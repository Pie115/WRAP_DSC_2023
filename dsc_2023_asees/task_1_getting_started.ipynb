{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "- Download the dataset from the [ECG Heartbeat Categorization Dataset](https://www.kaggle.com/datasets/shayanfazeli/heartbeat)\n",
    "- Unzip the `archive.zip` file\n",
    "- Rename the folder `archive` as `ecg_dataset` and place it in the root of the git repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- [ECG Heartbeat Classification: A Deep Transferable Representation](https://arxiv.org/pdf/1805.00794.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The PTB Diagnostic ECG Database\n",
    "\n",
    "- Number of Samples: 14552\n",
    "- Number of Categories: 2\n",
    "- Sampling Frequency: 125Hz\n",
    "- Data Source: Physionet's PTB Diagnostic Database\n",
    "- ECG lead II re-sampled to the sampling frequency of 125Hz as the input (from [ECG Heartbeat Classification: A Deep Transferable Representation](https://arxiv.org/pdf/1805.00794.pdf))\n",
    "- Remark: All the samples are cropped, downsampled and padded with zeroes if necessary to the fixed dimension of 188.\n",
    "- The final element of each row denotes the class to which that example belongs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the normal dataframe is :  (4046, 188)\n",
      "Class :  0.0\n",
      "The shape of the abnormal dataframe is :  (10506, 188)\n",
      "Class :  1.0\n",
      "The last time value is :  1488.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_ptbd_normal = pd.read_csv(\"ecg_dataset/ptbdb_normal.csv\", header = None)\n",
    "df_ptbd_abnormal = pd.read_csv(\"ecg_dataset/ptbdb_abnormal.csv\", header = None)\n",
    "\n",
    "# print shapes of the dataframes\n",
    "print(\"The shape of the normal dataframe is : \", df_ptbd_normal.shape)\n",
    "# print one example of the last column\n",
    "print(\"Class : \", df_ptbd_normal.iloc[:, -1][0])\n",
    "print(\"The shape of the abnormal dataframe is : \", df_ptbd_abnormal.shape)\n",
    "# print one example of the last column\n",
    "print(\"Class : \", df_ptbd_abnormal.iloc[:, -1][0])\n",
    "\n",
    "# classes are 0 (normal) and 1 (abnormal)\n",
    "classes_names = {0 : \"normal\", \n",
    "                 1 : \"abnormal\"}\n",
    "\n",
    "# get the number of columns in the dataframe\n",
    "# the last column is the label/class\n",
    "num_cols = df_ptbd_normal.shape[1] - 1 \n",
    "# the signal was resampled at frequency of 125Hz as the input\n",
    "# compute the time vector\n",
    "time = np.arange(0, num_cols) / 125\n",
    "# convert to milliseconds\n",
    "time = time * 1000\n",
    "# print last time value\n",
    "print(\"The last time value is : \", time[-1])\n",
    "\n",
    "# for the first \"num_cols\" and the time steps as column names\n",
    "df_ptbd_normal.columns = list(time) + [\"label\"]\n",
    "df_ptbd_abnormal.columns = list(time) + [\"label\"]\n",
    "\n",
    "# concatenate the two dataframes\n",
    "df_ptbd = pd.concat([df_ptbd_normal, df_ptbd_abnormal], axis = 0)\n",
    "\n",
    "# count the number of samples in each class and plot a bar chart\n",
    "# change the class names to \"normal\" and \"abnormal\"\n",
    "# count the number of samples in each class\n",
    "counts = df_ptbd[\"label\"].value_counts()\n",
    "# substitute the class names: 0.0 -> normal, 1.0 -> abnormal\n",
    "counts.index = counts.index.map(classes_names)\n",
    "# create a figure \n",
    "plt.figure(figsize = (10, 5))\n",
    "# plot a bar chart\n",
    "counts.plot(kind = \"bar\")\n",
    "# plot xticks in angle\n",
    "plt.xticks(rotation = 0)\n",
    "plt.title(\"Number of samples in each class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>16.0</th>\n",
       "      <th>24.0</th>\n",
       "      <th>32.0</th>\n",
       "      <th>40.0</th>\n",
       "      <th>48.0</th>\n",
       "      <th>56.0</th>\n",
       "      <th>64.0</th>\n",
       "      <th>72.0</th>\n",
       "      <th>...</th>\n",
       "      <th>1424.0</th>\n",
       "      <th>1432.0</th>\n",
       "      <th>1440.0</th>\n",
       "      <th>1448.0</th>\n",
       "      <th>1456.0</th>\n",
       "      <th>1464.0</th>\n",
       "      <th>1472.0</th>\n",
       "      <th>1480.0</th>\n",
       "      <th>1488.0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900324</td>\n",
       "      <td>0.358590</td>\n",
       "      <td>0.051459</td>\n",
       "      <td>0.046596</td>\n",
       "      <td>0.126823</td>\n",
       "      <td>0.133306</td>\n",
       "      <td>0.119125</td>\n",
       "      <td>0.110616</td>\n",
       "      <td>0.113047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.794681</td>\n",
       "      <td>0.375387</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171923</td>\n",
       "      <td>0.283859</td>\n",
       "      <td>0.293754</td>\n",
       "      <td>0.325912</td>\n",
       "      <td>0.345083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.909029</td>\n",
       "      <td>0.791482</td>\n",
       "      <td>0.423169</td>\n",
       "      <td>0.186712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007836</td>\n",
       "      <td>0.063032</td>\n",
       "      <td>0.077002</td>\n",
       "      <td>0.074957</td>\n",
       "      <td>0.077342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.478893</td>\n",
       "      <td>0.056760</td>\n",
       "      <td>0.064176</td>\n",
       "      <td>0.081289</td>\n",
       "      <td>0.072732</td>\n",
       "      <td>0.055619</td>\n",
       "      <td>0.048774</td>\n",
       "      <td>0.054478</td>\n",
       "      <td>0.041643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.867238</td>\n",
       "      <td>0.201360</td>\n",
       "      <td>0.099349</td>\n",
       "      <td>0.141336</td>\n",
       "      <td>0.120934</td>\n",
       "      <td>0.108516</td>\n",
       "      <td>0.096393</td>\n",
       "      <td>0.093436</td>\n",
       "      <td>0.100828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0.0       8.0      16.0      24.0      32.0      40.0      48.0  \\\n",
       "0  1.000000  0.900324  0.358590  0.051459  0.046596  0.126823  0.133306   \n",
       "1  1.000000  0.794681  0.375387  0.116883  0.000000  0.171923  0.283859   \n",
       "2  0.909029  0.791482  0.423169  0.186712  0.000000  0.007836  0.063032   \n",
       "3  1.000000  0.478893  0.056760  0.064176  0.081289  0.072732  0.055619   \n",
       "4  1.000000  0.867238  0.201360  0.099349  0.141336  0.120934  0.108516   \n",
       "\n",
       "       56.0      64.0      72.0  ...  1424.0  1432.0  1440.0  1448.0  1456.0  \\\n",
       "0  0.119125  0.110616  0.113047  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "1  0.293754  0.325912  0.345083  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "2  0.077002  0.074957  0.077342  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "3  0.048774  0.054478  0.041643  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "4  0.096393  0.093436  0.100828  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   1464.0  1472.0  1480.0  1488.0  label  \n",
       "0     0.0     0.0     0.0     0.0    0.0  \n",
       "1     0.0     0.0     0.0     0.0    0.0  \n",
       "2     0.0     0.0     0.0     0.0    0.0  \n",
       "3     0.0     0.0     0.0     0.0    0.0  \n",
       "4     0.0     0.0     0.0     0.0    0.0  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ptbd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_ptbd.loc[:, df_ptbd.columns != \"label\"]\n",
    "y = df_ptbd.loc[:, df_ptbd.columns == \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, shuffle = True)\n",
    "X_train, X_val, y_train, y_val   = train_test_split(X_train, y_train, test_size=0.2, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "x_val   = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "x_test  = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_val   = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "y_test  = torch.tensor(y_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(df_ptbd.columns)-1\n",
    "output_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50000\n",
    "train_loss_tens = torch.empty(epochs)\n",
    "train_loss_arr = np.zeros(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (70971884.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[24], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.linear = torch.nn.Linear(input_dim, output_dim)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#CLASSIFICATION\n",
    "##Logistic Regression Model\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "            \n",
    "            \n",
    "    def forward(self,x):\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs\n",
    "    \n",
    "model=LogisticRegression(input_dim, output_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 25)\n",
    "        self.fc4 = nn.Linear(25,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.softmax(self.fc4(x), dim = 0)\n",
    "        return x\n",
    "\n",
    "\n",
    "model1 = Net1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=torch.nn.BCELoss()\n",
    "optimizer=torch.optim.SGD(model1.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t\t Training Loss: 0.0005809849826619029 \t\n",
      "Epoch 2 \t\t Training Loss: 0.0005809849826619029 \t\n",
      "Epoch 3 \t\t Training Loss: 0.0005809849826619029 \t\n",
      "Epoch 4 \t\t Training Loss: 0.0005809849826619029 \t\n",
      "Epoch 5 \t\t Training Loss: 0.0005809849826619029 \t\n",
      "Epoch 6 \t\t Training Loss: 0.0005809849826619029 \t\n",
      "Epoch 7 \t\t Training Loss: 0.0005809849826619029 \t\n",
      "Epoch 8 \t\t Training Loss: 0.0005809849826619029 \t\n",
      "Epoch 9 \t\t Training Loss: 0.0005809849826619029 \t\n",
      "Epoch 10 \t\t Training Loss: 0.0005809849826619029 \t\n",
      "Epoch 11 \t\t Training Loss: 0.0005809849826619029 \t\n",
      "Epoch 12 \t\t Training Loss: 0.0005809849826619029 \t\n",
      "Epoch 13 \t\t Training Loss: 0.0005809849826619029 \t\n",
      "Epoch 14 \t\t Training Loss: 0.0005809849826619029 \t\n",
      "Epoch 15 \t\t Training Loss: 0.0005809849826619029 \t\n",
      "Epoch 16 \t\t Training Loss: 0.0005809849826619029 \t\n",
      "Epoch 17 \t\t Training Loss: 0.0005809848662465811 \t\n",
      "Epoch 18 \t\t Training Loss: 0.0005809848662465811 \t\n",
      "Epoch 19 \t\t Training Loss: 0.0005809848662465811 \t\n",
      "Epoch 20 \t\t Training Loss: 0.0005809848662465811 \t\n",
      "Epoch 21 \t\t Training Loss: 0.0005809848662465811 \t\n",
      "Epoch 22 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 23 \t\t Training Loss: 0.0005809848662465811 \t\n",
      "Epoch 24 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 25 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 26 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 27 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 28 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 29 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 30 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 31 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 32 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 33 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 34 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 35 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 36 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 37 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 38 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 39 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 40 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 41 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 42 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 43 \t\t Training Loss: 0.0005809847498312593 \t\n",
      "Epoch 44 \t\t Training Loss: 0.0005809846916235983 \t\n",
      "Epoch 45 \t\t Training Loss: 0.0005809846916235983 \t\n",
      "Epoch 46 \t\t Training Loss: 0.0005809846916235983 \t\n",
      "Epoch 47 \t\t Training Loss: 0.0005809846916235983 \t\n",
      "Epoch 48 \t\t Training Loss: 0.0005809846916235983 \t\n",
      "Epoch 49 \t\t Training Loss: 0.0005809846916235983 \t\n",
      "Epoch 50 \t\t Training Loss: 0.0005809845752082765 \t\n",
      "Epoch 51 \t\t Training Loss: 0.0005809846916235983 \t\n",
      "Epoch 52 \t\t Training Loss: 0.0005809846916235983 \t\n",
      "Epoch 53 \t\t Training Loss: 0.0005809845752082765 \t\n",
      "Epoch 54 \t\t Training Loss: 0.0005809845752082765 \t\n",
      "Epoch 55 \t\t Training Loss: 0.0005809845752082765 \t\n",
      "Epoch 56 \t\t Training Loss: 0.0005809845752082765 \t\n",
      "Epoch 57 \t\t Training Loss: 0.0005809845752082765 \t\n",
      "Epoch 58 \t\t Training Loss: 0.0005809845752082765 \t\n",
      "Epoch 59 \t\t Training Loss: 0.0005809845752082765 \t\n",
      "Epoch 60 \t\t Training Loss: 0.0005809845752082765 \t\n",
      "Epoch 61 \t\t Training Loss: 0.0005809845752082765 \t\n",
      "Epoch 62 \t\t Training Loss: 0.0005809845752082765 \t\n",
      "Epoch 63 \t\t Training Loss: 0.0005809845752082765 \t\n",
      "Epoch 64 \t\t Training Loss: 0.0005809845752082765 \t\n",
      "Epoch 65 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 66 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 67 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 68 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 69 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 70 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 71 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 72 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 73 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 74 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 75 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 76 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 77 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 78 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 79 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 80 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 81 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 82 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 83 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 84 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 85 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 86 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 87 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 88 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 89 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 90 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 91 \t\t Training Loss: 0.0005809844005852938 \t\n",
      "Epoch 92 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 93 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 94 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 95 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 96 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 97 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 98 \t\t Training Loss: 0.0005809844587929547 \t\n",
      "Epoch 99 \t\t Training Loss: 0.0005809844005852938 \t\n",
      "Epoch 100 \t\t Training Loss: 0.0005809844005852938 \t\n",
      "Epoch 101 \t\t Training Loss: 0.0005809844005852938 \t\n",
      "Epoch 102 \t\t Training Loss: 0.0005809844005852938 \t\n",
      "Epoch 103 \t\t Training Loss: 0.0005809844005852938 \t\n",
      "Epoch 104 \t\t Training Loss: 0.0005809844005852938 \t\n",
      "Epoch 105 \t\t Training Loss: 0.0005809844005852938 \t\n",
      "Epoch 106 \t\t Training Loss: 0.0005809844005852938 \t\n",
      "Epoch 107 \t\t Training Loss: 0.0005809844005852938 \t\n",
      "Epoch 108 \t\t Training Loss: 0.0005809843423776329 \t\n",
      "Epoch 109 \t\t Training Loss: 0.0005809844005852938 \t\n",
      "Epoch 110 \t\t Training Loss: 0.0005809843423776329 \t\n",
      "Epoch 111 \t\t Training Loss: 0.0005809843423776329 \t\n",
      "Epoch 112 \t\t Training Loss: 0.0005809843423776329 \t\n",
      "Epoch 113 \t\t Training Loss: 0.0005809843423776329 \t\n",
      "Epoch 114 \t\t Training Loss: 0.0005809842841699719 \t\n",
      "Epoch 115 \t\t Training Loss: 0.0005809842841699719 \t\n",
      "Epoch 116 \t\t Training Loss: 0.0005809842841699719 \t\n",
      "Epoch 117 \t\t Training Loss: 0.0005809842841699719 \t\n",
      "Epoch 118 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 119 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 120 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 121 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 122 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 123 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 124 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 125 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 126 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 127 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 128 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 129 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 130 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 131 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 132 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 133 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 134 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 135 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 136 \t\t Training Loss: 0.0005809841677546501 \t\n",
      "Epoch 137 \t\t Training Loss: 0.0005809841095469892 \t\n",
      "Epoch 138 \t\t Training Loss: 0.0005809841095469892 \t\n",
      "Epoch 139 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 140 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 141 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 142 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 143 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 144 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 145 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 146 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 147 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 148 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 149 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 150 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 151 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 152 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 153 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 154 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 155 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 156 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 157 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 158 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 159 \t\t Training Loss: 0.0005809839931316674 \t\n",
      "Epoch 160 \t\t Training Loss: 0.0005809839931316674 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 \t\t Training Loss: 0.0005809839931316674 \t\n",
      "Epoch 162 \t\t Training Loss: 0.0005809839931316674 \t\n",
      "Epoch 163 \t\t Training Loss: 0.0005809840513393283 \t\n",
      "Epoch 164 \t\t Training Loss: 0.0005809839931316674 \t\n",
      "Epoch 165 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 166 \t\t Training Loss: 0.0005809839931316674 \t\n",
      "Epoch 167 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 168 \t\t Training Loss: 0.0005809839931316674 \t\n",
      "Epoch 169 \t\t Training Loss: 0.0005809839931316674 \t\n",
      "Epoch 170 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 171 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 172 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 173 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 174 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 175 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 176 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 177 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 178 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 179 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 180 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 181 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 182 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 183 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 184 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 185 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 186 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 187 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 188 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 189 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 190 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 191 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 192 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 193 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 194 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 195 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 196 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 197 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 198 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 199 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 200 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 201 \t\t Training Loss: 0.0005809839349240065 \t\n",
      "Epoch 202 \t\t Training Loss: 0.0005809838767163455 \t\n",
      "Epoch 203 \t\t Training Loss: 0.0005809838767163455 \t\n",
      "Epoch 204 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 205 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 206 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 207 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 208 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 209 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 210 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 211 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 212 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 213 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 214 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 215 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 216 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 217 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 218 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 219 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 220 \t\t Training Loss: 0.0005809837020933628 \t\n",
      "Epoch 221 \t\t Training Loss: 0.0005809836438857019 \t\n",
      "Epoch 222 \t\t Training Loss: 0.0005809837020933628 \t\n",
      "Epoch 223 \t\t Training Loss: 0.0005809837020933628 \t\n",
      "Epoch 224 \t\t Training Loss: 0.0005809836438857019 \t\n",
      "Epoch 225 \t\t Training Loss: 0.0005809836438857019 \t\n",
      "Epoch 226 \t\t Training Loss: 0.0005809837020933628 \t\n",
      "Epoch 227 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 228 \t\t Training Loss: 0.0005809837603010237 \t\n",
      "Epoch 229 \t\t Training Loss: 0.0005809836438857019 \t\n",
      "Epoch 230 \t\t Training Loss: 0.0005809837020933628 \t\n",
      "Epoch 231 \t\t Training Loss: 0.0005809836438857019 \t\n",
      "Epoch 232 \t\t Training Loss: 0.0005809837020933628 \t\n",
      "Epoch 233 \t\t Training Loss: 0.0005809836438857019 \t\n",
      "Epoch 234 \t\t Training Loss: 0.0005809836438857019 \t\n",
      "Epoch 235 \t\t Training Loss: 0.0005809836438857019 \t\n",
      "Epoch 236 \t\t Training Loss: 0.0005809836438857019 \t\n",
      "Epoch 237 \t\t Training Loss: 0.0005809836438857019 \t\n",
      "Epoch 238 \t\t Training Loss: 0.0005809836438857019 \t\n",
      "Epoch 239 \t\t Training Loss: 0.0005809836438857019 \t\n",
      "Epoch 240 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 241 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 242 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 243 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 244 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 245 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 246 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 247 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 248 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 249 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 250 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 251 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 252 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 253 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 254 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 255 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 256 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 257 \t\t Training Loss: 0.0005809834692627192 \t\n",
      "Epoch 258 \t\t Training Loss: 0.0005809834110550582 \t\n",
      "Epoch 259 \t\t Training Loss: 0.0005809834110550582 \t\n",
      "Epoch 260 \t\t Training Loss: 0.0005809834110550582 \t\n",
      "Epoch 261 \t\t Training Loss: 0.0005809834110550582 \t\n",
      "Epoch 262 \t\t Training Loss: 0.0005809834110550582 \t\n",
      "Epoch 263 \t\t Training Loss: 0.0005809834110550582 \t\n",
      "Epoch 264 \t\t Training Loss: 0.0005809834110550582 \t\n",
      "Epoch 265 \t\t Training Loss: 0.0005809834110550582 \t\n",
      "Epoch 266 \t\t Training Loss: 0.0005809834110550582 \t\n",
      "Epoch 267 \t\t Training Loss: 0.0005809834110550582 \t\n",
      "Epoch 268 \t\t Training Loss: 0.0005809833528473973 \t\n",
      "Epoch 269 \t\t Training Loss: 0.0005809833528473973 \t\n",
      "Epoch 270 \t\t Training Loss: 0.0005809833528473973 \t\n",
      "Epoch 271 \t\t Training Loss: 0.0005809833528473973 \t\n",
      "Epoch 272 \t\t Training Loss: 0.0005809833528473973 \t\n",
      "Epoch 273 \t\t Training Loss: 0.0005809833528473973 \t\n",
      "Epoch 274 \t\t Training Loss: 0.0005809833528473973 \t\n",
      "Epoch 275 \t\t Training Loss: 0.0005809832946397364 \t\n",
      "Epoch 276 \t\t Training Loss: 0.0005809832946397364 \t\n",
      "Epoch 277 \t\t Training Loss: 0.0005809832946397364 \t\n",
      "Epoch 278 \t\t Training Loss: 0.0005809832946397364 \t\n",
      "Epoch 279 \t\t Training Loss: 0.0005809832946397364 \t\n",
      "Epoch 280 \t\t Training Loss: 0.0005809832946397364 \t\n",
      "Epoch 281 \t\t Training Loss: 0.0005809832946397364 \t\n",
      "Epoch 282 \t\t Training Loss: 0.0005809832946397364 \t\n",
      "Epoch 283 \t\t Training Loss: 0.0005809832946397364 \t\n",
      "Epoch 284 \t\t Training Loss: 0.0005809832946397364 \t\n",
      "Epoch 285 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 286 \t\t Training Loss: 0.0005809832946397364 \t\n",
      "Epoch 287 \t\t Training Loss: 0.0005809832946397364 \t\n",
      "Epoch 288 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 289 \t\t Training Loss: 0.0005809832364320755 \t\n",
      "Epoch 290 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 291 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 292 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 293 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 294 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 295 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 296 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 297 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 298 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 299 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 300 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 301 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 302 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 303 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 304 \t\t Training Loss: 0.0005809831782244146 \t\n",
      "Epoch 305 \t\t Training Loss: 0.0005809831200167537 \t\n",
      "Epoch 306 \t\t Training Loss: 0.0005809830618090928 \t\n",
      "Epoch 307 \t\t Training Loss: 0.0005809830618090928 \t\n",
      "Epoch 308 \t\t Training Loss: 0.0005809830618090928 \t\n",
      "Epoch 309 \t\t Training Loss: 0.0005809830618090928 \t\n",
      "Epoch 310 \t\t Training Loss: 0.0005809830618090928 \t\n",
      "Epoch 311 \t\t Training Loss: 0.0005809830036014318 \t\n",
      "Epoch 312 \t\t Training Loss: 0.0005809830036014318 \t\n",
      "Epoch 313 \t\t Training Loss: 0.0005809830036014318 \t\n",
      "Epoch 314 \t\t Training Loss: 0.0005809830036014318 \t\n",
      "Epoch 315 \t\t Training Loss: 0.0005809830036014318 \t\n",
      "Epoch 316 \t\t Training Loss: 0.0005809830036014318 \t\n",
      "Epoch 317 \t\t Training Loss: 0.0005809829453937709 \t\n",
      "Epoch 318 \t\t Training Loss: 0.0005809830036014318 \t\n",
      "Epoch 319 \t\t Training Loss: 0.0005809830036014318 \t\n",
      "Epoch 320 \t\t Training Loss: 0.0005809830036014318 \t\n",
      "Epoch 321 \t\t Training Loss: 0.0005809829453937709 \t\n",
      "Epoch 322 \t\t Training Loss: 0.0005809829453937709 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 323 \t\t Training Loss: 0.0005809829453937709 \t\n",
      "Epoch 324 \t\t Training Loss: 0.0005809829453937709 \t\n",
      "Epoch 325 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 326 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 327 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 328 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 329 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 330 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 331 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 332 \t\t Training Loss: 0.0005809829453937709 \t\n",
      "Epoch 333 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 334 \t\t Training Loss: 0.0005809829453937709 \t\n",
      "Epoch 335 \t\t Training Loss: 0.0005809829453937709 \t\n",
      "Epoch 336 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 337 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 338 \t\t Training Loss: 0.0005809829453937709 \t\n",
      "Epoch 339 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 340 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 341 \t\t Training Loss: 0.0005809829453937709 \t\n",
      "Epoch 342 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 343 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 344 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 345 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 346 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 347 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 348 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 349 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 350 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 351 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 352 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 353 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 354 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 355 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 356 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 357 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 358 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 359 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 360 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 361 \t\t Training Loss: 0.00058098288718611 \t\n",
      "Epoch 362 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 363 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 364 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 365 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 366 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 367 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 368 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 369 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 370 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 371 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 372 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 373 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 374 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 375 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 376 \t\t Training Loss: 0.0005809827125631273 \t\n",
      "Epoch 377 \t\t Training Loss: 0.0005809827125631273 \t\n",
      "Epoch 378 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 379 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 380 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 381 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 382 \t\t Training Loss: 0.0005809827707707882 \t\n",
      "Epoch 383 \t\t Training Loss: 0.0005809827125631273 \t\n",
      "Epoch 384 \t\t Training Loss: 0.0005809827125631273 \t\n",
      "Epoch 385 \t\t Training Loss: 0.0005809827125631273 \t\n",
      "Epoch 386 \t\t Training Loss: 0.0005809827125631273 \t\n",
      "Epoch 387 \t\t Training Loss: 0.0005809827125631273 \t\n",
      "Epoch 388 \t\t Training Loss: 0.0005809827125631273 \t\n",
      "Epoch 389 \t\t Training Loss: 0.0005809827125631273 \t\n",
      "Epoch 390 \t\t Training Loss: 0.0005809827125631273 \t\n",
      "Epoch 391 \t\t Training Loss: 0.0005809827125631273 \t\n",
      "Epoch 392 \t\t Training Loss: 0.0005809827125631273 \t\n",
      "Epoch 393 \t\t Training Loss: 0.0005809827125631273 \t\n",
      "Epoch 394 \t\t Training Loss: 0.0005809826543554664 \t\n",
      "Epoch 395 \t\t Training Loss: 0.0005809826543554664 \t\n",
      "Epoch 396 \t\t Training Loss: 0.0005809826543554664 \t\n",
      "Epoch 397 \t\t Training Loss: 0.0005809826543554664 \t\n",
      "Epoch 398 \t\t Training Loss: 0.0005809826543554664 \t\n",
      "Epoch 399 \t\t Training Loss: 0.0005809825961478055 \t\n",
      "Epoch 400 \t\t Training Loss: 0.0005809825961478055 \t\n",
      "Epoch 401 \t\t Training Loss: 0.0005809825961478055 \t\n",
      "Epoch 402 \t\t Training Loss: 0.0005809825961478055 \t\n",
      "Epoch 403 \t\t Training Loss: 0.0005809825961478055 \t\n",
      "Epoch 404 \t\t Training Loss: 0.0005809825961478055 \t\n",
      "Epoch 405 \t\t Training Loss: 0.0005809825961478055 \t\n",
      "Epoch 406 \t\t Training Loss: 0.0005809825961478055 \t\n",
      "Epoch 407 \t\t Training Loss: 0.0005809825961478055 \t\n",
      "Epoch 408 \t\t Training Loss: 0.0005809825961478055 \t\n",
      "Epoch 409 \t\t Training Loss: 0.0005809825961478055 \t\n",
      "Epoch 410 \t\t Training Loss: 0.0005809824797324836 \t\n",
      "Epoch 411 \t\t Training Loss: 0.0005809824797324836 \t\n",
      "Epoch 412 \t\t Training Loss: 0.0005809824797324836 \t\n",
      "Epoch 413 \t\t Training Loss: 0.0005809824797324836 \t\n",
      "Epoch 414 \t\t Training Loss: 0.0005809825961478055 \t\n",
      "Epoch 415 \t\t Training Loss: 0.0005809824797324836 \t\n",
      "Epoch 416 \t\t Training Loss: 0.0005809825961478055 \t\n",
      "Epoch 417 \t\t Training Loss: 0.0005809824797324836 \t\n",
      "Epoch 418 \t\t Training Loss: 0.0005809824797324836 \t\n",
      "Epoch 419 \t\t Training Loss: 0.0005809824797324836 \t\n",
      "Epoch 420 \t\t Training Loss: 0.0005809824797324836 \t\n",
      "Epoch 421 \t\t Training Loss: 0.0005809824797324836 \t\n",
      "Epoch 422 \t\t Training Loss: 0.0005809824215248227 \t\n",
      "Epoch 423 \t\t Training Loss: 0.0005809823633171618 \t\n",
      "Epoch 424 \t\t Training Loss: 0.0005809823633171618 \t\n",
      "Epoch 425 \t\t Training Loss: 0.0005809823633171618 \t\n",
      "Epoch 426 \t\t Training Loss: 0.0005809823633171618 \t\n",
      "Epoch 427 \t\t Training Loss: 0.0005809823633171618 \t\n",
      "Epoch 428 \t\t Training Loss: 0.0005809823051095009 \t\n",
      "Epoch 429 \t\t Training Loss: 0.0005809823051095009 \t\n",
      "Epoch 430 \t\t Training Loss: 0.0005809823051095009 \t\n",
      "Epoch 431 \t\t Training Loss: 0.0005809823051095009 \t\n",
      "Epoch 432 \t\t Training Loss: 0.0005809823051095009 \t\n",
      "Epoch 433 \t\t Training Loss: 0.0005809823051095009 \t\n",
      "Epoch 434 \t\t Training Loss: 0.0005809823051095009 \t\n",
      "Epoch 435 \t\t Training Loss: 0.0005809823051095009 \t\n",
      "Epoch 436 \t\t Training Loss: 0.0005809823051095009 \t\n",
      "Epoch 437 \t\t Training Loss: 0.0005809823051095009 \t\n",
      "Epoch 438 \t\t Training Loss: 0.0005809823051095009 \t\n",
      "Epoch 439 \t\t Training Loss: 0.00058098224690184 \t\n",
      "Epoch 440 \t\t Training Loss: 0.00058098224690184 \t\n",
      "Epoch 441 \t\t Training Loss: 0.00058098224690184 \t\n",
      "Epoch 442 \t\t Training Loss: 0.00058098224690184 \t\n",
      "Epoch 443 \t\t Training Loss: 0.00058098224690184 \t\n",
      "Epoch 444 \t\t Training Loss: 0.00058098224690184 \t\n",
      "Epoch 445 \t\t Training Loss: 0.00058098224690184 \t\n",
      "Epoch 446 \t\t Training Loss: 0.00058098224690184 \t\n",
      "Epoch 447 \t\t Training Loss: 0.00058098224690184 \t\n",
      "Epoch 448 \t\t Training Loss: 0.00058098224690184 \t\n",
      "Epoch 449 \t\t Training Loss: 0.00058098224690184 \t\n",
      "Epoch 450 \t\t Training Loss: 0.00058098224690184 \t\n",
      "Epoch 451 \t\t Training Loss: 0.00058098224690184 \t\n",
      "Epoch 452 \t\t Training Loss: 0.00058098224690184 \t\n",
      "Epoch 453 \t\t Training Loss: 0.00058098224690184 \t\n",
      "Epoch 454 \t\t Training Loss: 0.00058098224690184 \t\n",
      "Epoch 455 \t\t Training Loss: 0.0005809821886941791 \t\n",
      "Epoch 456 \t\t Training Loss: 0.0005809821886941791 \t\n",
      "Epoch 457 \t\t Training Loss: 0.0005809821886941791 \t\n",
      "Epoch 458 \t\t Training Loss: 0.0005809821886941791 \t\n",
      "Epoch 459 \t\t Training Loss: 0.0005809821886941791 \t\n",
      "Epoch 460 \t\t Training Loss: 0.0005809821886941791 \t\n",
      "Epoch 461 \t\t Training Loss: 0.0005809821886941791 \t\n",
      "Epoch 462 \t\t Training Loss: 0.0005809821886941791 \t\n",
      "Epoch 463 \t\t Training Loss: 0.0005809821886941791 \t\n",
      "Epoch 464 \t\t Training Loss: 0.0005809821886941791 \t\n",
      "Epoch 465 \t\t Training Loss: 0.0005809821886941791 \t\n",
      "Epoch 466 \t\t Training Loss: 0.0005809821886941791 \t\n",
      "Epoch 467 \t\t Training Loss: 0.0005809821304865181 \t\n",
      "Epoch 468 \t\t Training Loss: 0.0005809821304865181 \t\n",
      "Epoch 469 \t\t Training Loss: 0.0005809821304865181 \t\n",
      "Epoch 470 \t\t Training Loss: 0.0005809821304865181 \t\n",
      "Epoch 471 \t\t Training Loss: 0.0005809821304865181 \t\n",
      "Epoch 472 \t\t Training Loss: 0.0005809821304865181 \t\n",
      "Epoch 473 \t\t Training Loss: 0.0005809820722788572 \t\n",
      "Epoch 474 \t\t Training Loss: 0.0005809821304865181 \t\n",
      "Epoch 475 \t\t Training Loss: 0.0005809820722788572 \t\n",
      "Epoch 476 \t\t Training Loss: 0.0005809821304865181 \t\n",
      "Epoch 477 \t\t Training Loss: 0.0005809821886941791 \t\n",
      "Epoch 478 \t\t Training Loss: 0.0005809821886941791 \t\n",
      "Epoch 479 \t\t Training Loss: 0.0005809821304865181 \t\n",
      "Epoch 480 \t\t Training Loss: 0.0005809820722788572 \t\n",
      "Epoch 481 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 482 \t\t Training Loss: 0.0005809820140711963 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 483 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 484 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 485 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 486 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 487 \t\t Training Loss: 0.0005809821304865181 \t\n",
      "Epoch 488 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 489 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 490 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 491 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 492 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 493 \t\t Training Loss: 0.0005809820722788572 \t\n",
      "Epoch 494 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 495 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 496 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 497 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 498 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 499 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 500 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 501 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 502 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 503 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 504 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 505 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 506 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 507 \t\t Training Loss: 0.0005809820140711963 \t\n",
      "Epoch 508 \t\t Training Loss: 0.0005809818976558745 \t\n",
      "Epoch 509 \t\t Training Loss: 0.0005809818976558745 \t\n",
      "Epoch 510 \t\t Training Loss: 0.0005809818976558745 \t\n",
      "Epoch 511 \t\t Training Loss: 0.0005809818976558745 \t\n",
      "Epoch 512 \t\t Training Loss: 0.0005809818976558745 \t\n",
      "Epoch 513 \t\t Training Loss: 0.0005809818976558745 \t\n",
      "Epoch 514 \t\t Training Loss: 0.0005809818976558745 \t\n",
      "Epoch 515 \t\t Training Loss: 0.0005809818976558745 \t\n",
      "Epoch 516 \t\t Training Loss: 0.0005809818976558745 \t\n",
      "Epoch 517 \t\t Training Loss: 0.0005809818976558745 \t\n",
      "Epoch 518 \t\t Training Loss: 0.0005809818976558745 \t\n",
      "Epoch 519 \t\t Training Loss: 0.0005809818976558745 \t\n",
      "Epoch 520 \t\t Training Loss: 0.0005809818976558745 \t\n",
      "Epoch 521 \t\t Training Loss: 0.0005809818394482136 \t\n",
      "Epoch 522 \t\t Training Loss: 0.0005809818394482136 \t\n",
      "Epoch 523 \t\t Training Loss: 0.0005809818394482136 \t\n",
      "Epoch 524 \t\t Training Loss: 0.0005809818394482136 \t\n",
      "Epoch 525 \t\t Training Loss: 0.0005809818394482136 \t\n",
      "Epoch 526 \t\t Training Loss: 0.0005809818394482136 \t\n",
      "Epoch 527 \t\t Training Loss: 0.0005809818394482136 \t\n",
      "Epoch 528 \t\t Training Loss: 0.0005809818976558745 \t\n",
      "Epoch 529 \t\t Training Loss: 0.0005809818976558745 \t\n",
      "Epoch 530 \t\t Training Loss: 0.0005809818394482136 \t\n",
      "Epoch 531 \t\t Training Loss: 0.0005809818394482136 \t\n",
      "Epoch 532 \t\t Training Loss: 0.0005809818394482136 \t\n",
      "Epoch 533 \t\t Training Loss: 0.0005809818394482136 \t\n",
      "Epoch 534 \t\t Training Loss: 0.0005809818394482136 \t\n",
      "Epoch 535 \t\t Training Loss: 0.0005809817812405527 \t\n",
      "Epoch 536 \t\t Training Loss: 0.0005809817812405527 \t\n",
      "Epoch 537 \t\t Training Loss: 0.0005809817812405527 \t\n",
      "Epoch 538 \t\t Training Loss: 0.0005809817812405527 \t\n",
      "Epoch 539 \t\t Training Loss: 0.0005809817812405527 \t\n",
      "Epoch 540 \t\t Training Loss: 0.0005809817812405527 \t\n",
      "Epoch 541 \t\t Training Loss: 0.0005809817812405527 \t\n",
      "Epoch 542 \t\t Training Loss: 0.0005809817812405527 \t\n",
      "Epoch 543 \t\t Training Loss: 0.0005809817230328918 \t\n",
      "Epoch 544 \t\t Training Loss: 0.0005809817230328918 \t\n",
      "Epoch 545 \t\t Training Loss: 0.0005809817230328918 \t\n",
      "Epoch 546 \t\t Training Loss: 0.0005809817230328918 \t\n",
      "Epoch 547 \t\t Training Loss: 0.0005809817230328918 \t\n",
      "Epoch 548 \t\t Training Loss: 0.0005809817230328918 \t\n",
      "Epoch 549 \t\t Training Loss: 0.0005809817230328918 \t\n",
      "Epoch 550 \t\t Training Loss: 0.0005809817230328918 \t\n",
      "Epoch 551 \t\t Training Loss: 0.0005809816648252308 \t\n",
      "Epoch 552 \t\t Training Loss: 0.0005809817230328918 \t\n",
      "Epoch 553 \t\t Training Loss: 0.0005809816648252308 \t\n",
      "Epoch 554 \t\t Training Loss: 0.0005809816648252308 \t\n",
      "Epoch 555 \t\t Training Loss: 0.0005809816648252308 \t\n",
      "Epoch 556 \t\t Training Loss: 0.0005809816648252308 \t\n",
      "Epoch 557 \t\t Training Loss: 0.0005809816648252308 \t\n",
      "Epoch 558 \t\t Training Loss: 0.0005809816648252308 \t\n",
      "Epoch 559 \t\t Training Loss: 0.0005809816648252308 \t\n",
      "Epoch 560 \t\t Training Loss: 0.0005809816648252308 \t\n",
      "Epoch 561 \t\t Training Loss: 0.0005809816066175699 \t\n",
      "Epoch 562 \t\t Training Loss: 0.0005809816066175699 \t\n",
      "Epoch 563 \t\t Training Loss: 0.0005809816066175699 \t\n",
      "Epoch 564 \t\t Training Loss: 0.0005809816066175699 \t\n",
      "Epoch 565 \t\t Training Loss: 0.0005809816066175699 \t\n",
      "Epoch 566 \t\t Training Loss: 0.0005809816066175699 \t\n",
      "Epoch 567 \t\t Training Loss: 0.0005809816066175699 \t\n",
      "Epoch 568 \t\t Training Loss: 0.0005809816066175699 \t\n",
      "Epoch 569 \t\t Training Loss: 0.000580981548409909 \t\n",
      "Epoch 570 \t\t Training Loss: 0.0005809816066175699 \t\n",
      "Epoch 571 \t\t Training Loss: 0.0005809816066175699 \t\n",
      "Epoch 572 \t\t Training Loss: 0.0005809816066175699 \t\n",
      "Epoch 573 \t\t Training Loss: 0.000580981548409909 \t\n",
      "Epoch 574 \t\t Training Loss: 0.000580981548409909 \t\n",
      "Epoch 575 \t\t Training Loss: 0.000580981548409909 \t\n",
      "Epoch 576 \t\t Training Loss: 0.000580981548409909 \t\n",
      "Epoch 577 \t\t Training Loss: 0.0005809816066175699 \t\n",
      "Epoch 578 \t\t Training Loss: 0.0005809816066175699 \t\n",
      "Epoch 579 \t\t Training Loss: 0.000580981548409909 \t\n",
      "Epoch 580 \t\t Training Loss: 0.000580981548409909 \t\n",
      "Epoch 581 \t\t Training Loss: 0.000580981548409909 \t\n",
      "Epoch 582 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 583 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 584 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 585 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 586 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 587 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 588 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 589 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 590 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 591 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 592 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 593 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 594 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 595 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 596 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 597 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 598 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 599 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 600 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 601 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 602 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 603 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 604 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 605 \t\t Training Loss: 0.0005809814319945872 \t\n",
      "Epoch 606 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 607 \t\t Training Loss: 0.0005809813737869263 \t\n",
      "Epoch 608 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 609 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 610 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 611 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 612 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 613 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 614 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 615 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 616 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 617 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 618 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 619 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 620 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 621 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 622 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 623 \t\t Training Loss: 0.0005809813155792654 \t\n",
      "Epoch 624 \t\t Training Loss: 0.0005809812573716044 \t\n",
      "Epoch 625 \t\t Training Loss: 0.0005809812573716044 \t\n",
      "Epoch 626 \t\t Training Loss: 0.0005809812573716044 \t\n",
      "Epoch 627 \t\t Training Loss: 0.0005809812573716044 \t\n",
      "Epoch 628 \t\t Training Loss: 0.0005809812573716044 \t\n",
      "Epoch 629 \t\t Training Loss: 0.0005809812573716044 \t\n",
      "Epoch 630 \t\t Training Loss: 0.0005809812573716044 \t\n",
      "Epoch 631 \t\t Training Loss: 0.0005809811991639435 \t\n",
      "Epoch 632 \t\t Training Loss: 0.0005809811991639435 \t\n",
      "Epoch 633 \t\t Training Loss: 0.0005809812573716044 \t\n",
      "Epoch 634 \t\t Training Loss: 0.0005809811991639435 \t\n",
      "Epoch 635 \t\t Training Loss: 0.0005809811991639435 \t\n",
      "Epoch 636 \t\t Training Loss: 0.0005809811991639435 \t\n",
      "Epoch 637 \t\t Training Loss: 0.0005809811991639435 \t\n",
      "Epoch 638 \t\t Training Loss: 0.0005809811409562826 \t\n",
      "Epoch 639 \t\t Training Loss: 0.0005809811409562826 \t\n",
      "Epoch 640 \t\t Training Loss: 0.0005809811409562826 \t\n",
      "Epoch 641 \t\t Training Loss: 0.0005809811409562826 \t\n",
      "Epoch 642 \t\t Training Loss: 0.0005809811409562826 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 643 \t\t Training Loss: 0.0005809811409562826 \t\n",
      "Epoch 644 \t\t Training Loss: 0.0005809811409562826 \t\n",
      "Epoch 645 \t\t Training Loss: 0.0005809810827486217 \t\n",
      "Epoch 646 \t\t Training Loss: 0.0005809810827486217 \t\n",
      "Epoch 647 \t\t Training Loss: 0.0005809810827486217 \t\n",
      "Epoch 648 \t\t Training Loss: 0.0005809810827486217 \t\n",
      "Epoch 649 \t\t Training Loss: 0.0005809810827486217 \t\n",
      "Epoch 650 \t\t Training Loss: 0.0005809810827486217 \t\n",
      "Epoch 651 \t\t Training Loss: 0.0005809810827486217 \t\n",
      "Epoch 652 \t\t Training Loss: 0.0005809810827486217 \t\n",
      "Epoch 653 \t\t Training Loss: 0.0005809810827486217 \t\n",
      "Epoch 654 \t\t Training Loss: 0.0005809810827486217 \t\n",
      "Epoch 655 \t\t Training Loss: 0.0005809810827486217 \t\n",
      "Epoch 656 \t\t Training Loss: 0.0005809810827486217 \t\n",
      "Epoch 657 \t\t Training Loss: 0.0005809810827486217 \t\n",
      "Epoch 658 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 659 \t\t Training Loss: 0.0005809810827486217 \t\n",
      "Epoch 660 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 661 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 662 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 663 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 664 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 665 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 666 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 667 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 668 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 669 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 670 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 671 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 672 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 673 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 674 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 675 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 676 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 677 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 678 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 679 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 680 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 681 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 682 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 683 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 684 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 685 \t\t Training Loss: 0.000580980849917978 \t\n",
      "Epoch 686 \t\t Training Loss: 0.0005809810245409608 \t\n",
      "Epoch 687 \t\t Training Loss: 0.000580980849917978 \t\n",
      "Epoch 688 \t\t Training Loss: 0.000580980849917978 \t\n",
      "Epoch 689 \t\t Training Loss: 0.0005809807917103171 \t\n",
      "Epoch 690 \t\t Training Loss: 0.0005809807917103171 \t\n",
      "Epoch 691 \t\t Training Loss: 0.0005809807335026562 \t\n",
      "Epoch 692 \t\t Training Loss: 0.0005809807917103171 \t\n",
      "Epoch 693 \t\t Training Loss: 0.0005809807335026562 \t\n",
      "Epoch 694 \t\t Training Loss: 0.0005809807917103171 \t\n",
      "Epoch 695 \t\t Training Loss: 0.0005809807917103171 \t\n",
      "Epoch 696 \t\t Training Loss: 0.0005809807335026562 \t\n",
      "Epoch 697 \t\t Training Loss: 0.0005809807335026562 \t\n",
      "Epoch 698 \t\t Training Loss: 0.0005809807335026562 \t\n",
      "Epoch 699 \t\t Training Loss: 0.0005809807335026562 \t\n",
      "Epoch 700 \t\t Training Loss: 0.0005809807335026562 \t\n",
      "Epoch 701 \t\t Training Loss: 0.0005809807335026562 \t\n",
      "Epoch 702 \t\t Training Loss: 0.0005809807335026562 \t\n",
      "Epoch 703 \t\t Training Loss: 0.0005809807335026562 \t\n",
      "Epoch 704 \t\t Training Loss: 0.0005809806752949953 \t\n",
      "Epoch 705 \t\t Training Loss: 0.0005809806752949953 \t\n",
      "Epoch 706 \t\t Training Loss: 0.0005809806752949953 \t\n",
      "Epoch 707 \t\t Training Loss: 0.0005809806752949953 \t\n",
      "Epoch 708 \t\t Training Loss: 0.0005809806752949953 \t\n",
      "Epoch 709 \t\t Training Loss: 0.0005809806752949953 \t\n",
      "Epoch 710 \t\t Training Loss: 0.0005809806752949953 \t\n",
      "Epoch 711 \t\t Training Loss: 0.0005809806752949953 \t\n",
      "Epoch 712 \t\t Training Loss: 0.0005809806752949953 \t\n",
      "Epoch 713 \t\t Training Loss: 0.0005809806752949953 \t\n",
      "Epoch 714 \t\t Training Loss: 0.0005809806752949953 \t\n",
      "Epoch 715 \t\t Training Loss: 0.0005809806752949953 \t\n",
      "Epoch 716 \t\t Training Loss: 0.0005809806752949953 \t\n",
      "Epoch 717 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 718 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 719 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 720 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 721 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 722 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 723 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 724 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 725 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 726 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 727 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 728 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 729 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 730 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 731 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 732 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 733 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 734 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 735 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 736 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 737 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 738 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 739 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 740 \t\t Training Loss: 0.0005809805006720126 \t\n",
      "Epoch 741 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 742 \t\t Training Loss: 0.0005809806170873344 \t\n",
      "Epoch 743 \t\t Training Loss: 0.0005809804424643517 \t\n",
      "Epoch 744 \t\t Training Loss: 0.0005809804424643517 \t\n",
      "Epoch 745 \t\t Training Loss: 0.0005809804424643517 \t\n",
      "Epoch 746 \t\t Training Loss: 0.0005809804424643517 \t\n",
      "Epoch 747 \t\t Training Loss: 0.0005809804424643517 \t\n",
      "Epoch 748 \t\t Training Loss: 0.0005809804424643517 \t\n",
      "Epoch 749 \t\t Training Loss: 0.0005809804424643517 \t\n",
      "Epoch 750 \t\t Training Loss: 0.0005809804424643517 \t\n",
      "Epoch 751 \t\t Training Loss: 0.0005809804424643517 \t\n",
      "Epoch 752 \t\t Training Loss: 0.0005809804424643517 \t\n",
      "Epoch 753 \t\t Training Loss: 0.0005809804424643517 \t\n",
      "Epoch 754 \t\t Training Loss: 0.0005809803842566907 \t\n",
      "Epoch 755 \t\t Training Loss: 0.0005809803842566907 \t\n",
      "Epoch 756 \t\t Training Loss: 0.0005809803842566907 \t\n",
      "Epoch 757 \t\t Training Loss: 0.0005809803842566907 \t\n",
      "Epoch 758 \t\t Training Loss: 0.0005809803842566907 \t\n",
      "Epoch 759 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 760 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 761 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 762 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 763 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 764 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 765 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 766 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 767 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 768 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 769 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 770 \t\t Training Loss: 0.0005809803842566907 \t\n",
      "Epoch 771 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 772 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 773 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 774 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 775 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 776 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 777 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 778 \t\t Training Loss: 0.0005809803260490298 \t\n",
      "Epoch 779 \t\t Training Loss: 0.000580980209633708 \t\n",
      "Epoch 780 \t\t Training Loss: 0.000580980209633708 \t\n",
      "Epoch 781 \t\t Training Loss: 0.000580980209633708 \t\n",
      "Epoch 782 \t\t Training Loss: 0.000580980209633708 \t\n",
      "Epoch 783 \t\t Training Loss: 0.000580980209633708 \t\n",
      "Epoch 784 \t\t Training Loss: 0.000580980209633708 \t\n",
      "Epoch 785 \t\t Training Loss: 0.000580980209633708 \t\n",
      "Epoch 786 \t\t Training Loss: 0.000580980209633708 \t\n",
      "Epoch 787 \t\t Training Loss: 0.000580980209633708 \t\n",
      "Epoch 788 \t\t Training Loss: 0.000580980209633708 \t\n",
      "Epoch 789 \t\t Training Loss: 0.0005809801514260471 \t\n",
      "Epoch 790 \t\t Training Loss: 0.0005809801514260471 \t\n",
      "Epoch 791 \t\t Training Loss: 0.0005809801514260471 \t\n",
      "Epoch 792 \t\t Training Loss: 0.0005809801514260471 \t\n",
      "Epoch 793 \t\t Training Loss: 0.0005809801514260471 \t\n",
      "Epoch 794 \t\t Training Loss: 0.0005809801514260471 \t\n",
      "Epoch 795 \t\t Training Loss: 0.0005809801514260471 \t\n",
      "Epoch 796 \t\t Training Loss: 0.0005809801514260471 \t\n",
      "Epoch 797 \t\t Training Loss: 0.0005809801514260471 \t\n",
      "Epoch 798 \t\t Training Loss: 0.0005809801514260471 \t\n",
      "Epoch 799 \t\t Training Loss: 0.0005809800932183862 \t\n",
      "Epoch 800 \t\t Training Loss: 0.0005809800932183862 \t\n",
      "Epoch 801 \t\t Training Loss: 0.0005809800932183862 \t\n",
      "Epoch 802 \t\t Training Loss: 0.0005809801514260471 \t\n",
      "Epoch 803 \t\t Training Loss: 0.0005809800932183862 \t\n",
      "Epoch 804 \t\t Training Loss: 0.0005809800932183862 \t\n",
      "Epoch 805 \t\t Training Loss: 0.0005809800932183862 \t\n",
      "Epoch 806 \t\t Training Loss: 0.0005809800932183862 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 807 \t\t Training Loss: 0.0005809800932183862 \t\n",
      "Epoch 808 \t\t Training Loss: 0.0005809800932183862 \t\n",
      "Epoch 809 \t\t Training Loss: 0.0005809800932183862 \t\n",
      "Epoch 810 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 811 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 812 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 813 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 814 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 815 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 816 \t\t Training Loss: 0.0005809800932183862 \t\n",
      "Epoch 817 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 818 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 819 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 820 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 821 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 822 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 823 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 824 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 825 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 826 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 827 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 828 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 829 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 830 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 831 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 832 \t\t Training Loss: 0.0005809800350107253 \t\n",
      "Epoch 833 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 834 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 835 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 836 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 837 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 838 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 839 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 840 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 841 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 842 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 843 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 844 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 845 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 846 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 847 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 848 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 849 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 850 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 851 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 852 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 853 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 854 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 855 \t\t Training Loss: 0.0005809799185954034 \t\n",
      "Epoch 856 \t\t Training Loss: 0.0005809798021800816 \t\n",
      "Epoch 857 \t\t Training Loss: 0.0005809798021800816 \t\n",
      "Epoch 858 \t\t Training Loss: 0.0005809798021800816 \t\n",
      "Epoch 859 \t\t Training Loss: 0.0005809798021800816 \t\n",
      "Epoch 860 \t\t Training Loss: 0.0005809798021800816 \t\n",
      "Epoch 861 \t\t Training Loss: 0.0005809798021800816 \t\n",
      "Epoch 862 \t\t Training Loss: 0.0005809797439724207 \t\n",
      "Epoch 863 \t\t Training Loss: 0.0005809797439724207 \t\n",
      "Epoch 864 \t\t Training Loss: 0.0005809797439724207 \t\n",
      "Epoch 865 \t\t Training Loss: 0.0005809797439724207 \t\n",
      "Epoch 866 \t\t Training Loss: 0.0005809797439724207 \t\n",
      "Epoch 867 \t\t Training Loss: 0.0005809797439724207 \t\n",
      "Epoch 868 \t\t Training Loss: 0.0005809797439724207 \t\n",
      "Epoch 869 \t\t Training Loss: 0.0005809797439724207 \t\n",
      "Epoch 870 \t\t Training Loss: 0.0005809797439724207 \t\n",
      "Epoch 871 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 872 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 873 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 874 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 875 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 876 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 877 \t\t Training Loss: 0.0005809797439724207 \t\n",
      "Epoch 878 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 879 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 880 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 881 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 882 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 883 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 884 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 885 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 886 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 887 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 888 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 889 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 890 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 891 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 892 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 893 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 894 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 895 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 896 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 897 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 898 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 899 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 900 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 901 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 902 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 903 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 904 \t\t Training Loss: 0.0005809796275570989 \t\n",
      "Epoch 905 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 906 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 907 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 908 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 909 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 910 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 911 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 912 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 913 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 914 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 915 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 916 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 917 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 918 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 919 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 920 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 921 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 922 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 923 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 924 \t\t Training Loss: 0.000580979511141777 \t\n",
      "Epoch 925 \t\t Training Loss: 0.0005809794529341161 \t\n",
      "Epoch 926 \t\t Training Loss: 0.0005809794529341161 \t\n",
      "Epoch 927 \t\t Training Loss: 0.0005809794529341161 \t\n",
      "Epoch 928 \t\t Training Loss: 0.0005809794529341161 \t\n",
      "Epoch 929 \t\t Training Loss: 0.0005809794529341161 \t\n",
      "Epoch 930 \t\t Training Loss: 0.0005809794529341161 \t\n",
      "Epoch 931 \t\t Training Loss: 0.0005809794529341161 \t\n",
      "Epoch 932 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 933 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 934 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 935 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 936 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 937 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 938 \t\t Training Loss: 0.0005809794529341161 \t\n",
      "Epoch 939 \t\t Training Loss: 0.0005809794529341161 \t\n",
      "Epoch 940 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 941 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 942 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 943 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 944 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 945 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 946 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 947 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 948 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 949 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 950 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 951 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 952 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 953 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 954 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 955 \t\t Training Loss: 0.0005809792201034725 \t\n",
      "Epoch 956 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 957 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 958 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 959 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 960 \t\t Training Loss: 0.0005809793365187943 \t\n",
      "Epoch 961 \t\t Training Loss: 0.0005809792201034725 \t\n",
      "Epoch 962 \t\t Training Loss: 0.0005809792201034725 \t\n",
      "Epoch 963 \t\t Training Loss: 0.0005809791036881506 \t\n",
      "Epoch 964 \t\t Training Loss: 0.0005809791036881506 \t\n",
      "Epoch 965 \t\t Training Loss: 0.0005809791036881506 \t\n",
      "Epoch 966 \t\t Training Loss: 0.0005809791036881506 \t\n",
      "Epoch 967 \t\t Training Loss: 0.0005809791036881506 \t\n",
      "Epoch 968 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 969 \t\t Training Loss: 0.0005809790454804897 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 970 \t\t Training Loss: 0.0005809791036881506 \t\n",
      "Epoch 971 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 972 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 973 \t\t Training Loss: 0.0005809791036881506 \t\n",
      "Epoch 974 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 975 \t\t Training Loss: 0.0005809791036881506 \t\n",
      "Epoch 976 \t\t Training Loss: 0.0005809791036881506 \t\n",
      "Epoch 977 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 978 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 979 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 980 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 981 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 982 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 983 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 984 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 985 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 986 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 987 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 988 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 989 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 990 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 991 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 992 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 993 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 994 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 995 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 996 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 997 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 998 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 999 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 1000 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 1001 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 1002 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 1003 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 1004 \t\t Training Loss: 0.0005809790454804897 \t\n",
      "Epoch 1005 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1006 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1007 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1008 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1009 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1010 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1011 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1012 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1013 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1014 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1015 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1016 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1017 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1018 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1019 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1020 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1021 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1022 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1023 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1024 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1025 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1026 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1027 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1028 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1029 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1030 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1031 \t\t Training Loss: 0.0005809789290651679 \t\n",
      "Epoch 1032 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1033 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1034 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1035 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1036 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1037 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1038 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1039 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1040 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1041 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1042 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1043 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1044 \t\t Training Loss: 0.0005809788126498461 \t\n",
      "Epoch 1045 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1046 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1047 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1048 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1049 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1050 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1051 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1052 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1053 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1054 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1055 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1056 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1057 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1058 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1059 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1060 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1061 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1062 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1063 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1064 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1065 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1066 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1067 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1068 \t\t Training Loss: 0.0005809786380268633 \t\n",
      "Epoch 1069 \t\t Training Loss: 0.0005809785216115415 \t\n",
      "Epoch 1070 \t\t Training Loss: 0.0005809785216115415 \t\n",
      "Epoch 1071 \t\t Training Loss: 0.0005809785216115415 \t\n",
      "Epoch 1072 \t\t Training Loss: 0.0005809785216115415 \t\n",
      "Epoch 1073 \t\t Training Loss: 0.0005809785216115415 \t\n",
      "Epoch 1074 \t\t Training Loss: 0.0005809785216115415 \t\n",
      "Epoch 1075 \t\t Training Loss: 0.0005809785216115415 \t\n",
      "Epoch 1076 \t\t Training Loss: 0.0005809785216115415 \t\n",
      "Epoch 1077 \t\t Training Loss: 0.0005809785216115415 \t\n",
      "Epoch 1078 \t\t Training Loss: 0.0005809785216115415 \t\n",
      "Epoch 1079 \t\t Training Loss: 0.0005809784634038806 \t\n",
      "Epoch 1080 \t\t Training Loss: 0.0005809784634038806 \t\n",
      "Epoch 1081 \t\t Training Loss: 0.0005809784634038806 \t\n",
      "Epoch 1082 \t\t Training Loss: 0.0005809784634038806 \t\n",
      "Epoch 1083 \t\t Training Loss: 0.0005809784634038806 \t\n",
      "Epoch 1084 \t\t Training Loss: 0.0005809784634038806 \t\n",
      "Epoch 1085 \t\t Training Loss: 0.0005809784634038806 \t\n",
      "Epoch 1086 \t\t Training Loss: 0.0005809784634038806 \t\n",
      "Epoch 1087 \t\t Training Loss: 0.0005809784634038806 \t\n",
      "Epoch 1088 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1089 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1090 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1091 \t\t Training Loss: 0.0005809784634038806 \t\n",
      "Epoch 1092 \t\t Training Loss: 0.0005809784634038806 \t\n",
      "Epoch 1093 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1094 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1095 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1096 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1097 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1098 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1099 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1100 \t\t Training Loss: 0.0005809783469885588 \t\n",
      "Epoch 1101 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1102 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1103 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1104 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1105 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1106 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1107 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1108 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1109 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1110 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1111 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1112 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1113 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1114 \t\t Training Loss: 0.0005809784051962197 \t\n",
      "Epoch 1115 \t\t Training Loss: 0.0005809783469885588 \t\n",
      "Epoch 1116 \t\t Training Loss: 0.0005809783469885588 \t\n",
      "Epoch 1117 \t\t Training Loss: 0.0005809783469885588 \t\n",
      "Epoch 1118 \t\t Training Loss: 0.0005809783469885588 \t\n",
      "Epoch 1119 \t\t Training Loss: 0.0005809783469885588 \t\n",
      "Epoch 1120 \t\t Training Loss: 0.0005809783469885588 \t\n",
      "Epoch 1121 \t\t Training Loss: 0.0005809783469885588 \t\n",
      "Epoch 1122 \t\t Training Loss: 0.0005809782305732369 \t\n",
      "Epoch 1123 \t\t Training Loss: 0.0005809782305732369 \t\n",
      "Epoch 1124 \t\t Training Loss: 0.0005809782305732369 \t\n",
      "Epoch 1125 \t\t Training Loss: 0.0005809782305732369 \t\n",
      "Epoch 1126 \t\t Training Loss: 0.0005809782305732369 \t\n",
      "Epoch 1127 \t\t Training Loss: 0.0005809782305732369 \t\n",
      "Epoch 1128 \t\t Training Loss: 0.0005809782305732369 \t\n",
      "Epoch 1129 \t\t Training Loss: 0.0005809782305732369 \t\n",
      "Epoch 1130 \t\t Training Loss: 0.0005809782305732369 \t\n",
      "Epoch 1131 \t\t Training Loss: 0.0005809782305732369 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1132 \t\t Training Loss: 0.0005809782305732369 \t\n",
      "Epoch 1133 \t\t Training Loss: 0.0005809782305732369 \t\n",
      "Epoch 1134 \t\t Training Loss: 0.0005809782305732369 \t\n",
      "Epoch 1135 \t\t Training Loss: 0.0005809782305732369 \t\n",
      "Epoch 1136 \t\t Training Loss: 0.0005809782305732369 \t\n",
      "Epoch 1137 \t\t Training Loss: 0.0005809782305732369 \t\n",
      "Epoch 1138 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1139 \t\t Training Loss: 0.0005809782305732369 \t\n",
      "Epoch 1140 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1141 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1142 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1143 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1144 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1145 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1146 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1147 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1148 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1149 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1150 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1151 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1152 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1153 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1154 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1155 \t\t Training Loss: 0.0005809780559502542 \t\n",
      "Epoch 1156 \t\t Training Loss: 0.0005809780559502542 \t\n",
      "Epoch 1157 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1158 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1159 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1160 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1161 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1162 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1163 \t\t Training Loss: 0.0005809780559502542 \t\n",
      "Epoch 1164 \t\t Training Loss: 0.0005809780559502542 \t\n",
      "Epoch 1165 \t\t Training Loss: 0.0005809781141579151 \t\n",
      "Epoch 1166 \t\t Training Loss: 0.0005809779395349324 \t\n",
      "Epoch 1167 \t\t Training Loss: 0.0005809779395349324 \t\n",
      "Epoch 1168 \t\t Training Loss: 0.0005809780559502542 \t\n",
      "Epoch 1169 \t\t Training Loss: 0.0005809779395349324 \t\n",
      "Epoch 1170 \t\t Training Loss: 0.0005809779395349324 \t\n",
      "Epoch 1171 \t\t Training Loss: 0.0005809779395349324 \t\n",
      "Epoch 1172 \t\t Training Loss: 0.0005809779395349324 \t\n",
      "Epoch 1173 \t\t Training Loss: 0.0005809779395349324 \t\n",
      "Epoch 1174 \t\t Training Loss: 0.0005809779395349324 \t\n",
      "Epoch 1175 \t\t Training Loss: 0.0005809779395349324 \t\n",
      "Epoch 1176 \t\t Training Loss: 0.0005809779395349324 \t\n",
      "Epoch 1177 \t\t Training Loss: 0.0005809779395349324 \t\n",
      "Epoch 1178 \t\t Training Loss: 0.0005809779395349324 \t\n",
      "Epoch 1179 \t\t Training Loss: 0.0005809779395349324 \t\n",
      "Epoch 1180 \t\t Training Loss: 0.0005809779395349324 \t\n",
      "Epoch 1181 \t\t Training Loss: 0.0005809779395349324 \t\n",
      "Epoch 1182 \t\t Training Loss: 0.0005809779395349324 \t\n",
      "Epoch 1183 \t\t Training Loss: 0.0005809778813272715 \t\n",
      "Epoch 1184 \t\t Training Loss: 0.0005809778813272715 \t\n",
      "Epoch 1185 \t\t Training Loss: 0.0005809778813272715 \t\n",
      "Epoch 1186 \t\t Training Loss: 0.0005809778813272715 \t\n",
      "Epoch 1187 \t\t Training Loss: 0.0005809778813272715 \t\n",
      "Epoch 1188 \t\t Training Loss: 0.0005809778231196105 \t\n",
      "Epoch 1189 \t\t Training Loss: 0.0005809778813272715 \t\n",
      "Epoch 1190 \t\t Training Loss: 0.0005809778813272715 \t\n",
      "Epoch 1191 \t\t Training Loss: 0.0005809778231196105 \t\n",
      "Epoch 1192 \t\t Training Loss: 0.0005809778231196105 \t\n",
      "Epoch 1193 \t\t Training Loss: 0.0005809778231196105 \t\n",
      "Epoch 1194 \t\t Training Loss: 0.0005809778231196105 \t\n",
      "Epoch 1195 \t\t Training Loss: 0.0005809778231196105 \t\n",
      "Epoch 1196 \t\t Training Loss: 0.0005809778231196105 \t\n",
      "Epoch 1197 \t\t Training Loss: 0.0005809778231196105 \t\n",
      "Epoch 1198 \t\t Training Loss: 0.0005809778231196105 \t\n",
      "Epoch 1199 \t\t Training Loss: 0.0005809778231196105 \t\n",
      "Epoch 1200 \t\t Training Loss: 0.0005809777649119496 \t\n",
      "Epoch 1201 \t\t Training Loss: 0.0005809777649119496 \t\n",
      "Epoch 1202 \t\t Training Loss: 0.0005809778231196105 \t\n",
      "Epoch 1203 \t\t Training Loss: 0.0005809778231196105 \t\n",
      "Epoch 1204 \t\t Training Loss: 0.0005809778231196105 \t\n",
      "Epoch 1205 \t\t Training Loss: 0.0005809777649119496 \t\n",
      "Epoch 1206 \t\t Training Loss: 0.0005809777649119496 \t\n",
      "Epoch 1207 \t\t Training Loss: 0.0005809777649119496 \t\n",
      "Epoch 1208 \t\t Training Loss: 0.0005809777067042887 \t\n",
      "Epoch 1209 \t\t Training Loss: 0.0005809777067042887 \t\n",
      "Epoch 1210 \t\t Training Loss: 0.0005809777067042887 \t\n",
      "Epoch 1211 \t\t Training Loss: 0.0005809776484966278 \t\n",
      "Epoch 1212 \t\t Training Loss: 0.0005809776484966278 \t\n",
      "Epoch 1213 \t\t Training Loss: 0.0005809776484966278 \t\n",
      "Epoch 1214 \t\t Training Loss: 0.0005809776484966278 \t\n",
      "Epoch 1215 \t\t Training Loss: 0.0005809776484966278 \t\n",
      "Epoch 1216 \t\t Training Loss: 0.0005809776484966278 \t\n",
      "Epoch 1217 \t\t Training Loss: 0.0005809776484966278 \t\n",
      "Epoch 1218 \t\t Training Loss: 0.0005809776484966278 \t\n",
      "Epoch 1219 \t\t Training Loss: 0.0005809776484966278 \t\n",
      "Epoch 1220 \t\t Training Loss: 0.0005809776484966278 \t\n",
      "Epoch 1221 \t\t Training Loss: 0.0005809776484966278 \t\n",
      "Epoch 1222 \t\t Training Loss: 0.000580977532081306 \t\n",
      "Epoch 1223 \t\t Training Loss: 0.000580977532081306 \t\n",
      "Epoch 1224 \t\t Training Loss: 0.000580977532081306 \t\n",
      "Epoch 1225 \t\t Training Loss: 0.000580977532081306 \t\n",
      "Epoch 1226 \t\t Training Loss: 0.000580977532081306 \t\n",
      "Epoch 1227 \t\t Training Loss: 0.000580977532081306 \t\n",
      "Epoch 1228 \t\t Training Loss: 0.0005809774738736451 \t\n",
      "Epoch 1229 \t\t Training Loss: 0.0005809774738736451 \t\n",
      "Epoch 1230 \t\t Training Loss: 0.0005809774738736451 \t\n",
      "Epoch 1231 \t\t Training Loss: 0.0005809774738736451 \t\n",
      "Epoch 1232 \t\t Training Loss: 0.0005809774738736451 \t\n",
      "Epoch 1233 \t\t Training Loss: 0.0005809774738736451 \t\n",
      "Epoch 1234 \t\t Training Loss: 0.0005809774738736451 \t\n",
      "Epoch 1235 \t\t Training Loss: 0.0005809774738736451 \t\n",
      "Epoch 1236 \t\t Training Loss: 0.0005809774156659842 \t\n",
      "Epoch 1237 \t\t Training Loss: 0.0005809774156659842 \t\n",
      "Epoch 1238 \t\t Training Loss: 0.0005809774156659842 \t\n",
      "Epoch 1239 \t\t Training Loss: 0.0005809774156659842 \t\n",
      "Epoch 1240 \t\t Training Loss: 0.0005809774156659842 \t\n",
      "Epoch 1241 \t\t Training Loss: 0.0005809774156659842 \t\n",
      "Epoch 1242 \t\t Training Loss: 0.0005809774156659842 \t\n",
      "Epoch 1243 \t\t Training Loss: 0.0005809774156659842 \t\n",
      "Epoch 1244 \t\t Training Loss: 0.0005809774156659842 \t\n",
      "Epoch 1245 \t\t Training Loss: 0.0005809774156659842 \t\n",
      "Epoch 1246 \t\t Training Loss: 0.0005809774156659842 \t\n",
      "Epoch 1247 \t\t Training Loss: 0.0005809774156659842 \t\n",
      "Epoch 1248 \t\t Training Loss: 0.0005809774156659842 \t\n",
      "Epoch 1249 \t\t Training Loss: 0.0005809773574583232 \t\n",
      "Epoch 1250 \t\t Training Loss: 0.0005809774156659842 \t\n",
      "Epoch 1251 \t\t Training Loss: 0.0005809772992506623 \t\n",
      "Epoch 1252 \t\t Training Loss: 0.0005809772992506623 \t\n",
      "Epoch 1253 \t\t Training Loss: 0.0005809772992506623 \t\n",
      "Epoch 1254 \t\t Training Loss: 0.0005809772992506623 \t\n",
      "Epoch 1255 \t\t Training Loss: 0.0005809772992506623 \t\n",
      "Epoch 1256 \t\t Training Loss: 0.0005809772992506623 \t\n",
      "Epoch 1257 \t\t Training Loss: 0.0005809772992506623 \t\n",
      "Epoch 1258 \t\t Training Loss: 0.0005809772992506623 \t\n",
      "Epoch 1259 \t\t Training Loss: 0.0005809772410430014 \t\n",
      "Epoch 1260 \t\t Training Loss: 0.0005809772992506623 \t\n",
      "Epoch 1261 \t\t Training Loss: 0.0005809772410430014 \t\n",
      "Epoch 1262 \t\t Training Loss: 0.0005809772410430014 \t\n",
      "Epoch 1263 \t\t Training Loss: 0.0005809772410430014 \t\n",
      "Epoch 1264 \t\t Training Loss: 0.0005809772410430014 \t\n",
      "Epoch 1265 \t\t Training Loss: 0.0005809772410430014 \t\n",
      "Epoch 1266 \t\t Training Loss: 0.0005809772410430014 \t\n",
      "Epoch 1267 \t\t Training Loss: 0.0005809772410430014 \t\n",
      "Epoch 1268 \t\t Training Loss: 0.0005809771828353405 \t\n",
      "Epoch 1269 \t\t Training Loss: 0.0005809772410430014 \t\n",
      "Epoch 1270 \t\t Training Loss: 0.0005809772410430014 \t\n",
      "Epoch 1271 \t\t Training Loss: 0.0005809772410430014 \t\n",
      "Epoch 1272 \t\t Training Loss: 0.0005809771828353405 \t\n",
      "Epoch 1273 \t\t Training Loss: 0.0005809771828353405 \t\n",
      "Epoch 1274 \t\t Training Loss: 0.0005809772410430014 \t\n",
      "Epoch 1275 \t\t Training Loss: 0.0005809772410430014 \t\n",
      "Epoch 1276 \t\t Training Loss: 0.0005809772410430014 \t\n",
      "Epoch 1277 \t\t Training Loss: 0.0005809772410430014 \t\n",
      "Epoch 1278 \t\t Training Loss: 0.0005809771828353405 \t\n",
      "Epoch 1279 \t\t Training Loss: 0.0005809771828353405 \t\n",
      "Epoch 1280 \t\t Training Loss: 0.0005809771828353405 \t\n",
      "Epoch 1281 \t\t Training Loss: 0.0005809771828353405 \t\n",
      "Epoch 1282 \t\t Training Loss: 0.0005809771828353405 \t\n",
      "Epoch 1283 \t\t Training Loss: 0.0005809770664200187 \t\n",
      "Epoch 1284 \t\t Training Loss: 0.0005809770664200187 \t\n",
      "Epoch 1285 \t\t Training Loss: 0.0005809770664200187 \t\n",
      "Epoch 1286 \t\t Training Loss: 0.0005809770664200187 \t\n",
      "Epoch 1287 \t\t Training Loss: 0.0005809770664200187 \t\n",
      "Epoch 1288 \t\t Training Loss: 0.0005809770664200187 \t\n",
      "Epoch 1289 \t\t Training Loss: 0.0005809770664200187 \t\n",
      "Epoch 1290 \t\t Training Loss: 0.0005809770664200187 \t\n",
      "Epoch 1291 \t\t Training Loss: 0.0005809770664200187 \t\n",
      "Epoch 1292 \t\t Training Loss: 0.0005809770664200187 \t\n",
      "Epoch 1293 \t\t Training Loss: 0.0005809770082123578 \t\n",
      "Epoch 1294 \t\t Training Loss: 0.0005809770082123578 \t\n",
      "Epoch 1295 \t\t Training Loss: 0.0005809770082123578 \t\n",
      "Epoch 1296 \t\t Training Loss: 0.0005809770082123578 \t\n",
      "Epoch 1297 \t\t Training Loss: 0.0005809769500046968 \t\n",
      "Epoch 1298 \t\t Training Loss: 0.0005809769500046968 \t\n",
      "Epoch 1299 \t\t Training Loss: 0.0005809769500046968 \t\n",
      "Epoch 1300 \t\t Training Loss: 0.0005809769500046968 \t\n",
      "Epoch 1301 \t\t Training Loss: 0.0005809769500046968 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1302 \t\t Training Loss: 0.0005809769500046968 \t\n",
      "Epoch 1303 \t\t Training Loss: 0.0005809769500046968 \t\n",
      "Epoch 1304 \t\t Training Loss: 0.0005809769500046968 \t\n",
      "Epoch 1305 \t\t Training Loss: 0.0005809769500046968 \t\n",
      "Epoch 1306 \t\t Training Loss: 0.0005809769500046968 \t\n",
      "Epoch 1307 \t\t Training Loss: 0.0005809769500046968 \t\n",
      "Epoch 1308 \t\t Training Loss: 0.0005809769500046968 \t\n",
      "Epoch 1309 \t\t Training Loss: 0.0005809769500046968 \t\n",
      "Epoch 1310 \t\t Training Loss: 0.0005809769500046968 \t\n",
      "Epoch 1311 \t\t Training Loss: 0.0005809768917970359 \t\n",
      "Epoch 1312 \t\t Training Loss: 0.0005809768917970359 \t\n",
      "Epoch 1313 \t\t Training Loss: 0.0005809768917970359 \t\n",
      "Epoch 1314 \t\t Training Loss: 0.0005809769500046968 \t\n",
      "Epoch 1315 \t\t Training Loss: 0.0005809768917970359 \t\n",
      "Epoch 1316 \t\t Training Loss: 0.0005809768917970359 \t\n",
      "Epoch 1317 \t\t Training Loss: 0.0005809768917970359 \t\n",
      "Epoch 1318 \t\t Training Loss: 0.0005809768917970359 \t\n",
      "Epoch 1319 \t\t Training Loss: 0.0005809768917970359 \t\n",
      "Epoch 1320 \t\t Training Loss: 0.0005809768917970359 \t\n",
      "Epoch 1321 \t\t Training Loss: 0.0005809768917970359 \t\n",
      "Epoch 1322 \t\t Training Loss: 0.000580976833589375 \t\n",
      "Epoch 1323 \t\t Training Loss: 0.000580976833589375 \t\n",
      "Epoch 1324 \t\t Training Loss: 0.000580976833589375 \t\n",
      "Epoch 1325 \t\t Training Loss: 0.000580976833589375 \t\n",
      "Epoch 1326 \t\t Training Loss: 0.000580976833589375 \t\n",
      "Epoch 1327 \t\t Training Loss: 0.000580976833589375 \t\n",
      "Epoch 1328 \t\t Training Loss: 0.000580976833589375 \t\n",
      "Epoch 1329 \t\t Training Loss: 0.000580976833589375 \t\n",
      "Epoch 1330 \t\t Training Loss: 0.0005809767753817141 \t\n",
      "Epoch 1331 \t\t Training Loss: 0.0005809767753817141 \t\n",
      "Epoch 1332 \t\t Training Loss: 0.0005809767753817141 \t\n",
      "Epoch 1333 \t\t Training Loss: 0.0005809767753817141 \t\n",
      "Epoch 1334 \t\t Training Loss: 0.0005809767753817141 \t\n",
      "Epoch 1335 \t\t Training Loss: 0.0005809767753817141 \t\n",
      "Epoch 1336 \t\t Training Loss: 0.0005809767171740532 \t\n",
      "Epoch 1337 \t\t Training Loss: 0.0005809767171740532 \t\n",
      "Epoch 1338 \t\t Training Loss: 0.0005809767171740532 \t\n",
      "Epoch 1339 \t\t Training Loss: 0.0005809767171740532 \t\n",
      "Epoch 1340 \t\t Training Loss: 0.0005809767171740532 \t\n",
      "Epoch 1341 \t\t Training Loss: 0.0005809767171740532 \t\n",
      "Epoch 1342 \t\t Training Loss: 0.0005809767171740532 \t\n",
      "Epoch 1343 \t\t Training Loss: 0.0005809767171740532 \t\n",
      "Epoch 1344 \t\t Training Loss: 0.0005809767171740532 \t\n",
      "Epoch 1345 \t\t Training Loss: 0.0005809767171740532 \t\n",
      "Epoch 1346 \t\t Training Loss: 0.0005809767171740532 \t\n",
      "Epoch 1347 \t\t Training Loss: 0.0005809767171740532 \t\n",
      "Epoch 1348 \t\t Training Loss: 0.0005809767171740532 \t\n",
      "Epoch 1349 \t\t Training Loss: 0.0005809767171740532 \t\n",
      "Epoch 1350 \t\t Training Loss: 0.0005809766589663923 \t\n",
      "Epoch 1351 \t\t Training Loss: 0.0005809766589663923 \t\n",
      "Epoch 1352 \t\t Training Loss: 0.0005809766589663923 \t\n",
      "Epoch 1353 \t\t Training Loss: 0.0005809766589663923 \t\n",
      "Epoch 1354 \t\t Training Loss: 0.0005809767171740532 \t\n",
      "Epoch 1355 \t\t Training Loss: 0.0005809767171740532 \t\n",
      "Epoch 1356 \t\t Training Loss: 0.0005809766589663923 \t\n",
      "Epoch 1357 \t\t Training Loss: 0.0005809766589663923 \t\n",
      "Epoch 1358 \t\t Training Loss: 0.0005809766589663923 \t\n",
      "Epoch 1359 \t\t Training Loss: 0.0005809766007587314 \t\n",
      "Epoch 1360 \t\t Training Loss: 0.0005809766007587314 \t\n",
      "Epoch 1361 \t\t Training Loss: 0.0005809766007587314 \t\n",
      "Epoch 1362 \t\t Training Loss: 0.0005809766007587314 \t\n",
      "Epoch 1363 \t\t Training Loss: 0.0005809766007587314 \t\n",
      "Epoch 1364 \t\t Training Loss: 0.0005809766007587314 \t\n",
      "Epoch 1365 \t\t Training Loss: 0.0005809766007587314 \t\n",
      "Epoch 1366 \t\t Training Loss: 0.0005809766007587314 \t\n",
      "Epoch 1367 \t\t Training Loss: 0.0005809766007587314 \t\n",
      "Epoch 1368 \t\t Training Loss: 0.0005809766007587314 \t\n",
      "Epoch 1369 \t\t Training Loss: 0.0005809766007587314 \t\n",
      "Epoch 1370 \t\t Training Loss: 0.0005809766007587314 \t\n",
      "Epoch 1371 \t\t Training Loss: 0.0005809766007587314 \t\n",
      "Epoch 1372 \t\t Training Loss: 0.0005809766007587314 \t\n",
      "Epoch 1373 \t\t Training Loss: 0.0005809764843434095 \t\n",
      "Epoch 1374 \t\t Training Loss: 0.0005809764843434095 \t\n",
      "Epoch 1375 \t\t Training Loss: 0.0005809766007587314 \t\n",
      "Epoch 1376 \t\t Training Loss: 0.0005809764261357486 \t\n",
      "Epoch 1377 \t\t Training Loss: 0.0005809764261357486 \t\n",
      "Epoch 1378 \t\t Training Loss: 0.0005809764843434095 \t\n",
      "Epoch 1379 \t\t Training Loss: 0.0005809764261357486 \t\n",
      "Epoch 1380 \t\t Training Loss: 0.0005809764261357486 \t\n",
      "Epoch 1381 \t\t Training Loss: 0.0005809764261357486 \t\n",
      "Epoch 1382 \t\t Training Loss: 0.0005809764261357486 \t\n",
      "Epoch 1383 \t\t Training Loss: 0.0005809764261357486 \t\n",
      "Epoch 1384 \t\t Training Loss: 0.0005809764261357486 \t\n",
      "Epoch 1385 \t\t Training Loss: 0.0005809764261357486 \t\n",
      "Epoch 1386 \t\t Training Loss: 0.0005809764261357486 \t\n",
      "Epoch 1387 \t\t Training Loss: 0.0005809764261357486 \t\n",
      "Epoch 1388 \t\t Training Loss: 0.0005809764261357486 \t\n",
      "Epoch 1389 \t\t Training Loss: 0.0005809764261357486 \t\n",
      "Epoch 1390 \t\t Training Loss: 0.0005809763679280877 \t\n",
      "Epoch 1391 \t\t Training Loss: 0.0005809763679280877 \t\n",
      "Epoch 1392 \t\t Training Loss: 0.0005809763679280877 \t\n",
      "Epoch 1393 \t\t Training Loss: 0.0005809763679280877 \t\n",
      "Epoch 1394 \t\t Training Loss: 0.0005809763679280877 \t\n",
      "Epoch 1395 \t\t Training Loss: 0.0005809763679280877 \t\n",
      "Epoch 1396 \t\t Training Loss: 0.0005809763679280877 \t\n",
      "Epoch 1397 \t\t Training Loss: 0.0005809763097204268 \t\n",
      "Epoch 1398 \t\t Training Loss: 0.0005809763097204268 \t\n",
      "Epoch 1399 \t\t Training Loss: 0.0005809763097204268 \t\n",
      "Epoch 1400 \t\t Training Loss: 0.0005809763097204268 \t\n",
      "Epoch 1401 \t\t Training Loss: 0.0005809763097204268 \t\n",
      "Epoch 1402 \t\t Training Loss: 0.0005809763097204268 \t\n",
      "Epoch 1403 \t\t Training Loss: 0.0005809762515127659 \t\n",
      "Epoch 1404 \t\t Training Loss: 0.0005809762515127659 \t\n",
      "Epoch 1405 \t\t Training Loss: 0.0005809762515127659 \t\n",
      "Epoch 1406 \t\t Training Loss: 0.0005809762515127659 \t\n",
      "Epoch 1407 \t\t Training Loss: 0.0005809762515127659 \t\n",
      "Epoch 1408 \t\t Training Loss: 0.0005809762515127659 \t\n",
      "Epoch 1409 \t\t Training Loss: 0.0005809762515127659 \t\n",
      "Epoch 1410 \t\t Training Loss: 0.0005809762515127659 \t\n",
      "Epoch 1411 \t\t Training Loss: 0.0005809762515127659 \t\n",
      "Epoch 1412 \t\t Training Loss: 0.0005809762515127659 \t\n",
      "Epoch 1413 \t\t Training Loss: 0.0005809762515127659 \t\n",
      "Epoch 1414 \t\t Training Loss: 0.000580976193305105 \t\n",
      "Epoch 1415 \t\t Training Loss: 0.000580976193305105 \t\n",
      "Epoch 1416 \t\t Training Loss: 0.000580976193305105 \t\n",
      "Epoch 1417 \t\t Training Loss: 0.000580976193305105 \t\n",
      "Epoch 1418 \t\t Training Loss: 0.000580976193305105 \t\n",
      "Epoch 1419 \t\t Training Loss: 0.000580976193305105 \t\n",
      "Epoch 1420 \t\t Training Loss: 0.000580976193305105 \t\n",
      "Epoch 1421 \t\t Training Loss: 0.000580976193305105 \t\n",
      "Epoch 1422 \t\t Training Loss: 0.0005809761350974441 \t\n",
      "Epoch 1423 \t\t Training Loss: 0.0005809761350974441 \t\n",
      "Epoch 1424 \t\t Training Loss: 0.0005809761350974441 \t\n",
      "Epoch 1425 \t\t Training Loss: 0.0005809761350974441 \t\n",
      "Epoch 1426 \t\t Training Loss: 0.0005809760768897831 \t\n",
      "Epoch 1427 \t\t Training Loss: 0.0005809761350974441 \t\n",
      "Epoch 1428 \t\t Training Loss: 0.0005809760768897831 \t\n",
      "Epoch 1429 \t\t Training Loss: 0.0005809760768897831 \t\n",
      "Epoch 1430 \t\t Training Loss: 0.0005809761350974441 \t\n",
      "Epoch 1431 \t\t Training Loss: 0.0005809761350974441 \t\n",
      "Epoch 1432 \t\t Training Loss: 0.0005809760768897831 \t\n",
      "Epoch 1433 \t\t Training Loss: 0.0005809760768897831 \t\n",
      "Epoch 1434 \t\t Training Loss: 0.0005809760768897831 \t\n",
      "Epoch 1435 \t\t Training Loss: 0.0005809760768897831 \t\n",
      "Epoch 1436 \t\t Training Loss: 0.0005809760768897831 \t\n",
      "Epoch 1437 \t\t Training Loss: 0.0005809760768897831 \t\n",
      "Epoch 1438 \t\t Training Loss: 0.0005809760768897831 \t\n",
      "Epoch 1439 \t\t Training Loss: 0.0005809760768897831 \t\n",
      "Epoch 1440 \t\t Training Loss: 0.0005809760768897831 \t\n",
      "Epoch 1441 \t\t Training Loss: 0.0005809760768897831 \t\n",
      "Epoch 1442 \t\t Training Loss: 0.0005809760768897831 \t\n",
      "Epoch 1443 \t\t Training Loss: 0.0005809760768897831 \t\n",
      "Epoch 1444 \t\t Training Loss: 0.0005809760768897831 \t\n",
      "Epoch 1445 \t\t Training Loss: 0.0005809760186821222 \t\n",
      "Epoch 1446 \t\t Training Loss: 0.0005809760186821222 \t\n",
      "Epoch 1447 \t\t Training Loss: 0.0005809760186821222 \t\n",
      "Epoch 1448 \t\t Training Loss: 0.0005809760186821222 \t\n",
      "Epoch 1449 \t\t Training Loss: 0.0005809760186821222 \t\n",
      "Epoch 1450 \t\t Training Loss: 0.0005809760186821222 \t\n",
      "Epoch 1451 \t\t Training Loss: 0.0005809760186821222 \t\n",
      "Epoch 1452 \t\t Training Loss: 0.0005809759022668004 \t\n",
      "Epoch 1453 \t\t Training Loss: 0.0005809759022668004 \t\n",
      "Epoch 1454 \t\t Training Loss: 0.0005809759022668004 \t\n",
      "Epoch 1455 \t\t Training Loss: 0.0005809759022668004 \t\n",
      "Epoch 1456 \t\t Training Loss: 0.0005809759022668004 \t\n",
      "Epoch 1457 \t\t Training Loss: 0.0005809759022668004 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1458 \t\t Training Loss: 0.0005809759022668004 \t\n",
      "Epoch 1459 \t\t Training Loss: 0.0005809759022668004 \t\n",
      "Epoch 1460 \t\t Training Loss: 0.0005809759022668004 \t\n",
      "Epoch 1461 \t\t Training Loss: 0.0005809759022668004 \t\n",
      "Epoch 1462 \t\t Training Loss: 0.0005809759022668004 \t\n",
      "Epoch 1463 \t\t Training Loss: 0.0005809759022668004 \t\n",
      "Epoch 1464 \t\t Training Loss: 0.0005809758440591395 \t\n",
      "Epoch 1465 \t\t Training Loss: 0.0005809758440591395 \t\n",
      "Epoch 1466 \t\t Training Loss: 0.0005809757858514786 \t\n",
      "Epoch 1467 \t\t Training Loss: 0.0005809757858514786 \t\n",
      "Epoch 1468 \t\t Training Loss: 0.0005809757858514786 \t\n",
      "Epoch 1469 \t\t Training Loss: 0.0005809759022668004 \t\n",
      "Epoch 1470 \t\t Training Loss: 0.0005809758440591395 \t\n",
      "Epoch 1471 \t\t Training Loss: 0.0005809758440591395 \t\n",
      "Epoch 1472 \t\t Training Loss: 0.0005809758440591395 \t\n",
      "Epoch 1473 \t\t Training Loss: 0.0005809758440591395 \t\n",
      "Epoch 1474 \t\t Training Loss: 0.0005809757858514786 \t\n",
      "Epoch 1475 \t\t Training Loss: 0.0005809757858514786 \t\n",
      "Epoch 1476 \t\t Training Loss: 0.0005809758440591395 \t\n",
      "Epoch 1477 \t\t Training Loss: 0.0005809757858514786 \t\n",
      "Epoch 1478 \t\t Training Loss: 0.0005809757858514786 \t\n",
      "Epoch 1479 \t\t Training Loss: 0.0005809757858514786 \t\n",
      "Epoch 1480 \t\t Training Loss: 0.0005809757858514786 \t\n",
      "Epoch 1481 \t\t Training Loss: 0.0005809757858514786 \t\n",
      "Epoch 1482 \t\t Training Loss: 0.0005809757858514786 \t\n",
      "Epoch 1483 \t\t Training Loss: 0.0005809757858514786 \t\n",
      "Epoch 1484 \t\t Training Loss: 0.0005809757858514786 \t\n",
      "Epoch 1485 \t\t Training Loss: 0.0005809757858514786 \t\n",
      "Epoch 1486 \t\t Training Loss: 0.0005809757858514786 \t\n",
      "Epoch 1487 \t\t Training Loss: 0.0005809757276438177 \t\n",
      "Epoch 1488 \t\t Training Loss: 0.0005809757276438177 \t\n",
      "Epoch 1489 \t\t Training Loss: 0.0005809756694361567 \t\n",
      "Epoch 1490 \t\t Training Loss: 0.0005809756694361567 \t\n",
      "Epoch 1491 \t\t Training Loss: 0.0005809756694361567 \t\n",
      "Epoch 1492 \t\t Training Loss: 0.0005809756694361567 \t\n",
      "Epoch 1493 \t\t Training Loss: 0.0005809756694361567 \t\n",
      "Epoch 1494 \t\t Training Loss: 0.0005809756694361567 \t\n",
      "Epoch 1495 \t\t Training Loss: 0.0005809756694361567 \t\n",
      "Epoch 1496 \t\t Training Loss: 0.0005809756694361567 \t\n",
      "Epoch 1497 \t\t Training Loss: 0.0005809756694361567 \t\n",
      "Epoch 1498 \t\t Training Loss: 0.0005809756694361567 \t\n",
      "Epoch 1499 \t\t Training Loss: 0.0005809756694361567 \t\n",
      "Epoch 1500 \t\t Training Loss: 0.0005809756694361567 \t\n",
      "Epoch 1501 \t\t Training Loss: 0.0005809756694361567 \t\n",
      "Epoch 1502 \t\t Training Loss: 0.0005809756694361567 \t\n",
      "Epoch 1503 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1504 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1505 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1506 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1507 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1508 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1509 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1510 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1511 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1512 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1513 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1514 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1515 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1516 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1517 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1518 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1519 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1520 \t\t Training Loss: 0.0005809756112284958 \t\n",
      "Epoch 1521 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1522 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1523 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1524 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1525 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1526 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1527 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1528 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1529 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1530 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1531 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1532 \t\t Training Loss: 0.000580975494813174 \t\n",
      "Epoch 1533 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1534 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1535 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1536 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1537 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1538 \t\t Training Loss: 0.000580975494813174 \t\n",
      "Epoch 1539 \t\t Training Loss: 0.000580975494813174 \t\n",
      "Epoch 1540 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1541 \t\t Training Loss: 0.000580975494813174 \t\n",
      "Epoch 1542 \t\t Training Loss: 0.000580975494813174 \t\n",
      "Epoch 1543 \t\t Training Loss: 0.000580975494813174 \t\n",
      "Epoch 1544 \t\t Training Loss: 0.0005809755530208349 \t\n",
      "Epoch 1545 \t\t Training Loss: 0.000580975494813174 \t\n",
      "Epoch 1546 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1547 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1548 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1549 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1550 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1551 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1552 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1553 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1554 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1555 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1556 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1557 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1558 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1559 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1560 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1561 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1562 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1563 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1564 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1565 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1566 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1567 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1568 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1569 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1570 \t\t Training Loss: 0.0005809754366055131 \t\n",
      "Epoch 1571 \t\t Training Loss: 0.0005809753201901913 \t\n",
      "Epoch 1572 \t\t Training Loss: 0.0005809753201901913 \t\n",
      "Epoch 1573 \t\t Training Loss: 0.0005809753201901913 \t\n",
      "Epoch 1574 \t\t Training Loss: 0.0005809753201901913 \t\n",
      "Epoch 1575 \t\t Training Loss: 0.0005809753201901913 \t\n",
      "Epoch 1576 \t\t Training Loss: 0.0005809753201901913 \t\n",
      "Epoch 1577 \t\t Training Loss: 0.0005809753201901913 \t\n",
      "Epoch 1578 \t\t Training Loss: 0.0005809753201901913 \t\n",
      "Epoch 1579 \t\t Training Loss: 0.0005809753201901913 \t\n",
      "Epoch 1580 \t\t Training Loss: 0.0005809753201901913 \t\n",
      "Epoch 1581 \t\t Training Loss: 0.0005809752619825304 \t\n",
      "Epoch 1582 \t\t Training Loss: 0.0005809752619825304 \t\n",
      "Epoch 1583 \t\t Training Loss: 0.0005809752619825304 \t\n",
      "Epoch 1584 \t\t Training Loss: 0.0005809752619825304 \t\n",
      "Epoch 1585 \t\t Training Loss: 0.0005809752619825304 \t\n",
      "Epoch 1586 \t\t Training Loss: 0.0005809752619825304 \t\n",
      "Epoch 1587 \t\t Training Loss: 0.0005809752619825304 \t\n",
      "Epoch 1588 \t\t Training Loss: 0.0005809752619825304 \t\n",
      "Epoch 1589 \t\t Training Loss: 0.0005809752619825304 \t\n",
      "Epoch 1590 \t\t Training Loss: 0.0005809752619825304 \t\n",
      "Epoch 1591 \t\t Training Loss: 0.0005809752619825304 \t\n",
      "Epoch 1592 \t\t Training Loss: 0.0005809752619825304 \t\n",
      "Epoch 1593 \t\t Training Loss: 0.0005809752619825304 \t\n",
      "Epoch 1594 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1595 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1596 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1597 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1598 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1599 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1600 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1601 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1602 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1603 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1604 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1605 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1606 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1607 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1608 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1609 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1610 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1611 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1612 \t\t Training Loss: 0.0005809752037748694 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1613 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1614 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1615 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1616 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1617 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1618 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1619 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1620 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1621 \t\t Training Loss: 0.0005809752037748694 \t\n",
      "Epoch 1622 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1623 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1624 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1625 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1626 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1627 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1628 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1629 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1630 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1631 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1632 \t\t Training Loss: 0.0005809751455672085 \t\n",
      "Epoch 1633 \t\t Training Loss: 0.0005809750873595476 \t\n",
      "Epoch 1634 \t\t Training Loss: 0.0005809750873595476 \t\n",
      "Epoch 1635 \t\t Training Loss: 0.0005809750873595476 \t\n",
      "Epoch 1636 \t\t Training Loss: 0.0005809750873595476 \t\n",
      "Epoch 1637 \t\t Training Loss: 0.0005809750873595476 \t\n",
      "Epoch 1638 \t\t Training Loss: 0.0005809750873595476 \t\n",
      "Epoch 1639 \t\t Training Loss: 0.0005809750873595476 \t\n",
      "Epoch 1640 \t\t Training Loss: 0.0005809750873595476 \t\n",
      "Epoch 1641 \t\t Training Loss: 0.0005809749709442258 \t\n",
      "Epoch 1642 \t\t Training Loss: 0.0005809749709442258 \t\n",
      "Epoch 1643 \t\t Training Loss: 0.0005809750873595476 \t\n",
      "Epoch 1644 \t\t Training Loss: 0.0005809749709442258 \t\n",
      "Epoch 1645 \t\t Training Loss: 0.0005809749709442258 \t\n",
      "Epoch 1646 \t\t Training Loss: 0.0005809749709442258 \t\n",
      "Epoch 1647 \t\t Training Loss: 0.0005809749709442258 \t\n",
      "Epoch 1648 \t\t Training Loss: 0.0005809749709442258 \t\n",
      "Epoch 1649 \t\t Training Loss: 0.0005809749709442258 \t\n",
      "Epoch 1650 \t\t Training Loss: 0.0005809749709442258 \t\n",
      "Epoch 1651 \t\t Training Loss: 0.0005809749709442258 \t\n",
      "Epoch 1652 \t\t Training Loss: 0.0005809749709442258 \t\n",
      "Epoch 1653 \t\t Training Loss: 0.0005809749709442258 \t\n",
      "Epoch 1654 \t\t Training Loss: 0.0005809749709442258 \t\n",
      "Epoch 1655 \t\t Training Loss: 0.0005809749127365649 \t\n",
      "Epoch 1656 \t\t Training Loss: 0.0005809749709442258 \t\n",
      "Epoch 1657 \t\t Training Loss: 0.0005809749709442258 \t\n",
      "Epoch 1658 \t\t Training Loss: 0.0005809749709442258 \t\n",
      "Epoch 1659 \t\t Training Loss: 0.000580974854528904 \t\n",
      "Epoch 1660 \t\t Training Loss: 0.000580974854528904 \t\n",
      "Epoch 1661 \t\t Training Loss: 0.000580974854528904 \t\n",
      "Epoch 1662 \t\t Training Loss: 0.000580974854528904 \t\n",
      "Epoch 1663 \t\t Training Loss: 0.000580974854528904 \t\n",
      "Epoch 1664 \t\t Training Loss: 0.000580974854528904 \t\n",
      "Epoch 1665 \t\t Training Loss: 0.000580974854528904 \t\n",
      "Epoch 1666 \t\t Training Loss: 0.000580974854528904 \t\n",
      "Epoch 1667 \t\t Training Loss: 0.000580974854528904 \t\n",
      "Epoch 1668 \t\t Training Loss: 0.000580974854528904 \t\n",
      "Epoch 1669 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1670 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1671 \t\t Training Loss: 0.000580974854528904 \t\n",
      "Epoch 1672 \t\t Training Loss: 0.000580974854528904 \t\n",
      "Epoch 1673 \t\t Training Loss: 0.000580974854528904 \t\n",
      "Epoch 1674 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1675 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1676 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1677 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1678 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1679 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1680 \t\t Training Loss: 0.000580974854528904 \t\n",
      "Epoch 1681 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1682 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1683 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1684 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1685 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1686 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1687 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1688 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1689 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1690 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1691 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1692 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1693 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1694 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1695 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1696 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1697 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1698 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1699 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1700 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1701 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1702 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1703 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1704 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1705 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1706 \t\t Training Loss: 0.0005809746799059212 \t\n",
      "Epoch 1707 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1708 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1709 \t\t Training Loss: 0.0005809746799059212 \t\n",
      "Epoch 1710 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1711 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1712 \t\t Training Loss: 0.0005809746799059212 \t\n",
      "Epoch 1713 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1714 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1715 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1716 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1717 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1718 \t\t Training Loss: 0.000580974796321243 \t\n",
      "Epoch 1719 \t\t Training Loss: 0.0005809746799059212 \t\n",
      "Epoch 1720 \t\t Training Loss: 0.0005809746799059212 \t\n",
      "Epoch 1721 \t\t Training Loss: 0.0005809746799059212 \t\n",
      "Epoch 1722 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1723 \t\t Training Loss: 0.0005809746799059212 \t\n",
      "Epoch 1724 \t\t Training Loss: 0.0005809746799059212 \t\n",
      "Epoch 1725 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1726 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1727 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1728 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1729 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1730 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1731 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1732 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1733 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1734 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1735 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1736 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1737 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1738 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1739 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1740 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1741 \t\t Training Loss: 0.0005809746216982603 \t\n",
      "Epoch 1742 \t\t Training Loss: 0.0005809745634905994 \t\n",
      "Epoch 1743 \t\t Training Loss: 0.0005809745634905994 \t\n",
      "Epoch 1744 \t\t Training Loss: 0.0005809745634905994 \t\n",
      "Epoch 1745 \t\t Training Loss: 0.0005809745634905994 \t\n",
      "Epoch 1746 \t\t Training Loss: 0.0005809745634905994 \t\n",
      "Epoch 1747 \t\t Training Loss: 0.0005809745634905994 \t\n",
      "Epoch 1748 \t\t Training Loss: 0.0005809745634905994 \t\n",
      "Epoch 1749 \t\t Training Loss: 0.0005809745634905994 \t\n",
      "Epoch 1750 \t\t Training Loss: 0.0005809745634905994 \t\n",
      "Epoch 1751 \t\t Training Loss: 0.0005809745634905994 \t\n",
      "Epoch 1752 \t\t Training Loss: 0.0005809745052829385 \t\n",
      "Epoch 1753 \t\t Training Loss: 0.0005809745634905994 \t\n",
      "Epoch 1754 \t\t Training Loss: 0.0005809745052829385 \t\n",
      "Epoch 1755 \t\t Training Loss: 0.0005809745052829385 \t\n",
      "Epoch 1756 \t\t Training Loss: 0.0005809745052829385 \t\n",
      "Epoch 1757 \t\t Training Loss: 0.0005809745052829385 \t\n",
      "Epoch 1758 \t\t Training Loss: 0.0005809745052829385 \t\n",
      "Epoch 1759 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1760 \t\t Training Loss: 0.0005809745052829385 \t\n",
      "Epoch 1761 \t\t Training Loss: 0.0005809745052829385 \t\n",
      "Epoch 1762 \t\t Training Loss: 0.0005809745052829385 \t\n",
      "Epoch 1763 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1764 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1765 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1766 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1767 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1768 \t\t Training Loss: 0.0005809744470752776 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1769 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1770 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1771 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1772 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1773 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1774 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1775 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1776 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1777 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1778 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1779 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1780 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1781 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1782 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1783 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1784 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1785 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1786 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1787 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1788 \t\t Training Loss: 0.0005809744470752776 \t\n",
      "Epoch 1789 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1790 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1791 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1792 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1793 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1794 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1795 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1796 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1797 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1798 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1799 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1800 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1801 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1802 \t\t Training Loss: 0.0005809742724522948 \t\n",
      "Epoch 1803 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1804 \t\t Training Loss: 0.0005809742724522948 \t\n",
      "Epoch 1805 \t\t Training Loss: 0.0005809743888676167 \t\n",
      "Epoch 1806 \t\t Training Loss: 0.0005809742724522948 \t\n",
      "Epoch 1807 \t\t Training Loss: 0.0005809742724522948 \t\n",
      "Epoch 1808 \t\t Training Loss: 0.0005809742724522948 \t\n",
      "Epoch 1809 \t\t Training Loss: 0.0005809742724522948 \t\n",
      "Epoch 1810 \t\t Training Loss: 0.0005809742724522948 \t\n",
      "Epoch 1811 \t\t Training Loss: 0.0005809742142446339 \t\n",
      "Epoch 1812 \t\t Training Loss: 0.0005809742724522948 \t\n",
      "Epoch 1813 \t\t Training Loss: 0.0005809742142446339 \t\n",
      "Epoch 1814 \t\t Training Loss: 0.0005809742142446339 \t\n",
      "Epoch 1815 \t\t Training Loss: 0.0005809742142446339 \t\n",
      "Epoch 1816 \t\t Training Loss: 0.0005809742142446339 \t\n",
      "Epoch 1817 \t\t Training Loss: 0.000580974156036973 \t\n",
      "Epoch 1818 \t\t Training Loss: 0.000580974156036973 \t\n",
      "Epoch 1819 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1820 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1821 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1822 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1823 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1824 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1825 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1826 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1827 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1828 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1829 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1830 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1831 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1832 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1833 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1834 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1835 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1836 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1837 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1838 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1839 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1840 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1841 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1842 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1843 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1844 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1845 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1846 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1847 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1848 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1849 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1850 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1851 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1852 \t\t Training Loss: 0.0005809740978293121 \t\n",
      "Epoch 1853 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1854 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1855 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1856 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1857 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1858 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1859 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1860 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1861 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1862 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1863 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1864 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1865 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1866 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1867 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1868 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1869 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1870 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1871 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1872 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1873 \t\t Training Loss: 0.0005809739814139903 \t\n",
      "Epoch 1874 \t\t Training Loss: 0.0005809738649986684 \t\n",
      "Epoch 1875 \t\t Training Loss: 0.0005809738649986684 \t\n",
      "Epoch 1876 \t\t Training Loss: 0.0005809738649986684 \t\n",
      "Epoch 1877 \t\t Training Loss: 0.0005809738649986684 \t\n",
      "Epoch 1878 \t\t Training Loss: 0.0005809738649986684 \t\n",
      "Epoch 1879 \t\t Training Loss: 0.0005809738649986684 \t\n",
      "Epoch 1880 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1881 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1882 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1883 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1884 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1885 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1886 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1887 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1888 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1889 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1890 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1891 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1892 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1893 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1894 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1895 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1896 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1897 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1898 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1899 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1900 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1901 \t\t Training Loss: 0.0005809738067910075 \t\n",
      "Epoch 1902 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1903 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1904 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1905 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1906 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1907 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1908 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1909 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1910 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1911 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1912 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1913 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1914 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1915 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1916 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1917 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1918 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1919 \t\t Training Loss: 0.0005809736903756857 \t\n",
      "Epoch 1920 \t\t Training Loss: 0.0005809735739603639 \t\n",
      "Epoch 1921 \t\t Training Loss: 0.0005809735739603639 \t\n",
      "Epoch 1922 \t\t Training Loss: 0.0005809735739603639 \t\n",
      "Epoch 1923 \t\t Training Loss: 0.0005809735739603639 \t\n",
      "Epoch 1924 \t\t Training Loss: 0.0005809735739603639 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1925 \t\t Training Loss: 0.000580973515752703 \t\n",
      "Epoch 1926 \t\t Training Loss: 0.000580973515752703 \t\n",
      "Epoch 1927 \t\t Training Loss: 0.000580973515752703 \t\n",
      "Epoch 1928 \t\t Training Loss: 0.000580973515752703 \t\n",
      "Epoch 1929 \t\t Training Loss: 0.000580973515752703 \t\n",
      "Epoch 1930 \t\t Training Loss: 0.000580973515752703 \t\n",
      "Epoch 1931 \t\t Training Loss: 0.000580973515752703 \t\n",
      "Epoch 1932 \t\t Training Loss: 0.000580973515752703 \t\n",
      "Epoch 1933 \t\t Training Loss: 0.000580973515752703 \t\n",
      "Epoch 1934 \t\t Training Loss: 0.000580973515752703 \t\n",
      "Epoch 1935 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1936 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1937 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1938 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1939 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1940 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1941 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1942 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1943 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1944 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1945 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1946 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1947 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1948 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1949 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1950 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1951 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1952 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1953 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1954 \t\t Training Loss: 0.0005809733993373811 \t\n",
      "Epoch 1955 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1956 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1957 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1958 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1959 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1960 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1961 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1962 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1963 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1964 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1965 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1966 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1967 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1968 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1969 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1970 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1971 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1972 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1973 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1974 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1975 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1976 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1977 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1978 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1979 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1980 \t\t Training Loss: 0.0005809732829220593 \t\n",
      "Epoch 1981 \t\t Training Loss: 0.0005809731082990766 \t\n",
      "Epoch 1982 \t\t Training Loss: 0.0005809731082990766 \t\n",
      "Epoch 1983 \t\t Training Loss: 0.0005809731082990766 \t\n",
      "Epoch 1984 \t\t Training Loss: 0.0005809731082990766 \t\n",
      "Epoch 1985 \t\t Training Loss: 0.0005809731082990766 \t\n",
      "Epoch 1986 \t\t Training Loss: 0.0005809731082990766 \t\n",
      "Epoch 1987 \t\t Training Loss: 0.0005809731082990766 \t\n",
      "Epoch 1988 \t\t Training Loss: 0.0005809731082990766 \t\n",
      "Epoch 1989 \t\t Training Loss: 0.0005809731082990766 \t\n",
      "Epoch 1990 \t\t Training Loss: 0.0005809731082990766 \t\n",
      "Epoch 1991 \t\t Training Loss: 0.0005809731082990766 \t\n",
      "Epoch 1992 \t\t Training Loss: 0.0005809731082990766 \t\n",
      "Epoch 1993 \t\t Training Loss: 0.0005809731082990766 \t\n",
      "Epoch 1994 \t\t Training Loss: 0.0005809731082990766 \t\n",
      "Epoch 1995 \t\t Training Loss: 0.0005809731082990766 \t\n",
      "Epoch 1996 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 1997 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 1998 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 1999 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2000 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2001 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2002 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2003 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2004 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2005 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2006 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2007 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2008 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2009 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2010 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2011 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2012 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2013 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2014 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2015 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2016 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2017 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2018 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2019 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2020 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2021 \t\t Training Loss: 0.0005809729918837547 \t\n",
      "Epoch 2022 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2023 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2024 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2025 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2026 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2027 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2028 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2029 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2030 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2031 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2032 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2033 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2034 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2035 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2036 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2037 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2038 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2039 \t\t Training Loss: 0.000580972817260772 \t\n",
      "Epoch 2040 \t\t Training Loss: 0.000580972817260772 \t\n",
      "Epoch 2041 \t\t Training Loss: 0.0005809728754684329 \t\n",
      "Epoch 2042 \t\t Training Loss: 0.000580972817260772 \t\n",
      "Epoch 2043 \t\t Training Loss: 0.000580972817260772 \t\n",
      "Epoch 2044 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2045 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2046 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2047 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2048 \t\t Training Loss: 0.000580972817260772 \t\n",
      "Epoch 2049 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2050 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2051 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2052 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2053 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2054 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2055 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2056 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2057 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2058 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2059 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2060 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2061 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2062 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2063 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2064 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2065 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2066 \t\t Training Loss: 0.0005809727008454502 \t\n",
      "Epoch 2067 \t\t Training Loss: 0.0005809725844301283 \t\n",
      "Epoch 2068 \t\t Training Loss: 0.0005809725844301283 \t\n",
      "Epoch 2069 \t\t Training Loss: 0.0005809725844301283 \t\n",
      "Epoch 2070 \t\t Training Loss: 0.0005809725844301283 \t\n",
      "Epoch 2071 \t\t Training Loss: 0.0005809725844301283 \t\n",
      "Epoch 2072 \t\t Training Loss: 0.0005809725844301283 \t\n",
      "Epoch 2073 \t\t Training Loss: 0.0005809725844301283 \t\n",
      "Epoch 2074 \t\t Training Loss: 0.0005809725844301283 \t\n",
      "Epoch 2075 \t\t Training Loss: 0.0005809725844301283 \t\n",
      "Epoch 2076 \t\t Training Loss: 0.0005809725844301283 \t\n",
      "Epoch 2077 \t\t Training Loss: 0.0005809725844301283 \t\n",
      "Epoch 2078 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2079 \t\t Training Loss: 0.0005809725844301283 \t\n",
      "Epoch 2080 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2081 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2082 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2083 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2084 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2085 \t\t Training Loss: 0.0005809724098071456 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2086 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2087 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2088 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2089 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2090 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2091 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2092 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2093 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2094 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2095 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2096 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2097 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2098 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2099 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2100 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2101 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2102 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2103 \t\t Training Loss: 0.0005809724098071456 \t\n",
      "Epoch 2104 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2105 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2106 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2107 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2108 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2109 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2110 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2111 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2112 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2113 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2114 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2115 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2116 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2117 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2118 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2119 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2120 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2121 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2122 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2123 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2124 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2125 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2126 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2127 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2128 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2129 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2130 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2131 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2132 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2133 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2134 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2135 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2136 \t\t Training Loss: 0.0005809722933918238 \t\n",
      "Epoch 2137 \t\t Training Loss: 0.0005809722351841629 \t\n",
      "Epoch 2138 \t\t Training Loss: 0.0005809722351841629 \t\n",
      "Epoch 2139 \t\t Training Loss: 0.0005809721769765019 \t\n",
      "Epoch 2140 \t\t Training Loss: 0.000580972118768841 \t\n",
      "Epoch 2141 \t\t Training Loss: 0.000580972118768841 \t\n",
      "Epoch 2142 \t\t Training Loss: 0.000580972118768841 \t\n",
      "Epoch 2143 \t\t Training Loss: 0.000580972118768841 \t\n",
      "Epoch 2144 \t\t Training Loss: 0.000580972118768841 \t\n",
      "Epoch 2145 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2146 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2147 \t\t Training Loss: 0.000580972118768841 \t\n",
      "Epoch 2148 \t\t Training Loss: 0.000580972118768841 \t\n",
      "Epoch 2149 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2150 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2151 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2152 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2153 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2154 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2155 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2156 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2157 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2158 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2159 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2160 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2161 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2162 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2163 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2164 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2165 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2166 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2167 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2168 \t\t Training Loss: 0.0005809720023535192 \t\n",
      "Epoch 2169 \t\t Training Loss: 0.0005809719441458583 \t\n",
      "Epoch 2170 \t\t Training Loss: 0.0005809719441458583 \t\n",
      "Epoch 2171 \t\t Training Loss: 0.0005809719441458583 \t\n",
      "Epoch 2172 \t\t Training Loss: 0.0005809719441458583 \t\n",
      "Epoch 2173 \t\t Training Loss: 0.0005809719441458583 \t\n",
      "Epoch 2174 \t\t Training Loss: 0.0005809719441458583 \t\n",
      "Epoch 2175 \t\t Training Loss: 0.0005809719441458583 \t\n",
      "Epoch 2176 \t\t Training Loss: 0.0005809719441458583 \t\n",
      "Epoch 2177 \t\t Training Loss: 0.0005809719441458583 \t\n",
      "Epoch 2178 \t\t Training Loss: 0.0005809718859381974 \t\n",
      "Epoch 2179 \t\t Training Loss: 0.0005809719441458583 \t\n",
      "Epoch 2180 \t\t Training Loss: 0.0005809719441458583 \t\n",
      "Epoch 2181 \t\t Training Loss: 0.0005809718859381974 \t\n",
      "Epoch 2182 \t\t Training Loss: 0.0005809718859381974 \t\n",
      "Epoch 2183 \t\t Training Loss: 0.0005809718859381974 \t\n",
      "Epoch 2184 \t\t Training Loss: 0.0005809718859381974 \t\n",
      "Epoch 2185 \t\t Training Loss: 0.0005809718859381974 \t\n",
      "Epoch 2186 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2187 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2188 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2189 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2190 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2191 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2192 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2193 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2194 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2195 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2196 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2197 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2198 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2199 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2200 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2201 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2202 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2203 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2204 \t\t Training Loss: 0.0005809718277305365 \t\n",
      "Epoch 2205 \t\t Training Loss: 0.0005809717113152146 \t\n",
      "Epoch 2206 \t\t Training Loss: 0.0005809717113152146 \t\n",
      "Epoch 2207 \t\t Training Loss: 0.0005809717113152146 \t\n",
      "Epoch 2208 \t\t Training Loss: 0.0005809717113152146 \t\n",
      "Epoch 2209 \t\t Training Loss: 0.0005809717113152146 \t\n",
      "Epoch 2210 \t\t Training Loss: 0.0005809717113152146 \t\n",
      "Epoch 2211 \t\t Training Loss: 0.0005809717113152146 \t\n",
      "Epoch 2212 \t\t Training Loss: 0.0005809716531075537 \t\n",
      "Epoch 2213 \t\t Training Loss: 0.0005809716531075537 \t\n",
      "Epoch 2214 \t\t Training Loss: 0.0005809716531075537 \t\n",
      "Epoch 2215 \t\t Training Loss: 0.0005809716531075537 \t\n",
      "Epoch 2216 \t\t Training Loss: 0.0005809716531075537 \t\n",
      "Epoch 2217 \t\t Training Loss: 0.0005809716531075537 \t\n",
      "Epoch 2218 \t\t Training Loss: 0.0005809716531075537 \t\n",
      "Epoch 2219 \t\t Training Loss: 0.0005809716531075537 \t\n",
      "Epoch 2220 \t\t Training Loss: 0.0005809716531075537 \t\n",
      "Epoch 2221 \t\t Training Loss: 0.0005809716531075537 \t\n",
      "Epoch 2222 \t\t Training Loss: 0.0005809716531075537 \t\n",
      "Epoch 2223 \t\t Training Loss: 0.0005809716531075537 \t\n",
      "Epoch 2224 \t\t Training Loss: 0.0005809716531075537 \t\n",
      "Epoch 2225 \t\t Training Loss: 0.0005809716531075537 \t\n",
      "Epoch 2226 \t\t Training Loss: 0.0005809716531075537 \t\n",
      "Epoch 2227 \t\t Training Loss: 0.0005809715366922319 \t\n",
      "Epoch 2228 \t\t Training Loss: 0.0005809715366922319 \t\n",
      "Epoch 2229 \t\t Training Loss: 0.0005809715366922319 \t\n",
      "Epoch 2230 \t\t Training Loss: 0.0005809715366922319 \t\n",
      "Epoch 2231 \t\t Training Loss: 0.0005809715366922319 \t\n",
      "Epoch 2232 \t\t Training Loss: 0.0005809715366922319 \t\n",
      "Epoch 2233 \t\t Training Loss: 0.0005809715366922319 \t\n",
      "Epoch 2234 \t\t Training Loss: 0.000580971478484571 \t\n",
      "Epoch 2235 \t\t Training Loss: 0.0005809715366922319 \t\n",
      "Epoch 2236 \t\t Training Loss: 0.0005809715366922319 \t\n",
      "Epoch 2237 \t\t Training Loss: 0.0005809715366922319 \t\n",
      "Epoch 2238 \t\t Training Loss: 0.0005809715366922319 \t\n",
      "Epoch 2239 \t\t Training Loss: 0.0005809715366922319 \t\n",
      "Epoch 2240 \t\t Training Loss: 0.0005809715366922319 \t\n",
      "Epoch 2241 \t\t Training Loss: 0.000580971478484571 \t\n",
      "Epoch 2242 \t\t Training Loss: 0.0005809715366922319 \t\n",
      "Epoch 2243 \t\t Training Loss: 0.0005809715366922319 \t\n",
      "Epoch 2244 \t\t Training Loss: 0.0005809714202769101 \t\n",
      "Epoch 2245 \t\t Training Loss: 0.0005809714202769101 \t\n",
      "Epoch 2246 \t\t Training Loss: 0.000580971478484571 \t\n",
      "Epoch 2247 \t\t Training Loss: 0.000580971478484571 \t\n",
      "Epoch 2248 \t\t Training Loss: 0.0005809714202769101 \t\n",
      "Epoch 2249 \t\t Training Loss: 0.000580971478484571 \t\n",
      "Epoch 2250 \t\t Training Loss: 0.000580971478484571 \t\n",
      "Epoch 2251 \t\t Training Loss: 0.0005809714202769101 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2252 \t\t Training Loss: 0.0005809714202769101 \t\n",
      "Epoch 2253 \t\t Training Loss: 0.0005809714202769101 \t\n",
      "Epoch 2254 \t\t Training Loss: 0.0005809714202769101 \t\n",
      "Epoch 2255 \t\t Training Loss: 0.0005809714202769101 \t\n",
      "Epoch 2256 \t\t Training Loss: 0.0005809714202769101 \t\n",
      "Epoch 2257 \t\t Training Loss: 0.0005809714202769101 \t\n",
      "Epoch 2258 \t\t Training Loss: 0.0005809714202769101 \t\n",
      "Epoch 2259 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2260 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2261 \t\t Training Loss: 0.0005809714202769101 \t\n",
      "Epoch 2262 \t\t Training Loss: 0.0005809714202769101 \t\n",
      "Epoch 2263 \t\t Training Loss: 0.0005809714202769101 \t\n",
      "Epoch 2264 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2265 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2266 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2267 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2268 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2269 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2270 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2271 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2272 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2273 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2274 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2275 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2276 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2277 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2278 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2279 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2280 \t\t Training Loss: 0.0005809713038615882 \t\n",
      "Epoch 2281 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2282 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2283 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2284 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2285 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2286 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2287 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2288 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2289 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2290 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2291 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2292 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2293 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2294 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2295 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2296 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2297 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2298 \t\t Training Loss: 0.0005809712456539273 \t\n",
      "Epoch 2299 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2300 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2301 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2302 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2303 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2304 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2305 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2306 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2307 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2308 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2309 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2310 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2311 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2312 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2313 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2314 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2315 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2316 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2317 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2318 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2319 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2320 \t\t Training Loss: 0.0005809711874462664 \t\n",
      "Epoch 2321 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2322 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2323 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2324 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2325 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2326 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2327 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2328 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2329 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2330 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2331 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2332 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2333 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2334 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2335 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2336 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2337 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2338 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2339 \t\t Training Loss: 0.0005809710710309446 \t\n",
      "Epoch 2340 \t\t Training Loss: 0.0005809708964079618 \t\n",
      "Epoch 2341 \t\t Training Loss: 0.0005809709546156228 \t\n",
      "Epoch 2342 \t\t Training Loss: 0.0005809709546156228 \t\n",
      "Epoch 2343 \t\t Training Loss: 0.0005809708964079618 \t\n",
      "Epoch 2344 \t\t Training Loss: 0.0005809708964079618 \t\n",
      "Epoch 2345 \t\t Training Loss: 0.0005809708964079618 \t\n",
      "Epoch 2346 \t\t Training Loss: 0.0005809708964079618 \t\n",
      "Epoch 2347 \t\t Training Loss: 0.0005809708964079618 \t\n",
      "Epoch 2348 \t\t Training Loss: 0.0005809708964079618 \t\n",
      "Epoch 2349 \t\t Training Loss: 0.0005809708964079618 \t\n",
      "Epoch 2350 \t\t Training Loss: 0.0005809708964079618 \t\n",
      "Epoch 2351 \t\t Training Loss: 0.0005809708382003009 \t\n",
      "Epoch 2352 \t\t Training Loss: 0.0005809708382003009 \t\n",
      "Epoch 2353 \t\t Training Loss: 0.0005809708382003009 \t\n",
      "Epoch 2354 \t\t Training Loss: 0.0005809708382003009 \t\n",
      "Epoch 2355 \t\t Training Loss: 0.0005809708964079618 \t\n",
      "Epoch 2356 \t\t Training Loss: 0.0005809708964079618 \t\n",
      "Epoch 2357 \t\t Training Loss: 0.0005809708382003009 \t\n",
      "Epoch 2358 \t\t Training Loss: 0.0005809708382003009 \t\n",
      "Epoch 2359 \t\t Training Loss: 0.0005809708382003009 \t\n",
      "Epoch 2360 \t\t Training Loss: 0.0005809708382003009 \t\n",
      "Epoch 2361 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2362 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2363 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2364 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2365 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2366 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2367 \t\t Training Loss: 0.0005809708382003009 \t\n",
      "Epoch 2368 \t\t Training Loss: 0.0005809708382003009 \t\n",
      "Epoch 2369 \t\t Training Loss: 0.0005809708382003009 \t\n",
      "Epoch 2370 \t\t Training Loss: 0.0005809708382003009 \t\n",
      "Epoch 2371 \t\t Training Loss: 0.0005809708382003009 \t\n",
      "Epoch 2372 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2373 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2374 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2375 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2376 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2377 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2378 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2379 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2380 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2381 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2382 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2383 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2384 \t\t Training Loss: 0.00058097077999264 \t\n",
      "Epoch 2385 \t\t Training Loss: 0.0005809707217849791 \t\n",
      "Epoch 2386 \t\t Training Loss: 0.0005809707217849791 \t\n",
      "Epoch 2387 \t\t Training Loss: 0.0005809707217849791 \t\n",
      "Epoch 2388 \t\t Training Loss: 0.0005809707217849791 \t\n",
      "Epoch 2389 \t\t Training Loss: 0.0005809707217849791 \t\n",
      "Epoch 2390 \t\t Training Loss: 0.0005809707217849791 \t\n",
      "Epoch 2391 \t\t Training Loss: 0.0005809707217849791 \t\n",
      "Epoch 2392 \t\t Training Loss: 0.0005809706635773182 \t\n",
      "Epoch 2393 \t\t Training Loss: 0.0005809706635773182 \t\n",
      "Epoch 2394 \t\t Training Loss: 0.0005809706635773182 \t\n",
      "Epoch 2395 \t\t Training Loss: 0.0005809706635773182 \t\n",
      "Epoch 2396 \t\t Training Loss: 0.0005809706635773182 \t\n",
      "Epoch 2397 \t\t Training Loss: 0.0005809706053696573 \t\n",
      "Epoch 2398 \t\t Training Loss: 0.0005809706053696573 \t\n",
      "Epoch 2399 \t\t Training Loss: 0.0005809706635773182 \t\n",
      "Epoch 2400 \t\t Training Loss: 0.0005809706053696573 \t\n",
      "Epoch 2401 \t\t Training Loss: 0.0005809706635773182 \t\n",
      "Epoch 2402 \t\t Training Loss: 0.0005809706053696573 \t\n",
      "Epoch 2403 \t\t Training Loss: 0.0005809706053696573 \t\n",
      "Epoch 2404 \t\t Training Loss: 0.0005809706053696573 \t\n",
      "Epoch 2405 \t\t Training Loss: 0.0005809706053696573 \t\n",
      "Epoch 2406 \t\t Training Loss: 0.0005809706053696573 \t\n",
      "Epoch 2407 \t\t Training Loss: 0.0005809706053696573 \t\n",
      "Epoch 2408 \t\t Training Loss: 0.0005809706053696573 \t\n",
      "Epoch 2409 \t\t Training Loss: 0.0005809704889543355 \t\n",
      "Epoch 2410 \t\t Training Loss: 0.0005809706053696573 \t\n",
      "Epoch 2411 \t\t Training Loss: 0.0005809705471619964 \t\n",
      "Epoch 2412 \t\t Training Loss: 0.0005809705471619964 \t\n",
      "Epoch 2413 \t\t Training Loss: 0.0005809705471619964 \t\n",
      "Epoch 2414 \t\t Training Loss: 0.0005809705471619964 \t\n",
      "Epoch 2415 \t\t Training Loss: 0.0005809705471619964 \t\n",
      "Epoch 2416 \t\t Training Loss: 0.0005809705471619964 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2417 \t\t Training Loss: 0.0005809705471619964 \t\n",
      "Epoch 2418 \t\t Training Loss: 0.0005809704889543355 \t\n",
      "Epoch 2419 \t\t Training Loss: 0.0005809704889543355 \t\n",
      "Epoch 2420 \t\t Training Loss: 0.0005809704889543355 \t\n",
      "Epoch 2421 \t\t Training Loss: 0.0005809705471619964 \t\n",
      "Epoch 2422 \t\t Training Loss: 0.0005809704889543355 \t\n",
      "Epoch 2423 \t\t Training Loss: 0.0005809704889543355 \t\n",
      "Epoch 2424 \t\t Training Loss: 0.0005809704889543355 \t\n",
      "Epoch 2425 \t\t Training Loss: 0.0005809704889543355 \t\n",
      "Epoch 2426 \t\t Training Loss: 0.0005809704889543355 \t\n",
      "Epoch 2427 \t\t Training Loss: 0.0005809704889543355 \t\n",
      "Epoch 2428 \t\t Training Loss: 0.0005809704889543355 \t\n",
      "Epoch 2429 \t\t Training Loss: 0.0005809704889543355 \t\n",
      "Epoch 2430 \t\t Training Loss: 0.0005809703725390136 \t\n",
      "Epoch 2431 \t\t Training Loss: 0.0005809703725390136 \t\n",
      "Epoch 2432 \t\t Training Loss: 0.0005809704307466745 \t\n",
      "Epoch 2433 \t\t Training Loss: 0.0005809704307466745 \t\n",
      "Epoch 2434 \t\t Training Loss: 0.0005809703725390136 \t\n",
      "Epoch 2435 \t\t Training Loss: 0.0005809703725390136 \t\n",
      "Epoch 2436 \t\t Training Loss: 0.0005809703725390136 \t\n",
      "Epoch 2437 \t\t Training Loss: 0.0005809703725390136 \t\n",
      "Epoch 2438 \t\t Training Loss: 0.0005809703725390136 \t\n",
      "Epoch 2439 \t\t Training Loss: 0.0005809703725390136 \t\n",
      "Epoch 2440 \t\t Training Loss: 0.0005809703143313527 \t\n",
      "Epoch 2441 \t\t Training Loss: 0.0005809703143313527 \t\n",
      "Epoch 2442 \t\t Training Loss: 0.0005809703143313527 \t\n",
      "Epoch 2443 \t\t Training Loss: 0.0005809703143313527 \t\n",
      "Epoch 2444 \t\t Training Loss: 0.0005809703725390136 \t\n",
      "Epoch 2445 \t\t Training Loss: 0.0005809703143313527 \t\n",
      "Epoch 2446 \t\t Training Loss: 0.0005809703725390136 \t\n",
      "Epoch 2447 \t\t Training Loss: 0.0005809703143313527 \t\n",
      "Epoch 2448 \t\t Training Loss: 0.0005809703143313527 \t\n",
      "Epoch 2449 \t\t Training Loss: 0.0005809703143313527 \t\n",
      "Epoch 2450 \t\t Training Loss: 0.0005809703143313527 \t\n",
      "Epoch 2451 \t\t Training Loss: 0.0005809703143313527 \t\n",
      "Epoch 2452 \t\t Training Loss: 0.0005809703143313527 \t\n",
      "Epoch 2453 \t\t Training Loss: 0.0005809703143313527 \t\n",
      "Epoch 2454 \t\t Training Loss: 0.0005809703143313527 \t\n",
      "Epoch 2455 \t\t Training Loss: 0.0005809703143313527 \t\n",
      "Epoch 2456 \t\t Training Loss: 0.0005809702561236918 \t\n",
      "Epoch 2457 \t\t Training Loss: 0.0005809702561236918 \t\n",
      "Epoch 2458 \t\t Training Loss: 0.0005809702561236918 \t\n",
      "Epoch 2459 \t\t Training Loss: 0.0005809702561236918 \t\n",
      "Epoch 2460 \t\t Training Loss: 0.0005809702561236918 \t\n",
      "Epoch 2461 \t\t Training Loss: 0.0005809702561236918 \t\n",
      "Epoch 2462 \t\t Training Loss: 0.0005809702561236918 \t\n",
      "Epoch 2463 \t\t Training Loss: 0.0005809701979160309 \t\n",
      "Epoch 2464 \t\t Training Loss: 0.0005809701979160309 \t\n",
      "Epoch 2465 \t\t Training Loss: 0.0005809701979160309 \t\n",
      "Epoch 2466 \t\t Training Loss: 0.0005809702561236918 \t\n",
      "Epoch 2467 \t\t Training Loss: 0.0005809701979160309 \t\n",
      "Epoch 2468 \t\t Training Loss: 0.0005809701979160309 \t\n",
      "Epoch 2469 \t\t Training Loss: 0.0005809702561236918 \t\n",
      "Epoch 2470 \t\t Training Loss: 0.0005809702561236918 \t\n",
      "Epoch 2471 \t\t Training Loss: 0.0005809701979160309 \t\n",
      "Epoch 2472 \t\t Training Loss: 0.0005809701979160309 \t\n",
      "Epoch 2473 \t\t Training Loss: 0.0005809701979160309 \t\n",
      "Epoch 2474 \t\t Training Loss: 0.0005809701979160309 \t\n",
      "Epoch 2475 \t\t Training Loss: 0.0005809701979160309 \t\n",
      "Epoch 2476 \t\t Training Loss: 0.00058097013970837 \t\n",
      "Epoch 2477 \t\t Training Loss: 0.00058097013970837 \t\n",
      "Epoch 2478 \t\t Training Loss: 0.00058097013970837 \t\n",
      "Epoch 2479 \t\t Training Loss: 0.00058097013970837 \t\n",
      "Epoch 2480 \t\t Training Loss: 0.00058097013970837 \t\n",
      "Epoch 2481 \t\t Training Loss: 0.00058097013970837 \t\n",
      "Epoch 2482 \t\t Training Loss: 0.00058097013970837 \t\n",
      "Epoch 2483 \t\t Training Loss: 0.00058097013970837 \t\n",
      "Epoch 2484 \t\t Training Loss: 0.00058097013970837 \t\n",
      "Epoch 2485 \t\t Training Loss: 0.00058097013970837 \t\n",
      "Epoch 2486 \t\t Training Loss: 0.00058097013970837 \t\n",
      "Epoch 2487 \t\t Training Loss: 0.00058097013970837 \t\n",
      "Epoch 2488 \t\t Training Loss: 0.00058097013970837 \t\n",
      "Epoch 2489 \t\t Training Loss: 0.00058097013970837 \t\n",
      "Epoch 2490 \t\t Training Loss: 0.00058097013970837 \t\n",
      "Epoch 2491 \t\t Training Loss: 0.0005809700815007091 \t\n",
      "Epoch 2492 \t\t Training Loss: 0.0005809700232930481 \t\n",
      "Epoch 2493 \t\t Training Loss: 0.0005809700232930481 \t\n",
      "Epoch 2494 \t\t Training Loss: 0.0005809700232930481 \t\n",
      "Epoch 2495 \t\t Training Loss: 0.0005809700232930481 \t\n",
      "Epoch 2496 \t\t Training Loss: 0.0005809700232930481 \t\n",
      "Epoch 2497 \t\t Training Loss: 0.0005809700232930481 \t\n",
      "Epoch 2498 \t\t Training Loss: 0.0005809700232930481 \t\n",
      "Epoch 2499 \t\t Training Loss: 0.0005809700232930481 \t\n",
      "Epoch 2500 \t\t Training Loss: 0.0005809700232930481 \t\n",
      "Epoch 2501 \t\t Training Loss: 0.0005809700232930481 \t\n",
      "Epoch 2502 \t\t Training Loss: 0.0005809700232930481 \t\n",
      "Epoch 2503 \t\t Training Loss: 0.0005809700232930481 \t\n",
      "Epoch 2504 \t\t Training Loss: 0.0005809700232930481 \t\n",
      "Epoch 2505 \t\t Training Loss: 0.0005809700232930481 \t\n",
      "Epoch 2506 \t\t Training Loss: 0.0005809700232930481 \t\n",
      "Epoch 2507 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2508 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2509 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2510 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2511 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2512 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2513 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2514 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2515 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2516 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2517 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2518 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2519 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2520 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2521 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2522 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2523 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2524 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2525 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2526 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2527 \t\t Training Loss: 0.0005809699068777263 \t\n",
      "Epoch 2528 \t\t Training Loss: 0.0005809697904624045 \t\n",
      "Epoch 2529 \t\t Training Loss: 0.0005809698486700654 \t\n",
      "Epoch 2530 \t\t Training Loss: 0.0005809697904624045 \t\n",
      "Epoch 2531 \t\t Training Loss: 0.0005809697904624045 \t\n",
      "Epoch 2532 \t\t Training Loss: 0.0005809697904624045 \t\n",
      "Epoch 2533 \t\t Training Loss: 0.0005809697904624045 \t\n",
      "Epoch 2534 \t\t Training Loss: 0.0005809698486700654 \t\n",
      "Epoch 2535 \t\t Training Loss: 0.0005809697904624045 \t\n",
      "Epoch 2536 \t\t Training Loss: 0.0005809697904624045 \t\n",
      "Epoch 2537 \t\t Training Loss: 0.0005809697904624045 \t\n",
      "Epoch 2538 \t\t Training Loss: 0.0005809697904624045 \t\n",
      "Epoch 2539 \t\t Training Loss: 0.0005809697904624045 \t\n",
      "Epoch 2540 \t\t Training Loss: 0.0005809697904624045 \t\n",
      "Epoch 2541 \t\t Training Loss: 0.0005809697904624045 \t\n",
      "Epoch 2542 \t\t Training Loss: 0.0005809697904624045 \t\n",
      "Epoch 2543 \t\t Training Loss: 0.0005809697904624045 \t\n",
      "Epoch 2544 \t\t Training Loss: 0.0005809697904624045 \t\n",
      "Epoch 2545 \t\t Training Loss: 0.0005809697904624045 \t\n",
      "Epoch 2546 \t\t Training Loss: 0.0005809697322547436 \t\n",
      "Epoch 2547 \t\t Training Loss: 0.0005809697322547436 \t\n",
      "Epoch 2548 \t\t Training Loss: 0.0005809697322547436 \t\n",
      "Epoch 2549 \t\t Training Loss: 0.0005809697322547436 \t\n",
      "Epoch 2550 \t\t Training Loss: 0.0005809697322547436 \t\n",
      "Epoch 2551 \t\t Training Loss: 0.0005809697322547436 \t\n",
      "Epoch 2552 \t\t Training Loss: 0.0005809697322547436 \t\n",
      "Epoch 2553 \t\t Training Loss: 0.0005809697322547436 \t\n",
      "Epoch 2554 \t\t Training Loss: 0.0005809697322547436 \t\n",
      "Epoch 2555 \t\t Training Loss: 0.0005809697322547436 \t\n",
      "Epoch 2556 \t\t Training Loss: 0.0005809697322547436 \t\n",
      "Epoch 2557 \t\t Training Loss: 0.0005809697322547436 \t\n",
      "Epoch 2558 \t\t Training Loss: 0.0005809696158394217 \t\n",
      "Epoch 2559 \t\t Training Loss: 0.0005809696158394217 \t\n",
      "Epoch 2560 \t\t Training Loss: 0.0005809696158394217 \t\n",
      "Epoch 2561 \t\t Training Loss: 0.0005809696158394217 \t\n",
      "Epoch 2562 \t\t Training Loss: 0.0005809696158394217 \t\n",
      "Epoch 2563 \t\t Training Loss: 0.0005809696158394217 \t\n",
      "Epoch 2564 \t\t Training Loss: 0.0005809696158394217 \t\n",
      "Epoch 2565 \t\t Training Loss: 0.0005809696158394217 \t\n",
      "Epoch 2566 \t\t Training Loss: 0.0005809696158394217 \t\n",
      "Epoch 2567 \t\t Training Loss: 0.0005809696158394217 \t\n",
      "Epoch 2568 \t\t Training Loss: 0.0005809696158394217 \t\n",
      "Epoch 2569 \t\t Training Loss: 0.0005809696158394217 \t\n",
      "Epoch 2570 \t\t Training Loss: 0.0005809696158394217 \t\n",
      "Epoch 2571 \t\t Training Loss: 0.0005809696158394217 \t\n",
      "Epoch 2572 \t\t Training Loss: 0.0005809696158394217 \t\n",
      "Epoch 2573 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2574 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2575 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2576 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2577 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2578 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2579 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2580 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2581 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2582 \t\t Training Loss: 0.0005809695576317608 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2583 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2584 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2585 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2586 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2587 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2588 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2589 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2590 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2591 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2592 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2593 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2594 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2595 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2596 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2597 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2598 \t\t Training Loss: 0.0005809695576317608 \t\n",
      "Epoch 2599 \t\t Training Loss: 0.0005809694994240999 \t\n",
      "Epoch 2600 \t\t Training Loss: 0.0005809694994240999 \t\n",
      "Epoch 2601 \t\t Training Loss: 0.0005809694994240999 \t\n",
      "Epoch 2602 \t\t Training Loss: 0.0005809694994240999 \t\n",
      "Epoch 2603 \t\t Training Loss: 0.0005809694994240999 \t\n",
      "Epoch 2604 \t\t Training Loss: 0.0005809694994240999 \t\n",
      "Epoch 2605 \t\t Training Loss: 0.0005809694994240999 \t\n",
      "Epoch 2606 \t\t Training Loss: 0.0005809694994240999 \t\n",
      "Epoch 2607 \t\t Training Loss: 0.0005809694994240999 \t\n",
      "Epoch 2608 \t\t Training Loss: 0.000580969441216439 \t\n",
      "Epoch 2609 \t\t Training Loss: 0.0005809694994240999 \t\n",
      "Epoch 2610 \t\t Training Loss: 0.000580969441216439 \t\n",
      "Epoch 2611 \t\t Training Loss: 0.000580969441216439 \t\n",
      "Epoch 2612 \t\t Training Loss: 0.000580969441216439 \t\n",
      "Epoch 2613 \t\t Training Loss: 0.000580969441216439 \t\n",
      "Epoch 2614 \t\t Training Loss: 0.000580969441216439 \t\n",
      "Epoch 2615 \t\t Training Loss: 0.000580969441216439 \t\n",
      "Epoch 2616 \t\t Training Loss: 0.000580969441216439 \t\n",
      "Epoch 2617 \t\t Training Loss: 0.000580969441216439 \t\n",
      "Epoch 2618 \t\t Training Loss: 0.000580969441216439 \t\n",
      "Epoch 2619 \t\t Training Loss: 0.0005809693248011172 \t\n",
      "Epoch 2620 \t\t Training Loss: 0.0005809693248011172 \t\n",
      "Epoch 2621 \t\t Training Loss: 0.0005809693248011172 \t\n",
      "Epoch 2622 \t\t Training Loss: 0.0005809693248011172 \t\n",
      "Epoch 2623 \t\t Training Loss: 0.0005809693248011172 \t\n",
      "Epoch 2624 \t\t Training Loss: 0.0005809693248011172 \t\n",
      "Epoch 2625 \t\t Training Loss: 0.0005809693248011172 \t\n",
      "Epoch 2626 \t\t Training Loss: 0.0005809693248011172 \t\n",
      "Epoch 2627 \t\t Training Loss: 0.0005809693248011172 \t\n",
      "Epoch 2628 \t\t Training Loss: 0.0005809693248011172 \t\n",
      "Epoch 2629 \t\t Training Loss: 0.0005809693248011172 \t\n",
      "Epoch 2630 \t\t Training Loss: 0.0005809693248011172 \t\n",
      "Epoch 2631 \t\t Training Loss: 0.0005809692665934563 \t\n",
      "Epoch 2632 \t\t Training Loss: 0.0005809692665934563 \t\n",
      "Epoch 2633 \t\t Training Loss: 0.0005809692665934563 \t\n",
      "Epoch 2634 \t\t Training Loss: 0.0005809692665934563 \t\n",
      "Epoch 2635 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2636 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2637 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2638 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2639 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2640 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2641 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2642 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2643 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2644 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2645 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2646 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2647 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2648 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2649 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2650 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2651 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2652 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2653 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2654 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2655 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2656 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2657 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2658 \t\t Training Loss: 0.0005809692083857954 \t\n",
      "Epoch 2659 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2660 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2661 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2662 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2663 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2664 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2665 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2666 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2667 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2668 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2669 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2670 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2671 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2672 \t\t Training Loss: 0.0005809690919704735 \t\n",
      "Epoch 2673 \t\t Training Loss: 0.0005809691501781344 \t\n",
      "Epoch 2674 \t\t Training Loss: 0.0005809690919704735 \t\n",
      "Epoch 2675 \t\t Training Loss: 0.0005809690919704735 \t\n",
      "Epoch 2676 \t\t Training Loss: 0.0005809690919704735 \t\n",
      "Epoch 2677 \t\t Training Loss: 0.0005809690919704735 \t\n",
      "Epoch 2678 \t\t Training Loss: 0.0005809690919704735 \t\n",
      "Epoch 2679 \t\t Training Loss: 0.0005809690919704735 \t\n",
      "Epoch 2680 \t\t Training Loss: 0.0005809690919704735 \t\n",
      "Epoch 2681 \t\t Training Loss: 0.0005809690919704735 \t\n",
      "Epoch 2682 \t\t Training Loss: 0.0005809690919704735 \t\n",
      "Epoch 2683 \t\t Training Loss: 0.0005809690919704735 \t\n",
      "Epoch 2684 \t\t Training Loss: 0.0005809690919704735 \t\n",
      "Epoch 2685 \t\t Training Loss: 0.0005809690919704735 \t\n",
      "Epoch 2686 \t\t Training Loss: 0.0005809690919704735 \t\n",
      "Epoch 2687 \t\t Training Loss: 0.0005809690337628126 \t\n",
      "Epoch 2688 \t\t Training Loss: 0.0005809690337628126 \t\n",
      "Epoch 2689 \t\t Training Loss: 0.0005809690337628126 \t\n",
      "Epoch 2690 \t\t Training Loss: 0.0005809690337628126 \t\n",
      "Epoch 2691 \t\t Training Loss: 0.0005809690337628126 \t\n",
      "Epoch 2692 \t\t Training Loss: 0.0005809690337628126 \t\n",
      "Epoch 2693 \t\t Training Loss: 0.0005809690337628126 \t\n",
      "Epoch 2694 \t\t Training Loss: 0.0005809689755551517 \t\n",
      "Epoch 2695 \t\t Training Loss: 0.0005809690337628126 \t\n",
      "Epoch 2696 \t\t Training Loss: 0.0005809689755551517 \t\n",
      "Epoch 2697 \t\t Training Loss: 0.0005809689755551517 \t\n",
      "Epoch 2698 \t\t Training Loss: 0.0005809689755551517 \t\n",
      "Epoch 2699 \t\t Training Loss: 0.0005809689173474908 \t\n",
      "Epoch 2700 \t\t Training Loss: 0.0005809689173474908 \t\n",
      "Epoch 2701 \t\t Training Loss: 0.0005809688591398299 \t\n",
      "Epoch 2702 \t\t Training Loss: 0.0005809688591398299 \t\n",
      "Epoch 2703 \t\t Training Loss: 0.0005809689173474908 \t\n",
      "Epoch 2704 \t\t Training Loss: 0.0005809688591398299 \t\n",
      "Epoch 2705 \t\t Training Loss: 0.0005809688591398299 \t\n",
      "Epoch 2706 \t\t Training Loss: 0.0005809688591398299 \t\n",
      "Epoch 2707 \t\t Training Loss: 0.0005809688591398299 \t\n",
      "Epoch 2708 \t\t Training Loss: 0.0005809688591398299 \t\n",
      "Epoch 2709 \t\t Training Loss: 0.0005809688591398299 \t\n",
      "Epoch 2710 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2711 \t\t Training Loss: 0.0005809688591398299 \t\n",
      "Epoch 2712 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2713 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2714 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2715 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2716 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2717 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2718 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2719 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2720 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2721 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2722 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2723 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2724 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2725 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2726 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2727 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2728 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2729 \t\t Training Loss: 0.000580968742724508 \t\n",
      "Epoch 2730 \t\t Training Loss: 0.0005809686845168471 \t\n",
      "Epoch 2731 \t\t Training Loss: 0.0005809686845168471 \t\n",
      "Epoch 2732 \t\t Training Loss: 0.0005809686845168471 \t\n",
      "Epoch 2733 \t\t Training Loss: 0.0005809686845168471 \t\n",
      "Epoch 2734 \t\t Training Loss: 0.0005809686845168471 \t\n",
      "Epoch 2735 \t\t Training Loss: 0.0005809686845168471 \t\n",
      "Epoch 2736 \t\t Training Loss: 0.0005809686845168471 \t\n",
      "Epoch 2737 \t\t Training Loss: 0.0005809686845168471 \t\n",
      "Epoch 2738 \t\t Training Loss: 0.0005809686845168471 \t\n",
      "Epoch 2739 \t\t Training Loss: 0.0005809685681015253 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2740 \t\t Training Loss: 0.0005809686845168471 \t\n",
      "Epoch 2741 \t\t Training Loss: 0.0005809686263091862 \t\n",
      "Epoch 2742 \t\t Training Loss: 0.0005809686263091862 \t\n",
      "Epoch 2743 \t\t Training Loss: 0.0005809686263091862 \t\n",
      "Epoch 2744 \t\t Training Loss: 0.0005809686263091862 \t\n",
      "Epoch 2745 \t\t Training Loss: 0.0005809685681015253 \t\n",
      "Epoch 2746 \t\t Training Loss: 0.0005809685681015253 \t\n",
      "Epoch 2747 \t\t Training Loss: 0.0005809685681015253 \t\n",
      "Epoch 2748 \t\t Training Loss: 0.0005809685681015253 \t\n",
      "Epoch 2749 \t\t Training Loss: 0.0005809685681015253 \t\n",
      "Epoch 2750 \t\t Training Loss: 0.0005809685681015253 \t\n",
      "Epoch 2751 \t\t Training Loss: 0.0005809685681015253 \t\n",
      "Epoch 2752 \t\t Training Loss: 0.0005809685681015253 \t\n",
      "Epoch 2753 \t\t Training Loss: 0.0005809685681015253 \t\n",
      "Epoch 2754 \t\t Training Loss: 0.0005809685681015253 \t\n",
      "Epoch 2755 \t\t Training Loss: 0.0005809685681015253 \t\n",
      "Epoch 2756 \t\t Training Loss: 0.0005809685681015253 \t\n",
      "Epoch 2757 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2758 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2759 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2760 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2761 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2762 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2763 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2764 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2765 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2766 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2767 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2768 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2769 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2770 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2771 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2772 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2773 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2774 \t\t Training Loss: 0.0005809683352708817 \t\n",
      "Epoch 2775 \t\t Training Loss: 0.0005809683352708817 \t\n",
      "Epoch 2776 \t\t Training Loss: 0.0005809683352708817 \t\n",
      "Epoch 2777 \t\t Training Loss: 0.0005809683352708817 \t\n",
      "Epoch 2778 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2779 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2780 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2781 \t\t Training Loss: 0.0005809684516862035 \t\n",
      "Epoch 2782 \t\t Training Loss: 0.0005809683352708817 \t\n",
      "Epoch 2783 \t\t Training Loss: 0.0005809682770632207 \t\n",
      "Epoch 2784 \t\t Training Loss: 0.0005809682770632207 \t\n",
      "Epoch 2785 \t\t Training Loss: 0.0005809682770632207 \t\n",
      "Epoch 2786 \t\t Training Loss: 0.0005809682770632207 \t\n",
      "Epoch 2787 \t\t Training Loss: 0.0005809682770632207 \t\n",
      "Epoch 2788 \t\t Training Loss: 0.0005809682770632207 \t\n",
      "Epoch 2789 \t\t Training Loss: 0.0005809682770632207 \t\n",
      "Epoch 2790 \t\t Training Loss: 0.0005809682770632207 \t\n",
      "Epoch 2791 \t\t Training Loss: 0.0005809682770632207 \t\n",
      "Epoch 2792 \t\t Training Loss: 0.0005809682770632207 \t\n",
      "Epoch 2793 \t\t Training Loss: 0.0005809682770632207 \t\n",
      "Epoch 2794 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2795 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2796 \t\t Training Loss: 0.0005809682770632207 \t\n",
      "Epoch 2797 \t\t Training Loss: 0.0005809682188555598 \t\n",
      "Epoch 2798 \t\t Training Loss: 0.0005809682188555598 \t\n",
      "Epoch 2799 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2800 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2801 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2802 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2803 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2804 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2805 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2806 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2807 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2808 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2809 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2810 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2811 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2812 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2813 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2814 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2815 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2816 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2817 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2818 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2819 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2820 \t\t Training Loss: 0.0005809681606478989 \t\n",
      "Epoch 2821 \t\t Training Loss: 0.0005809680442325771 \t\n",
      "Epoch 2822 \t\t Training Loss: 0.0005809680442325771 \t\n",
      "Epoch 2823 \t\t Training Loss: 0.0005809680442325771 \t\n",
      "Epoch 2824 \t\t Training Loss: 0.0005809680442325771 \t\n",
      "Epoch 2825 \t\t Training Loss: 0.0005809680442325771 \t\n",
      "Epoch 2826 \t\t Training Loss: 0.0005809680442325771 \t\n",
      "Epoch 2827 \t\t Training Loss: 0.0005809680442325771 \t\n",
      "Epoch 2828 \t\t Training Loss: 0.0005809680442325771 \t\n",
      "Epoch 2829 \t\t Training Loss: 0.0005809680442325771 \t\n",
      "Epoch 2830 \t\t Training Loss: 0.0005809679860249162 \t\n",
      "Epoch 2831 \t\t Training Loss: 0.0005809679860249162 \t\n",
      "Epoch 2832 \t\t Training Loss: 0.0005809679860249162 \t\n",
      "Epoch 2833 \t\t Training Loss: 0.0005809679860249162 \t\n",
      "Epoch 2834 \t\t Training Loss: 0.0005809679860249162 \t\n",
      "Epoch 2835 \t\t Training Loss: 0.0005809679860249162 \t\n",
      "Epoch 2836 \t\t Training Loss: 0.0005809679860249162 \t\n",
      "Epoch 2837 \t\t Training Loss: 0.0005809679860249162 \t\n",
      "Epoch 2838 \t\t Training Loss: 0.0005809679860249162 \t\n",
      "Epoch 2839 \t\t Training Loss: 0.0005809679860249162 \t\n",
      "Epoch 2840 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2841 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2842 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2843 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2844 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2845 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2846 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2847 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2848 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2849 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2850 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2851 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2852 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2853 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2854 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2855 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2856 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2857 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2858 \t\t Training Loss: 0.0005809679278172553 \t\n",
      "Epoch 2859 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2860 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2861 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2862 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2863 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2864 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2865 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2866 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2867 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2868 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2869 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2870 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2871 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2872 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2873 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2874 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2875 \t\t Training Loss: 0.0005809678696095943 \t\n",
      "Epoch 2876 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2877 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2878 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2879 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2880 \t\t Training Loss: 0.0005809677531942725 \t\n",
      "Epoch 2881 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2882 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2883 \t\t Training Loss: 0.0005809677531942725 \t\n",
      "Epoch 2884 \t\t Training Loss: 0.0005809677531942725 \t\n",
      "Epoch 2885 \t\t Training Loss: 0.0005809677531942725 \t\n",
      "Epoch 2886 \t\t Training Loss: 0.0005809677531942725 \t\n",
      "Epoch 2887 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2888 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2889 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2890 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2891 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2892 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2893 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2894 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2895 \t\t Training Loss: 0.0005809676367789507 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2896 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2897 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2898 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2899 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2900 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2901 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2902 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2903 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2904 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2905 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2906 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2907 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2908 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2909 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2910 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2911 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2912 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2913 \t\t Training Loss: 0.0005809675785712898 \t\n",
      "Epoch 2914 \t\t Training Loss: 0.0005809675785712898 \t\n",
      "Epoch 2915 \t\t Training Loss: 0.0005809676367789507 \t\n",
      "Epoch 2916 \t\t Training Loss: 0.0005809675785712898 \t\n",
      "Epoch 2917 \t\t Training Loss: 0.0005809675785712898 \t\n",
      "Epoch 2918 \t\t Training Loss: 0.0005809675785712898 \t\n",
      "Epoch 2919 \t\t Training Loss: 0.0005809675785712898 \t\n",
      "Epoch 2920 \t\t Training Loss: 0.0005809675785712898 \t\n",
      "Epoch 2921 \t\t Training Loss: 0.0005809675785712898 \t\n",
      "Epoch 2922 \t\t Training Loss: 0.0005809675785712898 \t\n",
      "Epoch 2923 \t\t Training Loss: 0.0005809675785712898 \t\n",
      "Epoch 2924 \t\t Training Loss: 0.0005809675785712898 \t\n",
      "Epoch 2925 \t\t Training Loss: 0.0005809675785712898 \t\n",
      "Epoch 2926 \t\t Training Loss: 0.0005809675785712898 \t\n",
      "Epoch 2927 \t\t Training Loss: 0.0005809675785712898 \t\n",
      "Epoch 2928 \t\t Training Loss: 0.0005809675785712898 \t\n",
      "Epoch 2929 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2930 \t\t Training Loss: 0.0005809675785712898 \t\n",
      "Epoch 2931 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2932 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2933 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2934 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2935 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2936 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2937 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2938 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2939 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2940 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2941 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2942 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2943 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2944 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2945 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2946 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2947 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2948 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2949 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2950 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2951 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2952 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2953 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2954 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2955 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2956 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2957 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2958 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2959 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2960 \t\t Training Loss: 0.000580967462155968 \t\n",
      "Epoch 2961 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2962 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2963 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2964 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2965 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2966 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2967 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2968 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2969 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2970 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2971 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2972 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2973 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2974 \t\t Training Loss: 0.0005809672875329852 \t\n",
      "Epoch 2975 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2976 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2977 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2978 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2979 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2980 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2981 \t\t Training Loss: 0.0005809672875329852 \t\n",
      "Epoch 2982 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2983 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 2984 \t\t Training Loss: 0.0005809672875329852 \t\n",
      "Epoch 2985 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2986 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 2987 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2988 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2989 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2990 \t\t Training Loss: 0.0005809672875329852 \t\n",
      "Epoch 2991 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2992 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2993 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2994 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2995 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 2996 \t\t Training Loss: 0.0005809672875329852 \t\n",
      "Epoch 2997 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 2998 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 2999 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3000 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3001 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3002 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3003 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3004 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3005 \t\t Training Loss: 0.0005809672875329852 \t\n",
      "Epoch 3006 \t\t Training Loss: 0.0005809672875329852 \t\n",
      "Epoch 3007 \t\t Training Loss: 0.0005809672875329852 \t\n",
      "Epoch 3008 \t\t Training Loss: 0.0005809672875329852 \t\n",
      "Epoch 3009 \t\t Training Loss: 0.0005809673457406461 \t\n",
      "Epoch 3010 \t\t Training Loss: 0.0005809672875329852 \t\n",
      "Epoch 3011 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3012 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3013 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3014 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3015 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3016 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3017 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3018 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3019 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3020 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3021 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3022 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3023 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3024 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3025 \t\t Training Loss: 0.0005809671711176634 \t\n",
      "Epoch 3026 \t\t Training Loss: 0.0005809670547023416 \t\n",
      "Epoch 3027 \t\t Training Loss: 0.0005809670547023416 \t\n",
      "Epoch 3028 \t\t Training Loss: 0.0005809670547023416 \t\n",
      "Epoch 3029 \t\t Training Loss: 0.0005809670547023416 \t\n",
      "Epoch 3030 \t\t Training Loss: 0.0005809670547023416 \t\n",
      "Epoch 3031 \t\t Training Loss: 0.0005809670547023416 \t\n",
      "Epoch 3032 \t\t Training Loss: 0.0005809670547023416 \t\n",
      "Epoch 3033 \t\t Training Loss: 0.0005809670547023416 \t\n",
      "Epoch 3034 \t\t Training Loss: 0.0005809669964946806 \t\n",
      "Epoch 3035 \t\t Training Loss: 0.0005809669964946806 \t\n",
      "Epoch 3036 \t\t Training Loss: 0.0005809668800793588 \t\n",
      "Epoch 3037 \t\t Training Loss: 0.0005809669964946806 \t\n",
      "Epoch 3038 \t\t Training Loss: 0.0005809669964946806 \t\n",
      "Epoch 3039 \t\t Training Loss: 0.0005809668800793588 \t\n",
      "Epoch 3040 \t\t Training Loss: 0.0005809669964946806 \t\n",
      "Epoch 3041 \t\t Training Loss: 0.0005809669964946806 \t\n",
      "Epoch 3042 \t\t Training Loss: 0.0005809669964946806 \t\n",
      "Epoch 3043 \t\t Training Loss: 0.0005809669964946806 \t\n",
      "Epoch 3044 \t\t Training Loss: 0.0005809669964946806 \t\n",
      "Epoch 3045 \t\t Training Loss: 0.0005809669964946806 \t\n",
      "Epoch 3046 \t\t Training Loss: 0.0005809669964946806 \t\n",
      "Epoch 3047 \t\t Training Loss: 0.0005809668800793588 \t\n",
      "Epoch 3048 \t\t Training Loss: 0.0005809668800793588 \t\n",
      "Epoch 3049 \t\t Training Loss: 0.0005809668800793588 \t\n",
      "Epoch 3050 \t\t Training Loss: 0.0005809668800793588 \t\n",
      "Epoch 3051 \t\t Training Loss: 0.0005809668800793588 \t\n",
      "Epoch 3052 \t\t Training Loss: 0.0005809668800793588 \t\n",
      "Epoch 3053 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3054 \t\t Training Loss: 0.0005809668800793588 \t\n",
      "Epoch 3055 \t\t Training Loss: 0.0005809668800793588 \t\n",
      "Epoch 3056 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3057 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3058 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3059 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3060 \t\t Training Loss: 0.000580966763664037 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3061 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3062 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3063 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3064 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3065 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3066 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3067 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3068 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3069 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3070 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3071 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3072 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3073 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3074 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3075 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3076 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3077 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3078 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3079 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3080 \t\t Training Loss: 0.000580966763664037 \t\n",
      "Epoch 3081 \t\t Training Loss: 0.0005809666472487152 \t\n",
      "Epoch 3082 \t\t Training Loss: 0.0005809666472487152 \t\n",
      "Epoch 3083 \t\t Training Loss: 0.0005809666472487152 \t\n",
      "Epoch 3084 \t\t Training Loss: 0.0005809666472487152 \t\n",
      "Epoch 3085 \t\t Training Loss: 0.0005809666472487152 \t\n",
      "Epoch 3086 \t\t Training Loss: 0.0005809666472487152 \t\n",
      "Epoch 3087 \t\t Training Loss: 0.0005809666472487152 \t\n",
      "Epoch 3088 \t\t Training Loss: 0.0005809666472487152 \t\n",
      "Epoch 3089 \t\t Training Loss: 0.0005809666472487152 \t\n",
      "Epoch 3090 \t\t Training Loss: 0.0005809666472487152 \t\n",
      "Epoch 3091 \t\t Training Loss: 0.0005809666472487152 \t\n",
      "Epoch 3092 \t\t Training Loss: 0.0005809665890410542 \t\n",
      "Epoch 3093 \t\t Training Loss: 0.0005809665890410542 \t\n",
      "Epoch 3094 \t\t Training Loss: 0.0005809665890410542 \t\n",
      "Epoch 3095 \t\t Training Loss: 0.0005809665890410542 \t\n",
      "Epoch 3096 \t\t Training Loss: 0.0005809665890410542 \t\n",
      "Epoch 3097 \t\t Training Loss: 0.0005809665890410542 \t\n",
      "Epoch 3098 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3099 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3100 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3101 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3102 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3103 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3104 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3105 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3106 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3107 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3108 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3109 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3110 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3111 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3112 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3113 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3114 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3115 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3116 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3117 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3118 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3119 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3120 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3121 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3122 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3123 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3124 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3125 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3126 \t\t Training Loss: 0.0005809664726257324 \t\n",
      "Epoch 3127 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3128 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3129 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3130 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3131 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3132 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3133 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3134 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3135 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3136 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3137 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3138 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3139 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3140 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3141 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3142 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3143 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3144 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3145 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3146 \t\t Training Loss: 0.0005809662980027497 \t\n",
      "Epoch 3147 \t\t Training Loss: 0.0005809662980027497 \t\n",
      "Epoch 3148 \t\t Training Loss: 0.0005809662980027497 \t\n",
      "Epoch 3149 \t\t Training Loss: 0.0005809662980027497 \t\n",
      "Epoch 3150 \t\t Training Loss: 0.0005809662980027497 \t\n",
      "Epoch 3151 \t\t Training Loss: 0.0005809662980027497 \t\n",
      "Epoch 3152 \t\t Training Loss: 0.0005809662980027497 \t\n",
      "Epoch 3153 \t\t Training Loss: 0.0005809663562104106 \t\n",
      "Epoch 3154 \t\t Training Loss: 0.0005809662980027497 \t\n",
      "Epoch 3155 \t\t Training Loss: 0.0005809662980027497 \t\n",
      "Epoch 3156 \t\t Training Loss: 0.0005809662980027497 \t\n",
      "Epoch 3157 \t\t Training Loss: 0.0005809662980027497 \t\n",
      "Epoch 3158 \t\t Training Loss: 0.0005809662980027497 \t\n",
      "Epoch 3159 \t\t Training Loss: 0.0005809662397950888 \t\n",
      "Epoch 3160 \t\t Training Loss: 0.0005809662397950888 \t\n",
      "Epoch 3161 \t\t Training Loss: 0.0005809662397950888 \t\n",
      "Epoch 3162 \t\t Training Loss: 0.0005809662397950888 \t\n",
      "Epoch 3163 \t\t Training Loss: 0.0005809662397950888 \t\n",
      "Epoch 3164 \t\t Training Loss: 0.0005809662397950888 \t\n",
      "Epoch 3165 \t\t Training Loss: 0.0005809662397950888 \t\n",
      "Epoch 3166 \t\t Training Loss: 0.0005809661815874279 \t\n",
      "Epoch 3167 \t\t Training Loss: 0.000580966065172106 \t\n",
      "Epoch 3168 \t\t Training Loss: 0.000580966065172106 \t\n",
      "Epoch 3169 \t\t Training Loss: 0.000580966065172106 \t\n",
      "Epoch 3170 \t\t Training Loss: 0.000580966065172106 \t\n",
      "Epoch 3171 \t\t Training Loss: 0.000580966065172106 \t\n",
      "Epoch 3172 \t\t Training Loss: 0.000580966065172106 \t\n",
      "Epoch 3173 \t\t Training Loss: 0.000580966065172106 \t\n",
      "Epoch 3174 \t\t Training Loss: 0.000580966065172106 \t\n",
      "Epoch 3175 \t\t Training Loss: 0.000580966065172106 \t\n",
      "Epoch 3176 \t\t Training Loss: 0.000580966065172106 \t\n",
      "Epoch 3177 \t\t Training Loss: 0.000580966065172106 \t\n",
      "Epoch 3178 \t\t Training Loss: 0.000580966065172106 \t\n",
      "Epoch 3179 \t\t Training Loss: 0.000580966065172106 \t\n",
      "Epoch 3180 \t\t Training Loss: 0.0005809660069644451 \t\n",
      "Epoch 3181 \t\t Training Loss: 0.000580966065172106 \t\n",
      "Epoch 3182 \t\t Training Loss: 0.0005809660069644451 \t\n",
      "Epoch 3183 \t\t Training Loss: 0.0005809660069644451 \t\n",
      "Epoch 3184 \t\t Training Loss: 0.0005809660069644451 \t\n",
      "Epoch 3185 \t\t Training Loss: 0.0005809660069644451 \t\n",
      "Epoch 3186 \t\t Training Loss: 0.0005809659487567842 \t\n",
      "Epoch 3187 \t\t Training Loss: 0.0005809660069644451 \t\n",
      "Epoch 3188 \t\t Training Loss: 0.0005809660069644451 \t\n",
      "Epoch 3189 \t\t Training Loss: 0.0005809660069644451 \t\n",
      "Epoch 3190 \t\t Training Loss: 0.0005809660069644451 \t\n",
      "Epoch 3191 \t\t Training Loss: 0.0005809660069644451 \t\n",
      "Epoch 3192 \t\t Training Loss: 0.0005809660069644451 \t\n",
      "Epoch 3193 \t\t Training Loss: 0.0005809660069644451 \t\n",
      "Epoch 3194 \t\t Training Loss: 0.0005809659487567842 \t\n",
      "Epoch 3195 \t\t Training Loss: 0.0005809659487567842 \t\n",
      "Epoch 3196 \t\t Training Loss: 0.0005809659487567842 \t\n",
      "Epoch 3197 \t\t Training Loss: 0.0005809659487567842 \t\n",
      "Epoch 3198 \t\t Training Loss: 0.0005809659487567842 \t\n",
      "Epoch 3199 \t\t Training Loss: 0.0005809659487567842 \t\n",
      "Epoch 3200 \t\t Training Loss: 0.0005809659487567842 \t\n",
      "Epoch 3201 \t\t Training Loss: 0.0005809659487567842 \t\n",
      "Epoch 3202 \t\t Training Loss: 0.0005809659487567842 \t\n",
      "Epoch 3203 \t\t Training Loss: 0.0005809659487567842 \t\n",
      "Epoch 3204 \t\t Training Loss: 0.0005809659487567842 \t\n",
      "Epoch 3205 \t\t Training Loss: 0.0005809658905491233 \t\n",
      "Epoch 3206 \t\t Training Loss: 0.0005809658905491233 \t\n",
      "Epoch 3207 \t\t Training Loss: 0.0005809658905491233 \t\n",
      "Epoch 3208 \t\t Training Loss: 0.0005809658905491233 \t\n",
      "Epoch 3209 \t\t Training Loss: 0.0005809658905491233 \t\n",
      "Epoch 3210 \t\t Training Loss: 0.0005809658905491233 \t\n",
      "Epoch 3211 \t\t Training Loss: 0.0005809658905491233 \t\n",
      "Epoch 3212 \t\t Training Loss: 0.0005809658905491233 \t\n",
      "Epoch 3213 \t\t Training Loss: 0.0005809658905491233 \t\n",
      "Epoch 3214 \t\t Training Loss: 0.0005809659487567842 \t\n",
      "Epoch 3215 \t\t Training Loss: 0.0005809659487567842 \t\n",
      "Epoch 3216 \t\t Training Loss: 0.0005809659487567842 \t\n",
      "Epoch 3217 \t\t Training Loss: 0.0005809658905491233 \t\n",
      "Epoch 3218 \t\t Training Loss: 0.0005809658905491233 \t\n",
      "Epoch 3219 \t\t Training Loss: 0.0005809658905491233 \t\n",
      "Epoch 3220 \t\t Training Loss: 0.0005809658905491233 \t\n",
      "Epoch 3221 \t\t Training Loss: 0.0005809658905491233 \t\n",
      "Epoch 3222 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3223 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3224 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3225 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3226 \t\t Training Loss: 0.0005809657741338015 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3227 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3228 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3229 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3230 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3231 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3232 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3233 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3234 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3235 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3236 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3237 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3238 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3239 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3240 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3241 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3242 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3243 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3244 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3245 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3246 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3247 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3248 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3249 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3250 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3251 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3252 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3253 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3254 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3255 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3256 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3257 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3258 \t\t Training Loss: 0.0005809657741338015 \t\n",
      "Epoch 3259 \t\t Training Loss: 0.0005809657159261405 \t\n",
      "Epoch 3260 \t\t Training Loss: 0.0005809657159261405 \t\n",
      "Epoch 3261 \t\t Training Loss: 0.0005809657159261405 \t\n",
      "Epoch 3262 \t\t Training Loss: 0.0005809657159261405 \t\n",
      "Epoch 3263 \t\t Training Loss: 0.0005809656577184796 \t\n",
      "Epoch 3264 \t\t Training Loss: 0.0005809656577184796 \t\n",
      "Epoch 3265 \t\t Training Loss: 0.0005809656577184796 \t\n",
      "Epoch 3266 \t\t Training Loss: 0.0005809656577184796 \t\n",
      "Epoch 3267 \t\t Training Loss: 0.0005809656577184796 \t\n",
      "Epoch 3268 \t\t Training Loss: 0.0005809656577184796 \t\n",
      "Epoch 3269 \t\t Training Loss: 0.0005809656577184796 \t\n",
      "Epoch 3270 \t\t Training Loss: 0.0005809655413031578 \t\n",
      "Epoch 3271 \t\t Training Loss: 0.0005809656577184796 \t\n",
      "Epoch 3272 \t\t Training Loss: 0.0005809656577184796 \t\n",
      "Epoch 3273 \t\t Training Loss: 0.0005809656577184796 \t\n",
      "Epoch 3274 \t\t Training Loss: 0.0005809656577184796 \t\n",
      "Epoch 3275 \t\t Training Loss: 0.0005809656577184796 \t\n",
      "Epoch 3276 \t\t Training Loss: 0.0005809656577184796 \t\n",
      "Epoch 3277 \t\t Training Loss: 0.0005809655413031578 \t\n",
      "Epoch 3278 \t\t Training Loss: 0.0005809655413031578 \t\n",
      "Epoch 3279 \t\t Training Loss: 0.0005809655413031578 \t\n",
      "Epoch 3280 \t\t Training Loss: 0.0005809655413031578 \t\n",
      "Epoch 3281 \t\t Training Loss: 0.0005809655413031578 \t\n",
      "Epoch 3282 \t\t Training Loss: 0.0005809655413031578 \t\n",
      "Epoch 3283 \t\t Training Loss: 0.0005809655413031578 \t\n",
      "Epoch 3284 \t\t Training Loss: 0.0005809654830954969 \t\n",
      "Epoch 3285 \t\t Training Loss: 0.0005809654830954969 \t\n",
      "Epoch 3286 \t\t Training Loss: 0.0005809654830954969 \t\n",
      "Epoch 3287 \t\t Training Loss: 0.0005809654830954969 \t\n",
      "Epoch 3288 \t\t Training Loss: 0.0005809654830954969 \t\n",
      "Epoch 3289 \t\t Training Loss: 0.0005809654830954969 \t\n",
      "Epoch 3290 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3291 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3292 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3293 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3294 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3295 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3296 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3297 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3298 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3299 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3300 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3301 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3302 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3303 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3304 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3305 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3306 \t\t Training Loss: 0.0005809653084725142 \t\n",
      "Epoch 3307 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3308 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3309 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3310 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3311 \t\t Training Loss: 0.0005809653084725142 \t\n",
      "Epoch 3312 \t\t Training Loss: 0.0005809653084725142 \t\n",
      "Epoch 3313 \t\t Training Loss: 0.0005809653084725142 \t\n",
      "Epoch 3314 \t\t Training Loss: 0.0005809653084725142 \t\n",
      "Epoch 3315 \t\t Training Loss: 0.0005809653084725142 \t\n",
      "Epoch 3316 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3317 \t\t Training Loss: 0.0005809653666801751 \t\n",
      "Epoch 3318 \t\t Training Loss: 0.0005809653084725142 \t\n",
      "Epoch 3319 \t\t Training Loss: 0.0005809653084725142 \t\n",
      "Epoch 3320 \t\t Training Loss: 0.0005809653084725142 \t\n",
      "Epoch 3321 \t\t Training Loss: 0.0005809653084725142 \t\n",
      "Epoch 3322 \t\t Training Loss: 0.0005809653084725142 \t\n",
      "Epoch 3323 \t\t Training Loss: 0.0005809653084725142 \t\n",
      "Epoch 3324 \t\t Training Loss: 0.0005809652502648532 \t\n",
      "Epoch 3325 \t\t Training Loss: 0.0005809651920571923 \t\n",
      "Epoch 3326 \t\t Training Loss: 0.0005809651920571923 \t\n",
      "Epoch 3327 \t\t Training Loss: 0.0005809651920571923 \t\n",
      "Epoch 3328 \t\t Training Loss: 0.0005809651920571923 \t\n",
      "Epoch 3329 \t\t Training Loss: 0.0005809651920571923 \t\n",
      "Epoch 3330 \t\t Training Loss: 0.0005809651920571923 \t\n",
      "Epoch 3331 \t\t Training Loss: 0.0005809651920571923 \t\n",
      "Epoch 3332 \t\t Training Loss: 0.0005809651920571923 \t\n",
      "Epoch 3333 \t\t Training Loss: 0.0005809651920571923 \t\n",
      "Epoch 3334 \t\t Training Loss: 0.0005809651920571923 \t\n",
      "Epoch 3335 \t\t Training Loss: 0.0005809651920571923 \t\n",
      "Epoch 3336 \t\t Training Loss: 0.0005809651338495314 \t\n",
      "Epoch 3337 \t\t Training Loss: 0.0005809651338495314 \t\n",
      "Epoch 3338 \t\t Training Loss: 0.0005809651338495314 \t\n",
      "Epoch 3339 \t\t Training Loss: 0.0005809650756418705 \t\n",
      "Epoch 3340 \t\t Training Loss: 0.0005809650756418705 \t\n",
      "Epoch 3341 \t\t Training Loss: 0.0005809650756418705 \t\n",
      "Epoch 3342 \t\t Training Loss: 0.0005809651338495314 \t\n",
      "Epoch 3343 \t\t Training Loss: 0.0005809649592265487 \t\n",
      "Epoch 3344 \t\t Training Loss: 0.0005809649592265487 \t\n",
      "Epoch 3345 \t\t Training Loss: 0.0005809650756418705 \t\n",
      "Epoch 3346 \t\t Training Loss: 0.0005809649592265487 \t\n",
      "Epoch 3347 \t\t Training Loss: 0.0005809650174342096 \t\n",
      "Epoch 3348 \t\t Training Loss: 0.0005809650174342096 \t\n",
      "Epoch 3349 \t\t Training Loss: 0.0005809649592265487 \t\n",
      "Epoch 3350 \t\t Training Loss: 0.0005809649592265487 \t\n",
      "Epoch 3351 \t\t Training Loss: 0.0005809649592265487 \t\n",
      "Epoch 3352 \t\t Training Loss: 0.0005809649592265487 \t\n",
      "Epoch 3353 \t\t Training Loss: 0.0005809649592265487 \t\n",
      "Epoch 3354 \t\t Training Loss: 0.0005809649592265487 \t\n",
      "Epoch 3355 \t\t Training Loss: 0.0005809649592265487 \t\n",
      "Epoch 3356 \t\t Training Loss: 0.0005809649010188878 \t\n",
      "Epoch 3357 \t\t Training Loss: 0.0005809649010188878 \t\n",
      "Epoch 3358 \t\t Training Loss: 0.0005809649010188878 \t\n",
      "Epoch 3359 \t\t Training Loss: 0.0005809649010188878 \t\n",
      "Epoch 3360 \t\t Training Loss: 0.0005809648428112268 \t\n",
      "Epoch 3361 \t\t Training Loss: 0.0005809648428112268 \t\n",
      "Epoch 3362 \t\t Training Loss: 0.0005809648428112268 \t\n",
      "Epoch 3363 \t\t Training Loss: 0.0005809648428112268 \t\n",
      "Epoch 3364 \t\t Training Loss: 0.0005809648428112268 \t\n",
      "Epoch 3365 \t\t Training Loss: 0.0005809648428112268 \t\n",
      "Epoch 3366 \t\t Training Loss: 0.0005809648428112268 \t\n",
      "Epoch 3367 \t\t Training Loss: 0.0005809648428112268 \t\n",
      "Epoch 3368 \t\t Training Loss: 0.0005809648428112268 \t\n",
      "Epoch 3369 \t\t Training Loss: 0.0005809647846035659 \t\n",
      "Epoch 3370 \t\t Training Loss: 0.0005809647846035659 \t\n",
      "Epoch 3371 \t\t Training Loss: 0.0005809647846035659 \t\n",
      "Epoch 3372 \t\t Training Loss: 0.000580964726395905 \t\n",
      "Epoch 3373 \t\t Training Loss: 0.000580964726395905 \t\n",
      "Epoch 3374 \t\t Training Loss: 0.000580964726395905 \t\n",
      "Epoch 3375 \t\t Training Loss: 0.0005809647846035659 \t\n",
      "Epoch 3376 \t\t Training Loss: 0.000580964726395905 \t\n",
      "Epoch 3377 \t\t Training Loss: 0.000580964726395905 \t\n",
      "Epoch 3378 \t\t Training Loss: 0.000580964726395905 \t\n",
      "Epoch 3379 \t\t Training Loss: 0.000580964726395905 \t\n",
      "Epoch 3380 \t\t Training Loss: 0.000580964726395905 \t\n",
      "Epoch 3381 \t\t Training Loss: 0.0005809647846035659 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3382 \t\t Training Loss: 0.0005809647846035659 \t\n",
      "Epoch 3383 \t\t Training Loss: 0.000580964726395905 \t\n",
      "Epoch 3384 \t\t Training Loss: 0.000580964726395905 \t\n",
      "Epoch 3385 \t\t Training Loss: 0.000580964726395905 \t\n",
      "Epoch 3386 \t\t Training Loss: 0.000580964726395905 \t\n",
      "Epoch 3387 \t\t Training Loss: 0.000580964726395905 \t\n",
      "Epoch 3388 \t\t Training Loss: 0.000580964726395905 \t\n",
      "Epoch 3389 \t\t Training Loss: 0.0005809646681882441 \t\n",
      "Epoch 3390 \t\t Training Loss: 0.0005809646681882441 \t\n",
      "Epoch 3391 \t\t Training Loss: 0.0005809646099805832 \t\n",
      "Epoch 3392 \t\t Training Loss: 0.0005809646099805832 \t\n",
      "Epoch 3393 \t\t Training Loss: 0.0005809646099805832 \t\n",
      "Epoch 3394 \t\t Training Loss: 0.0005809646099805832 \t\n",
      "Epoch 3395 \t\t Training Loss: 0.0005809646099805832 \t\n",
      "Epoch 3396 \t\t Training Loss: 0.0005809646099805832 \t\n",
      "Epoch 3397 \t\t Training Loss: 0.0005809646099805832 \t\n",
      "Epoch 3398 \t\t Training Loss: 0.0005809646099805832 \t\n",
      "Epoch 3399 \t\t Training Loss: 0.0005809646099805832 \t\n",
      "Epoch 3400 \t\t Training Loss: 0.0005809646099805832 \t\n",
      "Epoch 3401 \t\t Training Loss: 0.0005809646099805832 \t\n",
      "Epoch 3402 \t\t Training Loss: 0.0005809646099805832 \t\n",
      "Epoch 3403 \t\t Training Loss: 0.0005809646099805832 \t\n",
      "Epoch 3404 \t\t Training Loss: 0.0005809646099805832 \t\n",
      "Epoch 3405 \t\t Training Loss: 0.0005809646099805832 \t\n",
      "Epoch 3406 \t\t Training Loss: 0.0005809645517729223 \t\n",
      "Epoch 3407 \t\t Training Loss: 0.0005809644935652614 \t\n",
      "Epoch 3408 \t\t Training Loss: 0.0005809645517729223 \t\n",
      "Epoch 3409 \t\t Training Loss: 0.0005809644935652614 \t\n",
      "Epoch 3410 \t\t Training Loss: 0.0005809644935652614 \t\n",
      "Epoch 3411 \t\t Training Loss: 0.0005809644935652614 \t\n",
      "Epoch 3412 \t\t Training Loss: 0.0005809644935652614 \t\n",
      "Epoch 3413 \t\t Training Loss: 0.0005809644935652614 \t\n",
      "Epoch 3414 \t\t Training Loss: 0.0005809644935652614 \t\n",
      "Epoch 3415 \t\t Training Loss: 0.0005809644935652614 \t\n",
      "Epoch 3416 \t\t Training Loss: 0.0005809644935652614 \t\n",
      "Epoch 3417 \t\t Training Loss: 0.0005809644935652614 \t\n",
      "Epoch 3418 \t\t Training Loss: 0.0005809644353576005 \t\n",
      "Epoch 3419 \t\t Training Loss: 0.0005809644935652614 \t\n",
      "Epoch 3420 \t\t Training Loss: 0.0005809644935652614 \t\n",
      "Epoch 3421 \t\t Training Loss: 0.0005809644353576005 \t\n",
      "Epoch 3422 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3423 \t\t Training Loss: 0.0005809644353576005 \t\n",
      "Epoch 3424 \t\t Training Loss: 0.0005809644935652614 \t\n",
      "Epoch 3425 \t\t Training Loss: 0.0005809644353576005 \t\n",
      "Epoch 3426 \t\t Training Loss: 0.0005809644353576005 \t\n",
      "Epoch 3427 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3428 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3429 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3430 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3431 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3432 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3433 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3434 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3435 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3436 \t\t Training Loss: 0.0005809644935652614 \t\n",
      "Epoch 3437 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3438 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3439 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3440 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3441 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3442 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3443 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3444 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3445 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3446 \t\t Training Loss: 0.0005809643771499395 \t\n",
      "Epoch 3447 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3448 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3449 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3450 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3451 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3452 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3453 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3454 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3455 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3456 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3457 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3458 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3459 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3460 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3461 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3462 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3463 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3464 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3465 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3466 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3467 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3468 \t\t Training Loss: 0.0005809643189422786 \t\n",
      "Epoch 3469 \t\t Training Loss: 0.0005809642607346177 \t\n",
      "Epoch 3470 \t\t Training Loss: 0.0005809642607346177 \t\n",
      "Epoch 3471 \t\t Training Loss: 0.0005809642607346177 \t\n",
      "Epoch 3472 \t\t Training Loss: 0.0005809642607346177 \t\n",
      "Epoch 3473 \t\t Training Loss: 0.0005809642607346177 \t\n",
      "Epoch 3474 \t\t Training Loss: 0.0005809642607346177 \t\n",
      "Epoch 3475 \t\t Training Loss: 0.0005809642607346177 \t\n",
      "Epoch 3476 \t\t Training Loss: 0.0005809642607346177 \t\n",
      "Epoch 3477 \t\t Training Loss: 0.0005809642025269568 \t\n",
      "Epoch 3478 \t\t Training Loss: 0.0005809642025269568 \t\n",
      "Epoch 3479 \t\t Training Loss: 0.0005809642025269568 \t\n",
      "Epoch 3480 \t\t Training Loss: 0.0005809642025269568 \t\n",
      "Epoch 3481 \t\t Training Loss: 0.0005809642025269568 \t\n",
      "Epoch 3482 \t\t Training Loss: 0.0005809642025269568 \t\n",
      "Epoch 3483 \t\t Training Loss: 0.0005809642025269568 \t\n",
      "Epoch 3484 \t\t Training Loss: 0.0005809642025269568 \t\n",
      "Epoch 3485 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3486 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3487 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3488 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3489 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3490 \t\t Training Loss: 0.0005809642025269568 \t\n",
      "Epoch 3491 \t\t Training Loss: 0.0005809642025269568 \t\n",
      "Epoch 3492 \t\t Training Loss: 0.0005809642025269568 \t\n",
      "Epoch 3493 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3494 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3495 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3496 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3497 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3498 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3499 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3500 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3501 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3502 \t\t Training Loss: 0.000580964086111635 \t\n",
      "Epoch 3503 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3504 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3505 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3506 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3507 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3508 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3509 \t\t Training Loss: 0.000580964086111635 \t\n",
      "Epoch 3510 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3511 \t\t Training Loss: 0.000580964086111635 \t\n",
      "Epoch 3512 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3513 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3514 \t\t Training Loss: 0.000580964086111635 \t\n",
      "Epoch 3515 \t\t Training Loss: 0.0005809641443192959 \t\n",
      "Epoch 3516 \t\t Training Loss: 0.000580964086111635 \t\n",
      "Epoch 3517 \t\t Training Loss: 0.000580964086111635 \t\n",
      "Epoch 3518 \t\t Training Loss: 0.000580964086111635 \t\n",
      "Epoch 3519 \t\t Training Loss: 0.000580964086111635 \t\n",
      "Epoch 3520 \t\t Training Loss: 0.000580964086111635 \t\n",
      "Epoch 3521 \t\t Training Loss: 0.000580964086111635 \t\n",
      "Epoch 3522 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3523 \t\t Training Loss: 0.000580964086111635 \t\n",
      "Epoch 3524 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3525 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3526 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3527 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3528 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3529 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3530 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3531 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3532 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3533 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3534 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3535 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3536 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3537 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3538 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3539 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3540 \t\t Training Loss: 0.0005809639696963131 \t\n",
      "Epoch 3541 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3542 \t\t Training Loss: 0.0005809640279039741 \t\n",
      "Epoch 3543 \t\t Training Loss: 0.0005809639114886522 \t\n",
      "Epoch 3544 \t\t Training Loss: 0.0005809639696963131 \t\n",
      "Epoch 3545 \t\t Training Loss: 0.0005809639114886522 \t\n",
      "Epoch 3546 \t\t Training Loss: 0.0005809639114886522 \t\n",
      "Epoch 3547 \t\t Training Loss: 0.0005809639114886522 \t\n",
      "Epoch 3548 \t\t Training Loss: 0.0005809639114886522 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3549 \t\t Training Loss: 0.0005809639114886522 \t\n",
      "Epoch 3550 \t\t Training Loss: 0.0005809639114886522 \t\n",
      "Epoch 3551 \t\t Training Loss: 0.0005809639114886522 \t\n",
      "Epoch 3552 \t\t Training Loss: 0.0005809639114886522 \t\n",
      "Epoch 3553 \t\t Training Loss: 0.0005809639114886522 \t\n",
      "Epoch 3554 \t\t Training Loss: 0.0005809639114886522 \t\n",
      "Epoch 3555 \t\t Training Loss: 0.0005809639114886522 \t\n",
      "Epoch 3556 \t\t Training Loss: 0.0005809639114886522 \t\n",
      "Epoch 3557 \t\t Training Loss: 0.0005809639114886522 \t\n",
      "Epoch 3558 \t\t Training Loss: 0.0005809637950733304 \t\n",
      "Epoch 3559 \t\t Training Loss: 0.0005809637950733304 \t\n",
      "Epoch 3560 \t\t Training Loss: 0.0005809637950733304 \t\n",
      "Epoch 3561 \t\t Training Loss: 0.0005809637950733304 \t\n",
      "Epoch 3562 \t\t Training Loss: 0.0005809637950733304 \t\n",
      "Epoch 3563 \t\t Training Loss: 0.0005809637950733304 \t\n",
      "Epoch 3564 \t\t Training Loss: 0.0005809637950733304 \t\n",
      "Epoch 3565 \t\t Training Loss: 0.0005809637950733304 \t\n",
      "Epoch 3566 \t\t Training Loss: 0.0005809637950733304 \t\n",
      "Epoch 3567 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3568 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3569 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3570 \t\t Training Loss: 0.0005809637950733304 \t\n",
      "Epoch 3571 \t\t Training Loss: 0.0005809637950733304 \t\n",
      "Epoch 3572 \t\t Training Loss: 0.0005809637950733304 \t\n",
      "Epoch 3573 \t\t Training Loss: 0.0005809637950733304 \t\n",
      "Epoch 3574 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3575 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3576 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3577 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3578 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3579 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3580 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3581 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3582 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3583 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3584 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3585 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3586 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3587 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3588 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3589 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3590 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3591 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3592 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3593 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3594 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3595 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3596 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3597 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3598 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3599 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3600 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3601 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3602 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3603 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3604 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3605 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3606 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3607 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3608 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3609 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3610 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3611 \t\t Training Loss: 0.0005809637368656695 \t\n",
      "Epoch 3612 \t\t Training Loss: 0.0005809636786580086 \t\n",
      "Epoch 3613 \t\t Training Loss: 0.0005809636786580086 \t\n",
      "Epoch 3614 \t\t Training Loss: 0.0005809636786580086 \t\n",
      "Epoch 3615 \t\t Training Loss: 0.0005809636786580086 \t\n",
      "Epoch 3616 \t\t Training Loss: 0.0005809636786580086 \t\n",
      "Epoch 3617 \t\t Training Loss: 0.0005809636786580086 \t\n",
      "Epoch 3618 \t\t Training Loss: 0.0005809636786580086 \t\n",
      "Epoch 3619 \t\t Training Loss: 0.0005809636204503477 \t\n",
      "Epoch 3620 \t\t Training Loss: 0.0005809636786580086 \t\n",
      "Epoch 3621 \t\t Training Loss: 0.0005809636204503477 \t\n",
      "Epoch 3622 \t\t Training Loss: 0.0005809636204503477 \t\n",
      "Epoch 3623 \t\t Training Loss: 0.0005809636204503477 \t\n",
      "Epoch 3624 \t\t Training Loss: 0.0005809635622426867 \t\n",
      "Epoch 3625 \t\t Training Loss: 0.0005809635622426867 \t\n",
      "Epoch 3626 \t\t Training Loss: 0.0005809635622426867 \t\n",
      "Epoch 3627 \t\t Training Loss: 0.0005809635622426867 \t\n",
      "Epoch 3628 \t\t Training Loss: 0.0005809635622426867 \t\n",
      "Epoch 3629 \t\t Training Loss: 0.0005809635622426867 \t\n",
      "Epoch 3630 \t\t Training Loss: 0.0005809635622426867 \t\n",
      "Epoch 3631 \t\t Training Loss: 0.0005809635622426867 \t\n",
      "Epoch 3632 \t\t Training Loss: 0.0005809635040350258 \t\n",
      "Epoch 3633 \t\t Training Loss: 0.0005809635040350258 \t\n",
      "Epoch 3634 \t\t Training Loss: 0.0005809635040350258 \t\n",
      "Epoch 3635 \t\t Training Loss: 0.0005809635040350258 \t\n",
      "Epoch 3636 \t\t Training Loss: 0.0005809635040350258 \t\n",
      "Epoch 3637 \t\t Training Loss: 0.0005809635040350258 \t\n",
      "Epoch 3638 \t\t Training Loss: 0.0005809635040350258 \t\n",
      "Epoch 3639 \t\t Training Loss: 0.0005809635040350258 \t\n",
      "Epoch 3640 \t\t Training Loss: 0.0005809635040350258 \t\n",
      "Epoch 3641 \t\t Training Loss: 0.0005809635040350258 \t\n",
      "Epoch 3642 \t\t Training Loss: 0.0005809635040350258 \t\n",
      "Epoch 3643 \t\t Training Loss: 0.0005809635040350258 \t\n",
      "Epoch 3644 \t\t Training Loss: 0.0005809635040350258 \t\n",
      "Epoch 3645 \t\t Training Loss: 0.0005809634458273649 \t\n",
      "Epoch 3646 \t\t Training Loss: 0.0005809635040350258 \t\n",
      "Epoch 3647 \t\t Training Loss: 0.0005809635040350258 \t\n",
      "Epoch 3648 \t\t Training Loss: 0.0005809634458273649 \t\n",
      "Epoch 3649 \t\t Training Loss: 0.0005809634458273649 \t\n",
      "Epoch 3650 \t\t Training Loss: 0.0005809634458273649 \t\n",
      "Epoch 3651 \t\t Training Loss: 0.0005809634458273649 \t\n",
      "Epoch 3652 \t\t Training Loss: 0.0005809634458273649 \t\n",
      "Epoch 3653 \t\t Training Loss: 0.0005809634458273649 \t\n",
      "Epoch 3654 \t\t Training Loss: 0.0005809634458273649 \t\n",
      "Epoch 3655 \t\t Training Loss: 0.0005809634458273649 \t\n",
      "Epoch 3656 \t\t Training Loss: 0.0005809634458273649 \t\n",
      "Epoch 3657 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3658 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3659 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3660 \t\t Training Loss: 0.000580963387619704 \t\n",
      "Epoch 3661 \t\t Training Loss: 0.000580963387619704 \t\n",
      "Epoch 3662 \t\t Training Loss: 0.000580963387619704 \t\n",
      "Epoch 3663 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3664 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3665 \t\t Training Loss: 0.000580963387619704 \t\n",
      "Epoch 3666 \t\t Training Loss: 0.000580963387619704 \t\n",
      "Epoch 3667 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3668 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3669 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3670 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3671 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3672 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3673 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3674 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3675 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3676 \t\t Training Loss: 0.0005809632712043822 \t\n",
      "Epoch 3677 \t\t Training Loss: 0.0005809632712043822 \t\n",
      "Epoch 3678 \t\t Training Loss: 0.0005809632712043822 \t\n",
      "Epoch 3679 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3680 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3681 \t\t Training Loss: 0.0005809632712043822 \t\n",
      "Epoch 3682 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3683 \t\t Training Loss: 0.0005809632712043822 \t\n",
      "Epoch 3684 \t\t Training Loss: 0.0005809632712043822 \t\n",
      "Epoch 3685 \t\t Training Loss: 0.0005809632712043822 \t\n",
      "Epoch 3686 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3687 \t\t Training Loss: 0.0005809632712043822 \t\n",
      "Epoch 3688 \t\t Training Loss: 0.0005809632712043822 \t\n",
      "Epoch 3689 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3690 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3691 \t\t Training Loss: 0.0005809633294120431 \t\n",
      "Epoch 3692 \t\t Training Loss: 0.0005809632712043822 \t\n",
      "Epoch 3693 \t\t Training Loss: 0.0005809632712043822 \t\n",
      "Epoch 3694 \t\t Training Loss: 0.0005809632712043822 \t\n",
      "Epoch 3695 \t\t Training Loss: 0.0005809632712043822 \t\n",
      "Epoch 3696 \t\t Training Loss: 0.0005809632712043822 \t\n",
      "Epoch 3697 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3698 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3699 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3700 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3701 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3702 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3703 \t\t Training Loss: 0.0005809632129967213 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3704 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3705 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3706 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3707 \t\t Training Loss: 0.0005809631547890604 \t\n",
      "Epoch 3708 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3709 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3710 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3711 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3712 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3713 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3714 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3715 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3716 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3717 \t\t Training Loss: 0.0005809632129967213 \t\n",
      "Epoch 3718 \t\t Training Loss: 0.0005809631547890604 \t\n",
      "Epoch 3719 \t\t Training Loss: 0.0005809631547890604 \t\n",
      "Epoch 3720 \t\t Training Loss: 0.0005809630965813994 \t\n",
      "Epoch 3721 \t\t Training Loss: 0.0005809630965813994 \t\n",
      "Epoch 3722 \t\t Training Loss: 0.0005809630965813994 \t\n",
      "Epoch 3723 \t\t Training Loss: 0.0005809630383737385 \t\n",
      "Epoch 3724 \t\t Training Loss: 0.0005809630965813994 \t\n",
      "Epoch 3725 \t\t Training Loss: 0.0005809630383737385 \t\n",
      "Epoch 3726 \t\t Training Loss: 0.0005809630383737385 \t\n",
      "Epoch 3727 \t\t Training Loss: 0.0005809630383737385 \t\n",
      "Epoch 3728 \t\t Training Loss: 0.0005809630965813994 \t\n",
      "Epoch 3729 \t\t Training Loss: 0.0005809630965813994 \t\n",
      "Epoch 3730 \t\t Training Loss: 0.0005809630383737385 \t\n",
      "Epoch 3731 \t\t Training Loss: 0.0005809630383737385 \t\n",
      "Epoch 3732 \t\t Training Loss: 0.0005809630383737385 \t\n",
      "Epoch 3733 \t\t Training Loss: 0.0005809630383737385 \t\n",
      "Epoch 3734 \t\t Training Loss: 0.0005809630383737385 \t\n",
      "Epoch 3735 \t\t Training Loss: 0.0005809630383737385 \t\n",
      "Epoch 3736 \t\t Training Loss: 0.0005809630383737385 \t\n",
      "Epoch 3737 \t\t Training Loss: 0.0005809629801660776 \t\n",
      "Epoch 3738 \t\t Training Loss: 0.0005809629801660776 \t\n",
      "Epoch 3739 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3740 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3741 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3742 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3743 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3744 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3745 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3746 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3747 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3748 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3749 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3750 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3751 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3752 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3753 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3754 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3755 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3756 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3757 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3758 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3759 \t\t Training Loss: 0.0005809629219584167 \t\n",
      "Epoch 3760 \t\t Training Loss: 0.000580962747335434 \t\n",
      "Epoch 3761 \t\t Training Loss: 0.000580962747335434 \t\n",
      "Epoch 3762 \t\t Training Loss: 0.000580962689127773 \t\n",
      "Epoch 3763 \t\t Training Loss: 0.000580962747335434 \t\n",
      "Epoch 3764 \t\t Training Loss: 0.000580962689127773 \t\n",
      "Epoch 3765 \t\t Training Loss: 0.000580962747335434 \t\n",
      "Epoch 3766 \t\t Training Loss: 0.000580962689127773 \t\n",
      "Epoch 3767 \t\t Training Loss: 0.000580962689127773 \t\n",
      "Epoch 3768 \t\t Training Loss: 0.000580962689127773 \t\n",
      "Epoch 3769 \t\t Training Loss: 0.000580962689127773 \t\n",
      "Epoch 3770 \t\t Training Loss: 0.000580962689127773 \t\n",
      "Epoch 3771 \t\t Training Loss: 0.000580962689127773 \t\n",
      "Epoch 3772 \t\t Training Loss: 0.000580962689127773 \t\n",
      "Epoch 3773 \t\t Training Loss: 0.0005809626309201121 \t\n",
      "Epoch 3774 \t\t Training Loss: 0.0005809626309201121 \t\n",
      "Epoch 3775 \t\t Training Loss: 0.0005809626309201121 \t\n",
      "Epoch 3776 \t\t Training Loss: 0.0005809626309201121 \t\n",
      "Epoch 3777 \t\t Training Loss: 0.0005809626309201121 \t\n",
      "Epoch 3778 \t\t Training Loss: 0.0005809626309201121 \t\n",
      "Epoch 3779 \t\t Training Loss: 0.0005809625145047903 \t\n",
      "Epoch 3780 \t\t Training Loss: 0.0005809626309201121 \t\n",
      "Epoch 3781 \t\t Training Loss: 0.0005809626309201121 \t\n",
      "Epoch 3782 \t\t Training Loss: 0.0005809626309201121 \t\n",
      "Epoch 3783 \t\t Training Loss: 0.0005809626309201121 \t\n",
      "Epoch 3784 \t\t Training Loss: 0.0005809625145047903 \t\n",
      "Epoch 3785 \t\t Training Loss: 0.0005809625145047903 \t\n",
      "Epoch 3786 \t\t Training Loss: 0.0005809625145047903 \t\n",
      "Epoch 3787 \t\t Training Loss: 0.0005809625145047903 \t\n",
      "Epoch 3788 \t\t Training Loss: 0.0005809625145047903 \t\n",
      "Epoch 3789 \t\t Training Loss: 0.0005809625145047903 \t\n",
      "Epoch 3790 \t\t Training Loss: 0.0005809625145047903 \t\n",
      "Epoch 3791 \t\t Training Loss: 0.0005809625145047903 \t\n",
      "Epoch 3792 \t\t Training Loss: 0.0005809625145047903 \t\n",
      "Epoch 3793 \t\t Training Loss: 0.0005809625145047903 \t\n",
      "Epoch 3794 \t\t Training Loss: 0.0005809625145047903 \t\n",
      "Epoch 3795 \t\t Training Loss: 0.0005809625145047903 \t\n",
      "Epoch 3796 \t\t Training Loss: 0.0005809624562971294 \t\n",
      "Epoch 3797 \t\t Training Loss: 0.0005809624562971294 \t\n",
      "Epoch 3798 \t\t Training Loss: 0.0005809624562971294 \t\n",
      "Epoch 3799 \t\t Training Loss: 0.0005809624562971294 \t\n",
      "Epoch 3800 \t\t Training Loss: 0.0005809624562971294 \t\n",
      "Epoch 3801 \t\t Training Loss: 0.0005809624562971294 \t\n",
      "Epoch 3802 \t\t Training Loss: 0.0005809624562971294 \t\n",
      "Epoch 3803 \t\t Training Loss: 0.0005809624562971294 \t\n",
      "Epoch 3804 \t\t Training Loss: 0.0005809624562971294 \t\n",
      "Epoch 3805 \t\t Training Loss: 0.0005809624562971294 \t\n",
      "Epoch 3806 \t\t Training Loss: 0.0005809624562971294 \t\n",
      "Epoch 3807 \t\t Training Loss: 0.0005809624562971294 \t\n",
      "Epoch 3808 \t\t Training Loss: 0.0005809624562971294 \t\n",
      "Epoch 3809 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3810 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3811 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3812 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3813 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3814 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3815 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3816 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3817 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3818 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3819 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3820 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3821 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3822 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3823 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3824 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3825 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3826 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3827 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3828 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3829 \t\t Training Loss: 0.0005809623980894685 \t\n",
      "Epoch 3830 \t\t Training Loss: 0.0005809623398818076 \t\n",
      "Epoch 3831 \t\t Training Loss: 0.0005809623398818076 \t\n",
      "Epoch 3832 \t\t Training Loss: 0.0005809623398818076 \t\n",
      "Epoch 3833 \t\t Training Loss: 0.0005809623398818076 \t\n",
      "Epoch 3834 \t\t Training Loss: 0.0005809623398818076 \t\n",
      "Epoch 3835 \t\t Training Loss: 0.0005809623398818076 \t\n",
      "Epoch 3836 \t\t Training Loss: 0.0005809623398818076 \t\n",
      "Epoch 3837 \t\t Training Loss: 0.0005809623398818076 \t\n",
      "Epoch 3838 \t\t Training Loss: 0.0005809623398818076 \t\n",
      "Epoch 3839 \t\t Training Loss: 0.0005809623398818076 \t\n",
      "Epoch 3840 \t\t Training Loss: 0.0005809623398818076 \t\n",
      "Epoch 3841 \t\t Training Loss: 0.0005809623398818076 \t\n",
      "Epoch 3842 \t\t Training Loss: 0.0005809623398818076 \t\n",
      "Epoch 3843 \t\t Training Loss: 0.0005809623398818076 \t\n",
      "Epoch 3844 \t\t Training Loss: 0.0005809623398818076 \t\n",
      "Epoch 3845 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3846 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3847 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3848 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3849 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3850 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3851 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3852 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3853 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3854 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3855 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3856 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3857 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3858 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3859 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3860 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3861 \t\t Training Loss: 0.0005809621070511639 \t\n",
      "Epoch 3862 \t\t Training Loss: 0.0005809621070511639 \t\n",
      "Epoch 3863 \t\t Training Loss: 0.0005809621070511639 \t\n",
      "Epoch 3864 \t\t Training Loss: 0.000580962048843503 \t\n",
      "Epoch 3865 \t\t Training Loss: 0.000580962048843503 \t\n",
      "Epoch 3866 \t\t Training Loss: 0.0005809621070511639 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3867 \t\t Training Loss: 0.0005809621070511639 \t\n",
      "Epoch 3868 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3869 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3870 \t\t Training Loss: 0.0005809621070511639 \t\n",
      "Epoch 3871 \t\t Training Loss: 0.0005809622234664857 \t\n",
      "Epoch 3872 \t\t Training Loss: 0.0005809621070511639 \t\n",
      "Epoch 3873 \t\t Training Loss: 0.0005809621070511639 \t\n",
      "Epoch 3874 \t\t Training Loss: 0.0005809621070511639 \t\n",
      "Epoch 3875 \t\t Training Loss: 0.0005809621070511639 \t\n",
      "Epoch 3876 \t\t Training Loss: 0.000580962048843503 \t\n",
      "Epoch 3877 \t\t Training Loss: 0.000580962048843503 \t\n",
      "Epoch 3878 \t\t Training Loss: 0.000580962048843503 \t\n",
      "Epoch 3879 \t\t Training Loss: 0.000580962048843503 \t\n",
      "Epoch 3880 \t\t Training Loss: 0.000580962048843503 \t\n",
      "Epoch 3881 \t\t Training Loss: 0.000580962048843503 \t\n",
      "Epoch 3882 \t\t Training Loss: 0.000580962048843503 \t\n",
      "Epoch 3883 \t\t Training Loss: 0.000580962048843503 \t\n",
      "Epoch 3884 \t\t Training Loss: 0.000580962048843503 \t\n",
      "Epoch 3885 \t\t Training Loss: 0.000580962048843503 \t\n",
      "Epoch 3886 \t\t Training Loss: 0.0005809621070511639 \t\n",
      "Epoch 3887 \t\t Training Loss: 0.000580962048843503 \t\n",
      "Epoch 3888 \t\t Training Loss: 0.000580962048843503 \t\n",
      "Epoch 3889 \t\t Training Loss: 0.0005809619906358421 \t\n",
      "Epoch 3890 \t\t Training Loss: 0.0005809619906358421 \t\n",
      "Epoch 3891 \t\t Training Loss: 0.0005809619906358421 \t\n",
      "Epoch 3892 \t\t Training Loss: 0.0005809619906358421 \t\n",
      "Epoch 3893 \t\t Training Loss: 0.0005809619906358421 \t\n",
      "Epoch 3894 \t\t Training Loss: 0.0005809619906358421 \t\n",
      "Epoch 3895 \t\t Training Loss: 0.0005809619906358421 \t\n",
      "Epoch 3896 \t\t Training Loss: 0.0005809619906358421 \t\n",
      "Epoch 3897 \t\t Training Loss: 0.0005809619906358421 \t\n",
      "Epoch 3898 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3899 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3900 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3901 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3902 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3903 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3904 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3905 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3906 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3907 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3908 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3909 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3910 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3911 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3912 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3913 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3914 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3915 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3916 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3917 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3918 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3919 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3920 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3921 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3922 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3923 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3924 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3925 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3926 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3927 \t\t Training Loss: 0.0005809619324281812 \t\n",
      "Epoch 3928 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3929 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3930 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3931 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3932 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3933 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3934 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3935 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3936 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3937 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3938 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3939 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3940 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3941 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3942 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3943 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3944 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3945 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3946 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3947 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3948 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3949 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3950 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3951 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3952 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3953 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3954 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3955 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3956 \t\t Training Loss: 0.0005809618160128593 \t\n",
      "Epoch 3957 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3958 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3959 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3960 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3961 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3962 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3963 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3964 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3965 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3966 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3967 \t\t Training Loss: 0.0005809616995975375 \t\n",
      "Epoch 3968 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3969 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3970 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3971 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3972 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3973 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3974 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3975 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3976 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3977 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3978 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3979 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3980 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3981 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3982 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3983 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3984 \t\t Training Loss: 0.0005809616413898766 \t\n",
      "Epoch 3985 \t\t Training Loss: 0.0005809615249745548 \t\n",
      "Epoch 3986 \t\t Training Loss: 0.0005809615249745548 \t\n",
      "Epoch 3987 \t\t Training Loss: 0.0005809615249745548 \t\n",
      "Epoch 3988 \t\t Training Loss: 0.0005809615249745548 \t\n",
      "Epoch 3989 \t\t Training Loss: 0.0005809615249745548 \t\n",
      "Epoch 3990 \t\t Training Loss: 0.0005809615249745548 \t\n",
      "Epoch 3991 \t\t Training Loss: 0.0005809615249745548 \t\n",
      "Epoch 3992 \t\t Training Loss: 0.0005809615249745548 \t\n",
      "Epoch 3993 \t\t Training Loss: 0.0005809615249745548 \t\n",
      "Epoch 3994 \t\t Training Loss: 0.0005809615249745548 \t\n",
      "Epoch 3995 \t\t Training Loss: 0.0005809615249745548 \t\n",
      "Epoch 3996 \t\t Training Loss: 0.0005809615249745548 \t\n",
      "Epoch 3997 \t\t Training Loss: 0.0005809615249745548 \t\n",
      "Epoch 3998 \t\t Training Loss: 0.000580961408559233 \t\n",
      "Epoch 3999 \t\t Training Loss: 0.0005809615249745548 \t\n",
      "Epoch 4000 \t\t Training Loss: 0.000580961408559233 \t\n",
      "Epoch 4001 \t\t Training Loss: 0.000580961408559233 \t\n",
      "Epoch 4002 \t\t Training Loss: 0.000580961408559233 \t\n",
      "Epoch 4003 \t\t Training Loss: 0.000580961350351572 \t\n",
      "Epoch 4004 \t\t Training Loss: 0.000580961408559233 \t\n",
      "Epoch 4005 \t\t Training Loss: 0.000580961408559233 \t\n",
      "Epoch 4006 \t\t Training Loss: 0.000580961408559233 \t\n",
      "Epoch 4007 \t\t Training Loss: 0.000580961408559233 \t\n",
      "Epoch 4008 \t\t Training Loss: 0.000580961408559233 \t\n",
      "Epoch 4009 \t\t Training Loss: 0.000580961350351572 \t\n",
      "Epoch 4010 \t\t Training Loss: 0.000580961350351572 \t\n",
      "Epoch 4011 \t\t Training Loss: 0.000580961350351572 \t\n",
      "Epoch 4012 \t\t Training Loss: 0.000580961350351572 \t\n",
      "Epoch 4013 \t\t Training Loss: 0.000580961350351572 \t\n",
      "Epoch 4014 \t\t Training Loss: 0.000580961350351572 \t\n",
      "Epoch 4015 \t\t Training Loss: 0.000580961350351572 \t\n",
      "Epoch 4016 \t\t Training Loss: 0.000580961350351572 \t\n",
      "Epoch 4017 \t\t Training Loss: 0.000580961350351572 \t\n",
      "Epoch 4018 \t\t Training Loss: 0.000580961350351572 \t\n",
      "Epoch 4019 \t\t Training Loss: 0.0005809612339362502 \t\n",
      "Epoch 4020 \t\t Training Loss: 0.0005809612339362502 \t\n",
      "Epoch 4021 \t\t Training Loss: 0.0005809612339362502 \t\n",
      "Epoch 4022 \t\t Training Loss: 0.0005809612339362502 \t\n",
      "Epoch 4023 \t\t Training Loss: 0.0005809612339362502 \t\n",
      "Epoch 4024 \t\t Training Loss: 0.0005809612339362502 \t\n",
      "Epoch 4025 \t\t Training Loss: 0.0005809612339362502 \t\n",
      "Epoch 4026 \t\t Training Loss: 0.0005809612339362502 \t\n",
      "Epoch 4027 \t\t Training Loss: 0.0005809612339362502 \t\n",
      "Epoch 4028 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4029 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4030 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4031 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4032 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4033 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4034 \t\t Training Loss: 0.0005809611175209284 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4035 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4036 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4037 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4038 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4039 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4040 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4041 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4042 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4043 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4044 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4045 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4046 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4047 \t\t Training Loss: 0.0005809610593132675 \t\n",
      "Epoch 4048 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4049 \t\t Training Loss: 0.0005809611175209284 \t\n",
      "Epoch 4050 \t\t Training Loss: 0.0005809610593132675 \t\n",
      "Epoch 4051 \t\t Training Loss: 0.0005809610593132675 \t\n",
      "Epoch 4052 \t\t Training Loss: 0.0005809610593132675 \t\n",
      "Epoch 4053 \t\t Training Loss: 0.0005809610593132675 \t\n",
      "Epoch 4054 \t\t Training Loss: 0.0005809610593132675 \t\n",
      "Epoch 4055 \t\t Training Loss: 0.0005809610593132675 \t\n",
      "Epoch 4056 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4057 \t\t Training Loss: 0.0005809610593132675 \t\n",
      "Epoch 4058 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4059 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4060 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4061 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4062 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4063 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4064 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4065 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4066 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4067 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4068 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4069 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4070 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4071 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4072 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4073 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4074 \t\t Training Loss: 0.0005809609428979456 \t\n",
      "Epoch 4075 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4076 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4077 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4078 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4079 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4080 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4081 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4082 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4083 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4084 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4085 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4086 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4087 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4088 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4089 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4090 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4091 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4092 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4093 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4094 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4095 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4096 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4097 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4098 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4099 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4100 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4101 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4102 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4103 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4104 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4105 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4106 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4107 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4108 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4109 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4110 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4111 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4112 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4113 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4114 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4115 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4116 \t\t Training Loss: 0.0005809608264826238 \t\n",
      "Epoch 4117 \t\t Training Loss: 0.0005809607682749629 \t\n",
      "Epoch 4118 \t\t Training Loss: 0.0005809607682749629 \t\n",
      "Epoch 4119 \t\t Training Loss: 0.0005809607682749629 \t\n",
      "Epoch 4120 \t\t Training Loss: 0.0005809607682749629 \t\n",
      "Epoch 4121 \t\t Training Loss: 0.0005809607682749629 \t\n",
      "Epoch 4122 \t\t Training Loss: 0.0005809607682749629 \t\n",
      "Epoch 4123 \t\t Training Loss: 0.0005809607682749629 \t\n",
      "Epoch 4124 \t\t Training Loss: 0.0005809607682749629 \t\n",
      "Epoch 4125 \t\t Training Loss: 0.0005809607682749629 \t\n",
      "Epoch 4126 \t\t Training Loss: 0.0005809607682749629 \t\n",
      "Epoch 4127 \t\t Training Loss: 0.0005809607682749629 \t\n",
      "Epoch 4128 \t\t Training Loss: 0.0005809607682749629 \t\n",
      "Epoch 4129 \t\t Training Loss: 0.0005809607682749629 \t\n",
      "Epoch 4130 \t\t Training Loss: 0.0005809607682749629 \t\n",
      "Epoch 4131 \t\t Training Loss: 0.0005809607682749629 \t\n",
      "Epoch 4132 \t\t Training Loss: 0.0005809607682749629 \t\n",
      "Epoch 4133 \t\t Training Loss: 0.000580960710067302 \t\n",
      "Epoch 4134 \t\t Training Loss: 0.000580960710067302 \t\n",
      "Epoch 4135 \t\t Training Loss: 0.000580960710067302 \t\n",
      "Epoch 4136 \t\t Training Loss: 0.000580960710067302 \t\n",
      "Epoch 4137 \t\t Training Loss: 0.000580960710067302 \t\n",
      "Epoch 4138 \t\t Training Loss: 0.000580960710067302 \t\n",
      "Epoch 4139 \t\t Training Loss: 0.000580960710067302 \t\n",
      "Epoch 4140 \t\t Training Loss: 0.000580960710067302 \t\n",
      "Epoch 4141 \t\t Training Loss: 0.000580960710067302 \t\n",
      "Epoch 4142 \t\t Training Loss: 0.000580960710067302 \t\n",
      "Epoch 4143 \t\t Training Loss: 0.000580960710067302 \t\n",
      "Epoch 4144 \t\t Training Loss: 0.000580960710067302 \t\n",
      "Epoch 4145 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4146 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4147 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4148 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4149 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4150 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4151 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4152 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4153 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4154 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4155 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4156 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4157 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4158 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4159 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4160 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4161 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4162 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4163 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4164 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4165 \t\t Training Loss: 0.0005809605354443192 \t\n",
      "Epoch 4166 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4167 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4168 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4169 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4170 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4171 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4172 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4173 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4174 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4175 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4176 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4177 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4178 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4179 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4180 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4181 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4182 \t\t Training Loss: 0.0005809604190289974 \t\n",
      "Epoch 4183 \t\t Training Loss: 0.0005809603608213365 \t\n",
      "Epoch 4184 \t\t Training Loss: 0.0005809603608213365 \t\n",
      "Epoch 4185 \t\t Training Loss: 0.0005809603608213365 \t\n",
      "Epoch 4186 \t\t Training Loss: 0.0005809603608213365 \t\n",
      "Epoch 4187 \t\t Training Loss: 0.0005809603608213365 \t\n",
      "Epoch 4188 \t\t Training Loss: 0.0005809603608213365 \t\n",
      "Epoch 4189 \t\t Training Loss: 0.0005809602444060147 \t\n",
      "Epoch 4190 \t\t Training Loss: 0.0005809602444060147 \t\n",
      "Epoch 4191 \t\t Training Loss: 0.0005809602444060147 \t\n",
      "Epoch 4192 \t\t Training Loss: 0.0005809602444060147 \t\n",
      "Epoch 4193 \t\t Training Loss: 0.0005809602444060147 \t\n",
      "Epoch 4194 \t\t Training Loss: 0.0005809602444060147 \t\n",
      "Epoch 4195 \t\t Training Loss: 0.0005809602444060147 \t\n",
      "Epoch 4196 \t\t Training Loss: 0.0005809602444060147 \t\n",
      "Epoch 4197 \t\t Training Loss: 0.0005809602444060147 \t\n",
      "Epoch 4198 \t\t Training Loss: 0.0005809602444060147 \t\n",
      "Epoch 4199 \t\t Training Loss: 0.0005809602444060147 \t\n",
      "Epoch 4200 \t\t Training Loss: 0.0005809602444060147 \t\n",
      "Epoch 4201 \t\t Training Loss: 0.0005809601279906929 \t\n",
      "Epoch 4202 \t\t Training Loss: 0.0005809601279906929 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4203 \t\t Training Loss: 0.0005809601279906929 \t\n",
      "Epoch 4204 \t\t Training Loss: 0.0005809601279906929 \t\n",
      "Epoch 4205 \t\t Training Loss: 0.0005809601279906929 \t\n",
      "Epoch 4206 \t\t Training Loss: 0.0005809601279906929 \t\n",
      "Epoch 4207 \t\t Training Loss: 0.0005809601279906929 \t\n",
      "Epoch 4208 \t\t Training Loss: 0.0005809600697830319 \t\n",
      "Epoch 4209 \t\t Training Loss: 0.0005809600697830319 \t\n",
      "Epoch 4210 \t\t Training Loss: 0.0005809600697830319 \t\n",
      "Epoch 4211 \t\t Training Loss: 0.000580960011575371 \t\n",
      "Epoch 4212 \t\t Training Loss: 0.000580960011575371 \t\n",
      "Epoch 4213 \t\t Training Loss: 0.000580960011575371 \t\n",
      "Epoch 4214 \t\t Training Loss: 0.000580960011575371 \t\n",
      "Epoch 4215 \t\t Training Loss: 0.000580960011575371 \t\n",
      "Epoch 4216 \t\t Training Loss: 0.000580960011575371 \t\n",
      "Epoch 4217 \t\t Training Loss: 0.000580960011575371 \t\n",
      "Epoch 4218 \t\t Training Loss: 0.000580960011575371 \t\n",
      "Epoch 4219 \t\t Training Loss: 0.0005809599533677101 \t\n",
      "Epoch 4220 \t\t Training Loss: 0.0005809599533677101 \t\n",
      "Epoch 4221 \t\t Training Loss: 0.0005809599533677101 \t\n",
      "Epoch 4222 \t\t Training Loss: 0.0005809599533677101 \t\n",
      "Epoch 4223 \t\t Training Loss: 0.0005809599533677101 \t\n",
      "Epoch 4224 \t\t Training Loss: 0.0005809599533677101 \t\n",
      "Epoch 4225 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4226 \t\t Training Loss: 0.0005809599533677101 \t\n",
      "Epoch 4227 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4228 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4229 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4230 \t\t Training Loss: 0.0005809599533677101 \t\n",
      "Epoch 4231 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4232 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4233 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4234 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4235 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4236 \t\t Training Loss: 0.0005809599533677101 \t\n",
      "Epoch 4237 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4238 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4239 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4240 \t\t Training Loss: 0.0005809599533677101 \t\n",
      "Epoch 4241 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4242 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4243 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4244 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4245 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4246 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4247 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4248 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4249 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4250 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4251 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4252 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4253 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4254 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4255 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4256 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4257 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4258 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4259 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4260 \t\t Training Loss: 0.0005809598369523883 \t\n",
      "Epoch 4261 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4262 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4263 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4264 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4265 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4266 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4267 \t\t Training Loss: 0.0005809597205370665 \t\n",
      "Epoch 4268 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4269 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4270 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4271 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4272 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4273 \t\t Training Loss: 0.0005809597787447274 \t\n",
      "Epoch 4274 \t\t Training Loss: 0.0005809597205370665 \t\n",
      "Epoch 4275 \t\t Training Loss: 0.0005809597205370665 \t\n",
      "Epoch 4276 \t\t Training Loss: 0.0005809597205370665 \t\n",
      "Epoch 4277 \t\t Training Loss: 0.0005809597205370665 \t\n",
      "Epoch 4278 \t\t Training Loss: 0.0005809597205370665 \t\n",
      "Epoch 4279 \t\t Training Loss: 0.0005809597205370665 \t\n",
      "Epoch 4280 \t\t Training Loss: 0.0005809597205370665 \t\n",
      "Epoch 4281 \t\t Training Loss: 0.0005809597205370665 \t\n",
      "Epoch 4282 \t\t Training Loss: 0.0005809597205370665 \t\n",
      "Epoch 4283 \t\t Training Loss: 0.0005809597205370665 \t\n",
      "Epoch 4284 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4285 \t\t Training Loss: 0.0005809597205370665 \t\n",
      "Epoch 4286 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4287 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4288 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4289 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4290 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4291 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4292 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4293 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4294 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4295 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4296 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4297 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4298 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4299 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4300 \t\t Training Loss: 0.0005809595459140837 \t\n",
      "Epoch 4301 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4302 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4303 \t\t Training Loss: 0.0005809595459140837 \t\n",
      "Epoch 4304 \t\t Training Loss: 0.0005809596623294055 \t\n",
      "Epoch 4305 \t\t Training Loss: 0.0005809595459140837 \t\n",
      "Epoch 4306 \t\t Training Loss: 0.0005809595459140837 \t\n",
      "Epoch 4307 \t\t Training Loss: 0.0005809595459140837 \t\n",
      "Epoch 4308 \t\t Training Loss: 0.0005809595459140837 \t\n",
      "Epoch 4309 \t\t Training Loss: 0.0005809595459140837 \t\n",
      "Epoch 4310 \t\t Training Loss: 0.0005809595459140837 \t\n",
      "Epoch 4311 \t\t Training Loss: 0.0005809594877064228 \t\n",
      "Epoch 4312 \t\t Training Loss: 0.0005809594877064228 \t\n",
      "Epoch 4313 \t\t Training Loss: 0.0005809594877064228 \t\n",
      "Epoch 4314 \t\t Training Loss: 0.0005809594877064228 \t\n",
      "Epoch 4315 \t\t Training Loss: 0.0005809594877064228 \t\n",
      "Epoch 4316 \t\t Training Loss: 0.0005809594877064228 \t\n",
      "Epoch 4317 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4318 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4319 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4320 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4321 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4322 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4323 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4324 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4325 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4326 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4327 \t\t Training Loss: 0.000580959371291101 \t\n",
      "Epoch 4328 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4329 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4330 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4331 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4332 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4333 \t\t Training Loss: 0.000580959371291101 \t\n",
      "Epoch 4334 \t\t Training Loss: 0.000580959371291101 \t\n",
      "Epoch 4335 \t\t Training Loss: 0.000580959371291101 \t\n",
      "Epoch 4336 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4337 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4338 \t\t Training Loss: 0.000580959371291101 \t\n",
      "Epoch 4339 \t\t Training Loss: 0.000580959371291101 \t\n",
      "Epoch 4340 \t\t Training Loss: 0.0005809594294987619 \t\n",
      "Epoch 4341 \t\t Training Loss: 0.000580959371291101 \t\n",
      "Epoch 4342 \t\t Training Loss: 0.000580959371291101 \t\n",
      "Epoch 4343 \t\t Training Loss: 0.000580959371291101 \t\n",
      "Epoch 4344 \t\t Training Loss: 0.000580959371291101 \t\n",
      "Epoch 4345 \t\t Training Loss: 0.000580959371291101 \t\n",
      "Epoch 4346 \t\t Training Loss: 0.000580959371291101 \t\n",
      "Epoch 4347 \t\t Training Loss: 0.000580959371291101 \t\n",
      "Epoch 4348 \t\t Training Loss: 0.000580959371291101 \t\n",
      "Epoch 4349 \t\t Training Loss: 0.0005809593130834401 \t\n",
      "Epoch 4350 \t\t Training Loss: 0.000580959371291101 \t\n",
      "Epoch 4351 \t\t Training Loss: 0.0005809593130834401 \t\n",
      "Epoch 4352 \t\t Training Loss: 0.0005809593130834401 \t\n",
      "Epoch 4353 \t\t Training Loss: 0.0005809593130834401 \t\n",
      "Epoch 4354 \t\t Training Loss: 0.0005809593130834401 \t\n",
      "Epoch 4355 \t\t Training Loss: 0.0005809593130834401 \t\n",
      "Epoch 4356 \t\t Training Loss: 0.0005809593130834401 \t\n",
      "Epoch 4357 \t\t Training Loss: 0.0005809592548757792 \t\n",
      "Epoch 4358 \t\t Training Loss: 0.0005809592548757792 \t\n",
      "Epoch 4359 \t\t Training Loss: 0.0005809592548757792 \t\n",
      "Epoch 4360 \t\t Training Loss: 0.0005809592548757792 \t\n",
      "Epoch 4361 \t\t Training Loss: 0.0005809592548757792 \t\n",
      "Epoch 4362 \t\t Training Loss: 0.0005809593130834401 \t\n",
      "Epoch 4363 \t\t Training Loss: 0.0005809593130834401 \t\n",
      "Epoch 4364 \t\t Training Loss: 0.0005809592548757792 \t\n",
      "Epoch 4365 \t\t Training Loss: 0.0005809592548757792 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4366 \t\t Training Loss: 0.0005809592548757792 \t\n",
      "Epoch 4367 \t\t Training Loss: 0.0005809592548757792 \t\n",
      "Epoch 4368 \t\t Training Loss: 0.0005809592548757792 \t\n",
      "Epoch 4369 \t\t Training Loss: 0.0005809592548757792 \t\n",
      "Epoch 4370 \t\t Training Loss: 0.0005809592548757792 \t\n",
      "Epoch 4371 \t\t Training Loss: 0.0005809592548757792 \t\n",
      "Epoch 4372 \t\t Training Loss: 0.0005809592548757792 \t\n",
      "Epoch 4373 \t\t Training Loss: 0.0005809591966681182 \t\n",
      "Epoch 4374 \t\t Training Loss: 0.0005809591966681182 \t\n",
      "Epoch 4375 \t\t Training Loss: 0.0005809591966681182 \t\n",
      "Epoch 4376 \t\t Training Loss: 0.0005809591966681182 \t\n",
      "Epoch 4377 \t\t Training Loss: 0.0005809591966681182 \t\n",
      "Epoch 4378 \t\t Training Loss: 0.0005809591966681182 \t\n",
      "Epoch 4379 \t\t Training Loss: 0.0005809591966681182 \t\n",
      "Epoch 4380 \t\t Training Loss: 0.0005809591384604573 \t\n",
      "Epoch 4381 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4382 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4383 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4384 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4385 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4386 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4387 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4388 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4389 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4390 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4391 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4392 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4393 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4394 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4395 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4396 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4397 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4398 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4399 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4400 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4401 \t\t Training Loss: 0.0005809590802527964 \t\n",
      "Epoch 4402 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4403 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4404 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4405 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4406 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4407 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4408 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4409 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4410 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4411 \t\t Training Loss: 0.0005809589056298137 \t\n",
      "Epoch 4412 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4413 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4414 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4415 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4416 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4417 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4418 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4419 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4420 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4421 \t\t Training Loss: 0.0005809589638374746 \t\n",
      "Epoch 4422 \t\t Training Loss: 0.0005809589056298137 \t\n",
      "Epoch 4423 \t\t Training Loss: 0.0005809589056298137 \t\n",
      "Epoch 4424 \t\t Training Loss: 0.0005809589056298137 \t\n",
      "Epoch 4425 \t\t Training Loss: 0.0005809589056298137 \t\n",
      "Epoch 4426 \t\t Training Loss: 0.0005809589056298137 \t\n",
      "Epoch 4427 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4428 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4429 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4430 \t\t Training Loss: 0.0005809589056298137 \t\n",
      "Epoch 4431 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4432 \t\t Training Loss: 0.0005809589056298137 \t\n",
      "Epoch 4433 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4434 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4435 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4436 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4437 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4438 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4439 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4440 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4441 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4442 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4443 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4444 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4445 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4446 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4447 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4448 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4449 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4450 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4451 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4452 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4453 \t\t Training Loss: 0.0005809587892144918 \t\n",
      "Epoch 4454 \t\t Training Loss: 0.0005809587892144918 \t\n",
      "Epoch 4455 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4456 \t\t Training Loss: 0.0005809587892144918 \t\n",
      "Epoch 4457 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4458 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4459 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4460 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4461 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4462 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4463 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4464 \t\t Training Loss: 0.0005809588474221528 \t\n",
      "Epoch 4465 \t\t Training Loss: 0.0005809587892144918 \t\n",
      "Epoch 4466 \t\t Training Loss: 0.0005809587892144918 \t\n",
      "Epoch 4467 \t\t Training Loss: 0.0005809587892144918 \t\n",
      "Epoch 4468 \t\t Training Loss: 0.0005809587892144918 \t\n",
      "Epoch 4469 \t\t Training Loss: 0.0005809587892144918 \t\n",
      "Epoch 4470 \t\t Training Loss: 0.0005809587892144918 \t\n",
      "Epoch 4471 \t\t Training Loss: 0.0005809587892144918 \t\n",
      "Epoch 4472 \t\t Training Loss: 0.0005809587892144918 \t\n",
      "Epoch 4473 \t\t Training Loss: 0.0005809587310068309 \t\n",
      "Epoch 4474 \t\t Training Loss: 0.0005809587310068309 \t\n",
      "Epoch 4475 \t\t Training Loss: 0.0005809587310068309 \t\n",
      "Epoch 4476 \t\t Training Loss: 0.0005809587310068309 \t\n",
      "Epoch 4477 \t\t Training Loss: 0.00058095867279917 \t\n",
      "Epoch 4478 \t\t Training Loss: 0.00058095867279917 \t\n",
      "Epoch 4479 \t\t Training Loss: 0.0005809587310068309 \t\n",
      "Epoch 4480 \t\t Training Loss: 0.00058095867279917 \t\n",
      "Epoch 4481 \t\t Training Loss: 0.00058095867279917 \t\n",
      "Epoch 4482 \t\t Training Loss: 0.00058095867279917 \t\n",
      "Epoch 4483 \t\t Training Loss: 0.00058095867279917 \t\n",
      "Epoch 4484 \t\t Training Loss: 0.00058095867279917 \t\n",
      "Epoch 4485 \t\t Training Loss: 0.00058095867279917 \t\n",
      "Epoch 4486 \t\t Training Loss: 0.00058095867279917 \t\n",
      "Epoch 4487 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4488 \t\t Training Loss: 0.00058095867279917 \t\n",
      "Epoch 4489 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4490 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4491 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4492 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4493 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4494 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4495 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4496 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4497 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4498 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4499 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4500 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4501 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4502 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4503 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4504 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4505 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4506 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4507 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4508 \t\t Training Loss: 0.0005809585563838482 \t\n",
      "Epoch 4509 \t\t Training Loss: 0.0005809586145915091 \t\n",
      "Epoch 4510 \t\t Training Loss: 0.0005809585563838482 \t\n",
      "Epoch 4511 \t\t Training Loss: 0.0005809585563838482 \t\n",
      "Epoch 4512 \t\t Training Loss: 0.0005809585563838482 \t\n",
      "Epoch 4513 \t\t Training Loss: 0.0005809584981761873 \t\n",
      "Epoch 4514 \t\t Training Loss: 0.0005809585563838482 \t\n",
      "Epoch 4515 \t\t Training Loss: 0.0005809585563838482 \t\n",
      "Epoch 4516 \t\t Training Loss: 0.0005809585563838482 \t\n",
      "Epoch 4517 \t\t Training Loss: 0.0005809585563838482 \t\n",
      "Epoch 4518 \t\t Training Loss: 0.0005809584981761873 \t\n",
      "Epoch 4519 \t\t Training Loss: 0.0005809584981761873 \t\n",
      "Epoch 4520 \t\t Training Loss: 0.0005809584981761873 \t\n",
      "Epoch 4521 \t\t Training Loss: 0.0005809584981761873 \t\n",
      "Epoch 4522 \t\t Training Loss: 0.0005809584981761873 \t\n",
      "Epoch 4523 \t\t Training Loss: 0.0005809584981761873 \t\n",
      "Epoch 4524 \t\t Training Loss: 0.0005809584981761873 \t\n",
      "Epoch 4525 \t\t Training Loss: 0.0005809584981761873 \t\n",
      "Epoch 4526 \t\t Training Loss: 0.0005809584981761873 \t\n",
      "Epoch 4527 \t\t Training Loss: 0.0005809584981761873 \t\n",
      "Epoch 4528 \t\t Training Loss: 0.0005809584981761873 \t\n",
      "Epoch 4529 \t\t Training Loss: 0.0005809584981761873 \t\n",
      "Epoch 4530 \t\t Training Loss: 0.0005809584981761873 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4531 \t\t Training Loss: 0.0005809584981761873 \t\n",
      "Epoch 4532 \t\t Training Loss: 0.0005809584981761873 \t\n",
      "Epoch 4533 \t\t Training Loss: 0.0005809583817608654 \t\n",
      "Epoch 4534 \t\t Training Loss: 0.0005809583817608654 \t\n",
      "Epoch 4535 \t\t Training Loss: 0.0005809583817608654 \t\n",
      "Epoch 4536 \t\t Training Loss: 0.0005809583817608654 \t\n",
      "Epoch 4537 \t\t Training Loss: 0.0005809583235532045 \t\n",
      "Epoch 4538 \t\t Training Loss: 0.0005809583235532045 \t\n",
      "Epoch 4539 \t\t Training Loss: 0.0005809582653455436 \t\n",
      "Epoch 4540 \t\t Training Loss: 0.0005809582653455436 \t\n",
      "Epoch 4541 \t\t Training Loss: 0.0005809582653455436 \t\n",
      "Epoch 4542 \t\t Training Loss: 0.0005809582653455436 \t\n",
      "Epoch 4543 \t\t Training Loss: 0.0005809582653455436 \t\n",
      "Epoch 4544 \t\t Training Loss: 0.0005809582653455436 \t\n",
      "Epoch 4545 \t\t Training Loss: 0.0005809582653455436 \t\n",
      "Epoch 4546 \t\t Training Loss: 0.0005809582653455436 \t\n",
      "Epoch 4547 \t\t Training Loss: 0.0005809582653455436 \t\n",
      "Epoch 4548 \t\t Training Loss: 0.0005809582653455436 \t\n",
      "Epoch 4549 \t\t Training Loss: 0.0005809582653455436 \t\n",
      "Epoch 4550 \t\t Training Loss: 0.0005809582653455436 \t\n",
      "Epoch 4551 \t\t Training Loss: 0.0005809582653455436 \t\n",
      "Epoch 4552 \t\t Training Loss: 0.0005809582071378827 \t\n",
      "Epoch 4553 \t\t Training Loss: 0.0005809582071378827 \t\n",
      "Epoch 4554 \t\t Training Loss: 0.0005809582071378827 \t\n",
      "Epoch 4555 \t\t Training Loss: 0.0005809582071378827 \t\n",
      "Epoch 4556 \t\t Training Loss: 0.0005809582071378827 \t\n",
      "Epoch 4557 \t\t Training Loss: 0.0005809582071378827 \t\n",
      "Epoch 4558 \t\t Training Loss: 0.0005809582071378827 \t\n",
      "Epoch 4559 \t\t Training Loss: 0.0005809582071378827 \t\n",
      "Epoch 4560 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4561 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4562 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4563 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4564 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4565 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4566 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4567 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4568 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4569 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4570 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4571 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4572 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4573 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4574 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4575 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4576 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4577 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4578 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4579 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4580 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4581 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4582 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4583 \t\t Training Loss: 0.0005809581489302218 \t\n",
      "Epoch 4584 \t\t Training Loss: 0.0005809580907225609 \t\n",
      "Epoch 4585 \t\t Training Loss: 0.0005809580907225609 \t\n",
      "Epoch 4586 \t\t Training Loss: 0.0005809580907225609 \t\n",
      "Epoch 4587 \t\t Training Loss: 0.0005809580325149 \t\n",
      "Epoch 4588 \t\t Training Loss: 0.0005809580325149 \t\n",
      "Epoch 4589 \t\t Training Loss: 0.0005809580325149 \t\n",
      "Epoch 4590 \t\t Training Loss: 0.0005809580325149 \t\n",
      "Epoch 4591 \t\t Training Loss: 0.0005809579160995781 \t\n",
      "Epoch 4592 \t\t Training Loss: 0.0005809579160995781 \t\n",
      "Epoch 4593 \t\t Training Loss: 0.0005809578578919172 \t\n",
      "Epoch 4594 \t\t Training Loss: 0.0005809578578919172 \t\n",
      "Epoch 4595 \t\t Training Loss: 0.0005809578578919172 \t\n",
      "Epoch 4596 \t\t Training Loss: 0.0005809578578919172 \t\n",
      "Epoch 4597 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4598 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4599 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4600 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4601 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4602 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4603 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4604 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4605 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4606 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4607 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4608 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4609 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4610 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4611 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4612 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4613 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4614 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4615 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4616 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4617 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4618 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4619 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4620 \t\t Training Loss: 0.0005809577996842563 \t\n",
      "Epoch 4621 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4622 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4623 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4624 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4625 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4626 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4627 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4628 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4629 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4630 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4631 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4632 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4633 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4634 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4635 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4636 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4637 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4638 \t\t Training Loss: 0.0005809576832689345 \t\n",
      "Epoch 4639 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4640 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4641 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4642 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4643 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4644 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4645 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4646 \t\t Training Loss: 0.0005809577414765954 \t\n",
      "Epoch 4647 \t\t Training Loss: 0.0005809576832689345 \t\n",
      "Epoch 4648 \t\t Training Loss: 0.0005809576832689345 \t\n",
      "Epoch 4649 \t\t Training Loss: 0.0005809576250612736 \t\n",
      "Epoch 4650 \t\t Training Loss: 0.0005809576250612736 \t\n",
      "Epoch 4651 \t\t Training Loss: 0.0005809576250612736 \t\n",
      "Epoch 4652 \t\t Training Loss: 0.0005809576250612736 \t\n",
      "Epoch 4653 \t\t Training Loss: 0.0005809576250612736 \t\n",
      "Epoch 4654 \t\t Training Loss: 0.0005809576250612736 \t\n",
      "Epoch 4655 \t\t Training Loss: 0.0005809576250612736 \t\n",
      "Epoch 4656 \t\t Training Loss: 0.0005809576250612736 \t\n",
      "Epoch 4657 \t\t Training Loss: 0.0005809576250612736 \t\n",
      "Epoch 4658 \t\t Training Loss: 0.0005809576250612736 \t\n",
      "Epoch 4659 \t\t Training Loss: 0.0005809575668536127 \t\n",
      "Epoch 4660 \t\t Training Loss: 0.0005809575668536127 \t\n",
      "Epoch 4661 \t\t Training Loss: 0.0005809575668536127 \t\n",
      "Epoch 4662 \t\t Training Loss: 0.0005809576250612736 \t\n",
      "Epoch 4663 \t\t Training Loss: 0.0005809575668536127 \t\n",
      "Epoch 4664 \t\t Training Loss: 0.0005809575668536127 \t\n",
      "Epoch 4665 \t\t Training Loss: 0.0005809575668536127 \t\n",
      "Epoch 4666 \t\t Training Loss: 0.0005809575668536127 \t\n",
      "Epoch 4667 \t\t Training Loss: 0.0005809575668536127 \t\n",
      "Epoch 4668 \t\t Training Loss: 0.0005809575668536127 \t\n",
      "Epoch 4669 \t\t Training Loss: 0.0005809575668536127 \t\n",
      "Epoch 4670 \t\t Training Loss: 0.0005809575668536127 \t\n",
      "Epoch 4671 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4672 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4673 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4674 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4675 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4676 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4677 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4678 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4679 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4680 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4681 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4682 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4683 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4684 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4685 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4686 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4687 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4688 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4689 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4690 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4691 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4692 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4693 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4694 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4695 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4696 \t\t Training Loss: 0.0005809575086459517 \t\n",
      "Epoch 4697 \t\t Training Loss: 0.0005809575086459517 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4698 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4699 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4700 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4701 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4702 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4703 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4704 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4705 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4706 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4707 \t\t Training Loss: 0.000580957334022969 \t\n",
      "Epoch 4708 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4709 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4710 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4711 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4712 \t\t Training Loss: 0.0005809573922306299 \t\n",
      "Epoch 4713 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4714 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4715 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4716 \t\t Training Loss: 0.0005809574504382908 \t\n",
      "Epoch 4717 \t\t Training Loss: 0.000580957334022969 \t\n",
      "Epoch 4718 \t\t Training Loss: 0.000580957334022969 \t\n",
      "Epoch 4719 \t\t Training Loss: 0.000580957334022969 \t\n",
      "Epoch 4720 \t\t Training Loss: 0.000580957334022969 \t\n",
      "Epoch 4721 \t\t Training Loss: 0.000580957334022969 \t\n",
      "Epoch 4722 \t\t Training Loss: 0.000580957334022969 \t\n",
      "Epoch 4723 \t\t Training Loss: 0.0005809572758153081 \t\n",
      "Epoch 4724 \t\t Training Loss: 0.0005809572758153081 \t\n",
      "Epoch 4725 \t\t Training Loss: 0.0005809572176076472 \t\n",
      "Epoch 4726 \t\t Training Loss: 0.0005809572176076472 \t\n",
      "Epoch 4727 \t\t Training Loss: 0.0005809572176076472 \t\n",
      "Epoch 4728 \t\t Training Loss: 0.0005809572176076472 \t\n",
      "Epoch 4729 \t\t Training Loss: 0.0005809572176076472 \t\n",
      "Epoch 4730 \t\t Training Loss: 0.0005809572176076472 \t\n",
      "Epoch 4731 \t\t Training Loss: 0.0005809572176076472 \t\n",
      "Epoch 4732 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4733 \t\t Training Loss: 0.0005809572176076472 \t\n",
      "Epoch 4734 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4735 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4736 \t\t Training Loss: 0.0005809572176076472 \t\n",
      "Epoch 4737 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4738 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4739 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4740 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4741 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4742 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4743 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4744 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4745 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4746 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4747 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4748 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4749 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4750 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4751 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4752 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4753 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4754 \t\t Training Loss: 0.0005809571011923254 \t\n",
      "Epoch 4755 \t\t Training Loss: 0.0005809571593999863 \t\n",
      "Epoch 4756 \t\t Training Loss: 0.0005809571011923254 \t\n",
      "Epoch 4757 \t\t Training Loss: 0.0005809571011923254 \t\n",
      "Epoch 4758 \t\t Training Loss: 0.0005809571011923254 \t\n",
      "Epoch 4759 \t\t Training Loss: 0.0005809571011923254 \t\n",
      "Epoch 4760 \t\t Training Loss: 0.0005809570429846644 \t\n",
      "Epoch 4761 \t\t Training Loss: 0.0005809570429846644 \t\n",
      "Epoch 4762 \t\t Training Loss: 0.0005809570429846644 \t\n",
      "Epoch 4763 \t\t Training Loss: 0.0005809570429846644 \t\n",
      "Epoch 4764 \t\t Training Loss: 0.0005809570429846644 \t\n",
      "Epoch 4765 \t\t Training Loss: 0.0005809570429846644 \t\n",
      "Epoch 4766 \t\t Training Loss: 0.0005809570429846644 \t\n",
      "Epoch 4767 \t\t Training Loss: 0.0005809570429846644 \t\n",
      "Epoch 4768 \t\t Training Loss: 0.0005809570429846644 \t\n",
      "Epoch 4769 \t\t Training Loss: 0.0005809570429846644 \t\n",
      "Epoch 4770 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4771 \t\t Training Loss: 0.0005809570429846644 \t\n",
      "Epoch 4772 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4773 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4774 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4775 \t\t Training Loss: 0.0005809570429846644 \t\n",
      "Epoch 4776 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4777 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4778 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4779 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4780 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4781 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4782 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4783 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4784 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4785 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4786 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4787 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4788 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4789 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4790 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4791 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4792 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4793 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4794 \t\t Training Loss: 0.0005809569265693426 \t\n",
      "Epoch 4795 \t\t Training Loss: 0.0005809569265693426 \t\n",
      "Epoch 4796 \t\t Training Loss: 0.0005809569847770035 \t\n",
      "Epoch 4797 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4798 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4799 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4800 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4801 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4802 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4803 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4804 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4805 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4806 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4807 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4808 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4809 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4810 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4811 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4812 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4813 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4814 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4815 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4816 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4817 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4818 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4819 \t\t Training Loss: 0.0005809568101540208 \t\n",
      "Epoch 4820 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4821 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4822 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4823 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4824 \t\t Training Loss: 0.0005809568101540208 \t\n",
      "Epoch 4825 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4826 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4827 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4828 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4829 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4830 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4831 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4832 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4833 \t\t Training Loss: 0.0005809568683616817 \t\n",
      "Epoch 4834 \t\t Training Loss: 0.0005809568101540208 \t\n",
      "Epoch 4835 \t\t Training Loss: 0.0005809568101540208 \t\n",
      "Epoch 4836 \t\t Training Loss: 0.0005809568101540208 \t\n",
      "Epoch 4837 \t\t Training Loss: 0.0005809567519463599 \t\n",
      "Epoch 4838 \t\t Training Loss: 0.0005809567519463599 \t\n",
      "Epoch 4839 \t\t Training Loss: 0.0005809567519463599 \t\n",
      "Epoch 4840 \t\t Training Loss: 0.0005809567519463599 \t\n",
      "Epoch 4841 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4842 \t\t Training Loss: 0.0005809567519463599 \t\n",
      "Epoch 4843 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4844 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4845 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4846 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4847 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4848 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4849 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4850 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4851 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4852 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4853 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4854 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4855 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4856 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4857 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4858 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4859 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4860 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4861 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4862 \t\t Training Loss: 0.0005809565773233771 \t\n",
      "Epoch 4863 \t\t Training Loss: 0.0005809565773233771 \t\n",
      "Epoch 4864 \t\t Training Loss: 0.0005809565773233771 \t\n",
      "Epoch 4865 \t\t Training Loss: 0.0005809565773233771 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4866 \t\t Training Loss: 0.0005809565773233771 \t\n",
      "Epoch 4867 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4868 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4869 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4870 \t\t Training Loss: 0.000580956693738699 \t\n",
      "Epoch 4871 \t\t Training Loss: 0.0005809565773233771 \t\n",
      "Epoch 4872 \t\t Training Loss: 0.0005809565773233771 \t\n",
      "Epoch 4873 \t\t Training Loss: 0.0005809565773233771 \t\n",
      "Epoch 4874 \t\t Training Loss: 0.0005809565773233771 \t\n",
      "Epoch 4875 \t\t Training Loss: 0.0005809565773233771 \t\n",
      "Epoch 4876 \t\t Training Loss: 0.0005809565773233771 \t\n",
      "Epoch 4877 \t\t Training Loss: 0.0005809565773233771 \t\n",
      "Epoch 4878 \t\t Training Loss: 0.0005809565773233771 \t\n",
      "Epoch 4879 \t\t Training Loss: 0.0005809565773233771 \t\n",
      "Epoch 4880 \t\t Training Loss: 0.0005809565773233771 \t\n",
      "Epoch 4881 \t\t Training Loss: 0.0005809565773233771 \t\n",
      "Epoch 4882 \t\t Training Loss: 0.0005809565773233771 \t\n",
      "Epoch 4883 \t\t Training Loss: 0.0005809565191157162 \t\n",
      "Epoch 4884 \t\t Training Loss: 0.0005809565191157162 \t\n",
      "Epoch 4885 \t\t Training Loss: 0.0005809565191157162 \t\n",
      "Epoch 4886 \t\t Training Loss: 0.0005809565191157162 \t\n",
      "Epoch 4887 \t\t Training Loss: 0.0005809565191157162 \t\n",
      "Epoch 4888 \t\t Training Loss: 0.0005809564609080553 \t\n",
      "Epoch 4889 \t\t Training Loss: 0.0005809564609080553 \t\n",
      "Epoch 4890 \t\t Training Loss: 0.0005809564609080553 \t\n",
      "Epoch 4891 \t\t Training Loss: 0.0005809564609080553 \t\n",
      "Epoch 4892 \t\t Training Loss: 0.0005809564609080553 \t\n",
      "Epoch 4893 \t\t Training Loss: 0.0005809564609080553 \t\n",
      "Epoch 4894 \t\t Training Loss: 0.0005809564027003944 \t\n",
      "Epoch 4895 \t\t Training Loss: 0.0005809564027003944 \t\n",
      "Epoch 4896 \t\t Training Loss: 0.0005809564027003944 \t\n",
      "Epoch 4897 \t\t Training Loss: 0.0005809564027003944 \t\n",
      "Epoch 4898 \t\t Training Loss: 0.0005809564027003944 \t\n",
      "Epoch 4899 \t\t Training Loss: 0.0005809564027003944 \t\n",
      "Epoch 4900 \t\t Training Loss: 0.0005809564027003944 \t\n",
      "Epoch 4901 \t\t Training Loss: 0.0005809564027003944 \t\n",
      "Epoch 4902 \t\t Training Loss: 0.0005809564027003944 \t\n",
      "Epoch 4903 \t\t Training Loss: 0.0005809564027003944 \t\n",
      "Epoch 4904 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4905 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4906 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4907 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4908 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4909 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4910 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4911 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4912 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4913 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4914 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4915 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4916 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4917 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4918 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4919 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4920 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4921 \t\t Training Loss: 0.0005809562862850726 \t\n",
      "Epoch 4922 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4923 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4924 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4925 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4926 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4927 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4928 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4929 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4930 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4931 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4932 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4933 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4934 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4935 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4936 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4937 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4938 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4939 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4940 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4941 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4942 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4943 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4944 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4945 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4946 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4947 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4948 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4949 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4950 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4951 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4952 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4953 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4954 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4955 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4956 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4957 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4958 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4959 \t\t Training Loss: 0.0005809561698697507 \t\n",
      "Epoch 4960 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4961 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4962 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4963 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4964 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4965 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4966 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4967 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4968 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4969 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4970 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4971 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4972 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4973 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4974 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4975 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4976 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4977 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4978 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4979 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4980 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4981 \t\t Training Loss: 0.0005809561116620898 \t\n",
      "Epoch 4982 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4983 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4984 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4985 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4986 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4987 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4988 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4989 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4990 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4991 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4992 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4993 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4994 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4995 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4996 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4997 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4998 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 4999 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 5000 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 5001 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 5002 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 5003 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 5004 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 5005 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 5006 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 5007 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 5008 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 5009 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 5010 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 5011 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 5012 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 5013 \t\t Training Loss: 0.000580955995246768 \t\n",
      "Epoch 5014 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5015 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5016 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5017 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5018 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5019 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5020 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5021 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5022 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5023 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5024 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5025 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5026 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5027 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5028 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5029 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5030 \t\t Training Loss: 0.0005809558206237853 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5031 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5032 \t\t Training Loss: 0.0005809557624161243 \t\n",
      "Epoch 5033 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5034 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5035 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5036 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5037 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5038 \t\t Training Loss: 0.0005809558206237853 \t\n",
      "Epoch 5039 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5040 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5041 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5042 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5043 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5044 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5045 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5046 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5047 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5048 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5049 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5050 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5051 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5052 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5053 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5054 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5055 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5056 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5057 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5058 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5059 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5060 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5061 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5062 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5063 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5064 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5065 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5066 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5067 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5068 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5069 \t\t Training Loss: 0.0005809557042084634 \t\n",
      "Epoch 5070 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5071 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5072 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5073 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5074 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5075 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5076 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5077 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5078 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5079 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5080 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5081 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5082 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5083 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5084 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5085 \t\t Training Loss: 0.0005809555877931416 \t\n",
      "Epoch 5086 \t\t Training Loss: 0.0005809554713778198 \t\n",
      "Epoch 5087 \t\t Training Loss: 0.0005809554131701589 \t\n",
      "Epoch 5088 \t\t Training Loss: 0.0005809554131701589 \t\n",
      "Epoch 5089 \t\t Training Loss: 0.0005809554131701589 \t\n",
      "Epoch 5090 \t\t Training Loss: 0.0005809554131701589 \t\n",
      "Epoch 5091 \t\t Training Loss: 0.0005809554131701589 \t\n",
      "Epoch 5092 \t\t Training Loss: 0.0005809554131701589 \t\n",
      "Epoch 5093 \t\t Training Loss: 0.0005809554131701589 \t\n",
      "Epoch 5094 \t\t Training Loss: 0.0005809554131701589 \t\n",
      "Epoch 5095 \t\t Training Loss: 0.0005809554131701589 \t\n",
      "Epoch 5096 \t\t Training Loss: 0.0005809554131701589 \t\n",
      "Epoch 5097 \t\t Training Loss: 0.0005809554131701589 \t\n",
      "Epoch 5098 \t\t Training Loss: 0.0005809554131701589 \t\n",
      "Epoch 5099 \t\t Training Loss: 0.0005809554131701589 \t\n",
      "Epoch 5100 \t\t Training Loss: 0.0005809554131701589 \t\n",
      "Epoch 5101 \t\t Training Loss: 0.0005809554131701589 \t\n",
      "Epoch 5102 \t\t Training Loss: 0.0005809554131701589 \t\n",
      "Epoch 5103 \t\t Training Loss: 0.000580955296754837 \t\n",
      "Epoch 5104 \t\t Training Loss: 0.000580955296754837 \t\n",
      "Epoch 5105 \t\t Training Loss: 0.000580955296754837 \t\n",
      "Epoch 5106 \t\t Training Loss: 0.000580955296754837 \t\n",
      "Epoch 5107 \t\t Training Loss: 0.000580955296754837 \t\n",
      "Epoch 5108 \t\t Training Loss: 0.000580955296754837 \t\n",
      "Epoch 5109 \t\t Training Loss: 0.000580955296754837 \t\n",
      "Epoch 5110 \t\t Training Loss: 0.000580955296754837 \t\n",
      "Epoch 5111 \t\t Training Loss: 0.0005809551803395152 \t\n",
      "Epoch 5112 \t\t Training Loss: 0.0005809551803395152 \t\n",
      "Epoch 5113 \t\t Training Loss: 0.0005809551803395152 \t\n",
      "Epoch 5114 \t\t Training Loss: 0.0005809551803395152 \t\n",
      "Epoch 5115 \t\t Training Loss: 0.0005809551803395152 \t\n",
      "Epoch 5116 \t\t Training Loss: 0.0005809551803395152 \t\n",
      "Epoch 5117 \t\t Training Loss: 0.0005809551803395152 \t\n",
      "Epoch 5118 \t\t Training Loss: 0.0005809551803395152 \t\n",
      "Epoch 5119 \t\t Training Loss: 0.0005809551803395152 \t\n",
      "Epoch 5120 \t\t Training Loss: 0.0005809551221318543 \t\n",
      "Epoch 5121 \t\t Training Loss: 0.0005809551221318543 \t\n",
      "Epoch 5122 \t\t Training Loss: 0.0005809551221318543 \t\n",
      "Epoch 5123 \t\t Training Loss: 0.0005809551221318543 \t\n",
      "Epoch 5124 \t\t Training Loss: 0.0005809551221318543 \t\n",
      "Epoch 5125 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5126 \t\t Training Loss: 0.0005809551221318543 \t\n",
      "Epoch 5127 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5128 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5129 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5130 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5131 \t\t Training Loss: 0.0005809551221318543 \t\n",
      "Epoch 5132 \t\t Training Loss: 0.0005809551221318543 \t\n",
      "Epoch 5133 \t\t Training Loss: 0.0005809551221318543 \t\n",
      "Epoch 5134 \t\t Training Loss: 0.0005809551221318543 \t\n",
      "Epoch 5135 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5136 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5137 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5138 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5139 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5140 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5141 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5142 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5143 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5144 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5145 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5146 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5147 \t\t Training Loss: 0.0005809550057165325 \t\n",
      "Epoch 5148 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5149 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5150 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5151 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5152 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5153 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5154 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5155 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5156 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5157 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5158 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5159 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5160 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5161 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5162 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5163 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5164 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5165 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5166 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5167 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5168 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5169 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5170 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5171 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5172 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5173 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5174 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5175 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5176 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5177 \t\t Training Loss: 0.0005809548893012106 \t\n",
      "Epoch 5178 \t\t Training Loss: 0.0005809548310935497 \t\n",
      "Epoch 5179 \t\t Training Loss: 0.0005809548310935497 \t\n",
      "Epoch 5180 \t\t Training Loss: 0.0005809548310935497 \t\n",
      "Epoch 5181 \t\t Training Loss: 0.0005809548310935497 \t\n",
      "Epoch 5182 \t\t Training Loss: 0.0005809548310935497 \t\n",
      "Epoch 5183 \t\t Training Loss: 0.0005809548310935497 \t\n",
      "Epoch 5184 \t\t Training Loss: 0.0005809548310935497 \t\n",
      "Epoch 5185 \t\t Training Loss: 0.0005809548310935497 \t\n",
      "Epoch 5186 \t\t Training Loss: 0.0005809548310935497 \t\n",
      "Epoch 5187 \t\t Training Loss: 0.0005809548310935497 \t\n",
      "Epoch 5188 \t\t Training Loss: 0.0005809548310935497 \t\n",
      "Epoch 5189 \t\t Training Loss: 0.0005809547146782279 \t\n",
      "Epoch 5190 \t\t Training Loss: 0.0005809547146782279 \t\n",
      "Epoch 5191 \t\t Training Loss: 0.0005809547146782279 \t\n",
      "Epoch 5192 \t\t Training Loss: 0.0005809547146782279 \t\n",
      "Epoch 5193 \t\t Training Loss: 0.0005809547146782279 \t\n",
      "Epoch 5194 \t\t Training Loss: 0.0005809547146782279 \t\n",
      "Epoch 5195 \t\t Training Loss: 0.0005809547146782279 \t\n",
      "Epoch 5196 \t\t Training Loss: 0.0005809547146782279 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5197 \t\t Training Loss: 0.0005809547146782279 \t\n",
      "Epoch 5198 \t\t Training Loss: 0.0005809547146782279 \t\n",
      "Epoch 5199 \t\t Training Loss: 0.0005809547146782279 \t\n",
      "Epoch 5200 \t\t Training Loss: 0.0005809547146782279 \t\n",
      "Epoch 5201 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5202 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5203 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5204 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5205 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5206 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5207 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5208 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5209 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5210 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5211 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5212 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5213 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5214 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5215 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5216 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5217 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5218 \t\t Training Loss: 0.0005809545982629061 \t\n",
      "Epoch 5219 \t\t Training Loss: 0.0005809545400552452 \t\n",
      "Epoch 5220 \t\t Training Loss: 0.0005809545400552452 \t\n",
      "Epoch 5221 \t\t Training Loss: 0.0005809545400552452 \t\n",
      "Epoch 5222 \t\t Training Loss: 0.0005809545400552452 \t\n",
      "Epoch 5223 \t\t Training Loss: 0.0005809545400552452 \t\n",
      "Epoch 5224 \t\t Training Loss: 0.0005809545400552452 \t\n",
      "Epoch 5225 \t\t Training Loss: 0.0005809545400552452 \t\n",
      "Epoch 5226 \t\t Training Loss: 0.0005809545400552452 \t\n",
      "Epoch 5227 \t\t Training Loss: 0.0005809545400552452 \t\n",
      "Epoch 5228 \t\t Training Loss: 0.0005809545400552452 \t\n",
      "Epoch 5229 \t\t Training Loss: 0.0005809545400552452 \t\n",
      "Epoch 5230 \t\t Training Loss: 0.0005809545400552452 \t\n",
      "Epoch 5231 \t\t Training Loss: 0.0005809545400552452 \t\n",
      "Epoch 5232 \t\t Training Loss: 0.0005809545400552452 \t\n",
      "Epoch 5233 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5234 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5235 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5236 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5237 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5238 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5239 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5240 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5241 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5242 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5243 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5244 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5245 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5246 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5247 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5248 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5249 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5250 \t\t Training Loss: 0.0005809544818475842 \t\n",
      "Epoch 5251 \t\t Training Loss: 0.0005809544236399233 \t\n",
      "Epoch 5252 \t\t Training Loss: 0.0005809544236399233 \t\n",
      "Epoch 5253 \t\t Training Loss: 0.0005809544236399233 \t\n",
      "Epoch 5254 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5255 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5256 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5257 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5258 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5259 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5260 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5261 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5262 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5263 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5264 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5265 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5266 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5267 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5268 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5269 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5270 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5271 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5272 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5273 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5274 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5275 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5276 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5277 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5278 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5279 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5280 \t\t Training Loss: 0.0005809543072246015 \t\n",
      "Epoch 5281 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5282 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5283 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5284 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5285 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5286 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5287 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5288 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5289 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5290 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5291 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5292 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5293 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5294 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5295 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5296 \t\t Training Loss: 0.0005809541908092797 \t\n",
      "Epoch 5297 \t\t Training Loss: 0.0005809541326016188 \t\n",
      "Epoch 5298 \t\t Training Loss: 0.0005809541326016188 \t\n",
      "Epoch 5299 \t\t Training Loss: 0.0005809541326016188 \t\n",
      "Epoch 5300 \t\t Training Loss: 0.0005809541326016188 \t\n",
      "Epoch 5301 \t\t Training Loss: 0.0005809541326016188 \t\n",
      "Epoch 5302 \t\t Training Loss: 0.0005809541326016188 \t\n",
      "Epoch 5303 \t\t Training Loss: 0.0005809541326016188 \t\n",
      "Epoch 5304 \t\t Training Loss: 0.0005809541326016188 \t\n",
      "Epoch 5305 \t\t Training Loss: 0.0005809540743939579 \t\n",
      "Epoch 5306 \t\t Training Loss: 0.0005809540743939579 \t\n",
      "Epoch 5307 \t\t Training Loss: 0.0005809541326016188 \t\n",
      "Epoch 5308 \t\t Training Loss: 0.0005809541326016188 \t\n",
      "Epoch 5309 \t\t Training Loss: 0.0005809540743939579 \t\n",
      "Epoch 5310 \t\t Training Loss: 0.0005809540743939579 \t\n",
      "Epoch 5311 \t\t Training Loss: 0.0005809540743939579 \t\n",
      "Epoch 5312 \t\t Training Loss: 0.0005809540743939579 \t\n",
      "Epoch 5313 \t\t Training Loss: 0.0005809540743939579 \t\n",
      "Epoch 5314 \t\t Training Loss: 0.0005809540743939579 \t\n",
      "Epoch 5315 \t\t Training Loss: 0.0005809540743939579 \t\n",
      "Epoch 5316 \t\t Training Loss: 0.0005809540161862969 \t\n",
      "Epoch 5317 \t\t Training Loss: 0.0005809540161862969 \t\n",
      "Epoch 5318 \t\t Training Loss: 0.0005809540161862969 \t\n",
      "Epoch 5319 \t\t Training Loss: 0.0005809540161862969 \t\n",
      "Epoch 5320 \t\t Training Loss: 0.0005809540161862969 \t\n",
      "Epoch 5321 \t\t Training Loss: 0.0005809540161862969 \t\n",
      "Epoch 5322 \t\t Training Loss: 0.0005809540161862969 \t\n",
      "Epoch 5323 \t\t Training Loss: 0.0005809540161862969 \t\n",
      "Epoch 5324 \t\t Training Loss: 0.0005809540161862969 \t\n",
      "Epoch 5325 \t\t Training Loss: 0.0005809540161862969 \t\n",
      "Epoch 5326 \t\t Training Loss: 0.0005809540161862969 \t\n",
      "Epoch 5327 \t\t Training Loss: 0.0005809540161862969 \t\n",
      "Epoch 5328 \t\t Training Loss: 0.0005809540161862969 \t\n",
      "Epoch 5329 \t\t Training Loss: 0.0005809538997709751 \t\n",
      "Epoch 5330 \t\t Training Loss: 0.0005809538997709751 \t\n",
      "Epoch 5331 \t\t Training Loss: 0.0005809540161862969 \t\n",
      "Epoch 5332 \t\t Training Loss: 0.0005809538997709751 \t\n",
      "Epoch 5333 \t\t Training Loss: 0.0005809538997709751 \t\n",
      "Epoch 5334 \t\t Training Loss: 0.0005809538997709751 \t\n",
      "Epoch 5335 \t\t Training Loss: 0.0005809540161862969 \t\n",
      "Epoch 5336 \t\t Training Loss: 0.0005809538997709751 \t\n",
      "Epoch 5337 \t\t Training Loss: 0.0005809538997709751 \t\n",
      "Epoch 5338 \t\t Training Loss: 0.0005809538997709751 \t\n",
      "Epoch 5339 \t\t Training Loss: 0.0005809538997709751 \t\n",
      "Epoch 5340 \t\t Training Loss: 0.0005809538997709751 \t\n",
      "Epoch 5341 \t\t Training Loss: 0.0005809538997709751 \t\n",
      "Epoch 5342 \t\t Training Loss: 0.0005809538997709751 \t\n",
      "Epoch 5343 \t\t Training Loss: 0.0005809538997709751 \t\n",
      "Epoch 5344 \t\t Training Loss: 0.0005809538997709751 \t\n",
      "Epoch 5345 \t\t Training Loss: 0.0005809537833556533 \t\n",
      "Epoch 5346 \t\t Training Loss: 0.0005809537833556533 \t\n",
      "Epoch 5347 \t\t Training Loss: 0.0005809537833556533 \t\n",
      "Epoch 5348 \t\t Training Loss: 0.0005809537833556533 \t\n",
      "Epoch 5349 \t\t Training Loss: 0.0005809537833556533 \t\n",
      "Epoch 5350 \t\t Training Loss: 0.0005809537833556533 \t\n",
      "Epoch 5351 \t\t Training Loss: 0.0005809537833556533 \t\n",
      "Epoch 5352 \t\t Training Loss: 0.0005809537833556533 \t\n",
      "Epoch 5353 \t\t Training Loss: 0.0005809537251479924 \t\n",
      "Epoch 5354 \t\t Training Loss: 0.0005809537251479924 \t\n",
      "Epoch 5355 \t\t Training Loss: 0.0005809537251479924 \t\n",
      "Epoch 5356 \t\t Training Loss: 0.0005809537251479924 \t\n",
      "Epoch 5357 \t\t Training Loss: 0.0005809537251479924 \t\n",
      "Epoch 5358 \t\t Training Loss: 0.0005809537251479924 \t\n",
      "Epoch 5359 \t\t Training Loss: 0.0005809537251479924 \t\n",
      "Epoch 5360 \t\t Training Loss: 0.0005809537251479924 \t\n",
      "Epoch 5361 \t\t Training Loss: 0.0005809537251479924 \t\n",
      "Epoch 5362 \t\t Training Loss: 0.0005809536087326705 \t\n",
      "Epoch 5363 \t\t Training Loss: 0.0005809536087326705 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5364 \t\t Training Loss: 0.0005809535505250096 \t\n",
      "Epoch 5365 \t\t Training Loss: 0.0005809535505250096 \t\n",
      "Epoch 5366 \t\t Training Loss: 0.0005809535505250096 \t\n",
      "Epoch 5367 \t\t Training Loss: 0.0005809535505250096 \t\n",
      "Epoch 5368 \t\t Training Loss: 0.0005809534923173487 \t\n",
      "Epoch 5369 \t\t Training Loss: 0.0005809534923173487 \t\n",
      "Epoch 5370 \t\t Training Loss: 0.0005809534923173487 \t\n",
      "Epoch 5371 \t\t Training Loss: 0.0005809534923173487 \t\n",
      "Epoch 5372 \t\t Training Loss: 0.0005809534923173487 \t\n",
      "Epoch 5373 \t\t Training Loss: 0.0005809534923173487 \t\n",
      "Epoch 5374 \t\t Training Loss: 0.0005809534923173487 \t\n",
      "Epoch 5375 \t\t Training Loss: 0.0005809534923173487 \t\n",
      "Epoch 5376 \t\t Training Loss: 0.0005809534923173487 \t\n",
      "Epoch 5377 \t\t Training Loss: 0.0005809534923173487 \t\n",
      "Epoch 5378 \t\t Training Loss: 0.0005809534923173487 \t\n",
      "Epoch 5379 \t\t Training Loss: 0.0005809534923173487 \t\n",
      "Epoch 5380 \t\t Training Loss: 0.0005809534923173487 \t\n",
      "Epoch 5381 \t\t Training Loss: 0.0005809534923173487 \t\n",
      "Epoch 5382 \t\t Training Loss: 0.0005809534341096878 \t\n",
      "Epoch 5383 \t\t Training Loss: 0.0005809534341096878 \t\n",
      "Epoch 5384 \t\t Training Loss: 0.0005809534341096878 \t\n",
      "Epoch 5385 \t\t Training Loss: 0.0005809534341096878 \t\n",
      "Epoch 5386 \t\t Training Loss: 0.0005809534341096878 \t\n",
      "Epoch 5387 \t\t Training Loss: 0.0005809534341096878 \t\n",
      "Epoch 5388 \t\t Training Loss: 0.0005809534341096878 \t\n",
      "Epoch 5389 \t\t Training Loss: 0.0005809534341096878 \t\n",
      "Epoch 5390 \t\t Training Loss: 0.0005809533759020269 \t\n",
      "Epoch 5391 \t\t Training Loss: 0.0005809533759020269 \t\n",
      "Epoch 5392 \t\t Training Loss: 0.0005809533759020269 \t\n",
      "Epoch 5393 \t\t Training Loss: 0.000580953317694366 \t\n",
      "Epoch 5394 \t\t Training Loss: 0.000580953317694366 \t\n",
      "Epoch 5395 \t\t Training Loss: 0.000580953317694366 \t\n",
      "Epoch 5396 \t\t Training Loss: 0.000580953317694366 \t\n",
      "Epoch 5397 \t\t Training Loss: 0.000580953317694366 \t\n",
      "Epoch 5398 \t\t Training Loss: 0.000580953317694366 \t\n",
      "Epoch 5399 \t\t Training Loss: 0.000580953317694366 \t\n",
      "Epoch 5400 \t\t Training Loss: 0.000580953317694366 \t\n",
      "Epoch 5401 \t\t Training Loss: 0.000580953317694366 \t\n",
      "Epoch 5402 \t\t Training Loss: 0.000580953317694366 \t\n",
      "Epoch 5403 \t\t Training Loss: 0.000580953317694366 \t\n",
      "Epoch 5404 \t\t Training Loss: 0.0005809532594867051 \t\n",
      "Epoch 5405 \t\t Training Loss: 0.0005809532594867051 \t\n",
      "Epoch 5406 \t\t Training Loss: 0.0005809532594867051 \t\n",
      "Epoch 5407 \t\t Training Loss: 0.0005809532594867051 \t\n",
      "Epoch 5408 \t\t Training Loss: 0.0005809532594867051 \t\n",
      "Epoch 5409 \t\t Training Loss: 0.0005809532594867051 \t\n",
      "Epoch 5410 \t\t Training Loss: 0.0005809532594867051 \t\n",
      "Epoch 5411 \t\t Training Loss: 0.0005809532594867051 \t\n",
      "Epoch 5412 \t\t Training Loss: 0.0005809532594867051 \t\n",
      "Epoch 5413 \t\t Training Loss: 0.0005809532594867051 \t\n",
      "Epoch 5414 \t\t Training Loss: 0.0005809532594867051 \t\n",
      "Epoch 5415 \t\t Training Loss: 0.0005809532012790442 \t\n",
      "Epoch 5416 \t\t Training Loss: 0.0005809532594867051 \t\n",
      "Epoch 5417 \t\t Training Loss: 0.0005809532012790442 \t\n",
      "Epoch 5418 \t\t Training Loss: 0.0005809532012790442 \t\n",
      "Epoch 5419 \t\t Training Loss: 0.0005809532012790442 \t\n",
      "Epoch 5420 \t\t Training Loss: 0.0005809530848637223 \t\n",
      "Epoch 5421 \t\t Training Loss: 0.0005809530848637223 \t\n",
      "Epoch 5422 \t\t Training Loss: 0.0005809530848637223 \t\n",
      "Epoch 5423 \t\t Training Loss: 0.0005809530848637223 \t\n",
      "Epoch 5424 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5425 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5426 \t\t Training Loss: 0.0005809530848637223 \t\n",
      "Epoch 5427 \t\t Training Loss: 0.0005809530848637223 \t\n",
      "Epoch 5428 \t\t Training Loss: 0.0005809530848637223 \t\n",
      "Epoch 5429 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5430 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5431 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5432 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5433 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5434 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5435 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5436 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5437 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5438 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5439 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5440 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5441 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5442 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5443 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5444 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5445 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5446 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5447 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5448 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5449 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5450 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5451 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5452 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5453 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5454 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5455 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5456 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5457 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5458 \t\t Training Loss: 0.0005809529102407396 \t\n",
      "Epoch 5459 \t\t Training Loss: 0.0005809529102407396 \t\n",
      "Epoch 5460 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5461 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5462 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5463 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5464 \t\t Training Loss: 0.0005809530266560614 \t\n",
      "Epoch 5465 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5466 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5467 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5468 \t\t Training Loss: 0.0005809529102407396 \t\n",
      "Epoch 5469 \t\t Training Loss: 0.0005809529102407396 \t\n",
      "Epoch 5470 \t\t Training Loss: 0.0005809529102407396 \t\n",
      "Epoch 5471 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5472 \t\t Training Loss: 0.0005809529102407396 \t\n",
      "Epoch 5473 \t\t Training Loss: 0.0005809529102407396 \t\n",
      "Epoch 5474 \t\t Training Loss: 0.0005809529684484005 \t\n",
      "Epoch 5475 \t\t Training Loss: 0.0005809529102407396 \t\n",
      "Epoch 5476 \t\t Training Loss: 0.0005809529102407396 \t\n",
      "Epoch 5477 \t\t Training Loss: 0.0005809529102407396 \t\n",
      "Epoch 5478 \t\t Training Loss: 0.0005809529102407396 \t\n",
      "Epoch 5479 \t\t Training Loss: 0.0005809529102407396 \t\n",
      "Epoch 5480 \t\t Training Loss: 0.0005809529102407396 \t\n",
      "Epoch 5481 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5482 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5483 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5484 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5485 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5486 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5487 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5488 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5489 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5490 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5491 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5492 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5493 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5494 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5495 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5496 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5497 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5498 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5499 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5500 \t\t Training Loss: 0.0005809527938254178 \t\n",
      "Epoch 5501 \t\t Training Loss: 0.0005809527938254178 \t\n",
      "Epoch 5502 \t\t Training Loss: 0.0005809527938254178 \t\n",
      "Epoch 5503 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5504 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5505 \t\t Training Loss: 0.0005809528520330787 \t\n",
      "Epoch 5506 \t\t Training Loss: 0.0005809527938254178 \t\n",
      "Epoch 5507 \t\t Training Loss: 0.0005809527938254178 \t\n",
      "Epoch 5508 \t\t Training Loss: 0.0005809527938254178 \t\n",
      "Epoch 5509 \t\t Training Loss: 0.0005809527938254178 \t\n",
      "Epoch 5510 \t\t Training Loss: 0.0005809527938254178 \t\n",
      "Epoch 5511 \t\t Training Loss: 0.0005809527356177568 \t\n",
      "Epoch 5512 \t\t Training Loss: 0.0005809527356177568 \t\n",
      "Epoch 5513 \t\t Training Loss: 0.0005809527356177568 \t\n",
      "Epoch 5514 \t\t Training Loss: 0.0005809527356177568 \t\n",
      "Epoch 5515 \t\t Training Loss: 0.0005809527356177568 \t\n",
      "Epoch 5516 \t\t Training Loss: 0.0005809527356177568 \t\n",
      "Epoch 5517 \t\t Training Loss: 0.0005809527356177568 \t\n",
      "Epoch 5518 \t\t Training Loss: 0.0005809527356177568 \t\n",
      "Epoch 5519 \t\t Training Loss: 0.0005809527356177568 \t\n",
      "Epoch 5520 \t\t Training Loss: 0.0005809527356177568 \t\n",
      "Epoch 5521 \t\t Training Loss: 0.0005809526774100959 \t\n",
      "Epoch 5522 \t\t Training Loss: 0.0005809527356177568 \t\n",
      "Epoch 5523 \t\t Training Loss: 0.0005809527356177568 \t\n",
      "Epoch 5524 \t\t Training Loss: 0.0005809527356177568 \t\n",
      "Epoch 5525 \t\t Training Loss: 0.0005809527356177568 \t\n",
      "Epoch 5526 \t\t Training Loss: 0.0005809526774100959 \t\n",
      "Epoch 5527 \t\t Training Loss: 0.0005809527356177568 \t\n",
      "Epoch 5528 \t\t Training Loss: 0.0005809526774100959 \t\n",
      "Epoch 5529 \t\t Training Loss: 0.0005809526774100959 \t\n",
      "Epoch 5530 \t\t Training Loss: 0.0005809526774100959 \t\n",
      "Epoch 5531 \t\t Training Loss: 0.0005809526774100959 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5532 \t\t Training Loss: 0.0005809526774100959 \t\n",
      "Epoch 5533 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5534 \t\t Training Loss: 0.0005809526774100959 \t\n",
      "Epoch 5535 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5536 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5537 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5538 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5539 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5540 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5541 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5542 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5543 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5544 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5545 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5546 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5547 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5548 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5549 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5550 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5551 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5552 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5553 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5554 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5555 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5556 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5557 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5558 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5559 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5560 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5561 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5562 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5563 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5564 \t\t Training Loss: 0.0005809525609947741 \t\n",
      "Epoch 5565 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5566 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5567 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5568 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5569 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5570 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5571 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5572 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5573 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5574 \t\t Training Loss: 0.000580952619202435 \t\n",
      "Epoch 5575 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5576 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5577 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5578 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5579 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5580 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5581 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5582 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5583 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5584 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5585 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5586 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5587 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5588 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5589 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5590 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5591 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5592 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5593 \t\t Training Loss: 0.0005809525027871132 \t\n",
      "Epoch 5594 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5595 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5596 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5597 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5598 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5599 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5600 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5601 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5602 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5603 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5604 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5605 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5606 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5607 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5608 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5609 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5610 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5611 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5612 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5613 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5614 \t\t Training Loss: 0.0005809524445794523 \t\n",
      "Epoch 5615 \t\t Training Loss: 0.0005809523863717914 \t\n",
      "Epoch 5616 \t\t Training Loss: 0.0005809523863717914 \t\n",
      "Epoch 5617 \t\t Training Loss: 0.0005809523863717914 \t\n",
      "Epoch 5618 \t\t Training Loss: 0.0005809523863717914 \t\n",
      "Epoch 5619 \t\t Training Loss: 0.0005809523863717914 \t\n",
      "Epoch 5620 \t\t Training Loss: 0.0005809523281641304 \t\n",
      "Epoch 5621 \t\t Training Loss: 0.0005809523281641304 \t\n",
      "Epoch 5622 \t\t Training Loss: 0.0005809523863717914 \t\n",
      "Epoch 5623 \t\t Training Loss: 0.0005809523281641304 \t\n",
      "Epoch 5624 \t\t Training Loss: 0.0005809523281641304 \t\n",
      "Epoch 5625 \t\t Training Loss: 0.0005809523281641304 \t\n",
      "Epoch 5626 \t\t Training Loss: 0.0005809523281641304 \t\n",
      "Epoch 5627 \t\t Training Loss: 0.0005809523281641304 \t\n",
      "Epoch 5628 \t\t Training Loss: 0.0005809522699564695 \t\n",
      "Epoch 5629 \t\t Training Loss: 0.0005809522699564695 \t\n",
      "Epoch 5630 \t\t Training Loss: 0.0005809523281641304 \t\n",
      "Epoch 5631 \t\t Training Loss: 0.0005809523281641304 \t\n",
      "Epoch 5632 \t\t Training Loss: 0.0005809523281641304 \t\n",
      "Epoch 5633 \t\t Training Loss: 0.0005809523281641304 \t\n",
      "Epoch 5634 \t\t Training Loss: 0.0005809522699564695 \t\n",
      "Epoch 5635 \t\t Training Loss: 0.0005809522699564695 \t\n",
      "Epoch 5636 \t\t Training Loss: 0.0005809522699564695 \t\n",
      "Epoch 5637 \t\t Training Loss: 0.0005809522699564695 \t\n",
      "Epoch 5638 \t\t Training Loss: 0.0005809522699564695 \t\n",
      "Epoch 5639 \t\t Training Loss: 0.0005809522699564695 \t\n",
      "Epoch 5640 \t\t Training Loss: 0.0005809522699564695 \t\n",
      "Epoch 5641 \t\t Training Loss: 0.0005809522117488086 \t\n",
      "Epoch 5642 \t\t Training Loss: 0.0005809522117488086 \t\n",
      "Epoch 5643 \t\t Training Loss: 0.0005809522117488086 \t\n",
      "Epoch 5644 \t\t Training Loss: 0.0005809522117488086 \t\n",
      "Epoch 5645 \t\t Training Loss: 0.0005809522699564695 \t\n",
      "Epoch 5646 \t\t Training Loss: 0.0005809522117488086 \t\n",
      "Epoch 5647 \t\t Training Loss: 0.0005809522117488086 \t\n",
      "Epoch 5648 \t\t Training Loss: 0.0005809522117488086 \t\n",
      "Epoch 5649 \t\t Training Loss: 0.0005809522117488086 \t\n",
      "Epoch 5650 \t\t Training Loss: 0.0005809522117488086 \t\n",
      "Epoch 5651 \t\t Training Loss: 0.0005809522117488086 \t\n",
      "Epoch 5652 \t\t Training Loss: 0.0005809521535411477 \t\n",
      "Epoch 5653 \t\t Training Loss: 0.0005809521535411477 \t\n",
      "Epoch 5654 \t\t Training Loss: 0.0005809521535411477 \t\n",
      "Epoch 5655 \t\t Training Loss: 0.0005809522117488086 \t\n",
      "Epoch 5656 \t\t Training Loss: 0.0005809522117488086 \t\n",
      "Epoch 5657 \t\t Training Loss: 0.0005809521535411477 \t\n",
      "Epoch 5658 \t\t Training Loss: 0.0005809521535411477 \t\n",
      "Epoch 5659 \t\t Training Loss: 0.0005809520953334868 \t\n",
      "Epoch 5660 \t\t Training Loss: 0.0005809520953334868 \t\n",
      "Epoch 5661 \t\t Training Loss: 0.0005809520953334868 \t\n",
      "Epoch 5662 \t\t Training Loss: 0.0005809520371258259 \t\n",
      "Epoch 5663 \t\t Training Loss: 0.0005809520371258259 \t\n",
      "Epoch 5664 \t\t Training Loss: 0.0005809520371258259 \t\n",
      "Epoch 5665 \t\t Training Loss: 0.0005809520371258259 \t\n",
      "Epoch 5666 \t\t Training Loss: 0.0005809520371258259 \t\n",
      "Epoch 5667 \t\t Training Loss: 0.0005809520371258259 \t\n",
      "Epoch 5668 \t\t Training Loss: 0.0005809520371258259 \t\n",
      "Epoch 5669 \t\t Training Loss: 0.0005809520371258259 \t\n",
      "Epoch 5670 \t\t Training Loss: 0.0005809520371258259 \t\n",
      "Epoch 5671 \t\t Training Loss: 0.0005809520371258259 \t\n",
      "Epoch 5672 \t\t Training Loss: 0.0005809520371258259 \t\n",
      "Epoch 5673 \t\t Training Loss: 0.0005809519207105041 \t\n",
      "Epoch 5674 \t\t Training Loss: 0.0005809519207105041 \t\n",
      "Epoch 5675 \t\t Training Loss: 0.0005809518625028431 \t\n",
      "Epoch 5676 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5677 \t\t Training Loss: 0.0005809519207105041 \t\n",
      "Epoch 5678 \t\t Training Loss: 0.0005809519207105041 \t\n",
      "Epoch 5679 \t\t Training Loss: 0.0005809519207105041 \t\n",
      "Epoch 5680 \t\t Training Loss: 0.0005809518625028431 \t\n",
      "Epoch 5681 \t\t Training Loss: 0.0005809519207105041 \t\n",
      "Epoch 5682 \t\t Training Loss: 0.0005809519207105041 \t\n",
      "Epoch 5683 \t\t Training Loss: 0.0005809518625028431 \t\n",
      "Epoch 5684 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5685 \t\t Training Loss: 0.0005809518625028431 \t\n",
      "Epoch 5686 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5687 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5688 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5689 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5690 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5691 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5692 \t\t Training Loss: 0.0005809518042951822 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5693 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5694 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5695 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5696 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5697 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5698 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5699 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5700 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5701 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5702 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5703 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5704 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5705 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5706 \t\t Training Loss: 0.0005809517460875213 \t\n",
      "Epoch 5707 \t\t Training Loss: 0.0005809516878798604 \t\n",
      "Epoch 5708 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5709 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5710 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5711 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5712 \t\t Training Loss: 0.0005809518042951822 \t\n",
      "Epoch 5713 \t\t Training Loss: 0.0005809516878798604 \t\n",
      "Epoch 5714 \t\t Training Loss: 0.0005809516878798604 \t\n",
      "Epoch 5715 \t\t Training Loss: 0.0005809516878798604 \t\n",
      "Epoch 5716 \t\t Training Loss: 0.0005809517460875213 \t\n",
      "Epoch 5717 \t\t Training Loss: 0.0005809516878798604 \t\n",
      "Epoch 5718 \t\t Training Loss: 0.0005809516878798604 \t\n",
      "Epoch 5719 \t\t Training Loss: 0.0005809516878798604 \t\n",
      "Epoch 5720 \t\t Training Loss: 0.0005809516878798604 \t\n",
      "Epoch 5721 \t\t Training Loss: 0.0005809516296721995 \t\n",
      "Epoch 5722 \t\t Training Loss: 0.0005809516296721995 \t\n",
      "Epoch 5723 \t\t Training Loss: 0.0005809516296721995 \t\n",
      "Epoch 5724 \t\t Training Loss: 0.0005809516296721995 \t\n",
      "Epoch 5725 \t\t Training Loss: 0.0005809516296721995 \t\n",
      "Epoch 5726 \t\t Training Loss: 0.0005809516296721995 \t\n",
      "Epoch 5727 \t\t Training Loss: 0.0005809516296721995 \t\n",
      "Epoch 5728 \t\t Training Loss: 0.0005809516296721995 \t\n",
      "Epoch 5729 \t\t Training Loss: 0.0005809516296721995 \t\n",
      "Epoch 5730 \t\t Training Loss: 0.0005809516296721995 \t\n",
      "Epoch 5731 \t\t Training Loss: 0.0005809516296721995 \t\n",
      "Epoch 5732 \t\t Training Loss: 0.0005809516296721995 \t\n",
      "Epoch 5733 \t\t Training Loss: 0.0005809516296721995 \t\n",
      "Epoch 5734 \t\t Training Loss: 0.0005809516296721995 \t\n",
      "Epoch 5735 \t\t Training Loss: 0.0005809515714645386 \t\n",
      "Epoch 5736 \t\t Training Loss: 0.0005809515714645386 \t\n",
      "Epoch 5737 \t\t Training Loss: 0.0005809515714645386 \t\n",
      "Epoch 5738 \t\t Training Loss: 0.0005809515714645386 \t\n",
      "Epoch 5739 \t\t Training Loss: 0.0005809515714645386 \t\n",
      "Epoch 5740 \t\t Training Loss: 0.0005809515714645386 \t\n",
      "Epoch 5741 \t\t Training Loss: 0.0005809515714645386 \t\n",
      "Epoch 5742 \t\t Training Loss: 0.0005809515714645386 \t\n",
      "Epoch 5743 \t\t Training Loss: 0.0005809515714645386 \t\n",
      "Epoch 5744 \t\t Training Loss: 0.0005809515132568777 \t\n",
      "Epoch 5745 \t\t Training Loss: 0.0005809514550492167 \t\n",
      "Epoch 5746 \t\t Training Loss: 0.0005809514550492167 \t\n",
      "Epoch 5747 \t\t Training Loss: 0.0005809514550492167 \t\n",
      "Epoch 5748 \t\t Training Loss: 0.0005809514550492167 \t\n",
      "Epoch 5749 \t\t Training Loss: 0.0005809514550492167 \t\n",
      "Epoch 5750 \t\t Training Loss: 0.0005809514550492167 \t\n",
      "Epoch 5751 \t\t Training Loss: 0.0005809514550492167 \t\n",
      "Epoch 5752 \t\t Training Loss: 0.0005809514550492167 \t\n",
      "Epoch 5753 \t\t Training Loss: 0.0005809514550492167 \t\n",
      "Epoch 5754 \t\t Training Loss: 0.0005809514550492167 \t\n",
      "Epoch 5755 \t\t Training Loss: 0.0005809514550492167 \t\n",
      "Epoch 5756 \t\t Training Loss: 0.0005809514550492167 \t\n",
      "Epoch 5757 \t\t Training Loss: 0.0005809513968415558 \t\n",
      "Epoch 5758 \t\t Training Loss: 0.0005809513968415558 \t\n",
      "Epoch 5759 \t\t Training Loss: 0.0005809514550492167 \t\n",
      "Epoch 5760 \t\t Training Loss: 0.0005809513968415558 \t\n",
      "Epoch 5761 \t\t Training Loss: 0.0005809513968415558 \t\n",
      "Epoch 5762 \t\t Training Loss: 0.0005809513386338949 \t\n",
      "Epoch 5763 \t\t Training Loss: 0.0005809513386338949 \t\n",
      "Epoch 5764 \t\t Training Loss: 0.0005809514550492167 \t\n",
      "Epoch 5765 \t\t Training Loss: 0.0005809513386338949 \t\n",
      "Epoch 5766 \t\t Training Loss: 0.0005809513386338949 \t\n",
      "Epoch 5767 \t\t Training Loss: 0.0005809513386338949 \t\n",
      "Epoch 5768 \t\t Training Loss: 0.0005809513968415558 \t\n",
      "Epoch 5769 \t\t Training Loss: 0.0005809513968415558 \t\n",
      "Epoch 5770 \t\t Training Loss: 0.0005809513968415558 \t\n",
      "Epoch 5771 \t\t Training Loss: 0.0005809513386338949 \t\n",
      "Epoch 5772 \t\t Training Loss: 0.0005809513386338949 \t\n",
      "Epoch 5773 \t\t Training Loss: 0.0005809513386338949 \t\n",
      "Epoch 5774 \t\t Training Loss: 0.0005809513386338949 \t\n",
      "Epoch 5775 \t\t Training Loss: 0.0005809513386338949 \t\n",
      "Epoch 5776 \t\t Training Loss: 0.000580951280426234 \t\n",
      "Epoch 5777 \t\t Training Loss: 0.000580951280426234 \t\n",
      "Epoch 5778 \t\t Training Loss: 0.000580951280426234 \t\n",
      "Epoch 5779 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5780 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5781 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5782 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5783 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5784 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5785 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5786 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5787 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5788 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5789 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5790 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5791 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5792 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5793 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5794 \t\t Training Loss: 0.0005809511640109122 \t\n",
      "Epoch 5795 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5796 \t\t Training Loss: 0.0005809511640109122 \t\n",
      "Epoch 5797 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5798 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5799 \t\t Training Loss: 0.0005809511640109122 \t\n",
      "Epoch 5800 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5801 \t\t Training Loss: 0.0005809512222185731 \t\n",
      "Epoch 5802 \t\t Training Loss: 0.0005809511640109122 \t\n",
      "Epoch 5803 \t\t Training Loss: 0.0005809511640109122 \t\n",
      "Epoch 5804 \t\t Training Loss: 0.0005809511640109122 \t\n",
      "Epoch 5805 \t\t Training Loss: 0.0005809511640109122 \t\n",
      "Epoch 5806 \t\t Training Loss: 0.0005809511640109122 \t\n",
      "Epoch 5807 \t\t Training Loss: 0.0005809511640109122 \t\n",
      "Epoch 5808 \t\t Training Loss: 0.0005809511640109122 \t\n",
      "Epoch 5809 \t\t Training Loss: 0.0005809511640109122 \t\n",
      "Epoch 5810 \t\t Training Loss: 0.0005809511640109122 \t\n",
      "Epoch 5811 \t\t Training Loss: 0.0005809511058032513 \t\n",
      "Epoch 5812 \t\t Training Loss: 0.0005809511058032513 \t\n",
      "Epoch 5813 \t\t Training Loss: 0.0005809510475955904 \t\n",
      "Epoch 5814 \t\t Training Loss: 0.0005809510475955904 \t\n",
      "Epoch 5815 \t\t Training Loss: 0.0005809510475955904 \t\n",
      "Epoch 5816 \t\t Training Loss: 0.0005809510475955904 \t\n",
      "Epoch 5817 \t\t Training Loss: 0.0005809510475955904 \t\n",
      "Epoch 5818 \t\t Training Loss: 0.0005809510475955904 \t\n",
      "Epoch 5819 \t\t Training Loss: 0.0005809509893879294 \t\n",
      "Epoch 5820 \t\t Training Loss: 0.0005809509893879294 \t\n",
      "Epoch 5821 \t\t Training Loss: 0.0005809509893879294 \t\n",
      "Epoch 5822 \t\t Training Loss: 0.0005809509893879294 \t\n",
      "Epoch 5823 \t\t Training Loss: 0.0005809509893879294 \t\n",
      "Epoch 5824 \t\t Training Loss: 0.0005809509893879294 \t\n",
      "Epoch 5825 \t\t Training Loss: 0.0005809508729726076 \t\n",
      "Epoch 5826 \t\t Training Loss: 0.0005809508729726076 \t\n",
      "Epoch 5827 \t\t Training Loss: 0.0005809508729726076 \t\n",
      "Epoch 5828 \t\t Training Loss: 0.0005809508729726076 \t\n",
      "Epoch 5829 \t\t Training Loss: 0.0005809508729726076 \t\n",
      "Epoch 5830 \t\t Training Loss: 0.0005809509311802685 \t\n",
      "Epoch 5831 \t\t Training Loss: 0.0005809508729726076 \t\n",
      "Epoch 5832 \t\t Training Loss: 0.0005809508729726076 \t\n",
      "Epoch 5833 \t\t Training Loss: 0.0005809508729726076 \t\n",
      "Epoch 5834 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5835 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5836 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5837 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5838 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5839 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5840 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5841 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5842 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5843 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5844 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5845 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5846 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5847 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5848 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5849 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5850 \t\t Training Loss: 0.0005809508147649467 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5851 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5852 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5853 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5854 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5855 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5856 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5857 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5858 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5859 \t\t Training Loss: 0.0005809508147649467 \t\n",
      "Epoch 5860 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5861 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5862 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5863 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5864 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5865 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5866 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5867 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5868 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5869 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5870 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5871 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5872 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5873 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5874 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5875 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5876 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5877 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5878 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5879 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5880 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5881 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5882 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5883 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5884 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5885 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5886 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5887 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5888 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5889 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5890 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5891 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5892 \t\t Training Loss: 0.000580950640141964 \t\n",
      "Epoch 5893 \t\t Training Loss: 0.000580950640141964 \t\n",
      "Epoch 5894 \t\t Training Loss: 0.000580950640141964 \t\n",
      "Epoch 5895 \t\t Training Loss: 0.000580950640141964 \t\n",
      "Epoch 5896 \t\t Training Loss: 0.000580950640141964 \t\n",
      "Epoch 5897 \t\t Training Loss: 0.000580950640141964 \t\n",
      "Epoch 5898 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5899 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5900 \t\t Training Loss: 0.0005809507565572858 \t\n",
      "Epoch 5901 \t\t Training Loss: 0.000580950640141964 \t\n",
      "Epoch 5902 \t\t Training Loss: 0.000580950640141964 \t\n",
      "Epoch 5903 \t\t Training Loss: 0.000580950640141964 \t\n",
      "Epoch 5904 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5905 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5906 \t\t Training Loss: 0.000580950640141964 \t\n",
      "Epoch 5907 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5908 \t\t Training Loss: 0.000580950640141964 \t\n",
      "Epoch 5909 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5910 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5911 \t\t Training Loss: 0.000580950640141964 \t\n",
      "Epoch 5912 \t\t Training Loss: 0.000580950640141964 \t\n",
      "Epoch 5913 \t\t Training Loss: 0.000580950640141964 \t\n",
      "Epoch 5914 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5915 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5916 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5917 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5918 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5919 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5920 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5921 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5922 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5923 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5924 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5925 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5926 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5927 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5928 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5929 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5930 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5931 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5932 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5933 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5934 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5935 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5936 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5937 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5938 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5939 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5940 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5941 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5942 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5943 \t\t Training Loss: 0.000580950581934303 \t\n",
      "Epoch 5944 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5945 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5946 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5947 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5948 \t\t Training Loss: 0.0005809505237266421 \t\n",
      "Epoch 5949 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5950 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5951 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5952 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5953 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5954 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5955 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5956 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5957 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5958 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5959 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5960 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5961 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5962 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5963 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5964 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5965 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5966 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5967 \t\t Training Loss: 0.0005809503491036594 \t\n",
      "Epoch 5968 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5969 \t\t Training Loss: 0.0005809502908959985 \t\n",
      "Epoch 5970 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5971 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5972 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5973 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5974 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5975 \t\t Training Loss: 0.0005809502908959985 \t\n",
      "Epoch 5976 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5977 \t\t Training Loss: 0.0005809502908959985 \t\n",
      "Epoch 5978 \t\t Training Loss: 0.0005809502908959985 \t\n",
      "Epoch 5979 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5980 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5981 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5982 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5983 \t\t Training Loss: 0.0005809504655189812 \t\n",
      "Epoch 5984 \t\t Training Loss: 0.0005809502326883376 \t\n",
      "Epoch 5985 \t\t Training Loss: 0.0005809502908959985 \t\n",
      "Epoch 5986 \t\t Training Loss: 0.0005809502908959985 \t\n",
      "Epoch 5987 \t\t Training Loss: 0.0005809502908959985 \t\n",
      "Epoch 5988 \t\t Training Loss: 0.0005809502908959985 \t\n",
      "Epoch 5989 \t\t Training Loss: 0.0005809502326883376 \t\n",
      "Epoch 5990 \t\t Training Loss: 0.0005809502326883376 \t\n",
      "Epoch 5991 \t\t Training Loss: 0.0005809502326883376 \t\n",
      "Epoch 5992 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 5993 \t\t Training Loss: 0.0005809502326883376 \t\n",
      "Epoch 5994 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 5995 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 5996 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 5997 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 5998 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 5999 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6000 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6001 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6002 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6003 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6004 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6005 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6006 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6007 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6008 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6009 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6010 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6011 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6012 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6013 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6014 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6015 \t\t Training Loss: 0.0005809500580653548 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6016 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6017 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6018 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6019 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6020 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6021 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6022 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6023 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6024 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6025 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6026 \t\t Training Loss: 0.0005809501744806767 \t\n",
      "Epoch 6027 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6028 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6029 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6030 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6031 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6032 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6033 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6034 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6035 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6036 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6037 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6038 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6039 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6040 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6041 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6042 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6043 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6044 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6045 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6046 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6047 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6048 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6049 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6050 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6051 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6052 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6053 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6054 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6055 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6056 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6057 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6058 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6059 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6060 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6061 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6062 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6063 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6064 \t\t Training Loss: 0.0005809500580653548 \t\n",
      "Epoch 6065 \t\t Training Loss: 0.000580949941650033 \t\n",
      "Epoch 6066 \t\t Training Loss: 0.0005809498834423721 \t\n",
      "Epoch 6067 \t\t Training Loss: 0.0005809498834423721 \t\n",
      "Epoch 6068 \t\t Training Loss: 0.0005809498834423721 \t\n",
      "Epoch 6069 \t\t Training Loss: 0.0005809498834423721 \t\n",
      "Epoch 6070 \t\t Training Loss: 0.0005809498834423721 \t\n",
      "Epoch 6071 \t\t Training Loss: 0.0005809498252347112 \t\n",
      "Epoch 6072 \t\t Training Loss: 0.0005809498252347112 \t\n",
      "Epoch 6073 \t\t Training Loss: 0.0005809498252347112 \t\n",
      "Epoch 6074 \t\t Training Loss: 0.0005809498252347112 \t\n",
      "Epoch 6075 \t\t Training Loss: 0.0005809498252347112 \t\n",
      "Epoch 6076 \t\t Training Loss: 0.0005809498252347112 \t\n",
      "Epoch 6077 \t\t Training Loss: 0.0005809497670270503 \t\n",
      "Epoch 6078 \t\t Training Loss: 0.0005809497670270503 \t\n",
      "Epoch 6079 \t\t Training Loss: 0.0005809497670270503 \t\n",
      "Epoch 6080 \t\t Training Loss: 0.0005809497670270503 \t\n",
      "Epoch 6081 \t\t Training Loss: 0.0005809497670270503 \t\n",
      "Epoch 6082 \t\t Training Loss: 0.0005809497670270503 \t\n",
      "Epoch 6083 \t\t Training Loss: 0.0005809497670270503 \t\n",
      "Epoch 6084 \t\t Training Loss: 0.0005809497670270503 \t\n",
      "Epoch 6085 \t\t Training Loss: 0.0005809497670270503 \t\n",
      "Epoch 6086 \t\t Training Loss: 0.0005809497670270503 \t\n",
      "Epoch 6087 \t\t Training Loss: 0.0005809497670270503 \t\n",
      "Epoch 6088 \t\t Training Loss: 0.0005809497670270503 \t\n",
      "Epoch 6089 \t\t Training Loss: 0.0005809497670270503 \t\n",
      "Epoch 6090 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6091 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6092 \t\t Training Loss: 0.0005809497670270503 \t\n",
      "Epoch 6093 \t\t Training Loss: 0.0005809497670270503 \t\n",
      "Epoch 6094 \t\t Training Loss: 0.0005809497670270503 \t\n",
      "Epoch 6095 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6096 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6097 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6098 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6099 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6100 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6101 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6102 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6103 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6104 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6105 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6106 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6107 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6108 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6109 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6110 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6111 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6112 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6113 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6114 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6115 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6116 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6117 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6118 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6119 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6120 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6121 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6122 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6123 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6124 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6125 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6126 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6127 \t\t Training Loss: 0.0005809496506117284 \t\n",
      "Epoch 6128 \t\t Training Loss: 0.0005809495341964066 \t\n",
      "Epoch 6129 \t\t Training Loss: 0.0005809495341964066 \t\n",
      "Epoch 6130 \t\t Training Loss: 0.0005809495341964066 \t\n",
      "Epoch 6131 \t\t Training Loss: 0.0005809495341964066 \t\n",
      "Epoch 6132 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6133 \t\t Training Loss: 0.0005809495341964066 \t\n",
      "Epoch 6134 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6135 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6136 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6137 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6138 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6139 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6140 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6141 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6142 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6143 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6144 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6145 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6146 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6147 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6148 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6149 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6150 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6151 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6152 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6153 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6154 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6155 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6156 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6157 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6158 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6159 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6160 \t\t Training Loss: 0.0005809494759887457 \t\n",
      "Epoch 6161 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6162 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6163 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6164 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6165 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6166 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6167 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6168 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6169 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6170 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6171 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6172 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6173 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6174 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6175 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6176 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6177 \t\t Training Loss: 0.000580949243158102 \t\n",
      "Epoch 6178 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6179 \t\t Training Loss: 0.000580949243158102 \t\n",
      "Epoch 6180 \t\t Training Loss: 0.000580949243158102 \t\n",
      "Epoch 6181 \t\t Training Loss: 0.000580949243158102 \t\n",
      "Epoch 6182 \t\t Training Loss: 0.0005809493595734239 \t\n",
      "Epoch 6183 \t\t Training Loss: 0.000580949243158102 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6184 \t\t Training Loss: 0.000580949243158102 \t\n",
      "Epoch 6185 \t\t Training Loss: 0.0005809491849504411 \t\n",
      "Epoch 6186 \t\t Training Loss: 0.0005809491849504411 \t\n",
      "Epoch 6187 \t\t Training Loss: 0.0005809491849504411 \t\n",
      "Epoch 6188 \t\t Training Loss: 0.0005809491849504411 \t\n",
      "Epoch 6189 \t\t Training Loss: 0.000580949243158102 \t\n",
      "Epoch 6190 \t\t Training Loss: 0.0005809491849504411 \t\n",
      "Epoch 6191 \t\t Training Loss: 0.0005809491849504411 \t\n",
      "Epoch 6192 \t\t Training Loss: 0.0005809491849504411 \t\n",
      "Epoch 6193 \t\t Training Loss: 0.0005809491849504411 \t\n",
      "Epoch 6194 \t\t Training Loss: 0.0005809491849504411 \t\n",
      "Epoch 6195 \t\t Training Loss: 0.0005809491849504411 \t\n",
      "Epoch 6196 \t\t Training Loss: 0.0005809491849504411 \t\n",
      "Epoch 6197 \t\t Training Loss: 0.0005809491849504411 \t\n",
      "Epoch 6198 \t\t Training Loss: 0.0005809491849504411 \t\n",
      "Epoch 6199 \t\t Training Loss: 0.0005809491849504411 \t\n",
      "Epoch 6200 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6201 \t\t Training Loss: 0.0005809491849504411 \t\n",
      "Epoch 6202 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6203 \t\t Training Loss: 0.0005809491849504411 \t\n",
      "Epoch 6204 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6205 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6206 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6207 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6208 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6209 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6210 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6211 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6212 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6213 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6214 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6215 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6216 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6217 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6218 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6219 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6220 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6221 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6222 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6223 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6224 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6225 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6226 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6227 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6228 \t\t Training Loss: 0.0005809490685351193 \t\n",
      "Epoch 6229 \t\t Training Loss: 0.0005809489521197975 \t\n",
      "Epoch 6230 \t\t Training Loss: 0.0005809489521197975 \t\n",
      "Epoch 6231 \t\t Training Loss: 0.0005809489521197975 \t\n",
      "Epoch 6232 \t\t Training Loss: 0.0005809489521197975 \t\n",
      "Epoch 6233 \t\t Training Loss: 0.0005809489521197975 \t\n",
      "Epoch 6234 \t\t Training Loss: 0.0005809489521197975 \t\n",
      "Epoch 6235 \t\t Training Loss: 0.0005809489521197975 \t\n",
      "Epoch 6236 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6237 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6238 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6239 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6240 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6241 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6242 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6243 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6244 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6245 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6246 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6247 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6248 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6249 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6250 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6251 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6252 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6253 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6254 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6255 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6256 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6257 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6258 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6259 \t\t Training Loss: 0.0005809488939121366 \t\n",
      "Epoch 6260 \t\t Training Loss: 0.0005809487774968147 \t\n",
      "Epoch 6261 \t\t Training Loss: 0.0005809487774968147 \t\n",
      "Epoch 6262 \t\t Training Loss: 0.0005809487774968147 \t\n",
      "Epoch 6263 \t\t Training Loss: 0.0005809487774968147 \t\n",
      "Epoch 6264 \t\t Training Loss: 0.0005809487774968147 \t\n",
      "Epoch 6265 \t\t Training Loss: 0.0005809487774968147 \t\n",
      "Epoch 6266 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6267 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6268 \t\t Training Loss: 0.0005809487774968147 \t\n",
      "Epoch 6269 \t\t Training Loss: 0.0005809487774968147 \t\n",
      "Epoch 6270 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6271 \t\t Training Loss: 0.0005809487774968147 \t\n",
      "Epoch 6272 \t\t Training Loss: 0.0005809487774968147 \t\n",
      "Epoch 6273 \t\t Training Loss: 0.0005809487774968147 \t\n",
      "Epoch 6274 \t\t Training Loss: 0.0005809487774968147 \t\n",
      "Epoch 6275 \t\t Training Loss: 0.0005809487774968147 \t\n",
      "Epoch 6276 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6277 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6278 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6279 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6280 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6281 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6282 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6283 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6284 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6285 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6286 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6287 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6288 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6289 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6290 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6291 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6292 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6293 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6294 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6295 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6296 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6297 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6298 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6299 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6300 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6301 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6302 \t\t Training Loss: 0.000580948602873832 \t\n",
      "Epoch 6303 \t\t Training Loss: 0.000580948602873832 \t\n",
      "Epoch 6304 \t\t Training Loss: 0.000580948602873832 \t\n",
      "Epoch 6305 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6306 \t\t Training Loss: 0.000580948602873832 \t\n",
      "Epoch 6307 \t\t Training Loss: 0.000580948602873832 \t\n",
      "Epoch 6308 \t\t Training Loss: 0.000580948602873832 \t\n",
      "Epoch 6309 \t\t Training Loss: 0.0005809486610814929 \t\n",
      "Epoch 6310 \t\t Training Loss: 0.000580948602873832 \t\n",
      "Epoch 6311 \t\t Training Loss: 0.000580948602873832 \t\n",
      "Epoch 6312 \t\t Training Loss: 0.000580948602873832 \t\n",
      "Epoch 6313 \t\t Training Loss: 0.000580948602873832 \t\n",
      "Epoch 6314 \t\t Training Loss: 0.000580948602873832 \t\n",
      "Epoch 6315 \t\t Training Loss: 0.0005809485446661711 \t\n",
      "Epoch 6316 \t\t Training Loss: 0.0005809485446661711 \t\n",
      "Epoch 6317 \t\t Training Loss: 0.000580948602873832 \t\n",
      "Epoch 6318 \t\t Training Loss: 0.0005809485446661711 \t\n",
      "Epoch 6319 \t\t Training Loss: 0.0005809484864585102 \t\n",
      "Epoch 6320 \t\t Training Loss: 0.0005809484864585102 \t\n",
      "Epoch 6321 \t\t Training Loss: 0.0005809484864585102 \t\n",
      "Epoch 6322 \t\t Training Loss: 0.0005809484864585102 \t\n",
      "Epoch 6323 \t\t Training Loss: 0.0005809484864585102 \t\n",
      "Epoch 6324 \t\t Training Loss: 0.0005809484864585102 \t\n",
      "Epoch 6325 \t\t Training Loss: 0.0005809484864585102 \t\n",
      "Epoch 6326 \t\t Training Loss: 0.0005809484864585102 \t\n",
      "Epoch 6327 \t\t Training Loss: 0.0005809484864585102 \t\n",
      "Epoch 6328 \t\t Training Loss: 0.0005809484864585102 \t\n",
      "Epoch 6329 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6330 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6331 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6332 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6333 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6334 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6335 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6336 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6337 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6338 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6339 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6340 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6341 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6342 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6343 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6344 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6345 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6346 \t\t Training Loss: 0.0005809483700431883 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6347 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6348 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6349 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6350 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6351 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6352 \t\t Training Loss: 0.0005809483700431883 \t\n",
      "Epoch 6353 \t\t Training Loss: 0.0005809482536278665 \t\n",
      "Epoch 6354 \t\t Training Loss: 0.0005809482536278665 \t\n",
      "Epoch 6355 \t\t Training Loss: 0.0005809482536278665 \t\n",
      "Epoch 6356 \t\t Training Loss: 0.0005809482536278665 \t\n",
      "Epoch 6357 \t\t Training Loss: 0.0005809482536278665 \t\n",
      "Epoch 6358 \t\t Training Loss: 0.0005809482536278665 \t\n",
      "Epoch 6359 \t\t Training Loss: 0.0005809482536278665 \t\n",
      "Epoch 6360 \t\t Training Loss: 0.0005809482536278665 \t\n",
      "Epoch 6361 \t\t Training Loss: 0.0005809482536278665 \t\n",
      "Epoch 6362 \t\t Training Loss: 0.0005809482536278665 \t\n",
      "Epoch 6363 \t\t Training Loss: 0.0005809481954202056 \t\n",
      "Epoch 6364 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6365 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6366 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6367 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6368 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6369 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6370 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6371 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6372 \t\t Training Loss: 0.0005809481954202056 \t\n",
      "Epoch 6373 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6374 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6375 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6376 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6377 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6378 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6379 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6380 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6381 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6382 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6383 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6384 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6385 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6386 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6387 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6388 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6389 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6390 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6391 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6392 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6393 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6394 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6395 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6396 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6397 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6398 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6399 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6400 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6401 \t\t Training Loss: 0.0005809480790048838 \t\n",
      "Epoch 6402 \t\t Training Loss: 0.0005809479625895619 \t\n",
      "Epoch 6403 \t\t Training Loss: 0.0005809479625895619 \t\n",
      "Epoch 6404 \t\t Training Loss: 0.0005809479625895619 \t\n",
      "Epoch 6405 \t\t Training Loss: 0.0005809479625895619 \t\n",
      "Epoch 6406 \t\t Training Loss: 0.0005809479625895619 \t\n",
      "Epoch 6407 \t\t Training Loss: 0.0005809479625895619 \t\n",
      "Epoch 6408 \t\t Training Loss: 0.0005809479625895619 \t\n",
      "Epoch 6409 \t\t Training Loss: 0.0005809479625895619 \t\n",
      "Epoch 6410 \t\t Training Loss: 0.000580947904381901 \t\n",
      "Epoch 6411 \t\t Training Loss: 0.0005809479625895619 \t\n",
      "Epoch 6412 \t\t Training Loss: 0.0005809479625895619 \t\n",
      "Epoch 6413 \t\t Training Loss: 0.0005809479625895619 \t\n",
      "Epoch 6414 \t\t Training Loss: 0.0005809479625895619 \t\n",
      "Epoch 6415 \t\t Training Loss: 0.0005809479625895619 \t\n",
      "Epoch 6416 \t\t Training Loss: 0.0005809479625895619 \t\n",
      "Epoch 6417 \t\t Training Loss: 0.0005809478461742401 \t\n",
      "Epoch 6418 \t\t Training Loss: 0.0005809479625895619 \t\n",
      "Epoch 6419 \t\t Training Loss: 0.0005809478461742401 \t\n",
      "Epoch 6420 \t\t Training Loss: 0.0005809478461742401 \t\n",
      "Epoch 6421 \t\t Training Loss: 0.0005809478461742401 \t\n",
      "Epoch 6422 \t\t Training Loss: 0.0005809478461742401 \t\n",
      "Epoch 6423 \t\t Training Loss: 0.0005809478461742401 \t\n",
      "Epoch 6424 \t\t Training Loss: 0.0005809478461742401 \t\n",
      "Epoch 6425 \t\t Training Loss: 0.0005809478461742401 \t\n",
      "Epoch 6426 \t\t Training Loss: 0.0005809477879665792 \t\n",
      "Epoch 6427 \t\t Training Loss: 0.0005809478461742401 \t\n",
      "Epoch 6428 \t\t Training Loss: 0.0005809477879665792 \t\n",
      "Epoch 6429 \t\t Training Loss: 0.0005809477879665792 \t\n",
      "Epoch 6430 \t\t Training Loss: 0.0005809477879665792 \t\n",
      "Epoch 6431 \t\t Training Loss: 0.0005809477879665792 \t\n",
      "Epoch 6432 \t\t Training Loss: 0.0005809476715512574 \t\n",
      "Epoch 6433 \t\t Training Loss: 0.0005809477879665792 \t\n",
      "Epoch 6434 \t\t Training Loss: 0.0005809477879665792 \t\n",
      "Epoch 6435 \t\t Training Loss: 0.0005809476715512574 \t\n",
      "Epoch 6436 \t\t Training Loss: 0.0005809476715512574 \t\n",
      "Epoch 6437 \t\t Training Loss: 0.0005809476715512574 \t\n",
      "Epoch 6438 \t\t Training Loss: 0.0005809476715512574 \t\n",
      "Epoch 6439 \t\t Training Loss: 0.0005809476715512574 \t\n",
      "Epoch 6440 \t\t Training Loss: 0.0005809476715512574 \t\n",
      "Epoch 6441 \t\t Training Loss: 0.0005809476715512574 \t\n",
      "Epoch 6442 \t\t Training Loss: 0.0005809476715512574 \t\n",
      "Epoch 6443 \t\t Training Loss: 0.0005809476715512574 \t\n",
      "Epoch 6444 \t\t Training Loss: 0.0005809476133435965 \t\n",
      "Epoch 6445 \t\t Training Loss: 0.0005809476715512574 \t\n",
      "Epoch 6446 \t\t Training Loss: 0.0005809476715512574 \t\n",
      "Epoch 6447 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6448 \t\t Training Loss: 0.0005809476133435965 \t\n",
      "Epoch 6449 \t\t Training Loss: 0.0005809476133435965 \t\n",
      "Epoch 6450 \t\t Training Loss: 0.0005809476133435965 \t\n",
      "Epoch 6451 \t\t Training Loss: 0.0005809476133435965 \t\n",
      "Epoch 6452 \t\t Training Loss: 0.0005809476133435965 \t\n",
      "Epoch 6453 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6454 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6455 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6456 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6457 \t\t Training Loss: 0.0005809476133435965 \t\n",
      "Epoch 6458 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6459 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6460 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6461 \t\t Training Loss: 0.0005809476133435965 \t\n",
      "Epoch 6462 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6463 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6464 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6465 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6466 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6467 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6468 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6469 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6470 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6471 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6472 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6473 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6474 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6475 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6476 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6477 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6478 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6479 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6480 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6481 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6482 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6483 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6484 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6485 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6486 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6487 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6488 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6489 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6490 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6491 \t\t Training Loss: 0.0005809474969282746 \t\n",
      "Epoch 6492 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6493 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6494 \t\t Training Loss: 0.0005809474969282746 \t\n",
      "Epoch 6495 \t\t Training Loss: 0.0005809474969282746 \t\n",
      "Epoch 6496 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6497 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6498 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6499 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6500 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6501 \t\t Training Loss: 0.0005809475551359355 \t\n",
      "Epoch 6502 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6503 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6504 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6505 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6506 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6507 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6508 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6509 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6510 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6511 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6512 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6513 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6514 \t\t Training Loss: 0.0005809474387206137 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6515 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6516 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6517 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6518 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6519 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6520 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6521 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6522 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6523 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6524 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6525 \t\t Training Loss: 0.0005809473805129528 \t\n",
      "Epoch 6526 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6527 \t\t Training Loss: 0.0005809473805129528 \t\n",
      "Epoch 6528 \t\t Training Loss: 0.0005809474387206137 \t\n",
      "Epoch 6529 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6530 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6531 \t\t Training Loss: 0.0005809473805129528 \t\n",
      "Epoch 6532 \t\t Training Loss: 0.0005809473805129528 \t\n",
      "Epoch 6533 \t\t Training Loss: 0.0005809473805129528 \t\n",
      "Epoch 6534 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6535 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6536 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6537 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6538 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6539 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6540 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6541 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6542 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6543 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6544 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6545 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6546 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6547 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6548 \t\t Training Loss: 0.000580947264097631 \t\n",
      "Epoch 6549 \t\t Training Loss: 0.000580947264097631 \t\n",
      "Epoch 6550 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6551 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6552 \t\t Training Loss: 0.0005809473223052919 \t\n",
      "Epoch 6553 \t\t Training Loss: 0.000580947264097631 \t\n",
      "Epoch 6554 \t\t Training Loss: 0.0005809472058899701 \t\n",
      "Epoch 6555 \t\t Training Loss: 0.0005809472058899701 \t\n",
      "Epoch 6556 \t\t Training Loss: 0.0005809472058899701 \t\n",
      "Epoch 6557 \t\t Training Loss: 0.0005809472058899701 \t\n",
      "Epoch 6558 \t\t Training Loss: 0.0005809472058899701 \t\n",
      "Epoch 6559 \t\t Training Loss: 0.0005809472058899701 \t\n",
      "Epoch 6560 \t\t Training Loss: 0.0005809472058899701 \t\n",
      "Epoch 6561 \t\t Training Loss: 0.0005809472058899701 \t\n",
      "Epoch 6562 \t\t Training Loss: 0.0005809472058899701 \t\n",
      "Epoch 6563 \t\t Training Loss: 0.0005809472058899701 \t\n",
      "Epoch 6564 \t\t Training Loss: 0.0005809472058899701 \t\n",
      "Epoch 6565 \t\t Training Loss: 0.0005809472058899701 \t\n",
      "Epoch 6566 \t\t Training Loss: 0.0005809472058899701 \t\n",
      "Epoch 6567 \t\t Training Loss: 0.0005809472058899701 \t\n",
      "Epoch 6568 \t\t Training Loss: 0.0005809472058899701 \t\n",
      "Epoch 6569 \t\t Training Loss: 0.0005809470894746482 \t\n",
      "Epoch 6570 \t\t Training Loss: 0.0005809472058899701 \t\n",
      "Epoch 6571 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6572 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6573 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6574 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6575 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6576 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6577 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6578 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6579 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6580 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6581 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6582 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6583 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6584 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6585 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6586 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6587 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6588 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6589 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6590 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6591 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6592 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6593 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6594 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6595 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6596 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6597 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6598 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6599 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6600 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6601 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6602 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6603 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6604 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6605 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6606 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6607 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6608 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6609 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6610 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6611 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6612 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6613 \t\t Training Loss: 0.0005809470312669873 \t\n",
      "Epoch 6614 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6615 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6616 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6617 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6618 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6619 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6620 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6621 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6622 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6623 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6624 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6625 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6626 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6627 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6628 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6629 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6630 \t\t Training Loss: 0.0005809469730593264 \t\n",
      "Epoch 6631 \t\t Training Loss: 0.0005809469148516655 \t\n",
      "Epoch 6632 \t\t Training Loss: 0.0005809469148516655 \t\n",
      "Epoch 6633 \t\t Training Loss: 0.0005809469148516655 \t\n",
      "Epoch 6634 \t\t Training Loss: 0.0005809469148516655 \t\n",
      "Epoch 6635 \t\t Training Loss: 0.0005809469148516655 \t\n",
      "Epoch 6636 \t\t Training Loss: 0.0005809469148516655 \t\n",
      "Epoch 6637 \t\t Training Loss: 0.0005809469148516655 \t\n",
      "Epoch 6638 \t\t Training Loss: 0.0005809468566440046 \t\n",
      "Epoch 6639 \t\t Training Loss: 0.0005809469148516655 \t\n",
      "Epoch 6640 \t\t Training Loss: 0.0005809468566440046 \t\n",
      "Epoch 6641 \t\t Training Loss: 0.0005809469148516655 \t\n",
      "Epoch 6642 \t\t Training Loss: 0.0005809468566440046 \t\n",
      "Epoch 6643 \t\t Training Loss: 0.0005809467984363437 \t\n",
      "Epoch 6644 \t\t Training Loss: 0.0005809467984363437 \t\n",
      "Epoch 6645 \t\t Training Loss: 0.0005809467984363437 \t\n",
      "Epoch 6646 \t\t Training Loss: 0.0005809467984363437 \t\n",
      "Epoch 6647 \t\t Training Loss: 0.0005809467984363437 \t\n",
      "Epoch 6648 \t\t Training Loss: 0.0005809467984363437 \t\n",
      "Epoch 6649 \t\t Training Loss: 0.0005809467984363437 \t\n",
      "Epoch 6650 \t\t Training Loss: 0.0005809467984363437 \t\n",
      "Epoch 6651 \t\t Training Loss: 0.0005809467984363437 \t\n",
      "Epoch 6652 \t\t Training Loss: 0.0005809467984363437 \t\n",
      "Epoch 6653 \t\t Training Loss: 0.0005809467984363437 \t\n",
      "Epoch 6654 \t\t Training Loss: 0.0005809467984363437 \t\n",
      "Epoch 6655 \t\t Training Loss: 0.0005809467984363437 \t\n",
      "Epoch 6656 \t\t Training Loss: 0.0005809467984363437 \t\n",
      "Epoch 6657 \t\t Training Loss: 0.0005809467984363437 \t\n",
      "Epoch 6658 \t\t Training Loss: 0.0005809467402286828 \t\n",
      "Epoch 6659 \t\t Training Loss: 0.0005809467402286828 \t\n",
      "Epoch 6660 \t\t Training Loss: 0.0005809466820210218 \t\n",
      "Epoch 6661 \t\t Training Loss: 0.0005809466820210218 \t\n",
      "Epoch 6662 \t\t Training Loss: 0.0005809466820210218 \t\n",
      "Epoch 6663 \t\t Training Loss: 0.0005809466820210218 \t\n",
      "Epoch 6664 \t\t Training Loss: 0.0005809466820210218 \t\n",
      "Epoch 6665 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6666 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6667 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6668 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6669 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6670 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6671 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6672 \t\t Training Loss: 0.0005809466820210218 \t\n",
      "Epoch 6673 \t\t Training Loss: 0.0005809466820210218 \t\n",
      "Epoch 6674 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6675 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6676 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6677 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6678 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6679 \t\t Training Loss: 0.0005809466238133609 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6680 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6681 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6682 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6683 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6684 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6685 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6686 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6687 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6688 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6689 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6690 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6691 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6692 \t\t Training Loss: 0.0005809466820210218 \t\n",
      "Epoch 6693 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6694 \t\t Training Loss: 0.0005809466820210218 \t\n",
      "Epoch 6695 \t\t Training Loss: 0.0005809466820210218 \t\n",
      "Epoch 6696 \t\t Training Loss: 0.0005809466820210218 \t\n",
      "Epoch 6697 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6698 \t\t Training Loss: 0.0005809466820210218 \t\n",
      "Epoch 6699 \t\t Training Loss: 0.0005809466820210218 \t\n",
      "Epoch 6700 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6701 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6702 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6703 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6704 \t\t Training Loss: 0.0005809466238133609 \t\n",
      "Epoch 6705 \t\t Training Loss: 0.0005809465073980391 \t\n",
      "Epoch 6706 \t\t Training Loss: 0.0005809465073980391 \t\n",
      "Epoch 6707 \t\t Training Loss: 0.0005809465073980391 \t\n",
      "Epoch 6708 \t\t Training Loss: 0.0005809464491903782 \t\n",
      "Epoch 6709 \t\t Training Loss: 0.0005809465073980391 \t\n",
      "Epoch 6710 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6711 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6712 \t\t Training Loss: 0.0005809464491903782 \t\n",
      "Epoch 6713 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6714 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6715 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6716 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6717 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6718 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6719 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6720 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6721 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6722 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6723 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6724 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6725 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6726 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6727 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6728 \t\t Training Loss: 0.0005809463909827173 \t\n",
      "Epoch 6729 \t\t Training Loss: 0.0005809462745673954 \t\n",
      "Epoch 6730 \t\t Training Loss: 0.0005809462745673954 \t\n",
      "Epoch 6731 \t\t Training Loss: 0.0005809462745673954 \t\n",
      "Epoch 6732 \t\t Training Loss: 0.0005809462745673954 \t\n",
      "Epoch 6733 \t\t Training Loss: 0.0005809463327750564 \t\n",
      "Epoch 6734 \t\t Training Loss: 0.0005809463327750564 \t\n",
      "Epoch 6735 \t\t Training Loss: 0.0005809462745673954 \t\n",
      "Epoch 6736 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6737 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6738 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6739 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6740 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6741 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6742 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6743 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6744 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6745 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6746 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6747 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6748 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6749 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6750 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6751 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6752 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6753 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6754 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6755 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6756 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6757 \t\t Training Loss: 0.0005809462163597345 \t\n",
      "Epoch 6758 \t\t Training Loss: 0.0005809461581520736 \t\n",
      "Epoch 6759 \t\t Training Loss: 0.0005809461581520736 \t\n",
      "Epoch 6760 \t\t Training Loss: 0.0005809460999444127 \t\n",
      "Epoch 6761 \t\t Training Loss: 0.0005809460417367518 \t\n",
      "Epoch 6762 \t\t Training Loss: 0.0005809460417367518 \t\n",
      "Epoch 6763 \t\t Training Loss: 0.0005809460417367518 \t\n",
      "Epoch 6764 \t\t Training Loss: 0.0005809460417367518 \t\n",
      "Epoch 6765 \t\t Training Loss: 0.0005809460417367518 \t\n",
      "Epoch 6766 \t\t Training Loss: 0.0005809460417367518 \t\n",
      "Epoch 6767 \t\t Training Loss: 0.0005809460417367518 \t\n",
      "Epoch 6768 \t\t Training Loss: 0.0005809460417367518 \t\n",
      "Epoch 6769 \t\t Training Loss: 0.0005809460417367518 \t\n",
      "Epoch 6770 \t\t Training Loss: 0.0005809460417367518 \t\n",
      "Epoch 6771 \t\t Training Loss: 0.0005809460417367518 \t\n",
      "Epoch 6772 \t\t Training Loss: 0.0005809460417367518 \t\n",
      "Epoch 6773 \t\t Training Loss: 0.0005809460417367518 \t\n",
      "Epoch 6774 \t\t Training Loss: 0.0005809459835290909 \t\n",
      "Epoch 6775 \t\t Training Loss: 0.0005809459835290909 \t\n",
      "Epoch 6776 \t\t Training Loss: 0.00058094592532143 \t\n",
      "Epoch 6777 \t\t Training Loss: 0.00058094592532143 \t\n",
      "Epoch 6778 \t\t Training Loss: 0.00058094592532143 \t\n",
      "Epoch 6779 \t\t Training Loss: 0.00058094592532143 \t\n",
      "Epoch 6780 \t\t Training Loss: 0.00058094592532143 \t\n",
      "Epoch 6781 \t\t Training Loss: 0.00058094592532143 \t\n",
      "Epoch 6782 \t\t Training Loss: 0.00058094592532143 \t\n",
      "Epoch 6783 \t\t Training Loss: 0.00058094592532143 \t\n",
      "Epoch 6784 \t\t Training Loss: 0.00058094592532143 \t\n",
      "Epoch 6785 \t\t Training Loss: 0.00058094592532143 \t\n",
      "Epoch 6786 \t\t Training Loss: 0.00058094592532143 \t\n",
      "Epoch 6787 \t\t Training Loss: 0.00058094592532143 \t\n",
      "Epoch 6788 \t\t Training Loss: 0.00058094592532143 \t\n",
      "Epoch 6789 \t\t Training Loss: 0.0005809458671137691 \t\n",
      "Epoch 6790 \t\t Training Loss: 0.0005809458671137691 \t\n",
      "Epoch 6791 \t\t Training Loss: 0.0005809458671137691 \t\n",
      "Epoch 6792 \t\t Training Loss: 0.0005809458671137691 \t\n",
      "Epoch 6793 \t\t Training Loss: 0.0005809458671137691 \t\n",
      "Epoch 6794 \t\t Training Loss: 0.0005809458671137691 \t\n",
      "Epoch 6795 \t\t Training Loss: 0.0005809458671137691 \t\n",
      "Epoch 6796 \t\t Training Loss: 0.0005809458671137691 \t\n",
      "Epoch 6797 \t\t Training Loss: 0.0005809458089061081 \t\n",
      "Epoch 6798 \t\t Training Loss: 0.0005809458671137691 \t\n",
      "Epoch 6799 \t\t Training Loss: 0.0005809458671137691 \t\n",
      "Epoch 6800 \t\t Training Loss: 0.0005809458671137691 \t\n",
      "Epoch 6801 \t\t Training Loss: 0.0005809458671137691 \t\n",
      "Epoch 6802 \t\t Training Loss: 0.0005809458671137691 \t\n",
      "Epoch 6803 \t\t Training Loss: 0.0005809458671137691 \t\n",
      "Epoch 6804 \t\t Training Loss: 0.0005809458089061081 \t\n",
      "Epoch 6805 \t\t Training Loss: 0.0005809458671137691 \t\n",
      "Epoch 6806 \t\t Training Loss: 0.0005809458089061081 \t\n",
      "Epoch 6807 \t\t Training Loss: 0.0005809458089061081 \t\n",
      "Epoch 6808 \t\t Training Loss: 0.0005809458089061081 \t\n",
      "Epoch 6809 \t\t Training Loss: 0.0005809458089061081 \t\n",
      "Epoch 6810 \t\t Training Loss: 0.0005809458089061081 \t\n",
      "Epoch 6811 \t\t Training Loss: 0.0005809458089061081 \t\n",
      "Epoch 6812 \t\t Training Loss: 0.0005809458089061081 \t\n",
      "Epoch 6813 \t\t Training Loss: 0.0005809458089061081 \t\n",
      "Epoch 6814 \t\t Training Loss: 0.0005809458089061081 \t\n",
      "Epoch 6815 \t\t Training Loss: 0.0005809458089061081 \t\n",
      "Epoch 6816 \t\t Training Loss: 0.0005809458089061081 \t\n",
      "Epoch 6817 \t\t Training Loss: 0.0005809458089061081 \t\n",
      "Epoch 6818 \t\t Training Loss: 0.0005809457506984472 \t\n",
      "Epoch 6819 \t\t Training Loss: 0.0005809457506984472 \t\n",
      "Epoch 6820 \t\t Training Loss: 0.0005809457506984472 \t\n",
      "Epoch 6821 \t\t Training Loss: 0.0005809457506984472 \t\n",
      "Epoch 6822 \t\t Training Loss: 0.0005809457506984472 \t\n",
      "Epoch 6823 \t\t Training Loss: 0.0005809457506984472 \t\n",
      "Epoch 6824 \t\t Training Loss: 0.0005809457506984472 \t\n",
      "Epoch 6825 \t\t Training Loss: 0.0005809457506984472 \t\n",
      "Epoch 6826 \t\t Training Loss: 0.0005809457506984472 \t\n",
      "Epoch 6827 \t\t Training Loss: 0.0005809457506984472 \t\n",
      "Epoch 6828 \t\t Training Loss: 0.0005809457506984472 \t\n",
      "Epoch 6829 \t\t Training Loss: 0.0005809457506984472 \t\n",
      "Epoch 6830 \t\t Training Loss: 0.0005809456924907863 \t\n",
      "Epoch 6831 \t\t Training Loss: 0.0005809456924907863 \t\n",
      "Epoch 6832 \t\t Training Loss: 0.0005809456924907863 \t\n",
      "Epoch 6833 \t\t Training Loss: 0.0005809456924907863 \t\n",
      "Epoch 6834 \t\t Training Loss: 0.0005809456924907863 \t\n",
      "Epoch 6835 \t\t Training Loss: 0.0005809456924907863 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6836 \t\t Training Loss: 0.0005809456342831254 \t\n",
      "Epoch 6837 \t\t Training Loss: 0.0005809456924907863 \t\n",
      "Epoch 6838 \t\t Training Loss: 0.0005809456342831254 \t\n",
      "Epoch 6839 \t\t Training Loss: 0.0005809456342831254 \t\n",
      "Epoch 6840 \t\t Training Loss: 0.0005809456342831254 \t\n",
      "Epoch 6841 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6842 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6843 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6844 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6845 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6846 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6847 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6848 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6849 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6850 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6851 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6852 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6853 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6854 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6855 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6856 \t\t Training Loss: 0.0005809454596601427 \t\n",
      "Epoch 6857 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6858 \t\t Training Loss: 0.0005809454596601427 \t\n",
      "Epoch 6859 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6860 \t\t Training Loss: 0.0005809455760754645 \t\n",
      "Epoch 6861 \t\t Training Loss: 0.0005809454596601427 \t\n",
      "Epoch 6862 \t\t Training Loss: 0.0005809454596601427 \t\n",
      "Epoch 6863 \t\t Training Loss: 0.0005809454596601427 \t\n",
      "Epoch 6864 \t\t Training Loss: 0.0005809454596601427 \t\n",
      "Epoch 6865 \t\t Training Loss: 0.0005809454596601427 \t\n",
      "Epoch 6866 \t\t Training Loss: 0.0005809454014524817 \t\n",
      "Epoch 6867 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6868 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6869 \t\t Training Loss: 0.0005809454596601427 \t\n",
      "Epoch 6870 \t\t Training Loss: 0.0005809454014524817 \t\n",
      "Epoch 6871 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6872 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6873 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6874 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6875 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6876 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6877 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6878 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6879 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6880 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6881 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6882 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6883 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6884 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6885 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6886 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6887 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6888 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6889 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6890 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6891 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6892 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6893 \t\t Training Loss: 0.0005809453432448208 \t\n",
      "Epoch 6894 \t\t Training Loss: 0.0005809452850371599 \t\n",
      "Epoch 6895 \t\t Training Loss: 0.000580945226829499 \t\n",
      "Epoch 6896 \t\t Training Loss: 0.000580945226829499 \t\n",
      "Epoch 6897 \t\t Training Loss: 0.000580945226829499 \t\n",
      "Epoch 6898 \t\t Training Loss: 0.0005809452850371599 \t\n",
      "Epoch 6899 \t\t Training Loss: 0.000580945226829499 \t\n",
      "Epoch 6900 \t\t Training Loss: 0.000580945226829499 \t\n",
      "Epoch 6901 \t\t Training Loss: 0.000580945226829499 \t\n",
      "Epoch 6902 \t\t Training Loss: 0.000580945226829499 \t\n",
      "Epoch 6903 \t\t Training Loss: 0.000580945226829499 \t\n",
      "Epoch 6904 \t\t Training Loss: 0.000580945226829499 \t\n",
      "Epoch 6905 \t\t Training Loss: 0.000580945226829499 \t\n",
      "Epoch 6906 \t\t Training Loss: 0.000580945226829499 \t\n",
      "Epoch 6907 \t\t Training Loss: 0.000580945226829499 \t\n",
      "Epoch 6908 \t\t Training Loss: 0.0005809451686218381 \t\n",
      "Epoch 6909 \t\t Training Loss: 0.0005809451686218381 \t\n",
      "Epoch 6910 \t\t Training Loss: 0.0005809451686218381 \t\n",
      "Epoch 6911 \t\t Training Loss: 0.0005809451686218381 \t\n",
      "Epoch 6912 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6913 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6914 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6915 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6916 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6917 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6918 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6919 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6920 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6921 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6922 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6923 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6924 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6925 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6926 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6927 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6928 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6929 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6930 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6931 \t\t Training Loss: 0.0005809451104141772 \t\n",
      "Epoch 6932 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6933 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6934 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6935 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6936 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6937 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6938 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6939 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6940 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6941 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6942 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6943 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6944 \t\t Training Loss: 0.0005809448775835335 \t\n",
      "Epoch 6945 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6946 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6947 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6948 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6949 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6950 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6951 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6952 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6953 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6954 \t\t Training Loss: 0.0005809449939988554 \t\n",
      "Epoch 6955 \t\t Training Loss: 0.0005809448775835335 \t\n",
      "Epoch 6956 \t\t Training Loss: 0.0005809448775835335 \t\n",
      "Epoch 6957 \t\t Training Loss: 0.0005809449357911944 \t\n",
      "Epoch 6958 \t\t Training Loss: 0.0005809448775835335 \t\n",
      "Epoch 6959 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6960 \t\t Training Loss: 0.0005809448775835335 \t\n",
      "Epoch 6961 \t\t Training Loss: 0.0005809448775835335 \t\n",
      "Epoch 6962 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6963 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6964 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6965 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6966 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6967 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6968 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6969 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6970 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6971 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6972 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6973 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6974 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6975 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6976 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6977 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6978 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6979 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6980 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6981 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6982 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6983 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6984 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6985 \t\t Training Loss: 0.0005809447611682117 \t\n",
      "Epoch 6986 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6987 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6988 \t\t Training Loss: 0.0005809447029605508 \t\n",
      "Epoch 6989 \t\t Training Loss: 0.0005809447611682117 \t\n",
      "Epoch 6990 \t\t Training Loss: 0.0005809448193758726 \t\n",
      "Epoch 6991 \t\t Training Loss: 0.0005809447029605508 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6992 \t\t Training Loss: 0.0005809447029605508 \t\n",
      "Epoch 6993 \t\t Training Loss: 0.0005809447029605508 \t\n",
      "Epoch 6994 \t\t Training Loss: 0.0005809447029605508 \t\n",
      "Epoch 6995 \t\t Training Loss: 0.0005809447029605508 \t\n",
      "Epoch 6996 \t\t Training Loss: 0.0005809447029605508 \t\n",
      "Epoch 6997 \t\t Training Loss: 0.0005809447029605508 \t\n",
      "Epoch 6998 \t\t Training Loss: 0.0005809447029605508 \t\n",
      "Epoch 6999 \t\t Training Loss: 0.0005809447029605508 \t\n",
      "Epoch 7000 \t\t Training Loss: 0.0005809447029605508 \t\n",
      "Epoch 7001 \t\t Training Loss: 0.0005809447029605508 \t\n",
      "Epoch 7002 \t\t Training Loss: 0.0005809446447528899 \t\n",
      "Epoch 7003 \t\t Training Loss: 0.0005809447029605508 \t\n",
      "Epoch 7004 \t\t Training Loss: 0.0005809447029605508 \t\n",
      "Epoch 7005 \t\t Training Loss: 0.0005809446447528899 \t\n",
      "Epoch 7006 \t\t Training Loss: 0.0005809446447528899 \t\n",
      "Epoch 7007 \t\t Training Loss: 0.0005809446447528899 \t\n",
      "Epoch 7008 \t\t Training Loss: 0.0005809446447528899 \t\n",
      "Epoch 7009 \t\t Training Loss: 0.000580944586545229 \t\n",
      "Epoch 7010 \t\t Training Loss: 0.000580944586545229 \t\n",
      "Epoch 7011 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7012 \t\t Training Loss: 0.000580944586545229 \t\n",
      "Epoch 7013 \t\t Training Loss: 0.000580944586545229 \t\n",
      "Epoch 7014 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7015 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7016 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7017 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7018 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7019 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7020 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7021 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7022 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7023 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7024 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7025 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7026 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7027 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7028 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7029 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7030 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7031 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7032 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7033 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7034 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7035 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7036 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7037 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7038 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7039 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7040 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7041 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7042 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7043 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7044 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7045 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7046 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7047 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7048 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7049 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7050 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7051 \t\t Training Loss: 0.000580944528337568 \t\n",
      "Epoch 7052 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7053 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7054 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7055 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7056 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7057 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7058 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7059 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7060 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7061 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7062 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7063 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7064 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7065 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7066 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7067 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7068 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7069 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7070 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7071 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7072 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7073 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7074 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7075 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7076 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7077 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7078 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7079 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7080 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7081 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7082 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7083 \t\t Training Loss: 0.0005809444119222462 \t\n",
      "Epoch 7084 \t\t Training Loss: 0.0005809443537145853 \t\n",
      "Epoch 7085 \t\t Training Loss: 0.0005809443537145853 \t\n",
      "Epoch 7086 \t\t Training Loss: 0.0005809442955069244 \t\n",
      "Epoch 7087 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7088 \t\t Training Loss: 0.0005809442955069244 \t\n",
      "Epoch 7089 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7090 \t\t Training Loss: 0.0005809442955069244 \t\n",
      "Epoch 7091 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7092 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7093 \t\t Training Loss: 0.0005809442955069244 \t\n",
      "Epoch 7094 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7095 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7096 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7097 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7098 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7099 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7100 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7101 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7102 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7103 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7104 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7105 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7106 \t\t Training Loss: 0.0005809442372992635 \t\n",
      "Epoch 7107 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7108 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7109 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7110 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7111 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7112 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7113 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7114 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7115 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7116 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7117 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7118 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7119 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7120 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7121 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7122 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7123 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7124 \t\t Training Loss: 0.0005809441208839417 \t\n",
      "Epoch 7125 \t\t Training Loss: 0.0005809440044686198 \t\n",
      "Epoch 7126 \t\t Training Loss: 0.0005809440044686198 \t\n",
      "Epoch 7127 \t\t Training Loss: 0.0005809440044686198 \t\n",
      "Epoch 7128 \t\t Training Loss: 0.0005809440044686198 \t\n",
      "Epoch 7129 \t\t Training Loss: 0.0005809440044686198 \t\n",
      "Epoch 7130 \t\t Training Loss: 0.0005809440044686198 \t\n",
      "Epoch 7131 \t\t Training Loss: 0.0005809440044686198 \t\n",
      "Epoch 7132 \t\t Training Loss: 0.0005809440044686198 \t\n",
      "Epoch 7133 \t\t Training Loss: 0.0005809440044686198 \t\n",
      "Epoch 7134 \t\t Training Loss: 0.0005809440044686198 \t\n",
      "Epoch 7135 \t\t Training Loss: 0.0005809440044686198 \t\n",
      "Epoch 7136 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7137 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7138 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7139 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7140 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7141 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7142 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7143 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7144 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7145 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7146 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7147 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7148 \t\t Training Loss: 0.0005809439462609589 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7149 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7150 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7151 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7152 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7153 \t\t Training Loss: 0.0005809439462609589 \t\n",
      "Epoch 7154 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7155 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7156 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7157 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7158 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7159 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7160 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7161 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7162 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7163 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7164 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7165 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7166 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7167 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7168 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7169 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7170 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7171 \t\t Training Loss: 0.0005809438298456371 \t\n",
      "Epoch 7172 \t\t Training Loss: 0.0005809437134303153 \t\n",
      "Epoch 7173 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7174 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7175 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7176 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7177 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7178 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7179 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7180 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7181 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7182 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7183 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7184 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7185 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7186 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7187 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7188 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7189 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7190 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7191 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7192 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7193 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7194 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7195 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7196 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7197 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7198 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7199 \t\t Training Loss: 0.0005809436552226543 \t\n",
      "Epoch 7200 \t\t Training Loss: 0.0005809435970149934 \t\n",
      "Epoch 7201 \t\t Training Loss: 0.0005809435970149934 \t\n",
      "Epoch 7202 \t\t Training Loss: 0.0005809435970149934 \t\n",
      "Epoch 7203 \t\t Training Loss: 0.0005809435970149934 \t\n",
      "Epoch 7204 \t\t Training Loss: 0.0005809435970149934 \t\n",
      "Epoch 7205 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7206 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7207 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7208 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7209 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7210 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7211 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7212 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7213 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7214 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7215 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7216 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7217 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7218 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7219 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7220 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7221 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7222 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7223 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7224 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7225 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7226 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7227 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7228 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7229 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7230 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7231 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7232 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7233 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7234 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7235 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7236 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7237 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7238 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7239 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7240 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7241 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7242 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7243 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7244 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7245 \t\t Training Loss: 0.0005809435388073325 \t\n",
      "Epoch 7246 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7247 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7248 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7249 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7250 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7251 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7252 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7253 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7254 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7255 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7256 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7257 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7258 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7259 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7260 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7261 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7262 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7263 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7264 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7265 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7266 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7267 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7268 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7269 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7270 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7271 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7272 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7273 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7274 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7275 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7276 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7277 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7278 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7279 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7280 \t\t Training Loss: 0.0005809434223920107 \t\n",
      "Epoch 7281 \t\t Training Loss: 0.0005809433059766889 \t\n",
      "Epoch 7282 \t\t Training Loss: 0.000580943247769028 \t\n",
      "Epoch 7283 \t\t Training Loss: 0.000580943247769028 \t\n",
      "Epoch 7284 \t\t Training Loss: 0.000580943247769028 \t\n",
      "Epoch 7285 \t\t Training Loss: 0.000580943247769028 \t\n",
      "Epoch 7286 \t\t Training Loss: 0.000580943247769028 \t\n",
      "Epoch 7287 \t\t Training Loss: 0.000580943247769028 \t\n",
      "Epoch 7288 \t\t Training Loss: 0.000580943247769028 \t\n",
      "Epoch 7289 \t\t Training Loss: 0.000580943247769028 \t\n",
      "Epoch 7290 \t\t Training Loss: 0.000580943247769028 \t\n",
      "Epoch 7291 \t\t Training Loss: 0.000580943247769028 \t\n",
      "Epoch 7292 \t\t Training Loss: 0.000580943247769028 \t\n",
      "Epoch 7293 \t\t Training Loss: 0.000580943247769028 \t\n",
      "Epoch 7294 \t\t Training Loss: 0.000580943247769028 \t\n",
      "Epoch 7295 \t\t Training Loss: 0.000580943247769028 \t\n",
      "Epoch 7296 \t\t Training Loss: 0.000580943247769028 \t\n",
      "Epoch 7297 \t\t Training Loss: 0.0005809431313537061 \t\n",
      "Epoch 7298 \t\t Training Loss: 0.000580943247769028 \t\n",
      "Epoch 7299 \t\t Training Loss: 0.0005809431313537061 \t\n",
      "Epoch 7300 \t\t Training Loss: 0.0005809431313537061 \t\n",
      "Epoch 7301 \t\t Training Loss: 0.0005809431313537061 \t\n",
      "Epoch 7302 \t\t Training Loss: 0.0005809431313537061 \t\n",
      "Epoch 7303 \t\t Training Loss: 0.0005809431313537061 \t\n",
      "Epoch 7304 \t\t Training Loss: 0.0005809431313537061 \t\n",
      "Epoch 7305 \t\t Training Loss: 0.0005809431313537061 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7306 \t\t Training Loss: 0.0005809431313537061 \t\n",
      "Epoch 7307 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7308 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7309 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7310 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7311 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7312 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7313 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7314 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7315 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7316 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7317 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7318 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7319 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7320 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7321 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7322 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7323 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7324 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7325 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7326 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7327 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7328 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7329 \t\t Training Loss: 0.0005809430149383843 \t\n",
      "Epoch 7330 \t\t Training Loss: 0.0005809429567307234 \t\n",
      "Epoch 7331 \t\t Training Loss: 0.0005809429567307234 \t\n",
      "Epoch 7332 \t\t Training Loss: 0.0005809429567307234 \t\n",
      "Epoch 7333 \t\t Training Loss: 0.0005809429567307234 \t\n",
      "Epoch 7334 \t\t Training Loss: 0.0005809429567307234 \t\n",
      "Epoch 7335 \t\t Training Loss: 0.0005809429567307234 \t\n",
      "Epoch 7336 \t\t Training Loss: 0.0005809429567307234 \t\n",
      "Epoch 7337 \t\t Training Loss: 0.0005809429567307234 \t\n",
      "Epoch 7338 \t\t Training Loss: 0.0005809429567307234 \t\n",
      "Epoch 7339 \t\t Training Loss: 0.0005809429567307234 \t\n",
      "Epoch 7340 \t\t Training Loss: 0.0005809429567307234 \t\n",
      "Epoch 7341 \t\t Training Loss: 0.0005809428403154016 \t\n",
      "Epoch 7342 \t\t Training Loss: 0.0005809429567307234 \t\n",
      "Epoch 7343 \t\t Training Loss: 0.0005809428403154016 \t\n",
      "Epoch 7344 \t\t Training Loss: 0.0005809429567307234 \t\n",
      "Epoch 7345 \t\t Training Loss: 0.0005809429567307234 \t\n",
      "Epoch 7346 \t\t Training Loss: 0.0005809428403154016 \t\n",
      "Epoch 7347 \t\t Training Loss: 0.0005809428403154016 \t\n",
      "Epoch 7348 \t\t Training Loss: 0.0005809428403154016 \t\n",
      "Epoch 7349 \t\t Training Loss: 0.0005809428403154016 \t\n",
      "Epoch 7350 \t\t Training Loss: 0.0005809428403154016 \t\n",
      "Epoch 7351 \t\t Training Loss: 0.0005809428403154016 \t\n",
      "Epoch 7352 \t\t Training Loss: 0.0005809428403154016 \t\n",
      "Epoch 7353 \t\t Training Loss: 0.0005809428403154016 \t\n",
      "Epoch 7354 \t\t Training Loss: 0.0005809428403154016 \t\n",
      "Epoch 7355 \t\t Training Loss: 0.0005809428403154016 \t\n",
      "Epoch 7356 \t\t Training Loss: 0.0005809428403154016 \t\n",
      "Epoch 7357 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7358 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7359 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7360 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7361 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7362 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7363 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7364 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7365 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7366 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7367 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7368 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7369 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7370 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7371 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7372 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7373 \t\t Training Loss: 0.0005809427239000797 \t\n",
      "Epoch 7374 \t\t Training Loss: 0.0005809426656924188 \t\n",
      "Epoch 7375 \t\t Training Loss: 0.0005809426656924188 \t\n",
      "Epoch 7376 \t\t Training Loss: 0.0005809426656924188 \t\n",
      "Epoch 7377 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7378 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7379 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7380 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7381 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7382 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7383 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7384 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7385 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7386 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7387 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7388 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7389 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7390 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7391 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7392 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7393 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7394 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7395 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7396 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7397 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7398 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7399 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7400 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7401 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7402 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7403 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7404 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7405 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7406 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7407 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7408 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7409 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7410 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7411 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7412 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7413 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7414 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7415 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7416 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7417 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7418 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7419 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7420 \t\t Training Loss: 0.000580942549277097 \t\n",
      "Epoch 7421 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7422 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7423 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7424 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7425 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7426 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7427 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7428 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7429 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7430 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7431 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7432 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7433 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7434 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7435 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7436 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7437 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7438 \t\t Training Loss: 0.0005809424328617752 \t\n",
      "Epoch 7439 \t\t Training Loss: 0.0005809423746541142 \t\n",
      "Epoch 7440 \t\t Training Loss: 0.0005809423746541142 \t\n",
      "Epoch 7441 \t\t Training Loss: 0.0005809423746541142 \t\n",
      "Epoch 7442 \t\t Training Loss: 0.0005809423746541142 \t\n",
      "Epoch 7443 \t\t Training Loss: 0.0005809423746541142 \t\n",
      "Epoch 7444 \t\t Training Loss: 0.0005809423746541142 \t\n",
      "Epoch 7445 \t\t Training Loss: 0.0005809423746541142 \t\n",
      "Epoch 7446 \t\t Training Loss: 0.0005809422582387924 \t\n",
      "Epoch 7447 \t\t Training Loss: 0.0005809422582387924 \t\n",
      "Epoch 7448 \t\t Training Loss: 0.0005809422582387924 \t\n",
      "Epoch 7449 \t\t Training Loss: 0.0005809422582387924 \t\n",
      "Epoch 7450 \t\t Training Loss: 0.0005809422582387924 \t\n",
      "Epoch 7451 \t\t Training Loss: 0.0005809422582387924 \t\n",
      "Epoch 7452 \t\t Training Loss: 0.0005809422582387924 \t\n",
      "Epoch 7453 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7454 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7455 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7456 \t\t Training Loss: 0.0005809422582387924 \t\n",
      "Epoch 7457 \t\t Training Loss: 0.0005809422582387924 \t\n",
      "Epoch 7458 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7459 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7460 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7461 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7462 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7463 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7464 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7465 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7466 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7467 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7468 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7469 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7470 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7471 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7472 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7473 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7474 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7475 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7476 \t\t Training Loss: 0.0005809421418234706 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7477 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7478 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7479 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7480 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7481 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7482 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7483 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7484 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7485 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7486 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7487 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7488 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7489 \t\t Training Loss: 0.0005809421418234706 \t\n",
      "Epoch 7490 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7491 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7492 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7493 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7494 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7495 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7496 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7497 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7498 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7499 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7500 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7501 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7502 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7503 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7504 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7505 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7506 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7507 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7508 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7509 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7510 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7511 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7512 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7513 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7514 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7515 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7516 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7517 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7518 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7519 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7520 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7521 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7522 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7523 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7524 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7525 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7526 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7527 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7528 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7529 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7530 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7531 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7532 \t\t Training Loss: 0.0005809420254081488 \t\n",
      "Epoch 7533 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7534 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7535 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7536 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7537 \t\t Training Loss: 0.0005809419089928269 \t\n",
      "Epoch 7538 \t\t Training Loss: 0.0005809419672004879 \t\n",
      "Epoch 7539 \t\t Training Loss: 0.0005809419089928269 \t\n",
      "Epoch 7540 \t\t Training Loss: 0.0005809419089928269 \t\n",
      "Epoch 7541 \t\t Training Loss: 0.0005809419089928269 \t\n",
      "Epoch 7542 \t\t Training Loss: 0.0005809419089928269 \t\n",
      "Epoch 7543 \t\t Training Loss: 0.0005809419089928269 \t\n",
      "Epoch 7544 \t\t Training Loss: 0.0005809419089928269 \t\n",
      "Epoch 7545 \t\t Training Loss: 0.0005809419089928269 \t\n",
      "Epoch 7546 \t\t Training Loss: 0.0005809417343698442 \t\n",
      "Epoch 7547 \t\t Training Loss: 0.0005809417343698442 \t\n",
      "Epoch 7548 \t\t Training Loss: 0.0005809417343698442 \t\n",
      "Epoch 7549 \t\t Training Loss: 0.0005809419089928269 \t\n",
      "Epoch 7550 \t\t Training Loss: 0.000580941850785166 \t\n",
      "Epoch 7551 \t\t Training Loss: 0.0005809417343698442 \t\n",
      "Epoch 7552 \t\t Training Loss: 0.0005809417343698442 \t\n",
      "Epoch 7553 \t\t Training Loss: 0.0005809417343698442 \t\n",
      "Epoch 7554 \t\t Training Loss: 0.0005809417343698442 \t\n",
      "Epoch 7555 \t\t Training Loss: 0.0005809417343698442 \t\n",
      "Epoch 7556 \t\t Training Loss: 0.0005809417343698442 \t\n",
      "Epoch 7557 \t\t Training Loss: 0.0005809417343698442 \t\n",
      "Epoch 7558 \t\t Training Loss: 0.0005809417343698442 \t\n",
      "Epoch 7559 \t\t Training Loss: 0.0005809417343698442 \t\n",
      "Epoch 7560 \t\t Training Loss: 0.0005809417343698442 \t\n",
      "Epoch 7561 \t\t Training Loss: 0.0005809417343698442 \t\n",
      "Epoch 7562 \t\t Training Loss: 0.0005809417343698442 \t\n",
      "Epoch 7563 \t\t Training Loss: 0.0005809417343698442 \t\n",
      "Epoch 7564 \t\t Training Loss: 0.0005809416179545224 \t\n",
      "Epoch 7565 \t\t Training Loss: 0.0005809416179545224 \t\n",
      "Epoch 7566 \t\t Training Loss: 0.0005809416179545224 \t\n",
      "Epoch 7567 \t\t Training Loss: 0.0005809416179545224 \t\n",
      "Epoch 7568 \t\t Training Loss: 0.0005809416179545224 \t\n",
      "Epoch 7569 \t\t Training Loss: 0.0005809416179545224 \t\n",
      "Epoch 7570 \t\t Training Loss: 0.0005809416179545224 \t\n",
      "Epoch 7571 \t\t Training Loss: 0.0005809415597468615 \t\n",
      "Epoch 7572 \t\t Training Loss: 0.0005809416179545224 \t\n",
      "Epoch 7573 \t\t Training Loss: 0.0005809416179545224 \t\n",
      "Epoch 7574 \t\t Training Loss: 0.0005809415597468615 \t\n",
      "Epoch 7575 \t\t Training Loss: 0.0005809415597468615 \t\n",
      "Epoch 7576 \t\t Training Loss: 0.0005809415597468615 \t\n",
      "Epoch 7577 \t\t Training Loss: 0.0005809415597468615 \t\n",
      "Epoch 7578 \t\t Training Loss: 0.0005809415597468615 \t\n",
      "Epoch 7579 \t\t Training Loss: 0.0005809415597468615 \t\n",
      "Epoch 7580 \t\t Training Loss: 0.0005809415597468615 \t\n",
      "Epoch 7581 \t\t Training Loss: 0.0005809415597468615 \t\n",
      "Epoch 7582 \t\t Training Loss: 0.0005809415597468615 \t\n",
      "Epoch 7583 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7584 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7585 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7586 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7587 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7588 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7589 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7590 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7591 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7592 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7593 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7594 \t\t Training Loss: 0.0005809413851238787 \t\n",
      "Epoch 7595 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7596 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7597 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7598 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7599 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7600 \t\t Training Loss: 0.0005809414433315396 \t\n",
      "Epoch 7601 \t\t Training Loss: 0.0005809413851238787 \t\n",
      "Epoch 7602 \t\t Training Loss: 0.0005809413851238787 \t\n",
      "Epoch 7603 \t\t Training Loss: 0.0005809413851238787 \t\n",
      "Epoch 7604 \t\t Training Loss: 0.0005809413851238787 \t\n",
      "Epoch 7605 \t\t Training Loss: 0.0005809413851238787 \t\n",
      "Epoch 7606 \t\t Training Loss: 0.0005809413851238787 \t\n",
      "Epoch 7607 \t\t Training Loss: 0.0005809413851238787 \t\n",
      "Epoch 7608 \t\t Training Loss: 0.0005809413851238787 \t\n",
      "Epoch 7609 \t\t Training Loss: 0.0005809413851238787 \t\n",
      "Epoch 7610 \t\t Training Loss: 0.0005809413851238787 \t\n",
      "Epoch 7611 \t\t Training Loss: 0.0005809413851238787 \t\n",
      "Epoch 7612 \t\t Training Loss: 0.0005809413851238787 \t\n",
      "Epoch 7613 \t\t Training Loss: 0.0005809413269162178 \t\n",
      "Epoch 7614 \t\t Training Loss: 0.0005809413269162178 \t\n",
      "Epoch 7615 \t\t Training Loss: 0.0005809413269162178 \t\n",
      "Epoch 7616 \t\t Training Loss: 0.0005809413269162178 \t\n",
      "Epoch 7617 \t\t Training Loss: 0.0005809412687085569 \t\n",
      "Epoch 7618 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7619 \t\t Training Loss: 0.0005809413269162178 \t\n",
      "Epoch 7620 \t\t Training Loss: 0.0005809412687085569 \t\n",
      "Epoch 7621 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7622 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7623 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7624 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7625 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7626 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7627 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7628 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7629 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7630 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7631 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7632 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7633 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7634 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7635 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7636 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7637 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7638 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7639 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7640 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7641 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7642 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7643 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7644 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7645 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7646 \t\t Training Loss: 0.0005809410940855742 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7647 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7648 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7649 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7650 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7651 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7652 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7653 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7654 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7655 \t\t Training Loss: 0.0005809411522932351 \t\n",
      "Epoch 7656 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7657 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7658 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7659 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7660 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7661 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7662 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7663 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7664 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7665 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7666 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7667 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7668 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7669 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7670 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7671 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7672 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7673 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7674 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7675 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7676 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7677 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7678 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7679 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7680 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7681 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7682 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7683 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7684 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7685 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7686 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7687 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7688 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7689 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7690 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7691 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7692 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7693 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7694 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7695 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7696 \t\t Training Loss: 0.000580941210500896 \t\n",
      "Epoch 7697 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7698 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7699 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7700 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7701 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7702 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7703 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7704 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7705 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7706 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7707 \t\t Training Loss: 0.0005809410940855742 \t\n",
      "Epoch 7708 \t\t Training Loss: 0.0005809410358779132 \t\n",
      "Epoch 7709 \t\t Training Loss: 0.0005809410358779132 \t\n",
      "Epoch 7710 \t\t Training Loss: 0.0005809410358779132 \t\n",
      "Epoch 7711 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7712 \t\t Training Loss: 0.0005809410358779132 \t\n",
      "Epoch 7713 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7714 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7715 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7716 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7717 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7718 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7719 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7720 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7721 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7722 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7723 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7724 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7725 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7726 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7727 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7728 \t\t Training Loss: 0.0005809410358779132 \t\n",
      "Epoch 7729 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7730 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7731 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7732 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7733 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7734 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7735 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7736 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7737 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7738 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7739 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7740 \t\t Training Loss: 0.0005809409194625914 \t\n",
      "Epoch 7741 \t\t Training Loss: 0.0005809409194625914 \t\n",
      "Epoch 7742 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7743 \t\t Training Loss: 0.0005809409776702523 \t\n",
      "Epoch 7744 \t\t Training Loss: 0.0005809409194625914 \t\n",
      "Epoch 7745 \t\t Training Loss: 0.0005809409194625914 \t\n",
      "Epoch 7746 \t\t Training Loss: 0.0005809409194625914 \t\n",
      "Epoch 7747 \t\t Training Loss: 0.0005809409194625914 \t\n",
      "Epoch 7748 \t\t Training Loss: 0.0005809409194625914 \t\n",
      "Epoch 7749 \t\t Training Loss: 0.0005809409194625914 \t\n",
      "Epoch 7750 \t\t Training Loss: 0.0005809408612549305 \t\n",
      "Epoch 7751 \t\t Training Loss: 0.0005809408612549305 \t\n",
      "Epoch 7752 \t\t Training Loss: 0.0005809408612549305 \t\n",
      "Epoch 7753 \t\t Training Loss: 0.0005809408612549305 \t\n",
      "Epoch 7754 \t\t Training Loss: 0.0005809408612549305 \t\n",
      "Epoch 7755 \t\t Training Loss: 0.0005809408030472696 \t\n",
      "Epoch 7756 \t\t Training Loss: 0.0005809408030472696 \t\n",
      "Epoch 7757 \t\t Training Loss: 0.0005809408030472696 \t\n",
      "Epoch 7758 \t\t Training Loss: 0.0005809408030472696 \t\n",
      "Epoch 7759 \t\t Training Loss: 0.0005809408030472696 \t\n",
      "Epoch 7760 \t\t Training Loss: 0.0005809408030472696 \t\n",
      "Epoch 7761 \t\t Training Loss: 0.0005809408030472696 \t\n",
      "Epoch 7762 \t\t Training Loss: 0.0005809408030472696 \t\n",
      "Epoch 7763 \t\t Training Loss: 0.0005809408030472696 \t\n",
      "Epoch 7764 \t\t Training Loss: 0.0005809408030472696 \t\n",
      "Epoch 7765 \t\t Training Loss: 0.0005809408030472696 \t\n",
      "Epoch 7766 \t\t Training Loss: 0.0005809408030472696 \t\n",
      "Epoch 7767 \t\t Training Loss: 0.0005809407448396087 \t\n",
      "Epoch 7768 \t\t Training Loss: 0.0005809407448396087 \t\n",
      "Epoch 7769 \t\t Training Loss: 0.0005809407448396087 \t\n",
      "Epoch 7770 \t\t Training Loss: 0.0005809407448396087 \t\n",
      "Epoch 7771 \t\t Training Loss: 0.0005809407448396087 \t\n",
      "Epoch 7772 \t\t Training Loss: 0.0005809407448396087 \t\n",
      "Epoch 7773 \t\t Training Loss: 0.0005809407448396087 \t\n",
      "Epoch 7774 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7775 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7776 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7777 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7778 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7779 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7780 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7781 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7782 \t\t Training Loss: 0.0005809406866319478 \t\n",
      "Epoch 7783 \t\t Training Loss: 0.0005809406866319478 \t\n",
      "Epoch 7784 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7785 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7786 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7787 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7788 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7789 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7790 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7791 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7792 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7793 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7794 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7795 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7796 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7797 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7798 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7799 \t\t Training Loss: 0.000580940512008965 \t\n",
      "Epoch 7800 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7801 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7802 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7803 \t\t Training Loss: 0.0005809406284242868 \t\n",
      "Epoch 7804 \t\t Training Loss: 0.000580940512008965 \t\n",
      "Epoch 7805 \t\t Training Loss: 0.000580940512008965 \t\n",
      "Epoch 7806 \t\t Training Loss: 0.000580940512008965 \t\n",
      "Epoch 7807 \t\t Training Loss: 0.000580940512008965 \t\n",
      "Epoch 7808 \t\t Training Loss: 0.000580940512008965 \t\n",
      "Epoch 7809 \t\t Training Loss: 0.000580940512008965 \t\n",
      "Epoch 7810 \t\t Training Loss: 0.000580940512008965 \t\n",
      "Epoch 7811 \t\t Training Loss: 0.000580940512008965 \t\n",
      "Epoch 7812 \t\t Training Loss: 0.000580940512008965 \t\n",
      "Epoch 7813 \t\t Training Loss: 0.000580940512008965 \t\n",
      "Epoch 7814 \t\t Training Loss: 0.000580940512008965 \t\n",
      "Epoch 7815 \t\t Training Loss: 0.000580940512008965 \t\n",
      "Epoch 7816 \t\t Training Loss: 0.000580940512008965 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7817 \t\t Training Loss: 0.0005809404538013041 \t\n",
      "Epoch 7818 \t\t Training Loss: 0.0005809404538013041 \t\n",
      "Epoch 7819 \t\t Training Loss: 0.0005809404538013041 \t\n",
      "Epoch 7820 \t\t Training Loss: 0.0005809403955936432 \t\n",
      "Epoch 7821 \t\t Training Loss: 0.0005809403955936432 \t\n",
      "Epoch 7822 \t\t Training Loss: 0.0005809403955936432 \t\n",
      "Epoch 7823 \t\t Training Loss: 0.0005809403955936432 \t\n",
      "Epoch 7824 \t\t Training Loss: 0.0005809403955936432 \t\n",
      "Epoch 7825 \t\t Training Loss: 0.0005809403373859823 \t\n",
      "Epoch 7826 \t\t Training Loss: 0.0005809403373859823 \t\n",
      "Epoch 7827 \t\t Training Loss: 0.0005809403373859823 \t\n",
      "Epoch 7828 \t\t Training Loss: 0.0005809403373859823 \t\n",
      "Epoch 7829 \t\t Training Loss: 0.0005809403373859823 \t\n",
      "Epoch 7830 \t\t Training Loss: 0.0005809402791783214 \t\n",
      "Epoch 7831 \t\t Training Loss: 0.0005809403373859823 \t\n",
      "Epoch 7832 \t\t Training Loss: 0.0005809402791783214 \t\n",
      "Epoch 7833 \t\t Training Loss: 0.0005809402791783214 \t\n",
      "Epoch 7834 \t\t Training Loss: 0.0005809402791783214 \t\n",
      "Epoch 7835 \t\t Training Loss: 0.0005809402791783214 \t\n",
      "Epoch 7836 \t\t Training Loss: 0.0005809402791783214 \t\n",
      "Epoch 7837 \t\t Training Loss: 0.0005809402209706604 \t\n",
      "Epoch 7838 \t\t Training Loss: 0.0005809402209706604 \t\n",
      "Epoch 7839 \t\t Training Loss: 0.0005809402209706604 \t\n",
      "Epoch 7840 \t\t Training Loss: 0.0005809402209706604 \t\n",
      "Epoch 7841 \t\t Training Loss: 0.0005809402791783214 \t\n",
      "Epoch 7842 \t\t Training Loss: 0.0005809402209706604 \t\n",
      "Epoch 7843 \t\t Training Loss: 0.0005809402209706604 \t\n",
      "Epoch 7844 \t\t Training Loss: 0.0005809402209706604 \t\n",
      "Epoch 7845 \t\t Training Loss: 0.0005809402209706604 \t\n",
      "Epoch 7846 \t\t Training Loss: 0.0005809402209706604 \t\n",
      "Epoch 7847 \t\t Training Loss: 0.0005809401627629995 \t\n",
      "Epoch 7848 \t\t Training Loss: 0.0005809402209706604 \t\n",
      "Epoch 7849 \t\t Training Loss: 0.0005809401627629995 \t\n",
      "Epoch 7850 \t\t Training Loss: 0.0005809401627629995 \t\n",
      "Epoch 7851 \t\t Training Loss: 0.0005809401627629995 \t\n",
      "Epoch 7852 \t\t Training Loss: 0.0005809401627629995 \t\n",
      "Epoch 7853 \t\t Training Loss: 0.0005809401627629995 \t\n",
      "Epoch 7854 \t\t Training Loss: 0.0005809401627629995 \t\n",
      "Epoch 7855 \t\t Training Loss: 0.0005809401627629995 \t\n",
      "Epoch 7856 \t\t Training Loss: 0.0005809401627629995 \t\n",
      "Epoch 7857 \t\t Training Loss: 0.0005809400463476777 \t\n",
      "Epoch 7858 \t\t Training Loss: 0.0005809400463476777 \t\n",
      "Epoch 7859 \t\t Training Loss: 0.0005809400463476777 \t\n",
      "Epoch 7860 \t\t Training Loss: 0.0005809400463476777 \t\n",
      "Epoch 7861 \t\t Training Loss: 0.0005809400463476777 \t\n",
      "Epoch 7862 \t\t Training Loss: 0.0005809400463476777 \t\n",
      "Epoch 7863 \t\t Training Loss: 0.0005809400463476777 \t\n",
      "Epoch 7864 \t\t Training Loss: 0.0005809400463476777 \t\n",
      "Epoch 7865 \t\t Training Loss: 0.0005809400463476777 \t\n",
      "Epoch 7866 \t\t Training Loss: 0.0005809400463476777 \t\n",
      "Epoch 7867 \t\t Training Loss: 0.0005809400463476777 \t\n",
      "Epoch 7868 \t\t Training Loss: 0.0005809400463476777 \t\n",
      "Epoch 7869 \t\t Training Loss: 0.0005809400463476777 \t\n",
      "Epoch 7870 \t\t Training Loss: 0.0005809400463476777 \t\n",
      "Epoch 7871 \t\t Training Loss: 0.0005809400463476777 \t\n",
      "Epoch 7872 \t\t Training Loss: 0.0005809399881400168 \t\n",
      "Epoch 7873 \t\t Training Loss: 0.0005809399881400168 \t\n",
      "Epoch 7874 \t\t Training Loss: 0.0005809399881400168 \t\n",
      "Epoch 7875 \t\t Training Loss: 0.0005809399881400168 \t\n",
      "Epoch 7876 \t\t Training Loss: 0.0005809399881400168 \t\n",
      "Epoch 7877 \t\t Training Loss: 0.0005809399881400168 \t\n",
      "Epoch 7878 \t\t Training Loss: 0.0005809399881400168 \t\n",
      "Epoch 7879 \t\t Training Loss: 0.0005809399299323559 \t\n",
      "Epoch 7880 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7881 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7882 \t\t Training Loss: 0.0005809399299323559 \t\n",
      "Epoch 7883 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7884 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7885 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7886 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7887 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7888 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7889 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7890 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7891 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7892 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7893 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7894 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7895 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7896 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7897 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7898 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7899 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7900 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7901 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7902 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7903 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7904 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7905 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7906 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7907 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7908 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7909 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7910 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7911 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7912 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7913 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7914 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7915 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7916 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7917 \t\t Training Loss: 0.000580939871724695 \t\n",
      "Epoch 7918 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7919 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7920 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7921 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7922 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7923 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7924 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7925 \t\t Training Loss: 0.000580939813517034 \t\n",
      "Epoch 7926 \t\t Training Loss: 0.0005809397553093731 \t\n",
      "Epoch 7927 \t\t Training Loss: 0.0005809397553093731 \t\n",
      "Epoch 7928 \t\t Training Loss: 0.0005809397553093731 \t\n",
      "Epoch 7929 \t\t Training Loss: 0.0005809397553093731 \t\n",
      "Epoch 7930 \t\t Training Loss: 0.0005809396971017122 \t\n",
      "Epoch 7931 \t\t Training Loss: 0.0005809396971017122 \t\n",
      "Epoch 7932 \t\t Training Loss: 0.0005809396971017122 \t\n",
      "Epoch 7933 \t\t Training Loss: 0.0005809397553093731 \t\n",
      "Epoch 7934 \t\t Training Loss: 0.0005809397553093731 \t\n",
      "Epoch 7935 \t\t Training Loss: 0.0005809397553093731 \t\n",
      "Epoch 7936 \t\t Training Loss: 0.0005809397553093731 \t\n",
      "Epoch 7937 \t\t Training Loss: 0.0005809397553093731 \t\n",
      "Epoch 7938 \t\t Training Loss: 0.0005809397553093731 \t\n",
      "Epoch 7939 \t\t Training Loss: 0.0005809397553093731 \t\n",
      "Epoch 7940 \t\t Training Loss: 0.0005809396971017122 \t\n",
      "Epoch 7941 \t\t Training Loss: 0.0005809396971017122 \t\n",
      "Epoch 7942 \t\t Training Loss: 0.0005809396971017122 \t\n",
      "Epoch 7943 \t\t Training Loss: 0.0005809396971017122 \t\n",
      "Epoch 7944 \t\t Training Loss: 0.0005809396388940513 \t\n",
      "Epoch 7945 \t\t Training Loss: 0.0005809396388940513 \t\n",
      "Epoch 7946 \t\t Training Loss: 0.0005809395806863904 \t\n",
      "Epoch 7947 \t\t Training Loss: 0.0005809395806863904 \t\n",
      "Epoch 7948 \t\t Training Loss: 0.0005809395806863904 \t\n",
      "Epoch 7949 \t\t Training Loss: 0.0005809395806863904 \t\n",
      "Epoch 7950 \t\t Training Loss: 0.0005809395806863904 \t\n",
      "Epoch 7951 \t\t Training Loss: 0.0005809395806863904 \t\n",
      "Epoch 7952 \t\t Training Loss: 0.0005809395806863904 \t\n",
      "Epoch 7953 \t\t Training Loss: 0.0005809395806863904 \t\n",
      "Epoch 7954 \t\t Training Loss: 0.0005809395806863904 \t\n",
      "Epoch 7955 \t\t Training Loss: 0.0005809395806863904 \t\n",
      "Epoch 7956 \t\t Training Loss: 0.0005809395806863904 \t\n",
      "Epoch 7957 \t\t Training Loss: 0.0005809395806863904 \t\n",
      "Epoch 7958 \t\t Training Loss: 0.0005809395806863904 \t\n",
      "Epoch 7959 \t\t Training Loss: 0.0005809395806863904 \t\n",
      "Epoch 7960 \t\t Training Loss: 0.0005809395806863904 \t\n",
      "Epoch 7961 \t\t Training Loss: 0.0005809395806863904 \t\n",
      "Epoch 7962 \t\t Training Loss: 0.0005809394642710686 \t\n",
      "Epoch 7963 \t\t Training Loss: 0.0005809394642710686 \t\n",
      "Epoch 7964 \t\t Training Loss: 0.0005809394642710686 \t\n",
      "Epoch 7965 \t\t Training Loss: 0.0005809394642710686 \t\n",
      "Epoch 7966 \t\t Training Loss: 0.0005809394642710686 \t\n",
      "Epoch 7967 \t\t Training Loss: 0.0005809394642710686 \t\n",
      "Epoch 7968 \t\t Training Loss: 0.0005809394642710686 \t\n",
      "Epoch 7969 \t\t Training Loss: 0.0005809394642710686 \t\n",
      "Epoch 7970 \t\t Training Loss: 0.0005809394642710686 \t\n",
      "Epoch 7971 \t\t Training Loss: 0.0005809394642710686 \t\n",
      "Epoch 7972 \t\t Training Loss: 0.0005809394642710686 \t\n",
      "Epoch 7973 \t\t Training Loss: 0.0005809394642710686 \t\n",
      "Epoch 7974 \t\t Training Loss: 0.0005809394642710686 \t\n",
      "Epoch 7975 \t\t Training Loss: 0.0005809394642710686 \t\n",
      "Epoch 7976 \t\t Training Loss: 0.0005809394642710686 \t\n",
      "Epoch 7977 \t\t Training Loss: 0.0005809394060634077 \t\n",
      "Epoch 7978 \t\t Training Loss: 0.0005809394060634077 \t\n",
      "Epoch 7979 \t\t Training Loss: 0.0005809394060634077 \t\n",
      "Epoch 7980 \t\t Training Loss: 0.0005809394060634077 \t\n",
      "Epoch 7981 \t\t Training Loss: 0.0005809394060634077 \t\n",
      "Epoch 7982 \t\t Training Loss: 0.0005809394060634077 \t\n",
      "Epoch 7983 \t\t Training Loss: 0.0005809393478557467 \t\n",
      "Epoch 7984 \t\t Training Loss: 0.0005809394060634077 \t\n",
      "Epoch 7985 \t\t Training Loss: 0.0005809393478557467 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7986 \t\t Training Loss: 0.0005809393478557467 \t\n",
      "Epoch 7987 \t\t Training Loss: 0.0005809393478557467 \t\n",
      "Epoch 7988 \t\t Training Loss: 0.0005809393478557467 \t\n",
      "Epoch 7989 \t\t Training Loss: 0.0005809393478557467 \t\n",
      "Epoch 7990 \t\t Training Loss: 0.0005809393478557467 \t\n",
      "Epoch 7991 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 7992 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 7993 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 7994 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 7995 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 7996 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 7997 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 7998 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 7999 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 8000 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 8001 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 8002 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 8003 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 8004 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 8005 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 8006 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 8007 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 8008 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 8009 \t\t Training Loss: 0.0005809392896480858 \t\n",
      "Epoch 8010 \t\t Training Loss: 0.0005809392314404249 \t\n",
      "Epoch 8011 \t\t Training Loss: 0.0005809392314404249 \t\n",
      "Epoch 8012 \t\t Training Loss: 0.0005809392314404249 \t\n",
      "Epoch 8013 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8014 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8015 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8016 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8017 \t\t Training Loss: 0.0005809392314404249 \t\n",
      "Epoch 8018 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8019 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8020 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8021 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8022 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8023 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8024 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8025 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8026 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8027 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8028 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8029 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8030 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8031 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8032 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8033 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8034 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8035 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8036 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8037 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8038 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8039 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8040 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8041 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8042 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8043 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8044 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8045 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8046 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8047 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8048 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8049 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8050 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8051 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8052 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8053 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8054 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8055 \t\t Training Loss: 0.000580939173232764 \t\n",
      "Epoch 8056 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8057 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8058 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8059 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8060 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8061 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8062 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8063 \t\t Training Loss: 0.0005809391150251031 \t\n",
      "Epoch 8064 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8065 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8066 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8067 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8068 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8069 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8070 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8071 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8072 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8073 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8074 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8075 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8076 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8077 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8078 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8079 \t\t Training Loss: 0.0005809390568174422 \t\n",
      "Epoch 8080 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8081 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8082 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8083 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8084 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8085 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8086 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8087 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8088 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8089 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8090 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8091 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8092 \t\t Training Loss: 0.0005809389986097813 \t\n",
      "Epoch 8093 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8094 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8095 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8096 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8097 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8098 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8099 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8100 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8101 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8102 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8103 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8104 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8105 \t\t Training Loss: 0.0005809388821944594 \t\n",
      "Epoch 8106 \t\t Training Loss: 0.0005809388239867985 \t\n",
      "Epoch 8107 \t\t Training Loss: 0.0005809388239867985 \t\n",
      "Epoch 8108 \t\t Training Loss: 0.0005809388239867985 \t\n",
      "Epoch 8109 \t\t Training Loss: 0.0005809388239867985 \t\n",
      "Epoch 8110 \t\t Training Loss: 0.0005809388239867985 \t\n",
      "Epoch 8111 \t\t Training Loss: 0.0005809388239867985 \t\n",
      "Epoch 8112 \t\t Training Loss: 0.0005809388239867985 \t\n",
      "Epoch 8113 \t\t Training Loss: 0.0005809388239867985 \t\n",
      "Epoch 8114 \t\t Training Loss: 0.0005809388239867985 \t\n",
      "Epoch 8115 \t\t Training Loss: 0.0005809388239867985 \t\n",
      "Epoch 8116 \t\t Training Loss: 0.0005809388239867985 \t\n",
      "Epoch 8117 \t\t Training Loss: 0.0005809388239867985 \t\n",
      "Epoch 8118 \t\t Training Loss: 0.0005809388239867985 \t\n",
      "Epoch 8119 \t\t Training Loss: 0.0005809387657791376 \t\n",
      "Epoch 8120 \t\t Training Loss: 0.0005809387657791376 \t\n",
      "Epoch 8121 \t\t Training Loss: 0.0005809387657791376 \t\n",
      "Epoch 8122 \t\t Training Loss: 0.0005809387657791376 \t\n",
      "Epoch 8123 \t\t Training Loss: 0.0005809387657791376 \t\n",
      "Epoch 8124 \t\t Training Loss: 0.0005809387657791376 \t\n",
      "Epoch 8125 \t\t Training Loss: 0.0005809387657791376 \t\n",
      "Epoch 8126 \t\t Training Loss: 0.0005809387657791376 \t\n",
      "Epoch 8127 \t\t Training Loss: 0.0005809387657791376 \t\n",
      "Epoch 8128 \t\t Training Loss: 0.0005809387657791376 \t\n",
      "Epoch 8129 \t\t Training Loss: 0.0005809387657791376 \t\n",
      "Epoch 8130 \t\t Training Loss: 0.0005809387657791376 \t\n",
      "Epoch 8131 \t\t Training Loss: 0.0005809387657791376 \t\n",
      "Epoch 8132 \t\t Training Loss: 0.0005809387075714767 \t\n",
      "Epoch 8133 \t\t Training Loss: 0.0005809387075714767 \t\n",
      "Epoch 8134 \t\t Training Loss: 0.0005809387075714767 \t\n",
      "Epoch 8135 \t\t Training Loss: 0.0005809387075714767 \t\n",
      "Epoch 8136 \t\t Training Loss: 0.0005809387075714767 \t\n",
      "Epoch 8137 \t\t Training Loss: 0.0005809387075714767 \t\n",
      "Epoch 8138 \t\t Training Loss: 0.0005809387075714767 \t\n",
      "Epoch 8139 \t\t Training Loss: 0.0005809386493638158 \t\n",
      "Epoch 8140 \t\t Training Loss: 0.0005809386493638158 \t\n",
      "Epoch 8141 \t\t Training Loss: 0.0005809386493638158 \t\n",
      "Epoch 8142 \t\t Training Loss: 0.0005809386493638158 \t\n",
      "Epoch 8143 \t\t Training Loss: 0.0005809386493638158 \t\n",
      "Epoch 8144 \t\t Training Loss: 0.0005809386493638158 \t\n",
      "Epoch 8145 \t\t Training Loss: 0.0005809386493638158 \t\n",
      "Epoch 8146 \t\t Training Loss: 0.0005809386493638158 \t\n",
      "Epoch 8147 \t\t Training Loss: 0.0005809385911561549 \t\n",
      "Epoch 8148 \t\t Training Loss: 0.0005809385911561549 \t\n",
      "Epoch 8149 \t\t Training Loss: 0.0005809385911561549 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8150 \t\t Training Loss: 0.0005809385911561549 \t\n",
      "Epoch 8151 \t\t Training Loss: 0.0005809385911561549 \t\n",
      "Epoch 8152 \t\t Training Loss: 0.0005809385911561549 \t\n",
      "Epoch 8153 \t\t Training Loss: 0.0005809385911561549 \t\n",
      "Epoch 8154 \t\t Training Loss: 0.0005809385911561549 \t\n",
      "Epoch 8155 \t\t Training Loss: 0.0005809385911561549 \t\n",
      "Epoch 8156 \t\t Training Loss: 0.0005809385911561549 \t\n",
      "Epoch 8157 \t\t Training Loss: 0.0005809385911561549 \t\n",
      "Epoch 8158 \t\t Training Loss: 0.0005809385911561549 \t\n",
      "Epoch 8159 \t\t Training Loss: 0.0005809385911561549 \t\n",
      "Epoch 8160 \t\t Training Loss: 0.0005809385911561549 \t\n",
      "Epoch 8161 \t\t Training Loss: 0.0005809385911561549 \t\n",
      "Epoch 8162 \t\t Training Loss: 0.0005809385911561549 \t\n",
      "Epoch 8163 \t\t Training Loss: 0.0005809384165331721 \t\n",
      "Epoch 8164 \t\t Training Loss: 0.0005809384165331721 \t\n",
      "Epoch 8165 \t\t Training Loss: 0.000580938474740833 \t\n",
      "Epoch 8166 \t\t Training Loss: 0.000580938474740833 \t\n",
      "Epoch 8167 \t\t Training Loss: 0.0005809384165331721 \t\n",
      "Epoch 8168 \t\t Training Loss: 0.0005809384165331721 \t\n",
      "Epoch 8169 \t\t Training Loss: 0.0005809384165331721 \t\n",
      "Epoch 8170 \t\t Training Loss: 0.0005809384165331721 \t\n",
      "Epoch 8171 \t\t Training Loss: 0.0005809384165331721 \t\n",
      "Epoch 8172 \t\t Training Loss: 0.0005809384165331721 \t\n",
      "Epoch 8173 \t\t Training Loss: 0.0005809384165331721 \t\n",
      "Epoch 8174 \t\t Training Loss: 0.0005809384165331721 \t\n",
      "Epoch 8175 \t\t Training Loss: 0.0005809384165331721 \t\n",
      "Epoch 8176 \t\t Training Loss: 0.0005809384165331721 \t\n",
      "Epoch 8177 \t\t Training Loss: 0.0005809384165331721 \t\n",
      "Epoch 8178 \t\t Training Loss: 0.0005809384165331721 \t\n",
      "Epoch 8179 \t\t Training Loss: 0.0005809384165331721 \t\n",
      "Epoch 8180 \t\t Training Loss: 0.0005809384165331721 \t\n",
      "Epoch 8181 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8182 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8183 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8184 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8185 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8186 \t\t Training Loss: 0.0005809383001178503 \t\n",
      "Epoch 8187 \t\t Training Loss: 0.0005809383001178503 \t\n",
      "Epoch 8188 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8189 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8190 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8191 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8192 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8193 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8194 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8195 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8196 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8197 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8198 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8199 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8200 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8201 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8202 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8203 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8204 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8205 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8206 \t\t Training Loss: 0.0005809383001178503 \t\n",
      "Epoch 8207 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8208 \t\t Training Loss: 0.0005809383583255112 \t\n",
      "Epoch 8209 \t\t Training Loss: 0.0005809383001178503 \t\n",
      "Epoch 8210 \t\t Training Loss: 0.0005809383001178503 \t\n",
      "Epoch 8211 \t\t Training Loss: 0.0005809383001178503 \t\n",
      "Epoch 8212 \t\t Training Loss: 0.0005809383001178503 \t\n",
      "Epoch 8213 \t\t Training Loss: 0.0005809383001178503 \t\n",
      "Epoch 8214 \t\t Training Loss: 0.0005809383001178503 \t\n",
      "Epoch 8215 \t\t Training Loss: 0.0005809383001178503 \t\n",
      "Epoch 8216 \t\t Training Loss: 0.0005809383001178503 \t\n",
      "Epoch 8217 \t\t Training Loss: 0.0005809383001178503 \t\n",
      "Epoch 8218 \t\t Training Loss: 0.0005809383001178503 \t\n",
      "Epoch 8219 \t\t Training Loss: 0.0005809383001178503 \t\n",
      "Epoch 8220 \t\t Training Loss: 0.0005809383001178503 \t\n",
      "Epoch 8221 \t\t Training Loss: 0.0005809383001178503 \t\n",
      "Epoch 8222 \t\t Training Loss: 0.0005809381837025285 \t\n",
      "Epoch 8223 \t\t Training Loss: 0.0005809381254948676 \t\n",
      "Epoch 8224 \t\t Training Loss: 0.0005809381254948676 \t\n",
      "Epoch 8225 \t\t Training Loss: 0.0005809381254948676 \t\n",
      "Epoch 8226 \t\t Training Loss: 0.0005809381254948676 \t\n",
      "Epoch 8227 \t\t Training Loss: 0.0005809381254948676 \t\n",
      "Epoch 8228 \t\t Training Loss: 0.0005809381254948676 \t\n",
      "Epoch 8229 \t\t Training Loss: 0.0005809381254948676 \t\n",
      "Epoch 8230 \t\t Training Loss: 0.0005809380672872066 \t\n",
      "Epoch 8231 \t\t Training Loss: 0.0005809381254948676 \t\n",
      "Epoch 8232 \t\t Training Loss: 0.0005809380672872066 \t\n",
      "Epoch 8233 \t\t Training Loss: 0.0005809380672872066 \t\n",
      "Epoch 8234 \t\t Training Loss: 0.0005809380672872066 \t\n",
      "Epoch 8235 \t\t Training Loss: 0.0005809380672872066 \t\n",
      "Epoch 8236 \t\t Training Loss: 0.0005809380672872066 \t\n",
      "Epoch 8237 \t\t Training Loss: 0.0005809380672872066 \t\n",
      "Epoch 8238 \t\t Training Loss: 0.0005809380672872066 \t\n",
      "Epoch 8239 \t\t Training Loss: 0.0005809380672872066 \t\n",
      "Epoch 8240 \t\t Training Loss: 0.0005809380672872066 \t\n",
      "Epoch 8241 \t\t Training Loss: 0.0005809380672872066 \t\n",
      "Epoch 8242 \t\t Training Loss: 0.0005809380672872066 \t\n",
      "Epoch 8243 \t\t Training Loss: 0.0005809380672872066 \t\n",
      "Epoch 8244 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8245 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8246 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8247 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8248 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8249 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8250 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8251 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8252 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8253 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8254 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8255 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8256 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8257 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8258 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8259 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8260 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8261 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8262 \t\t Training Loss: 0.0005809380090795457 \t\n",
      "Epoch 8263 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8264 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8265 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8266 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8267 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8268 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8269 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8270 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8271 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8272 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8273 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8274 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8275 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8276 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8277 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8278 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8279 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8280 \t\t Training Loss: 0.0005809378926642239 \t\n",
      "Epoch 8281 \t\t Training Loss: 0.0005809377762489021 \t\n",
      "Epoch 8282 \t\t Training Loss: 0.0005809377762489021 \t\n",
      "Epoch 8283 \t\t Training Loss: 0.0005809377762489021 \t\n",
      "Epoch 8284 \t\t Training Loss: 0.0005809377762489021 \t\n",
      "Epoch 8285 \t\t Training Loss: 0.0005809377762489021 \t\n",
      "Epoch 8286 \t\t Training Loss: 0.0005809377762489021 \t\n",
      "Epoch 8287 \t\t Training Loss: 0.0005809377762489021 \t\n",
      "Epoch 8288 \t\t Training Loss: 0.0005809377762489021 \t\n",
      "Epoch 8289 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8290 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8291 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8292 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8293 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8294 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8295 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8296 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8297 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8298 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8299 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8300 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8301 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8302 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8303 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8304 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8305 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8306 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8307 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8308 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8309 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8310 \t\t Training Loss: 0.0005809377180412412 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8311 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8312 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8313 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8314 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8315 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8316 \t\t Training Loss: 0.0005809376016259193 \t\n",
      "Epoch 8317 \t\t Training Loss: 0.0005809376016259193 \t\n",
      "Epoch 8318 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8319 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8320 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8321 \t\t Training Loss: 0.0005809377180412412 \t\n",
      "Epoch 8322 \t\t Training Loss: 0.0005809376016259193 \t\n",
      "Epoch 8323 \t\t Training Loss: 0.0005809376016259193 \t\n",
      "Epoch 8324 \t\t Training Loss: 0.0005809376016259193 \t\n",
      "Epoch 8325 \t\t Training Loss: 0.0005809376016259193 \t\n",
      "Epoch 8326 \t\t Training Loss: 0.0005809376016259193 \t\n",
      "Epoch 8327 \t\t Training Loss: 0.0005809376016259193 \t\n",
      "Epoch 8328 \t\t Training Loss: 0.0005809376016259193 \t\n",
      "Epoch 8329 \t\t Training Loss: 0.0005809376016259193 \t\n",
      "Epoch 8330 \t\t Training Loss: 0.0005809376016259193 \t\n",
      "Epoch 8331 \t\t Training Loss: 0.0005809376016259193 \t\n",
      "Epoch 8332 \t\t Training Loss: 0.0005809376016259193 \t\n",
      "Epoch 8333 \t\t Training Loss: 0.0005809376016259193 \t\n",
      "Epoch 8334 \t\t Training Loss: 0.0005809376016259193 \t\n",
      "Epoch 8335 \t\t Training Loss: 0.0005809376016259193 \t\n",
      "Epoch 8336 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8337 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8338 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8339 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8340 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8341 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8342 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8343 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8344 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8345 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8346 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8347 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8348 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8349 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8350 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8351 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8352 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8353 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8354 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8355 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8356 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8357 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8358 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8359 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8360 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8361 \t\t Training Loss: 0.0005809374852105975 \t\n",
      "Epoch 8362 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8363 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8364 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8365 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8366 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8367 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8368 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8369 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8370 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8371 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8372 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8373 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8374 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8375 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8376 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8377 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8378 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8379 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8380 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8381 \t\t Training Loss: 0.0005809373687952757 \t\n",
      "Epoch 8382 \t\t Training Loss: 0.0005809373105876148 \t\n",
      "Epoch 8383 \t\t Training Loss: 0.0005809373105876148 \t\n",
      "Epoch 8384 \t\t Training Loss: 0.0005809373105876148 \t\n",
      "Epoch 8385 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8386 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8387 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8388 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8389 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8390 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8391 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8392 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8393 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8394 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8395 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8396 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8397 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8398 \t\t Training Loss: 0.0005809370777569711 \t\n",
      "Epoch 8399 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8400 \t\t Training Loss: 0.0005809370777569711 \t\n",
      "Epoch 8401 \t\t Training Loss: 0.0005809370777569711 \t\n",
      "Epoch 8402 \t\t Training Loss: 0.0005809370777569711 \t\n",
      "Epoch 8403 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8404 \t\t Training Loss: 0.0005809370777569711 \t\n",
      "Epoch 8405 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8406 \t\t Training Loss: 0.000580937194172293 \t\n",
      "Epoch 8407 \t\t Training Loss: 0.0005809370777569711 \t\n",
      "Epoch 8408 \t\t Training Loss: 0.0005809370777569711 \t\n",
      "Epoch 8409 \t\t Training Loss: 0.0005809370777569711 \t\n",
      "Epoch 8410 \t\t Training Loss: 0.0005809370777569711 \t\n",
      "Epoch 8411 \t\t Training Loss: 0.0005809370777569711 \t\n",
      "Epoch 8412 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8413 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8414 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8415 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8416 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8417 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8418 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8419 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8420 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8421 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8422 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8423 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8424 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8425 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8426 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8427 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8428 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8429 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8430 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8431 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8432 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8433 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8434 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8435 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8436 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8437 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8438 \t\t Training Loss: 0.0005809370195493102 \t\n",
      "Epoch 8439 \t\t Training Loss: 0.0005809369031339884 \t\n",
      "Epoch 8440 \t\t Training Loss: 0.0005809369031339884 \t\n",
      "Epoch 8441 \t\t Training Loss: 0.0005809369031339884 \t\n",
      "Epoch 8442 \t\t Training Loss: 0.0005809369031339884 \t\n",
      "Epoch 8443 \t\t Training Loss: 0.0005809369031339884 \t\n",
      "Epoch 8444 \t\t Training Loss: 0.0005809369031339884 \t\n",
      "Epoch 8445 \t\t Training Loss: 0.0005809369031339884 \t\n",
      "Epoch 8446 \t\t Training Loss: 0.0005809367867186666 \t\n",
      "Epoch 8447 \t\t Training Loss: 0.0005809369031339884 \t\n",
      "Epoch 8448 \t\t Training Loss: 0.0005809367867186666 \t\n",
      "Epoch 8449 \t\t Training Loss: 0.0005809369031339884 \t\n",
      "Epoch 8450 \t\t Training Loss: 0.0005809369031339884 \t\n",
      "Epoch 8451 \t\t Training Loss: 0.0005809367867186666 \t\n",
      "Epoch 8452 \t\t Training Loss: 0.0005809367867186666 \t\n",
      "Epoch 8453 \t\t Training Loss: 0.0005809367867186666 \t\n",
      "Epoch 8454 \t\t Training Loss: 0.0005809367867186666 \t\n",
      "Epoch 8455 \t\t Training Loss: 0.0005809367867186666 \t\n",
      "Epoch 8456 \t\t Training Loss: 0.0005809367867186666 \t\n",
      "Epoch 8457 \t\t Training Loss: 0.0005809367867186666 \t\n",
      "Epoch 8458 \t\t Training Loss: 0.0005809367285110056 \t\n",
      "Epoch 8459 \t\t Training Loss: 0.0005809367285110056 \t\n",
      "Epoch 8460 \t\t Training Loss: 0.0005809367285110056 \t\n",
      "Epoch 8461 \t\t Training Loss: 0.0005809367285110056 \t\n",
      "Epoch 8462 \t\t Training Loss: 0.0005809367285110056 \t\n",
      "Epoch 8463 \t\t Training Loss: 0.0005809367285110056 \t\n",
      "Epoch 8464 \t\t Training Loss: 0.0005809367285110056 \t\n",
      "Epoch 8465 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8466 \t\t Training Loss: 0.0005809367285110056 \t\n",
      "Epoch 8467 \t\t Training Loss: 0.0005809367285110056 \t\n",
      "Epoch 8468 \t\t Training Loss: 0.0005809367285110056 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8469 \t\t Training Loss: 0.0005809367285110056 \t\n",
      "Epoch 8470 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8471 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8472 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8473 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8474 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8475 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8476 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8477 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8478 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8479 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8480 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8481 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8482 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8483 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8484 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8485 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8486 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8487 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8488 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8489 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8490 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8491 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8492 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8493 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8494 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8495 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8496 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8497 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8498 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8499 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8500 \t\t Training Loss: 0.0005809366120956838 \t\n",
      "Epoch 8501 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8502 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8503 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8504 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8505 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8506 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8507 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8508 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8509 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8510 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8511 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8512 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8513 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8514 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8515 \t\t Training Loss: 0.000580936495680362 \t\n",
      "Epoch 8516 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8517 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8518 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8519 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8520 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8521 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8522 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8523 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8524 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8525 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8526 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8527 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8528 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8529 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8530 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8531 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8532 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8533 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8534 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8535 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8536 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8537 \t\t Training Loss: 0.0005809363792650402 \t\n",
      "Epoch 8538 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8539 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8540 \t\t Training Loss: 0.0005809364374727011 \t\n",
      "Epoch 8541 \t\t Training Loss: 0.0005809363792650402 \t\n",
      "Epoch 8542 \t\t Training Loss: 0.0005809363792650402 \t\n",
      "Epoch 8543 \t\t Training Loss: 0.0005809363792650402 \t\n",
      "Epoch 8544 \t\t Training Loss: 0.0005809363792650402 \t\n",
      "Epoch 8545 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8546 \t\t Training Loss: 0.0005809363792650402 \t\n",
      "Epoch 8547 \t\t Training Loss: 0.0005809363792650402 \t\n",
      "Epoch 8548 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8549 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8550 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8551 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8552 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8553 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8554 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8555 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8556 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8557 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8558 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8559 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8560 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8561 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8562 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8563 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8564 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8565 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8566 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8567 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8568 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8569 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8570 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8571 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8572 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8573 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8574 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8575 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8576 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8577 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8578 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8579 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8580 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8581 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8582 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8583 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8584 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8585 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8586 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8587 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8588 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8589 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8590 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8591 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8592 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8593 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8594 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8595 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8596 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8597 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8598 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8599 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8600 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8601 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8602 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8603 \t\t Training Loss: 0.0005809361464343965 \t\n",
      "Epoch 8604 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8605 \t\t Training Loss: 0.0005809362046420574 \t\n",
      "Epoch 8606 \t\t Training Loss: 0.0005809361464343965 \t\n",
      "Epoch 8607 \t\t Training Loss: 0.0005809361464343965 \t\n",
      "Epoch 8608 \t\t Training Loss: 0.0005809361464343965 \t\n",
      "Epoch 8609 \t\t Training Loss: 0.0005809360882267356 \t\n",
      "Epoch 8610 \t\t Training Loss: 0.0005809360882267356 \t\n",
      "Epoch 8611 \t\t Training Loss: 0.0005809360882267356 \t\n",
      "Epoch 8612 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8613 \t\t Training Loss: 0.0005809360882267356 \t\n",
      "Epoch 8614 \t\t Training Loss: 0.0005809360882267356 \t\n",
      "Epoch 8615 \t\t Training Loss: 0.0005809360882267356 \t\n",
      "Epoch 8616 \t\t Training Loss: 0.0005809360882267356 \t\n",
      "Epoch 8617 \t\t Training Loss: 0.0005809360882267356 \t\n",
      "Epoch 8618 \t\t Training Loss: 0.0005809360882267356 \t\n",
      "Epoch 8619 \t\t Training Loss: 0.0005809360882267356 \t\n",
      "Epoch 8620 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8621 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8622 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8623 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8624 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8625 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8626 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8627 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8628 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8629 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8630 \t\t Training Loss: 0.0005809360300190747 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8631 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8632 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8633 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8634 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8635 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8636 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8637 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8638 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8639 \t\t Training Loss: 0.0005809360300190747 \t\n",
      "Epoch 8640 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8641 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8642 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8643 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8644 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8645 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8646 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8647 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8648 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8649 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8650 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8651 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8652 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8653 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8654 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8655 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8656 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8657 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8658 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8659 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8660 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8661 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8662 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8663 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8664 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8665 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8666 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8667 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8668 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8669 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8670 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8671 \t\t Training Loss: 0.0005809359136037529 \t\n",
      "Epoch 8672 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8673 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8674 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8675 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8676 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8677 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8678 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8679 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8680 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8681 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8682 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8683 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8684 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8685 \t\t Training Loss: 0.000580935797188431 \t\n",
      "Epoch 8686 \t\t Training Loss: 0.0005809356807731092 \t\n",
      "Epoch 8687 \t\t Training Loss: 0.0005809356807731092 \t\n",
      "Epoch 8688 \t\t Training Loss: 0.0005809356807731092 \t\n",
      "Epoch 8689 \t\t Training Loss: 0.0005809356807731092 \t\n",
      "Epoch 8690 \t\t Training Loss: 0.0005809356807731092 \t\n",
      "Epoch 8691 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8692 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8693 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8694 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8695 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8696 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8697 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8698 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8699 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8700 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8701 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8702 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8703 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8704 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8705 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8706 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8707 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8708 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8709 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8710 \t\t Training Loss: 0.0005809356225654483 \t\n",
      "Epoch 8711 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8712 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8713 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8714 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8715 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8716 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8717 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8718 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8719 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8720 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8721 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8722 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8723 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8724 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8725 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8726 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8727 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8728 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8729 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8730 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8731 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8732 \t\t Training Loss: 0.0005809354479424655 \t\n",
      "Epoch 8733 \t\t Training Loss: 0.0005809355061501265 \t\n",
      "Epoch 8734 \t\t Training Loss: 0.0005809354479424655 \t\n",
      "Epoch 8735 \t\t Training Loss: 0.0005809354479424655 \t\n",
      "Epoch 8736 \t\t Training Loss: 0.0005809353897348046 \t\n",
      "Epoch 8737 \t\t Training Loss: 0.0005809353897348046 \t\n",
      "Epoch 8738 \t\t Training Loss: 0.0005809353897348046 \t\n",
      "Epoch 8739 \t\t Training Loss: 0.0005809354479424655 \t\n",
      "Epoch 8740 \t\t Training Loss: 0.0005809354479424655 \t\n",
      "Epoch 8741 \t\t Training Loss: 0.0005809354479424655 \t\n",
      "Epoch 8742 \t\t Training Loss: 0.0005809353897348046 \t\n",
      "Epoch 8743 \t\t Training Loss: 0.0005809353897348046 \t\n",
      "Epoch 8744 \t\t Training Loss: 0.0005809353897348046 \t\n",
      "Epoch 8745 \t\t Training Loss: 0.0005809353897348046 \t\n",
      "Epoch 8746 \t\t Training Loss: 0.0005809353897348046 \t\n",
      "Epoch 8747 \t\t Training Loss: 0.0005809353897348046 \t\n",
      "Epoch 8748 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8749 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8750 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8751 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8752 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8753 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8754 \t\t Training Loss: 0.0005809353897348046 \t\n",
      "Epoch 8755 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8756 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8757 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8758 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8759 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8760 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8761 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8762 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8763 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8764 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8765 \t\t Training Loss: 0.0005809353897348046 \t\n",
      "Epoch 8766 \t\t Training Loss: 0.0005809352733194828 \t\n",
      "Epoch 8767 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8768 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8769 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8770 \t\t Training Loss: 0.0005809353315271437 \t\n",
      "Epoch 8771 \t\t Training Loss: 0.0005809352733194828 \t\n",
      "Epoch 8772 \t\t Training Loss: 0.0005809352733194828 \t\n",
      "Epoch 8773 \t\t Training Loss: 0.0005809352733194828 \t\n",
      "Epoch 8774 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8775 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8776 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8777 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8778 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8779 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8780 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8781 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8782 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8783 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8784 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8785 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8786 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8787 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8788 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8789 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8790 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8791 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8792 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8793 \t\t Training Loss: 0.0005809352151118219 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8794 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8795 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8796 \t\t Training Loss: 0.0005809350986965001 \t\n",
      "Epoch 8797 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8798 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8799 \t\t Training Loss: 0.0005809350986965001 \t\n",
      "Epoch 8800 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8801 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8802 \t\t Training Loss: 0.0005809350986965001 \t\n",
      "Epoch 8803 \t\t Training Loss: 0.0005809350986965001 \t\n",
      "Epoch 8804 \t\t Training Loss: 0.0005809350986965001 \t\n",
      "Epoch 8805 \t\t Training Loss: 0.0005809350986965001 \t\n",
      "Epoch 8806 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8807 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8808 \t\t Training Loss: 0.000580935156904161 \t\n",
      "Epoch 8809 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8810 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8811 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8812 \t\t Training Loss: 0.0005809352151118219 \t\n",
      "Epoch 8813 \t\t Training Loss: 0.0005809350986965001 \t\n",
      "Epoch 8814 \t\t Training Loss: 0.0005809350986965001 \t\n",
      "Epoch 8815 \t\t Training Loss: 0.0005809350986965001 \t\n",
      "Epoch 8816 \t\t Training Loss: 0.0005809350986965001 \t\n",
      "Epoch 8817 \t\t Training Loss: 0.0005809350986965001 \t\n",
      "Epoch 8818 \t\t Training Loss: 0.0005809350986965001 \t\n",
      "Epoch 8819 \t\t Training Loss: 0.0005809350986965001 \t\n",
      "Epoch 8820 \t\t Training Loss: 0.0005809350986965001 \t\n",
      "Epoch 8821 \t\t Training Loss: 0.0005809350986965001 \t\n",
      "Epoch 8822 \t\t Training Loss: 0.0005809350404888391 \t\n",
      "Epoch 8823 \t\t Training Loss: 0.0005809350404888391 \t\n",
      "Epoch 8824 \t\t Training Loss: 0.0005809349822811782 \t\n",
      "Epoch 8825 \t\t Training Loss: 0.0005809350404888391 \t\n",
      "Epoch 8826 \t\t Training Loss: 0.0005809350404888391 \t\n",
      "Epoch 8827 \t\t Training Loss: 0.0005809350404888391 \t\n",
      "Epoch 8828 \t\t Training Loss: 0.0005809350404888391 \t\n",
      "Epoch 8829 \t\t Training Loss: 0.0005809349822811782 \t\n",
      "Epoch 8830 \t\t Training Loss: 0.0005809349822811782 \t\n",
      "Epoch 8831 \t\t Training Loss: 0.0005809349822811782 \t\n",
      "Epoch 8832 \t\t Training Loss: 0.0005809349822811782 \t\n",
      "Epoch 8833 \t\t Training Loss: 0.0005809349822811782 \t\n",
      "Epoch 8834 \t\t Training Loss: 0.0005809349822811782 \t\n",
      "Epoch 8835 \t\t Training Loss: 0.0005809349822811782 \t\n",
      "Epoch 8836 \t\t Training Loss: 0.0005809349240735173 \t\n",
      "Epoch 8837 \t\t Training Loss: 0.0005809349240735173 \t\n",
      "Epoch 8838 \t\t Training Loss: 0.0005809349240735173 \t\n",
      "Epoch 8839 \t\t Training Loss: 0.0005809349240735173 \t\n",
      "Epoch 8840 \t\t Training Loss: 0.0005809349240735173 \t\n",
      "Epoch 8841 \t\t Training Loss: 0.0005809349240735173 \t\n",
      "Epoch 8842 \t\t Training Loss: 0.0005809349240735173 \t\n",
      "Epoch 8843 \t\t Training Loss: 0.0005809349240735173 \t\n",
      "Epoch 8844 \t\t Training Loss: 0.0005809349240735173 \t\n",
      "Epoch 8845 \t\t Training Loss: 0.0005809348076581955 \t\n",
      "Epoch 8846 \t\t Training Loss: 0.0005809348076581955 \t\n",
      "Epoch 8847 \t\t Training Loss: 0.0005809348076581955 \t\n",
      "Epoch 8848 \t\t Training Loss: 0.0005809348076581955 \t\n",
      "Epoch 8849 \t\t Training Loss: 0.0005809348076581955 \t\n",
      "Epoch 8850 \t\t Training Loss: 0.0005809348076581955 \t\n",
      "Epoch 8851 \t\t Training Loss: 0.0005809348076581955 \t\n",
      "Epoch 8852 \t\t Training Loss: 0.0005809348076581955 \t\n",
      "Epoch 8853 \t\t Training Loss: 0.0005809348076581955 \t\n",
      "Epoch 8854 \t\t Training Loss: 0.0005809348076581955 \t\n",
      "Epoch 8855 \t\t Training Loss: 0.0005809348076581955 \t\n",
      "Epoch 8856 \t\t Training Loss: 0.0005809348076581955 \t\n",
      "Epoch 8857 \t\t Training Loss: 0.0005809348076581955 \t\n",
      "Epoch 8858 \t\t Training Loss: 0.0005809348076581955 \t\n",
      "Epoch 8859 \t\t Training Loss: 0.0005809348076581955 \t\n",
      "Epoch 8860 \t\t Training Loss: 0.0005809348076581955 \t\n",
      "Epoch 8861 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8862 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8863 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8864 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8865 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8866 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8867 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8868 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8869 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8870 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8871 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8872 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8873 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8874 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8875 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8876 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8877 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8878 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8879 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8880 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8881 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8882 \t\t Training Loss: 0.0005809347494505346 \t\n",
      "Epoch 8883 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8884 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8885 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8886 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8887 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8888 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8889 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8890 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8891 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8892 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8893 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8894 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8895 \t\t Training Loss: 0.0005809346330352128 \t\n",
      "Epoch 8896 \t\t Training Loss: 0.0005809346330352128 \t\n",
      "Epoch 8897 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8898 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8899 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8900 \t\t Training Loss: 0.0005809346330352128 \t\n",
      "Epoch 8901 \t\t Training Loss: 0.0005809346912428737 \t\n",
      "Epoch 8902 \t\t Training Loss: 0.0005809346330352128 \t\n",
      "Epoch 8903 \t\t Training Loss: 0.0005809345166198909 \t\n",
      "Epoch 8904 \t\t Training Loss: 0.0005809345166198909 \t\n",
      "Epoch 8905 \t\t Training Loss: 0.0005809346330352128 \t\n",
      "Epoch 8906 \t\t Training Loss: 0.0005809345166198909 \t\n",
      "Epoch 8907 \t\t Training Loss: 0.0005809345166198909 \t\n",
      "Epoch 8908 \t\t Training Loss: 0.0005809345166198909 \t\n",
      "Epoch 8909 \t\t Training Loss: 0.0005809345166198909 \t\n",
      "Epoch 8910 \t\t Training Loss: 0.0005809345166198909 \t\n",
      "Epoch 8911 \t\t Training Loss: 0.0005809345166198909 \t\n",
      "Epoch 8912 \t\t Training Loss: 0.0005809345166198909 \t\n",
      "Epoch 8913 \t\t Training Loss: 0.00058093445841223 \t\n",
      "Epoch 8914 \t\t Training Loss: 0.00058093445841223 \t\n",
      "Epoch 8915 \t\t Training Loss: 0.00058093445841223 \t\n",
      "Epoch 8916 \t\t Training Loss: 0.0005809344002045691 \t\n",
      "Epoch 8917 \t\t Training Loss: 0.00058093445841223 \t\n",
      "Epoch 8918 \t\t Training Loss: 0.0005809344002045691 \t\n",
      "Epoch 8919 \t\t Training Loss: 0.0005809343419969082 \t\n",
      "Epoch 8920 \t\t Training Loss: 0.0005809343419969082 \t\n",
      "Epoch 8921 \t\t Training Loss: 0.0005809343419969082 \t\n",
      "Epoch 8922 \t\t Training Loss: 0.0005809343419969082 \t\n",
      "Epoch 8923 \t\t Training Loss: 0.0005809343419969082 \t\n",
      "Epoch 8924 \t\t Training Loss: 0.0005809343419969082 \t\n",
      "Epoch 8925 \t\t Training Loss: 0.0005809343419969082 \t\n",
      "Epoch 8926 \t\t Training Loss: 0.0005809343419969082 \t\n",
      "Epoch 8927 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8928 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8929 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8930 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8931 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8932 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8933 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8934 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8935 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8936 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8937 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8938 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8939 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8940 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8941 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8942 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8943 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8944 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8945 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8946 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8947 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8948 \t\t Training Loss: 0.0005809342837892473 \t\n",
      "Epoch 8949 \t\t Training Loss: 0.0005809342255815864 \t\n",
      "Epoch 8950 \t\t Training Loss: 0.0005809342255815864 \t\n",
      "Epoch 8951 \t\t Training Loss: 0.0005809342255815864 \t\n",
      "Epoch 8952 \t\t Training Loss: 0.0005809341673739254 \t\n",
      "Epoch 8953 \t\t Training Loss: 0.0005809341673739254 \t\n",
      "Epoch 8954 \t\t Training Loss: 0.0005809341673739254 \t\n",
      "Epoch 8955 \t\t Training Loss: 0.0005809341673739254 \t\n",
      "Epoch 8956 \t\t Training Loss: 0.0005809341673739254 \t\n",
      "Epoch 8957 \t\t Training Loss: 0.0005809341673739254 \t\n",
      "Epoch 8958 \t\t Training Loss: 0.0005809341673739254 \t\n",
      "Epoch 8959 \t\t Training Loss: 0.0005809341673739254 \t\n",
      "Epoch 8960 \t\t Training Loss: 0.0005809340509586036 \t\n",
      "Epoch 8961 \t\t Training Loss: 0.0005809340509586036 \t\n",
      "Epoch 8962 \t\t Training Loss: 0.0005809340509586036 \t\n",
      "Epoch 8963 \t\t Training Loss: 0.0005809340509586036 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8964 \t\t Training Loss: 0.0005809339927509427 \t\n",
      "Epoch 8965 \t\t Training Loss: 0.0005809339927509427 \t\n",
      "Epoch 8966 \t\t Training Loss: 0.0005809339927509427 \t\n",
      "Epoch 8967 \t\t Training Loss: 0.0005809339927509427 \t\n",
      "Epoch 8968 \t\t Training Loss: 0.0005809339345432818 \t\n",
      "Epoch 8969 \t\t Training Loss: 0.0005809339345432818 \t\n",
      "Epoch 8970 \t\t Training Loss: 0.0005809339927509427 \t\n",
      "Epoch 8971 \t\t Training Loss: 0.0005809339345432818 \t\n",
      "Epoch 8972 \t\t Training Loss: 0.0005809339345432818 \t\n",
      "Epoch 8973 \t\t Training Loss: 0.0005809339345432818 \t\n",
      "Epoch 8974 \t\t Training Loss: 0.0005809339345432818 \t\n",
      "Epoch 8975 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8976 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8977 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8978 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8979 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8980 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8981 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8982 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8983 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8984 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8985 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8986 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8987 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8988 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8989 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8990 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8991 \t\t Training Loss: 0.0005809338763356209 \t\n",
      "Epoch 8992 \t\t Training Loss: 0.00058093381812796 \t\n",
      "Epoch 8993 \t\t Training Loss: 0.00058093381812796 \t\n",
      "Epoch 8994 \t\t Training Loss: 0.00058093381812796 \t\n",
      "Epoch 8995 \t\t Training Loss: 0.00058093381812796 \t\n",
      "Epoch 8996 \t\t Training Loss: 0.00058093381812796 \t\n",
      "Epoch 8997 \t\t Training Loss: 0.00058093381812796 \t\n",
      "Epoch 8998 \t\t Training Loss: 0.00058093381812796 \t\n",
      "Epoch 8999 \t\t Training Loss: 0.00058093381812796 \t\n",
      "Epoch 9000 \t\t Training Loss: 0.00058093381812796 \t\n",
      "Epoch 9001 \t\t Training Loss: 0.00058093381812796 \t\n",
      "Epoch 9002 \t\t Training Loss: 0.00058093381812796 \t\n",
      "Epoch 9003 \t\t Training Loss: 0.00058093381812796 \t\n",
      "Epoch 9004 \t\t Training Loss: 0.00058093381812796 \t\n",
      "Epoch 9005 \t\t Training Loss: 0.000580933759920299 \t\n",
      "Epoch 9006 \t\t Training Loss: 0.000580933759920299 \t\n",
      "Epoch 9007 \t\t Training Loss: 0.000580933759920299 \t\n",
      "Epoch 9008 \t\t Training Loss: 0.000580933759920299 \t\n",
      "Epoch 9009 \t\t Training Loss: 0.000580933759920299 \t\n",
      "Epoch 9010 \t\t Training Loss: 0.000580933759920299 \t\n",
      "Epoch 9011 \t\t Training Loss: 0.000580933759920299 \t\n",
      "Epoch 9012 \t\t Training Loss: 0.000580933759920299 \t\n",
      "Epoch 9013 \t\t Training Loss: 0.000580933759920299 \t\n",
      "Epoch 9014 \t\t Training Loss: 0.0005809337017126381 \t\n",
      "Epoch 9015 \t\t Training Loss: 0.000580933759920299 \t\n",
      "Epoch 9016 \t\t Training Loss: 0.0005809336435049772 \t\n",
      "Epoch 9017 \t\t Training Loss: 0.0005809337017126381 \t\n",
      "Epoch 9018 \t\t Training Loss: 0.0005809337017126381 \t\n",
      "Epoch 9019 \t\t Training Loss: 0.0005809337017126381 \t\n",
      "Epoch 9020 \t\t Training Loss: 0.0005809337017126381 \t\n",
      "Epoch 9021 \t\t Training Loss: 0.0005809337017126381 \t\n",
      "Epoch 9022 \t\t Training Loss: 0.0005809337017126381 \t\n",
      "Epoch 9023 \t\t Training Loss: 0.0005809336435049772 \t\n",
      "Epoch 9024 \t\t Training Loss: 0.0005809336435049772 \t\n",
      "Epoch 9025 \t\t Training Loss: 0.0005809336435049772 \t\n",
      "Epoch 9026 \t\t Training Loss: 0.0005809336435049772 \t\n",
      "Epoch 9027 \t\t Training Loss: 0.0005809336435049772 \t\n",
      "Epoch 9028 \t\t Training Loss: 0.0005809336435049772 \t\n",
      "Epoch 9029 \t\t Training Loss: 0.0005809336435049772 \t\n",
      "Epoch 9030 \t\t Training Loss: 0.0005809336435049772 \t\n",
      "Epoch 9031 \t\t Training Loss: 0.0005809336435049772 \t\n",
      "Epoch 9032 \t\t Training Loss: 0.0005809335852973163 \t\n",
      "Epoch 9033 \t\t Training Loss: 0.0005809335852973163 \t\n",
      "Epoch 9034 \t\t Training Loss: 0.0005809335852973163 \t\n",
      "Epoch 9035 \t\t Training Loss: 0.0005809335852973163 \t\n",
      "Epoch 9036 \t\t Training Loss: 0.0005809335852973163 \t\n",
      "Epoch 9037 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9038 \t\t Training Loss: 0.0005809335852973163 \t\n",
      "Epoch 9039 \t\t Training Loss: 0.0005809335852973163 \t\n",
      "Epoch 9040 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9041 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9042 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9043 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9044 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9045 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9046 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9047 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9048 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9049 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9050 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9051 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9052 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9053 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9054 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9055 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9056 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9057 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9058 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9059 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9060 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9061 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9062 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9063 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9064 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9065 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9066 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9067 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9068 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9069 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9070 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9071 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9072 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9073 \t\t Training Loss: 0.0005809334688819945 \t\n",
      "Epoch 9074 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9075 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9076 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9077 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9078 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9079 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9080 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9081 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9082 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9083 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9084 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9085 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9086 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9087 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9088 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9089 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9090 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9091 \t\t Training Loss: 0.0005809333524666727 \t\n",
      "Epoch 9092 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9093 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9094 \t\t Training Loss: 0.0005809334106743336 \t\n",
      "Epoch 9095 \t\t Training Loss: 0.0005809333524666727 \t\n",
      "Epoch 9096 \t\t Training Loss: 0.0005809333524666727 \t\n",
      "Epoch 9097 \t\t Training Loss: 0.0005809333524666727 \t\n",
      "Epoch 9098 \t\t Training Loss: 0.0005809333524666727 \t\n",
      "Epoch 9099 \t\t Training Loss: 0.0005809333524666727 \t\n",
      "Epoch 9100 \t\t Training Loss: 0.0005809333524666727 \t\n",
      "Epoch 9101 \t\t Training Loss: 0.0005809333524666727 \t\n",
      "Epoch 9102 \t\t Training Loss: 0.0005809333524666727 \t\n",
      "Epoch 9103 \t\t Training Loss: 0.0005809333524666727 \t\n",
      "Epoch 9104 \t\t Training Loss: 0.0005809333524666727 \t\n",
      "Epoch 9105 \t\t Training Loss: 0.0005809333524666727 \t\n",
      "Epoch 9106 \t\t Training Loss: 0.0005809333524666727 \t\n",
      "Epoch 9107 \t\t Training Loss: 0.0005809332942590117 \t\n",
      "Epoch 9108 \t\t Training Loss: 0.0005809332942590117 \t\n",
      "Epoch 9109 \t\t Training Loss: 0.0005809333524666727 \t\n",
      "Epoch 9110 \t\t Training Loss: 0.0005809332942590117 \t\n",
      "Epoch 9111 \t\t Training Loss: 0.0005809332942590117 \t\n",
      "Epoch 9112 \t\t Training Loss: 0.0005809333524666727 \t\n",
      "Epoch 9113 \t\t Training Loss: 0.0005809332942590117 \t\n",
      "Epoch 9114 \t\t Training Loss: 0.0005809332942590117 \t\n",
      "Epoch 9115 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9116 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9117 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9118 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9119 \t\t Training Loss: 0.0005809332360513508 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9120 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9121 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9122 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9123 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9124 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9125 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9126 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9127 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9128 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9129 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9130 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9131 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9132 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9133 \t\t Training Loss: 0.0005809331778436899 \t\n",
      "Epoch 9134 \t\t Training Loss: 0.0005809332360513508 \t\n",
      "Epoch 9135 \t\t Training Loss: 0.0005809331778436899 \t\n",
      "Epoch 9136 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9137 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9138 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9139 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9140 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9141 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9142 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9143 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9144 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9145 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9146 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9147 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9148 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9149 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9150 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9151 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9152 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9153 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9154 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9155 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9156 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9157 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9158 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9159 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9160 \t\t Training Loss: 0.0005809330614283681 \t\n",
      "Epoch 9161 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9162 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9163 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9164 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9165 \t\t Training Loss: 0.000580933119636029 \t\n",
      "Epoch 9166 \t\t Training Loss: 0.0005809330614283681 \t\n",
      "Epoch 9167 \t\t Training Loss: 0.0005809330614283681 \t\n",
      "Epoch 9168 \t\t Training Loss: 0.0005809330614283681 \t\n",
      "Epoch 9169 \t\t Training Loss: 0.0005809330614283681 \t\n",
      "Epoch 9170 \t\t Training Loss: 0.0005809330614283681 \t\n",
      "Epoch 9171 \t\t Training Loss: 0.0005809330614283681 \t\n",
      "Epoch 9172 \t\t Training Loss: 0.0005809330614283681 \t\n",
      "Epoch 9173 \t\t Training Loss: 0.0005809330614283681 \t\n",
      "Epoch 9174 \t\t Training Loss: 0.0005809330614283681 \t\n",
      "Epoch 9175 \t\t Training Loss: 0.0005809330614283681 \t\n",
      "Epoch 9176 \t\t Training Loss: 0.0005809330614283681 \t\n",
      "Epoch 9177 \t\t Training Loss: 0.0005809328868053854 \t\n",
      "Epoch 9178 \t\t Training Loss: 0.0005809328868053854 \t\n",
      "Epoch 9179 \t\t Training Loss: 0.0005809329450130463 \t\n",
      "Epoch 9180 \t\t Training Loss: 0.0005809329450130463 \t\n",
      "Epoch 9181 \t\t Training Loss: 0.0005809329450130463 \t\n",
      "Epoch 9182 \t\t Training Loss: 0.0005809328868053854 \t\n",
      "Epoch 9183 \t\t Training Loss: 0.0005809328868053854 \t\n",
      "Epoch 9184 \t\t Training Loss: 0.0005809328868053854 \t\n",
      "Epoch 9185 \t\t Training Loss: 0.0005809328868053854 \t\n",
      "Epoch 9186 \t\t Training Loss: 0.0005809328868053854 \t\n",
      "Epoch 9187 \t\t Training Loss: 0.0005809328285977244 \t\n",
      "Epoch 9188 \t\t Training Loss: 0.0005809328285977244 \t\n",
      "Epoch 9189 \t\t Training Loss: 0.0005809328285977244 \t\n",
      "Epoch 9190 \t\t Training Loss: 0.0005809328285977244 \t\n",
      "Epoch 9191 \t\t Training Loss: 0.0005809328285977244 \t\n",
      "Epoch 9192 \t\t Training Loss: 0.0005809328285977244 \t\n",
      "Epoch 9193 \t\t Training Loss: 0.0005809328285977244 \t\n",
      "Epoch 9194 \t\t Training Loss: 0.0005809328285977244 \t\n",
      "Epoch 9195 \t\t Training Loss: 0.0005809328285977244 \t\n",
      "Epoch 9196 \t\t Training Loss: 0.0005809328285977244 \t\n",
      "Epoch 9197 \t\t Training Loss: 0.0005809328285977244 \t\n",
      "Epoch 9198 \t\t Training Loss: 0.0005809327703900635 \t\n",
      "Epoch 9199 \t\t Training Loss: 0.0005809327703900635 \t\n",
      "Epoch 9200 \t\t Training Loss: 0.0005809327703900635 \t\n",
      "Epoch 9201 \t\t Training Loss: 0.0005809327703900635 \t\n",
      "Epoch 9202 \t\t Training Loss: 0.0005809327703900635 \t\n",
      "Epoch 9203 \t\t Training Loss: 0.0005809327703900635 \t\n",
      "Epoch 9204 \t\t Training Loss: 0.0005809327703900635 \t\n",
      "Epoch 9205 \t\t Training Loss: 0.0005809327703900635 \t\n",
      "Epoch 9206 \t\t Training Loss: 0.0005809327703900635 \t\n",
      "Epoch 9207 \t\t Training Loss: 0.0005809327703900635 \t\n",
      "Epoch 9208 \t\t Training Loss: 0.0005809327121824026 \t\n",
      "Epoch 9209 \t\t Training Loss: 0.0005809327121824026 \t\n",
      "Epoch 9210 \t\t Training Loss: 0.0005809327121824026 \t\n",
      "Epoch 9211 \t\t Training Loss: 0.0005809327121824026 \t\n",
      "Epoch 9212 \t\t Training Loss: 0.0005809327121824026 \t\n",
      "Epoch 9213 \t\t Training Loss: 0.0005809327703900635 \t\n",
      "Epoch 9214 \t\t Training Loss: 0.0005809327121824026 \t\n",
      "Epoch 9215 \t\t Training Loss: 0.0005809327121824026 \t\n",
      "Epoch 9216 \t\t Training Loss: 0.0005809327121824026 \t\n",
      "Epoch 9217 \t\t Training Loss: 0.0005809327121824026 \t\n",
      "Epoch 9218 \t\t Training Loss: 0.0005809326539747417 \t\n",
      "Epoch 9219 \t\t Training Loss: 0.0005809327121824026 \t\n",
      "Epoch 9220 \t\t Training Loss: 0.0005809326539747417 \t\n",
      "Epoch 9221 \t\t Training Loss: 0.0005809327121824026 \t\n",
      "Epoch 9222 \t\t Training Loss: 0.0005809326539747417 \t\n",
      "Epoch 9223 \t\t Training Loss: 0.0005809326539747417 \t\n",
      "Epoch 9224 \t\t Training Loss: 0.0005809326539747417 \t\n",
      "Epoch 9225 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9226 \t\t Training Loss: 0.0005809326539747417 \t\n",
      "Epoch 9227 \t\t Training Loss: 0.0005809326539747417 \t\n",
      "Epoch 9228 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9229 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9230 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9231 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9232 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9233 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9234 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9235 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9236 \t\t Training Loss: 0.0005809326539747417 \t\n",
      "Epoch 9237 \t\t Training Loss: 0.0005809326539747417 \t\n",
      "Epoch 9238 \t\t Training Loss: 0.0005809326539747417 \t\n",
      "Epoch 9239 \t\t Training Loss: 0.0005809326539747417 \t\n",
      "Epoch 9240 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9241 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9242 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9243 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9244 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9245 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9246 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9247 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9248 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9249 \t\t Training Loss: 0.0005809326539747417 \t\n",
      "Epoch 9250 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9251 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9252 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9253 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9254 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9255 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9256 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9257 \t\t Training Loss: 0.0005809325375594199 \t\n",
      "Epoch 9258 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9259 \t\t Training Loss: 0.0005809325957670808 \t\n",
      "Epoch 9260 \t\t Training Loss: 0.0005809325375594199 \t\n",
      "Epoch 9261 \t\t Training Loss: 0.0005809325375594199 \t\n",
      "Epoch 9262 \t\t Training Loss: 0.0005809325375594199 \t\n",
      "Epoch 9263 \t\t Training Loss: 0.0005809325375594199 \t\n",
      "Epoch 9264 \t\t Training Loss: 0.0005809325375594199 \t\n",
      "Epoch 9265 \t\t Training Loss: 0.0005809325375594199 \t\n",
      "Epoch 9266 \t\t Training Loss: 0.0005809325375594199 \t\n",
      "Epoch 9267 \t\t Training Loss: 0.0005809325375594199 \t\n",
      "Epoch 9268 \t\t Training Loss: 0.0005809325375594199 \t\n",
      "Epoch 9269 \t\t Training Loss: 0.0005809325375594199 \t\n",
      "Epoch 9270 \t\t Training Loss: 0.0005809325375594199 \t\n",
      "Epoch 9271 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9272 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9273 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9274 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9275 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9276 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9277 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9278 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9279 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9280 \t\t Training Loss: 0.000580932479351759 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9281 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9282 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9283 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9284 \t\t Training Loss: 0.000580932421144098 \t\n",
      "Epoch 9285 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9286 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9287 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9288 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9289 \t\t Training Loss: 0.000580932421144098 \t\n",
      "Epoch 9290 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9291 \t\t Training Loss: 0.000580932479351759 \t\n",
      "Epoch 9292 \t\t Training Loss: 0.000580932421144098 \t\n",
      "Epoch 9293 \t\t Training Loss: 0.000580932421144098 \t\n",
      "Epoch 9294 \t\t Training Loss: 0.000580932421144098 \t\n",
      "Epoch 9295 \t\t Training Loss: 0.000580932421144098 \t\n",
      "Epoch 9296 \t\t Training Loss: 0.000580932421144098 \t\n",
      "Epoch 9297 \t\t Training Loss: 0.0005809323629364371 \t\n",
      "Epoch 9298 \t\t Training Loss: 0.000580932421144098 \t\n",
      "Epoch 9299 \t\t Training Loss: 0.000580932421144098 \t\n",
      "Epoch 9300 \t\t Training Loss: 0.000580932421144098 \t\n",
      "Epoch 9301 \t\t Training Loss: 0.000580932421144098 \t\n",
      "Epoch 9302 \t\t Training Loss: 0.000580932421144098 \t\n",
      "Epoch 9303 \t\t Training Loss: 0.000580932421144098 \t\n",
      "Epoch 9304 \t\t Training Loss: 0.000580932421144098 \t\n",
      "Epoch 9305 \t\t Training Loss: 0.000580932421144098 \t\n",
      "Epoch 9306 \t\t Training Loss: 0.0005809323629364371 \t\n",
      "Epoch 9307 \t\t Training Loss: 0.0005809323629364371 \t\n",
      "Epoch 9308 \t\t Training Loss: 0.0005809323629364371 \t\n",
      "Epoch 9309 \t\t Training Loss: 0.0005809323629364371 \t\n",
      "Epoch 9310 \t\t Training Loss: 0.0005809323629364371 \t\n",
      "Epoch 9311 \t\t Training Loss: 0.0005809323629364371 \t\n",
      "Epoch 9312 \t\t Training Loss: 0.0005809323629364371 \t\n",
      "Epoch 9313 \t\t Training Loss: 0.0005809323629364371 \t\n",
      "Epoch 9314 \t\t Training Loss: 0.0005809323629364371 \t\n",
      "Epoch 9315 \t\t Training Loss: 0.0005809323629364371 \t\n",
      "Epoch 9316 \t\t Training Loss: 0.0005809322465211153 \t\n",
      "Epoch 9317 \t\t Training Loss: 0.0005809322465211153 \t\n",
      "Epoch 9318 \t\t Training Loss: 0.0005809322465211153 \t\n",
      "Epoch 9319 \t\t Training Loss: 0.0005809322465211153 \t\n",
      "Epoch 9320 \t\t Training Loss: 0.0005809322465211153 \t\n",
      "Epoch 9321 \t\t Training Loss: 0.0005809323629364371 \t\n",
      "Epoch 9322 \t\t Training Loss: 0.0005809322465211153 \t\n",
      "Epoch 9323 \t\t Training Loss: 0.0005809322465211153 \t\n",
      "Epoch 9324 \t\t Training Loss: 0.0005809323629364371 \t\n",
      "Epoch 9325 \t\t Training Loss: 0.0005809322465211153 \t\n",
      "Epoch 9326 \t\t Training Loss: 0.0005809321883134544 \t\n",
      "Epoch 9327 \t\t Training Loss: 0.0005809321883134544 \t\n",
      "Epoch 9328 \t\t Training Loss: 0.0005809321883134544 \t\n",
      "Epoch 9329 \t\t Training Loss: 0.0005809321883134544 \t\n",
      "Epoch 9330 \t\t Training Loss: 0.0005809321883134544 \t\n",
      "Epoch 9331 \t\t Training Loss: 0.0005809321883134544 \t\n",
      "Epoch 9332 \t\t Training Loss: 0.0005809321883134544 \t\n",
      "Epoch 9333 \t\t Training Loss: 0.0005809321883134544 \t\n",
      "Epoch 9334 \t\t Training Loss: 0.0005809321883134544 \t\n",
      "Epoch 9335 \t\t Training Loss: 0.0005809321301057935 \t\n",
      "Epoch 9336 \t\t Training Loss: 0.0005809321301057935 \t\n",
      "Epoch 9337 \t\t Training Loss: 0.0005809321301057935 \t\n",
      "Epoch 9338 \t\t Training Loss: 0.0005809321301057935 \t\n",
      "Epoch 9339 \t\t Training Loss: 0.0005809321883134544 \t\n",
      "Epoch 9340 \t\t Training Loss: 0.0005809321301057935 \t\n",
      "Epoch 9341 \t\t Training Loss: 0.0005809321301057935 \t\n",
      "Epoch 9342 \t\t Training Loss: 0.0005809321301057935 \t\n",
      "Epoch 9343 \t\t Training Loss: 0.0005809321301057935 \t\n",
      "Epoch 9344 \t\t Training Loss: 0.0005809321301057935 \t\n",
      "Epoch 9345 \t\t Training Loss: 0.0005809321301057935 \t\n",
      "Epoch 9346 \t\t Training Loss: 0.0005809321301057935 \t\n",
      "Epoch 9347 \t\t Training Loss: 0.0005809321301057935 \t\n",
      "Epoch 9348 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9349 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9350 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9351 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9352 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9353 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9354 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9355 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9356 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9357 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9358 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9359 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9360 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9361 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9362 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9363 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9364 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9365 \t\t Training Loss: 0.0005809320718981326 \t\n",
      "Epoch 9366 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9367 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9368 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9369 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9370 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9371 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9372 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9373 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9374 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9375 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9376 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9377 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9378 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9379 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9380 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9381 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9382 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9383 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9384 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9385 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9386 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9387 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9388 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9389 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9390 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9391 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9392 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9393 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9394 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9395 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9396 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9397 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9398 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9399 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9400 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9401 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9402 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9403 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9404 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9405 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9406 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9407 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9408 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9409 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9410 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9411 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9412 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9413 \t\t Training Loss: 0.0005809319554828107 \t\n",
      "Epoch 9414 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9415 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9416 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9417 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9418 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9419 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9420 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9421 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9422 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9423 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9424 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9425 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9426 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9427 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9428 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9429 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9430 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9431 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9432 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9433 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9434 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9435 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9436 \t\t Training Loss: 0.000580931780859828 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9437 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9438 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9439 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9440 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9441 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9442 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9443 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9444 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9445 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9446 \t\t Training Loss: 0.0005809318390674889 \t\n",
      "Epoch 9447 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9448 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9449 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9450 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9451 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9452 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9453 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9454 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9455 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9456 \t\t Training Loss: 0.000580931780859828 \t\n",
      "Epoch 9457 \t\t Training Loss: 0.0005809316644445062 \t\n",
      "Epoch 9458 \t\t Training Loss: 0.0005809316644445062 \t\n",
      "Epoch 9459 \t\t Training Loss: 0.0005809316644445062 \t\n",
      "Epoch 9460 \t\t Training Loss: 0.0005809316644445062 \t\n",
      "Epoch 9461 \t\t Training Loss: 0.0005809316644445062 \t\n",
      "Epoch 9462 \t\t Training Loss: 0.0005809316644445062 \t\n",
      "Epoch 9463 \t\t Training Loss: 0.0005809316644445062 \t\n",
      "Epoch 9464 \t\t Training Loss: 0.0005809316644445062 \t\n",
      "Epoch 9465 \t\t Training Loss: 0.0005809315480291843 \t\n",
      "Epoch 9466 \t\t Training Loss: 0.0005809315480291843 \t\n",
      "Epoch 9467 \t\t Training Loss: 0.0005809315480291843 \t\n",
      "Epoch 9468 \t\t Training Loss: 0.0005809315480291843 \t\n",
      "Epoch 9469 \t\t Training Loss: 0.0005809315480291843 \t\n",
      "Epoch 9470 \t\t Training Loss: 0.0005809315480291843 \t\n",
      "Epoch 9471 \t\t Training Loss: 0.0005809315480291843 \t\n",
      "Epoch 9472 \t\t Training Loss: 0.0005809315480291843 \t\n",
      "Epoch 9473 \t\t Training Loss: 0.0005809315480291843 \t\n",
      "Epoch 9474 \t\t Training Loss: 0.0005809314898215234 \t\n",
      "Epoch 9475 \t\t Training Loss: 0.0005809314898215234 \t\n",
      "Epoch 9476 \t\t Training Loss: 0.0005809314898215234 \t\n",
      "Epoch 9477 \t\t Training Loss: 0.0005809314898215234 \t\n",
      "Epoch 9478 \t\t Training Loss: 0.0005809314898215234 \t\n",
      "Epoch 9479 \t\t Training Loss: 0.0005809314898215234 \t\n",
      "Epoch 9480 \t\t Training Loss: 0.0005809314898215234 \t\n",
      "Epoch 9481 \t\t Training Loss: 0.0005809314898215234 \t\n",
      "Epoch 9482 \t\t Training Loss: 0.0005809314898215234 \t\n",
      "Epoch 9483 \t\t Training Loss: 0.0005809314898215234 \t\n",
      "Epoch 9484 \t\t Training Loss: 0.0005809314898215234 \t\n",
      "Epoch 9485 \t\t Training Loss: 0.0005809314898215234 \t\n",
      "Epoch 9486 \t\t Training Loss: 0.0005809314898215234 \t\n",
      "Epoch 9487 \t\t Training Loss: 0.0005809314898215234 \t\n",
      "Epoch 9488 \t\t Training Loss: 0.0005809314898215234 \t\n",
      "Epoch 9489 \t\t Training Loss: 0.0005809314898215234 \t\n",
      "Epoch 9490 \t\t Training Loss: 0.0005809313734062016 \t\n",
      "Epoch 9491 \t\t Training Loss: 0.0005809313734062016 \t\n",
      "Epoch 9492 \t\t Training Loss: 0.0005809313734062016 \t\n",
      "Epoch 9493 \t\t Training Loss: 0.0005809313734062016 \t\n",
      "Epoch 9494 \t\t Training Loss: 0.0005809313734062016 \t\n",
      "Epoch 9495 \t\t Training Loss: 0.0005809313734062016 \t\n",
      "Epoch 9496 \t\t Training Loss: 0.0005809313734062016 \t\n",
      "Epoch 9497 \t\t Training Loss: 0.0005809313734062016 \t\n",
      "Epoch 9498 \t\t Training Loss: 0.0005809312569908798 \t\n",
      "Epoch 9499 \t\t Training Loss: 0.0005809312569908798 \t\n",
      "Epoch 9500 \t\t Training Loss: 0.0005809312569908798 \t\n",
      "Epoch 9501 \t\t Training Loss: 0.0005809312569908798 \t\n",
      "Epoch 9502 \t\t Training Loss: 0.0005809312569908798 \t\n",
      "Epoch 9503 \t\t Training Loss: 0.0005809312569908798 \t\n",
      "Epoch 9504 \t\t Training Loss: 0.0005809312569908798 \t\n",
      "Epoch 9505 \t\t Training Loss: 0.0005809312569908798 \t\n",
      "Epoch 9506 \t\t Training Loss: 0.0005809312569908798 \t\n",
      "Epoch 9507 \t\t Training Loss: 0.0005809312569908798 \t\n",
      "Epoch 9508 \t\t Training Loss: 0.0005809312569908798 \t\n",
      "Epoch 9509 \t\t Training Loss: 0.000580931140575558 \t\n",
      "Epoch 9510 \t\t Training Loss: 0.000580931140575558 \t\n",
      "Epoch 9511 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9512 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9513 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9514 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9515 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9516 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9517 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9518 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9519 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9520 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9521 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9522 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9523 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9524 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9525 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9526 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9527 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9528 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9529 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9530 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9531 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9532 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9533 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9534 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9535 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9536 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9537 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9538 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9539 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9540 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9541 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9542 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9543 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9544 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9545 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9546 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9547 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9548 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9549 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9550 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9551 \t\t Training Loss: 0.000580931082367897 \t\n",
      "Epoch 9552 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9553 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9554 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9555 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9556 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9557 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9558 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9559 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9560 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9561 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9562 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9563 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9564 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9565 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9566 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9567 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9568 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9569 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9570 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9571 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9572 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9573 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9574 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9575 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9576 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9577 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9578 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9579 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9580 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9581 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9582 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9583 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9584 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9585 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9586 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9587 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9588 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9589 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9590 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9591 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9592 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9593 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9594 \t\t Training Loss: 0.0005809309659525752 \t\n",
      "Epoch 9595 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9596 \t\t Training Loss: 0.0005809309659525752 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9597 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9598 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9599 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9600 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9601 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9602 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9603 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9604 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9605 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9606 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9607 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9608 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9609 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9610 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9611 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9612 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9613 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9614 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9615 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9616 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9617 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9618 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9619 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9620 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9621 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9622 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9623 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9624 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9625 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9626 \t\t Training Loss: 0.0005809308495372534 \t\n",
      "Epoch 9627 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9628 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9629 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9630 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9631 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9632 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9633 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9634 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9635 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9636 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9637 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9638 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9639 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9640 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9641 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9642 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9643 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9644 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9645 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9646 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9647 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9648 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9649 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9650 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9651 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9652 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9653 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9654 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9655 \t\t Training Loss: 0.0005809306749142706 \t\n",
      "Epoch 9656 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9657 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9658 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9659 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9660 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9661 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9662 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9663 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9664 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9665 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9666 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9667 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9668 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9669 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9670 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9671 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9672 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9673 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9674 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9675 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9676 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9677 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9678 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9679 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9680 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9681 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9682 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9683 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9684 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9685 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9686 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9687 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9688 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9689 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9690 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9691 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9692 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9693 \t\t Training Loss: 0.0005809305584989488 \t\n",
      "Epoch 9694 \t\t Training Loss: 0.0005809305002912879 \t\n",
      "Epoch 9695 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9696 \t\t Training Loss: 0.0005809305002912879 \t\n",
      "Epoch 9697 \t\t Training Loss: 0.0005809305002912879 \t\n",
      "Epoch 9698 \t\t Training Loss: 0.0005809305002912879 \t\n",
      "Epoch 9699 \t\t Training Loss: 0.0005809305002912879 \t\n",
      "Epoch 9700 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9701 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9702 \t\t Training Loss: 0.0005809305002912879 \t\n",
      "Epoch 9703 \t\t Training Loss: 0.0005809305002912879 \t\n",
      "Epoch 9704 \t\t Training Loss: 0.0005809305002912879 \t\n",
      "Epoch 9705 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9706 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9707 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9708 \t\t Training Loss: 0.0005809305002912879 \t\n",
      "Epoch 9709 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9710 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9711 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9712 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9713 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9714 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9715 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9716 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9717 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9718 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9719 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9720 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9721 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9722 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9723 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9724 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9725 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9726 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9727 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9728 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9729 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9730 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9731 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9732 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9733 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9734 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9735 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9736 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9737 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9738 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9739 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9740 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9741 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9742 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9743 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9744 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9745 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9746 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9747 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9748 \t\t Training Loss: 0.0005809303838759661 \t\n",
      "Epoch 9749 \t\t Training Loss: 0.0005809302674606442 \t\n",
      "Epoch 9750 \t\t Training Loss: 0.0005809302674606442 \t\n",
      "Epoch 9751 \t\t Training Loss: 0.0005809302674606442 \t\n",
      "Epoch 9752 \t\t Training Loss: 0.0005809302674606442 \t\n",
      "Epoch 9753 \t\t Training Loss: 0.0005809302674606442 \t\n",
      "Epoch 9754 \t\t Training Loss: 0.0005809302674606442 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9755 \t\t Training Loss: 0.0005809302674606442 \t\n",
      "Epoch 9756 \t\t Training Loss: 0.0005809302674606442 \t\n",
      "Epoch 9757 \t\t Training Loss: 0.0005809302674606442 \t\n",
      "Epoch 9758 \t\t Training Loss: 0.0005809302674606442 \t\n",
      "Epoch 9759 \t\t Training Loss: 0.0005809302674606442 \t\n",
      "Epoch 9760 \t\t Training Loss: 0.0005809302674606442 \t\n",
      "Epoch 9761 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9762 \t\t Training Loss: 0.0005809302674606442 \t\n",
      "Epoch 9763 \t\t Training Loss: 0.0005809302674606442 \t\n",
      "Epoch 9764 \t\t Training Loss: 0.0005809302674606442 \t\n",
      "Epoch 9765 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9766 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9767 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9768 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9769 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9770 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9771 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9772 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9773 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9774 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9775 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9776 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9777 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9778 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9779 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9780 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9781 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9782 \t\t Training Loss: 0.0005809302092529833 \t\n",
      "Epoch 9783 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9784 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9785 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9786 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9787 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9788 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9789 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9790 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9791 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9792 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9793 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9794 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9795 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9796 \t\t Training Loss: 0.0005809300928376615 \t\n",
      "Epoch 9797 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9798 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9799 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9800 \t\t Training Loss: 0.0005809301510453224 \t\n",
      "Epoch 9801 \t\t Training Loss: 0.0005809300928376615 \t\n",
      "Epoch 9802 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9803 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9804 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9805 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9806 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9807 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9808 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9809 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9810 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9811 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9812 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9813 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9814 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9815 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9816 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9817 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9818 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9819 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9820 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9821 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9822 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9823 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9824 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9825 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9826 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9827 \t\t Training Loss: 0.0005809299764223397 \t\n",
      "Epoch 9828 \t\t Training Loss: 0.0005809299182146788 \t\n",
      "Epoch 9829 \t\t Training Loss: 0.0005809299182146788 \t\n",
      "Epoch 9830 \t\t Training Loss: 0.0005809299182146788 \t\n",
      "Epoch 9831 \t\t Training Loss: 0.0005809299182146788 \t\n",
      "Epoch 9832 \t\t Training Loss: 0.0005809299182146788 \t\n",
      "Epoch 9833 \t\t Training Loss: 0.0005809299182146788 \t\n",
      "Epoch 9834 \t\t Training Loss: 0.0005809299182146788 \t\n",
      "Epoch 9835 \t\t Training Loss: 0.0005809298600070179 \t\n",
      "Epoch 9836 \t\t Training Loss: 0.0005809298600070179 \t\n",
      "Epoch 9837 \t\t Training Loss: 0.0005809298600070179 \t\n",
      "Epoch 9838 \t\t Training Loss: 0.0005809298600070179 \t\n",
      "Epoch 9839 \t\t Training Loss: 0.0005809298600070179 \t\n",
      "Epoch 9840 \t\t Training Loss: 0.0005809298600070179 \t\n",
      "Epoch 9841 \t\t Training Loss: 0.0005809298600070179 \t\n",
      "Epoch 9842 \t\t Training Loss: 0.0005809298017993569 \t\n",
      "Epoch 9843 \t\t Training Loss: 0.0005809298017993569 \t\n",
      "Epoch 9844 \t\t Training Loss: 0.0005809298017993569 \t\n",
      "Epoch 9845 \t\t Training Loss: 0.0005809298017993569 \t\n",
      "Epoch 9846 \t\t Training Loss: 0.0005809298017993569 \t\n",
      "Epoch 9847 \t\t Training Loss: 0.0005809298017993569 \t\n",
      "Epoch 9848 \t\t Training Loss: 0.0005809298017993569 \t\n",
      "Epoch 9849 \t\t Training Loss: 0.0005809298017993569 \t\n",
      "Epoch 9850 \t\t Training Loss: 0.0005809298017993569 \t\n",
      "Epoch 9851 \t\t Training Loss: 0.0005809298017993569 \t\n",
      "Epoch 9852 \t\t Training Loss: 0.0005809298017993569 \t\n",
      "Epoch 9853 \t\t Training Loss: 0.0005809298017993569 \t\n",
      "Epoch 9854 \t\t Training Loss: 0.0005809298017993569 \t\n",
      "Epoch 9855 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9856 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9857 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9858 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9859 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9860 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9861 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9862 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9863 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9864 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9865 \t\t Training Loss: 0.0005809298017993569 \t\n",
      "Epoch 9866 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9867 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9868 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9869 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9870 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9871 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9872 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9873 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9874 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9875 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9876 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9877 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9878 \t\t Training Loss: 0.0005809296853840351 \t\n",
      "Epoch 9879 \t\t Training Loss: 0.0005809295689687133 \t\n",
      "Epoch 9880 \t\t Training Loss: 0.0005809295689687133 \t\n",
      "Epoch 9881 \t\t Training Loss: 0.0005809295689687133 \t\n",
      "Epoch 9882 \t\t Training Loss: 0.0005809295689687133 \t\n",
      "Epoch 9883 \t\t Training Loss: 0.0005809295107610524 \t\n",
      "Epoch 9884 \t\t Training Loss: 0.0005809295107610524 \t\n",
      "Epoch 9885 \t\t Training Loss: 0.0005809295107610524 \t\n",
      "Epoch 9886 \t\t Training Loss: 0.0005809295107610524 \t\n",
      "Epoch 9887 \t\t Training Loss: 0.0005809295107610524 \t\n",
      "Epoch 9888 \t\t Training Loss: 0.0005809295107610524 \t\n",
      "Epoch 9889 \t\t Training Loss: 0.0005809295107610524 \t\n",
      "Epoch 9890 \t\t Training Loss: 0.0005809295107610524 \t\n",
      "Epoch 9891 \t\t Training Loss: 0.0005809294525533915 \t\n",
      "Epoch 9892 \t\t Training Loss: 0.0005809294525533915 \t\n",
      "Epoch 9893 \t\t Training Loss: 0.0005809294525533915 \t\n",
      "Epoch 9894 \t\t Training Loss: 0.0005809294525533915 \t\n",
      "Epoch 9895 \t\t Training Loss: 0.0005809293943457305 \t\n",
      "Epoch 9896 \t\t Training Loss: 0.0005809294525533915 \t\n",
      "Epoch 9897 \t\t Training Loss: 0.0005809294525533915 \t\n",
      "Epoch 9898 \t\t Training Loss: 0.0005809294525533915 \t\n",
      "Epoch 9899 \t\t Training Loss: 0.0005809293943457305 \t\n",
      "Epoch 9900 \t\t Training Loss: 0.0005809294525533915 \t\n",
      "Epoch 9901 \t\t Training Loss: 0.0005809294525533915 \t\n",
      "Epoch 9902 \t\t Training Loss: 0.0005809294525533915 \t\n",
      "Epoch 9903 \t\t Training Loss: 0.0005809293943457305 \t\n",
      "Epoch 9904 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9905 \t\t Training Loss: 0.0005809293943457305 \t\n",
      "Epoch 9906 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9907 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9908 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9909 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9910 \t\t Training Loss: 0.0005809292779304087 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9911 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9912 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9913 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9914 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9915 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9916 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9917 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9918 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9919 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9920 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9921 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9922 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9923 \t\t Training Loss: 0.0005809292779304087 \t\n",
      "Epoch 9924 \t\t Training Loss: 0.0005809291615150869 \t\n",
      "Epoch 9925 \t\t Training Loss: 0.0005809292197227478 \t\n",
      "Epoch 9926 \t\t Training Loss: 0.0005809292197227478 \t\n",
      "Epoch 9927 \t\t Training Loss: 0.0005809292197227478 \t\n",
      "Epoch 9928 \t\t Training Loss: 0.0005809292197227478 \t\n",
      "Epoch 9929 \t\t Training Loss: 0.0005809292197227478 \t\n",
      "Epoch 9930 \t\t Training Loss: 0.0005809291615150869 \t\n",
      "Epoch 9931 \t\t Training Loss: 0.0005809291615150869 \t\n",
      "Epoch 9932 \t\t Training Loss: 0.000580929103307426 \t\n",
      "Epoch 9933 \t\t Training Loss: 0.000580929103307426 \t\n",
      "Epoch 9934 \t\t Training Loss: 0.000580929103307426 \t\n",
      "Epoch 9935 \t\t Training Loss: 0.000580929103307426 \t\n",
      "Epoch 9936 \t\t Training Loss: 0.000580929103307426 \t\n",
      "Epoch 9937 \t\t Training Loss: 0.000580929103307426 \t\n",
      "Epoch 9938 \t\t Training Loss: 0.000580929103307426 \t\n",
      "Epoch 9939 \t\t Training Loss: 0.000580929103307426 \t\n",
      "Epoch 9940 \t\t Training Loss: 0.0005809291615150869 \t\n",
      "Epoch 9941 \t\t Training Loss: 0.000580929103307426 \t\n",
      "Epoch 9942 \t\t Training Loss: 0.000580929103307426 \t\n",
      "Epoch 9943 \t\t Training Loss: 0.000580929103307426 \t\n",
      "Epoch 9944 \t\t Training Loss: 0.000580929103307426 \t\n",
      "Epoch 9945 \t\t Training Loss: 0.000580929103307426 \t\n",
      "Epoch 9946 \t\t Training Loss: 0.000580929103307426 \t\n",
      "Epoch 9947 \t\t Training Loss: 0.0005809289868921041 \t\n",
      "Epoch 9948 \t\t Training Loss: 0.0005809289868921041 \t\n",
      "Epoch 9949 \t\t Training Loss: 0.0005809289868921041 \t\n",
      "Epoch 9950 \t\t Training Loss: 0.0005809290450997651 \t\n",
      "Epoch 9951 \t\t Training Loss: 0.0005809290450997651 \t\n",
      "Epoch 9952 \t\t Training Loss: 0.0005809289868921041 \t\n",
      "Epoch 9953 \t\t Training Loss: 0.0005809289868921041 \t\n",
      "Epoch 9954 \t\t Training Loss: 0.0005809289868921041 \t\n",
      "Epoch 9955 \t\t Training Loss: 0.0005809289868921041 \t\n",
      "Epoch 9956 \t\t Training Loss: 0.0005809289868921041 \t\n",
      "Epoch 9957 \t\t Training Loss: 0.0005809289286844432 \t\n",
      "Epoch 9958 \t\t Training Loss: 0.0005809289868921041 \t\n",
      "Epoch 9959 \t\t Training Loss: 0.0005809289286844432 \t\n",
      "Epoch 9960 \t\t Training Loss: 0.0005809289868921041 \t\n",
      "Epoch 9961 \t\t Training Loss: 0.0005809289868921041 \t\n",
      "Epoch 9962 \t\t Training Loss: 0.0005809289868921041 \t\n",
      "Epoch 9963 \t\t Training Loss: 0.0005809289868921041 \t\n",
      "Epoch 9964 \t\t Training Loss: 0.0005809289868921041 \t\n",
      "Epoch 9965 \t\t Training Loss: 0.0005809289286844432 \t\n",
      "Epoch 9966 \t\t Training Loss: 0.0005809289286844432 \t\n",
      "Epoch 9967 \t\t Training Loss: 0.0005809289286844432 \t\n",
      "Epoch 9968 \t\t Training Loss: 0.0005809289286844432 \t\n",
      "Epoch 9969 \t\t Training Loss: 0.0005809289286844432 \t\n",
      "Epoch 9970 \t\t Training Loss: 0.0005809289286844432 \t\n",
      "Epoch 9971 \t\t Training Loss: 0.0005809289286844432 \t\n",
      "Epoch 9972 \t\t Training Loss: 0.0005809289286844432 \t\n",
      "Epoch 9973 \t\t Training Loss: 0.0005809289286844432 \t\n",
      "Epoch 9974 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9975 \t\t Training Loss: 0.0005809289286844432 \t\n",
      "Epoch 9976 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9977 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9978 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9979 \t\t Training Loss: 0.0005809289286844432 \t\n",
      "Epoch 9980 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9981 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9982 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9983 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9984 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9985 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9986 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9987 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9988 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9989 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9990 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9991 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9992 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9993 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9994 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9995 \t\t Training Loss: 0.0005809288704767823 \t\n",
      "Epoch 9996 \t\t Training Loss: 0.0005809288122691214 \t\n",
      "Epoch 9997 \t\t Training Loss: 0.0005809287540614605 \t\n",
      "Epoch 9998 \t\t Training Loss: 0.0005809287540614605 \t\n",
      "Epoch 9999 \t\t Training Loss: 0.0005809288122691214 \t\n",
      "Epoch 10000 \t\t Training Loss: 0.0005809288122691214 \t\n",
      "Epoch 10001 \t\t Training Loss: 0.0005809287540614605 \t\n",
      "Epoch 10002 \t\t Training Loss: 0.0005809287540614605 \t\n",
      "Epoch 10003 \t\t Training Loss: 0.0005809287540614605 \t\n",
      "Epoch 10004 \t\t Training Loss: 0.0005809287540614605 \t\n",
      "Epoch 10005 \t\t Training Loss: 0.0005809286958537996 \t\n",
      "Epoch 10006 \t\t Training Loss: 0.0005809287540614605 \t\n",
      "Epoch 10007 \t\t Training Loss: 0.0005809287540614605 \t\n",
      "Epoch 10008 \t\t Training Loss: 0.0005809287540614605 \t\n",
      "Epoch 10009 \t\t Training Loss: 0.0005809287540614605 \t\n",
      "Epoch 10010 \t\t Training Loss: 0.0005809287540614605 \t\n",
      "Epoch 10011 \t\t Training Loss: 0.0005809287540614605 \t\n",
      "Epoch 10012 \t\t Training Loss: 0.0005809287540614605 \t\n",
      "Epoch 10013 \t\t Training Loss: 0.0005809287540614605 \t\n",
      "Epoch 10014 \t\t Training Loss: 0.0005809287540614605 \t\n",
      "Epoch 10015 \t\t Training Loss: 0.0005809286958537996 \t\n",
      "Epoch 10016 \t\t Training Loss: 0.0005809286958537996 \t\n",
      "Epoch 10017 \t\t Training Loss: 0.0005809286376461387 \t\n",
      "Epoch 10018 \t\t Training Loss: 0.0005809286958537996 \t\n",
      "Epoch 10019 \t\t Training Loss: 0.0005809286376461387 \t\n",
      "Epoch 10020 \t\t Training Loss: 0.0005809286958537996 \t\n",
      "Epoch 10021 \t\t Training Loss: 0.0005809286376461387 \t\n",
      "Epoch 10022 \t\t Training Loss: 0.0005809286376461387 \t\n",
      "Epoch 10023 \t\t Training Loss: 0.0005809285212308168 \t\n",
      "Epoch 10024 \t\t Training Loss: 0.0005809285212308168 \t\n",
      "Epoch 10025 \t\t Training Loss: 0.0005809285212308168 \t\n",
      "Epoch 10026 \t\t Training Loss: 0.0005809285212308168 \t\n",
      "Epoch 10027 \t\t Training Loss: 0.0005809285212308168 \t\n",
      "Epoch 10028 \t\t Training Loss: 0.0005809284630231559 \t\n",
      "Epoch 10029 \t\t Training Loss: 0.0005809284630231559 \t\n",
      "Epoch 10030 \t\t Training Loss: 0.0005809284630231559 \t\n",
      "Epoch 10031 \t\t Training Loss: 0.0005809284630231559 \t\n",
      "Epoch 10032 \t\t Training Loss: 0.0005809284630231559 \t\n",
      "Epoch 10033 \t\t Training Loss: 0.0005809284630231559 \t\n",
      "Epoch 10034 \t\t Training Loss: 0.0005809284630231559 \t\n",
      "Epoch 10035 \t\t Training Loss: 0.0005809284630231559 \t\n",
      "Epoch 10036 \t\t Training Loss: 0.0005809284630231559 \t\n",
      "Epoch 10037 \t\t Training Loss: 0.0005809284630231559 \t\n",
      "Epoch 10038 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10039 \t\t Training Loss: 0.0005809284630231559 \t\n",
      "Epoch 10040 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10041 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10042 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10043 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10044 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10045 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10046 \t\t Training Loss: 0.0005809283466078341 \t\n",
      "Epoch 10047 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10048 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10049 \t\t Training Loss: 0.0005809283466078341 \t\n",
      "Epoch 10050 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10051 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10052 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10053 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10054 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10055 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10056 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10057 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10058 \t\t Training Loss: 0.0005809283466078341 \t\n",
      "Epoch 10059 \t\t Training Loss: 0.0005809283466078341 \t\n",
      "Epoch 10060 \t\t Training Loss: 0.0005809283466078341 \t\n",
      "Epoch 10061 \t\t Training Loss: 0.000580928404815495 \t\n",
      "Epoch 10062 \t\t Training Loss: 0.0005809283466078341 \t\n",
      "Epoch 10063 \t\t Training Loss: 0.0005809283466078341 \t\n",
      "Epoch 10064 \t\t Training Loss: 0.0005809283466078341 \t\n",
      "Epoch 10065 \t\t Training Loss: 0.0005809283466078341 \t\n",
      "Epoch 10066 \t\t Training Loss: 0.0005809283466078341 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10067 \t\t Training Loss: 0.0005809282884001732 \t\n",
      "Epoch 10068 \t\t Training Loss: 0.0005809282884001732 \t\n",
      "Epoch 10069 \t\t Training Loss: 0.0005809282884001732 \t\n",
      "Epoch 10070 \t\t Training Loss: 0.0005809282884001732 \t\n",
      "Epoch 10071 \t\t Training Loss: 0.0005809282884001732 \t\n",
      "Epoch 10072 \t\t Training Loss: 0.0005809282884001732 \t\n",
      "Epoch 10073 \t\t Training Loss: 0.0005809282301925123 \t\n",
      "Epoch 10074 \t\t Training Loss: 0.0005809282301925123 \t\n",
      "Epoch 10075 \t\t Training Loss: 0.0005809282301925123 \t\n",
      "Epoch 10076 \t\t Training Loss: 0.0005809282301925123 \t\n",
      "Epoch 10077 \t\t Training Loss: 0.0005809282301925123 \t\n",
      "Epoch 10078 \t\t Training Loss: 0.0005809282301925123 \t\n",
      "Epoch 10079 \t\t Training Loss: 0.0005809282301925123 \t\n",
      "Epoch 10080 \t\t Training Loss: 0.0005809282301925123 \t\n",
      "Epoch 10081 \t\t Training Loss: 0.0005809282301925123 \t\n",
      "Epoch 10082 \t\t Training Loss: 0.0005809281719848514 \t\n",
      "Epoch 10083 \t\t Training Loss: 0.0005809282301925123 \t\n",
      "Epoch 10084 \t\t Training Loss: 0.0005809282301925123 \t\n",
      "Epoch 10085 \t\t Training Loss: 0.0005809281719848514 \t\n",
      "Epoch 10086 \t\t Training Loss: 0.0005809281719848514 \t\n",
      "Epoch 10087 \t\t Training Loss: 0.0005809281719848514 \t\n",
      "Epoch 10088 \t\t Training Loss: 0.0005809281719848514 \t\n",
      "Epoch 10089 \t\t Training Loss: 0.0005809281719848514 \t\n",
      "Epoch 10090 \t\t Training Loss: 0.0005809281719848514 \t\n",
      "Epoch 10091 \t\t Training Loss: 0.0005809281719848514 \t\n",
      "Epoch 10092 \t\t Training Loss: 0.0005809281719848514 \t\n",
      "Epoch 10093 \t\t Training Loss: 0.0005809280555695295 \t\n",
      "Epoch 10094 \t\t Training Loss: 0.0005809280555695295 \t\n",
      "Epoch 10095 \t\t Training Loss: 0.0005809281137771904 \t\n",
      "Epoch 10096 \t\t Training Loss: 0.0005809281137771904 \t\n",
      "Epoch 10097 \t\t Training Loss: 0.0005809281137771904 \t\n",
      "Epoch 10098 \t\t Training Loss: 0.0005809280555695295 \t\n",
      "Epoch 10099 \t\t Training Loss: 0.0005809280555695295 \t\n",
      "Epoch 10100 \t\t Training Loss: 0.0005809280555695295 \t\n",
      "Epoch 10101 \t\t Training Loss: 0.0005809280555695295 \t\n",
      "Epoch 10102 \t\t Training Loss: 0.0005809280555695295 \t\n",
      "Epoch 10103 \t\t Training Loss: 0.0005809280555695295 \t\n",
      "Epoch 10104 \t\t Training Loss: 0.0005809280555695295 \t\n",
      "Epoch 10105 \t\t Training Loss: 0.0005809280555695295 \t\n",
      "Epoch 10106 \t\t Training Loss: 0.0005809280555695295 \t\n",
      "Epoch 10107 \t\t Training Loss: 0.0005809280555695295 \t\n",
      "Epoch 10108 \t\t Training Loss: 0.0005809280555695295 \t\n",
      "Epoch 10109 \t\t Training Loss: 0.0005809280555695295 \t\n",
      "Epoch 10110 \t\t Training Loss: 0.0005809280555695295 \t\n",
      "Epoch 10111 \t\t Training Loss: 0.0005809280555695295 \t\n",
      "Epoch 10112 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10113 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10114 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10115 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10116 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10117 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10118 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10119 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10120 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10121 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10122 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10123 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10124 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10125 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10126 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10127 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10128 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10129 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10130 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10131 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10132 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10133 \t\t Training Loss: 0.0005809278809465468 \t\n",
      "Epoch 10134 \t\t Training Loss: 0.0005809278809465468 \t\n",
      "Epoch 10135 \t\t Training Loss: 0.0005809279391542077 \t\n",
      "Epoch 10136 \t\t Training Loss: 0.0005809278227388859 \t\n",
      "Epoch 10137 \t\t Training Loss: 0.0005809278227388859 \t\n",
      "Epoch 10138 \t\t Training Loss: 0.0005809278227388859 \t\n",
      "Epoch 10139 \t\t Training Loss: 0.0005809278227388859 \t\n",
      "Epoch 10140 \t\t Training Loss: 0.0005809278227388859 \t\n",
      "Epoch 10141 \t\t Training Loss: 0.0005809278227388859 \t\n",
      "Epoch 10142 \t\t Training Loss: 0.0005809278227388859 \t\n",
      "Epoch 10143 \t\t Training Loss: 0.000580927764531225 \t\n",
      "Epoch 10144 \t\t Training Loss: 0.0005809278227388859 \t\n",
      "Epoch 10145 \t\t Training Loss: 0.000580927764531225 \t\n",
      "Epoch 10146 \t\t Training Loss: 0.000580927764531225 \t\n",
      "Epoch 10147 \t\t Training Loss: 0.000580927764531225 \t\n",
      "Epoch 10148 \t\t Training Loss: 0.000580927764531225 \t\n",
      "Epoch 10149 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10150 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10151 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10152 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10153 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10154 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10155 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10156 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10157 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10158 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10159 \t\t Training Loss: 0.000580927764531225 \t\n",
      "Epoch 10160 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10161 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10162 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10163 \t\t Training Loss: 0.000580927764531225 \t\n",
      "Epoch 10164 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10165 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10166 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10167 \t\t Training Loss: 0.000580927764531225 \t\n",
      "Epoch 10168 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10169 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10170 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10171 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10172 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10173 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10174 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10175 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10176 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10177 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10178 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10179 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10180 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10181 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10182 \t\t Training Loss: 0.000580927706323564 \t\n",
      "Epoch 10183 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10184 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10185 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10186 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10187 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10188 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10189 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10190 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10191 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10192 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10193 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10194 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10195 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10196 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10197 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10198 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10199 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10200 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10201 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10202 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10203 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10204 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10205 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10206 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10207 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10208 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10209 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10210 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10211 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10212 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10213 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10214 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10215 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10216 \t\t Training Loss: 0.0005809276481159031 \t\n",
      "Epoch 10217 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10218 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10219 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10220 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10221 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10222 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10223 \t\t Training Loss: 0.0005809275899082422 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10224 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10225 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10226 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10227 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10228 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10229 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10230 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10231 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10232 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10233 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10234 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10235 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10236 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10237 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10238 \t\t Training Loss: 0.0005809275899082422 \t\n",
      "Epoch 10239 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10240 \t\t Training Loss: 0.0005809275317005813 \t\n",
      "Epoch 10241 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10242 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10243 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10244 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10245 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10246 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10247 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10248 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10249 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10250 \t\t Training Loss: 0.0005809273570775986 \t\n",
      "Epoch 10251 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10252 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10253 \t\t Training Loss: 0.0005809273570775986 \t\n",
      "Epoch 10254 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10255 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10256 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10257 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10258 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10259 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10260 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10261 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10262 \t\t Training Loss: 0.0005809273570775986 \t\n",
      "Epoch 10263 \t\t Training Loss: 0.0005809273570775986 \t\n",
      "Epoch 10264 \t\t Training Loss: 0.0005809273570775986 \t\n",
      "Epoch 10265 \t\t Training Loss: 0.0005809274734929204 \t\n",
      "Epoch 10266 \t\t Training Loss: 0.0005809273570775986 \t\n",
      "Epoch 10267 \t\t Training Loss: 0.0005809273570775986 \t\n",
      "Epoch 10268 \t\t Training Loss: 0.0005809273570775986 \t\n",
      "Epoch 10269 \t\t Training Loss: 0.0005809273570775986 \t\n",
      "Epoch 10270 \t\t Training Loss: 0.0005809272988699377 \t\n",
      "Epoch 10271 \t\t Training Loss: 0.0005809272988699377 \t\n",
      "Epoch 10272 \t\t Training Loss: 0.0005809272988699377 \t\n",
      "Epoch 10273 \t\t Training Loss: 0.0005809272988699377 \t\n",
      "Epoch 10274 \t\t Training Loss: 0.0005809272406622767 \t\n",
      "Epoch 10275 \t\t Training Loss: 0.0005809272406622767 \t\n",
      "Epoch 10276 \t\t Training Loss: 0.0005809271824546158 \t\n",
      "Epoch 10277 \t\t Training Loss: 0.0005809271824546158 \t\n",
      "Epoch 10278 \t\t Training Loss: 0.0005809271824546158 \t\n",
      "Epoch 10279 \t\t Training Loss: 0.0005809271824546158 \t\n",
      "Epoch 10280 \t\t Training Loss: 0.0005809271824546158 \t\n",
      "Epoch 10281 \t\t Training Loss: 0.0005809271824546158 \t\n",
      "Epoch 10282 \t\t Training Loss: 0.0005809271824546158 \t\n",
      "Epoch 10283 \t\t Training Loss: 0.0005809271824546158 \t\n",
      "Epoch 10284 \t\t Training Loss: 0.0005809271824546158 \t\n",
      "Epoch 10285 \t\t Training Loss: 0.0005809271824546158 \t\n",
      "Epoch 10286 \t\t Training Loss: 0.0005809271824546158 \t\n",
      "Epoch 10287 \t\t Training Loss: 0.0005809271242469549 \t\n",
      "Epoch 10288 \t\t Training Loss: 0.0005809271824546158 \t\n",
      "Epoch 10289 \t\t Training Loss: 0.0005809271824546158 \t\n",
      "Epoch 10290 \t\t Training Loss: 0.0005809271824546158 \t\n",
      "Epoch 10291 \t\t Training Loss: 0.0005809271242469549 \t\n",
      "Epoch 10292 \t\t Training Loss: 0.0005809271242469549 \t\n",
      "Epoch 10293 \t\t Training Loss: 0.000580927066039294 \t\n",
      "Epoch 10294 \t\t Training Loss: 0.0005809271242469549 \t\n",
      "Epoch 10295 \t\t Training Loss: 0.000580927066039294 \t\n",
      "Epoch 10296 \t\t Training Loss: 0.0005809271242469549 \t\n",
      "Epoch 10297 \t\t Training Loss: 0.0005809271242469549 \t\n",
      "Epoch 10298 \t\t Training Loss: 0.0005809271242469549 \t\n",
      "Epoch 10299 \t\t Training Loss: 0.0005809271242469549 \t\n",
      "Epoch 10300 \t\t Training Loss: 0.0005809271242469549 \t\n",
      "Epoch 10301 \t\t Training Loss: 0.000580927066039294 \t\n",
      "Epoch 10302 \t\t Training Loss: 0.000580927066039294 \t\n",
      "Epoch 10303 \t\t Training Loss: 0.000580927066039294 \t\n",
      "Epoch 10304 \t\t Training Loss: 0.000580927066039294 \t\n",
      "Epoch 10305 \t\t Training Loss: 0.000580927066039294 \t\n",
      "Epoch 10306 \t\t Training Loss: 0.000580927066039294 \t\n",
      "Epoch 10307 \t\t Training Loss: 0.000580927066039294 \t\n",
      "Epoch 10308 \t\t Training Loss: 0.000580927066039294 \t\n",
      "Epoch 10309 \t\t Training Loss: 0.000580927066039294 \t\n",
      "Epoch 10310 \t\t Training Loss: 0.0005809270078316331 \t\n",
      "Epoch 10311 \t\t Training Loss: 0.0005809270078316331 \t\n",
      "Epoch 10312 \t\t Training Loss: 0.0005809270078316331 \t\n",
      "Epoch 10313 \t\t Training Loss: 0.0005809270078316331 \t\n",
      "Epoch 10314 \t\t Training Loss: 0.000580927066039294 \t\n",
      "Epoch 10315 \t\t Training Loss: 0.0005809270078316331 \t\n",
      "Epoch 10316 \t\t Training Loss: 0.0005809270078316331 \t\n",
      "Epoch 10317 \t\t Training Loss: 0.0005809270078316331 \t\n",
      "Epoch 10318 \t\t Training Loss: 0.0005809269496239722 \t\n",
      "Epoch 10319 \t\t Training Loss: 0.0005809270078316331 \t\n",
      "Epoch 10320 \t\t Training Loss: 0.0005809269496239722 \t\n",
      "Epoch 10321 \t\t Training Loss: 0.0005809268914163113 \t\n",
      "Epoch 10322 \t\t Training Loss: 0.0005809268914163113 \t\n",
      "Epoch 10323 \t\t Training Loss: 0.0005809268914163113 \t\n",
      "Epoch 10324 \t\t Training Loss: 0.0005809268914163113 \t\n",
      "Epoch 10325 \t\t Training Loss: 0.0005809268914163113 \t\n",
      "Epoch 10326 \t\t Training Loss: 0.0005809268914163113 \t\n",
      "Epoch 10327 \t\t Training Loss: 0.0005809268914163113 \t\n",
      "Epoch 10328 \t\t Training Loss: 0.0005809268914163113 \t\n",
      "Epoch 10329 \t\t Training Loss: 0.0005809268914163113 \t\n",
      "Epoch 10330 \t\t Training Loss: 0.0005809268914163113 \t\n",
      "Epoch 10331 \t\t Training Loss: 0.0005809268914163113 \t\n",
      "Epoch 10332 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10333 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10334 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10335 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10336 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10337 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10338 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10339 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10340 \t\t Training Loss: 0.0005809268914163113 \t\n",
      "Epoch 10341 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10342 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10343 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10344 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10345 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10346 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10347 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10348 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10349 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10350 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10351 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10352 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10353 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10354 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10355 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10356 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10357 \t\t Training Loss: 0.0005809268332086504 \t\n",
      "Epoch 10358 \t\t Training Loss: 0.0005809267167933285 \t\n",
      "Epoch 10359 \t\t Training Loss: 0.0005809267167933285 \t\n",
      "Epoch 10360 \t\t Training Loss: 0.0005809267167933285 \t\n",
      "Epoch 10361 \t\t Training Loss: 0.0005809267167933285 \t\n",
      "Epoch 10362 \t\t Training Loss: 0.0005809266585856676 \t\n",
      "Epoch 10363 \t\t Training Loss: 0.0005809267167933285 \t\n",
      "Epoch 10364 \t\t Training Loss: 0.0005809267167933285 \t\n",
      "Epoch 10365 \t\t Training Loss: 0.0005809267167933285 \t\n",
      "Epoch 10366 \t\t Training Loss: 0.0005809266585856676 \t\n",
      "Epoch 10367 \t\t Training Loss: 0.0005809266585856676 \t\n",
      "Epoch 10368 \t\t Training Loss: 0.0005809266585856676 \t\n",
      "Epoch 10369 \t\t Training Loss: 0.0005809266585856676 \t\n",
      "Epoch 10370 \t\t Training Loss: 0.0005809266585856676 \t\n",
      "Epoch 10371 \t\t Training Loss: 0.0005809266585856676 \t\n",
      "Epoch 10372 \t\t Training Loss: 0.0005809266003780067 \t\n",
      "Epoch 10373 \t\t Training Loss: 0.0005809266585856676 \t\n",
      "Epoch 10374 \t\t Training Loss: 0.0005809266585856676 \t\n",
      "Epoch 10375 \t\t Training Loss: 0.0005809266003780067 \t\n",
      "Epoch 10376 \t\t Training Loss: 0.0005809266003780067 \t\n",
      "Epoch 10377 \t\t Training Loss: 0.0005809266003780067 \t\n",
      "Epoch 10378 \t\t Training Loss: 0.0005809266003780067 \t\n",
      "Epoch 10379 \t\t Training Loss: 0.0005809266003780067 \t\n",
      "Epoch 10380 \t\t Training Loss: 0.0005809266003780067 \t\n",
      "Epoch 10381 \t\t Training Loss: 0.0005809266003780067 \t\n",
      "Epoch 10382 \t\t Training Loss: 0.0005809266003780067 \t\n",
      "Epoch 10383 \t\t Training Loss: 0.0005809266003780067 \t\n",
      "Epoch 10384 \t\t Training Loss: 0.0005809265421703458 \t\n",
      "Epoch 10385 \t\t Training Loss: 0.0005809265421703458 \t\n",
      "Epoch 10386 \t\t Training Loss: 0.0005809265421703458 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10387 \t\t Training Loss: 0.0005809264839626849 \t\n",
      "Epoch 10388 \t\t Training Loss: 0.0005809264839626849 \t\n",
      "Epoch 10389 \t\t Training Loss: 0.0005809264839626849 \t\n",
      "Epoch 10390 \t\t Training Loss: 0.0005809264839626849 \t\n",
      "Epoch 10391 \t\t Training Loss: 0.0005809264839626849 \t\n",
      "Epoch 10392 \t\t Training Loss: 0.0005809264839626849 \t\n",
      "Epoch 10393 \t\t Training Loss: 0.0005809264839626849 \t\n",
      "Epoch 10394 \t\t Training Loss: 0.0005809264839626849 \t\n",
      "Epoch 10395 \t\t Training Loss: 0.0005809264839626849 \t\n",
      "Epoch 10396 \t\t Training Loss: 0.0005809264839626849 \t\n",
      "Epoch 10397 \t\t Training Loss: 0.0005809264839626849 \t\n",
      "Epoch 10398 \t\t Training Loss: 0.000580926425755024 \t\n",
      "Epoch 10399 \t\t Training Loss: 0.000580926425755024 \t\n",
      "Epoch 10400 \t\t Training Loss: 0.000580926425755024 \t\n",
      "Epoch 10401 \t\t Training Loss: 0.000580926425755024 \t\n",
      "Epoch 10402 \t\t Training Loss: 0.000580926425755024 \t\n",
      "Epoch 10403 \t\t Training Loss: 0.000580926425755024 \t\n",
      "Epoch 10404 \t\t Training Loss: 0.000580926425755024 \t\n",
      "Epoch 10405 \t\t Training Loss: 0.000580926425755024 \t\n",
      "Epoch 10406 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10407 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10408 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10409 \t\t Training Loss: 0.000580926367547363 \t\n",
      "Epoch 10410 \t\t Training Loss: 0.000580926367547363 \t\n",
      "Epoch 10411 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10412 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10413 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10414 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10415 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10416 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10417 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10418 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10419 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10420 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10421 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10422 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10423 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10424 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10425 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10426 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10427 \t\t Training Loss: 0.0005809262511320412 \t\n",
      "Epoch 10428 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10429 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10430 \t\t Training Loss: 0.0005809262511320412 \t\n",
      "Epoch 10431 \t\t Training Loss: 0.0005809262511320412 \t\n",
      "Epoch 10432 \t\t Training Loss: 0.0005809262511320412 \t\n",
      "Epoch 10433 \t\t Training Loss: 0.0005809263093397021 \t\n",
      "Epoch 10434 \t\t Training Loss: 0.0005809262511320412 \t\n",
      "Epoch 10435 \t\t Training Loss: 0.0005809262511320412 \t\n",
      "Epoch 10436 \t\t Training Loss: 0.0005809262511320412 \t\n",
      "Epoch 10437 \t\t Training Loss: 0.0005809262511320412 \t\n",
      "Epoch 10438 \t\t Training Loss: 0.0005809261929243803 \t\n",
      "Epoch 10439 \t\t Training Loss: 0.0005809261929243803 \t\n",
      "Epoch 10440 \t\t Training Loss: 0.0005809261929243803 \t\n",
      "Epoch 10441 \t\t Training Loss: 0.0005809261929243803 \t\n",
      "Epoch 10442 \t\t Training Loss: 0.0005809261929243803 \t\n",
      "Epoch 10443 \t\t Training Loss: 0.0005809261929243803 \t\n",
      "Epoch 10444 \t\t Training Loss: 0.0005809261929243803 \t\n",
      "Epoch 10445 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10446 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10447 \t\t Training Loss: 0.0005809261929243803 \t\n",
      "Epoch 10448 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10449 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10450 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10451 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10452 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10453 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10454 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10455 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10456 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10457 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10458 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10459 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10460 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10461 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10462 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10463 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10464 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10465 \t\t Training Loss: 0.0005809261347167194 \t\n",
      "Epoch 10466 \t\t Training Loss: 0.0005809260183013976 \t\n",
      "Epoch 10467 \t\t Training Loss: 0.0005809260183013976 \t\n",
      "Epoch 10468 \t\t Training Loss: 0.0005809260183013976 \t\n",
      "Epoch 10469 \t\t Training Loss: 0.0005809260183013976 \t\n",
      "Epoch 10470 \t\t Training Loss: 0.0005809260183013976 \t\n",
      "Epoch 10471 \t\t Training Loss: 0.0005809260183013976 \t\n",
      "Epoch 10472 \t\t Training Loss: 0.0005809260183013976 \t\n",
      "Epoch 10473 \t\t Training Loss: 0.0005809260183013976 \t\n",
      "Epoch 10474 \t\t Training Loss: 0.0005809260183013976 \t\n",
      "Epoch 10475 \t\t Training Loss: 0.0005809260183013976 \t\n",
      "Epoch 10476 \t\t Training Loss: 0.0005809260183013976 \t\n",
      "Epoch 10477 \t\t Training Loss: 0.0005809260183013976 \t\n",
      "Epoch 10478 \t\t Training Loss: 0.0005809260183013976 \t\n",
      "Epoch 10479 \t\t Training Loss: 0.0005809260183013976 \t\n",
      "Epoch 10480 \t\t Training Loss: 0.0005809260183013976 \t\n",
      "Epoch 10481 \t\t Training Loss: 0.0005809259600937366 \t\n",
      "Epoch 10482 \t\t Training Loss: 0.0005809259600937366 \t\n",
      "Epoch 10483 \t\t Training Loss: 0.0005809259600937366 \t\n",
      "Epoch 10484 \t\t Training Loss: 0.0005809259600937366 \t\n",
      "Epoch 10485 \t\t Training Loss: 0.0005809259600937366 \t\n",
      "Epoch 10486 \t\t Training Loss: 0.0005809260183013976 \t\n",
      "Epoch 10487 \t\t Training Loss: 0.0005809259600937366 \t\n",
      "Epoch 10488 \t\t Training Loss: 0.0005809259600937366 \t\n",
      "Epoch 10489 \t\t Training Loss: 0.0005809259600937366 \t\n",
      "Epoch 10490 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10491 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10492 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10493 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10494 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10495 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10496 \t\t Training Loss: 0.0005809259600937366 \t\n",
      "Epoch 10497 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10498 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10499 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10500 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10501 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10502 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10503 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10504 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10505 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10506 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10507 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10508 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10509 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10510 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10511 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10512 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10513 \t\t Training Loss: 0.0005809259018860757 \t\n",
      "Epoch 10514 \t\t Training Loss: 0.0005809258436784148 \t\n",
      "Epoch 10515 \t\t Training Loss: 0.0005809258436784148 \t\n",
      "Epoch 10516 \t\t Training Loss: 0.0005809258436784148 \t\n",
      "Epoch 10517 \t\t Training Loss: 0.0005809258436784148 \t\n",
      "Epoch 10518 \t\t Training Loss: 0.0005809258436784148 \t\n",
      "Epoch 10519 \t\t Training Loss: 0.0005809258436784148 \t\n",
      "Epoch 10520 \t\t Training Loss: 0.0005809258436784148 \t\n",
      "Epoch 10521 \t\t Training Loss: 0.0005809258436784148 \t\n",
      "Epoch 10522 \t\t Training Loss: 0.0005809258436784148 \t\n",
      "Epoch 10523 \t\t Training Loss: 0.0005809258436784148 \t\n",
      "Epoch 10524 \t\t Training Loss: 0.0005809258436784148 \t\n",
      "Epoch 10525 \t\t Training Loss: 0.0005809258436784148 \t\n",
      "Epoch 10526 \t\t Training Loss: 0.0005809258436784148 \t\n",
      "Epoch 10527 \t\t Training Loss: 0.0005809258436784148 \t\n",
      "Epoch 10528 \t\t Training Loss: 0.0005809258436784148 \t\n",
      "Epoch 10529 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10530 \t\t Training Loss: 0.0005809258436784148 \t\n",
      "Epoch 10531 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10532 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10533 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10534 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10535 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10536 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10537 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10538 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10539 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10540 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10541 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10542 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10543 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10544 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10545 \t\t Training Loss: 0.000580925727263093 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10546 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10547 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10548 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10549 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10550 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10551 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10552 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10553 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10554 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10555 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10556 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10557 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10558 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10559 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10560 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10561 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10562 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10563 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10564 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10565 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10566 \t\t Training Loss: 0.0005809256108477712 \t\n",
      "Epoch 10567 \t\t Training Loss: 0.0005809256108477712 \t\n",
      "Epoch 10568 \t\t Training Loss: 0.000580925727263093 \t\n",
      "Epoch 10569 \t\t Training Loss: 0.0005809256108477712 \t\n",
      "Epoch 10570 \t\t Training Loss: 0.0005809256108477712 \t\n",
      "Epoch 10571 \t\t Training Loss: 0.0005809256108477712 \t\n",
      "Epoch 10572 \t\t Training Loss: 0.0005809256108477712 \t\n",
      "Epoch 10573 \t\t Training Loss: 0.0005809256108477712 \t\n",
      "Epoch 10574 \t\t Training Loss: 0.0005809256108477712 \t\n",
      "Epoch 10575 \t\t Training Loss: 0.0005809256108477712 \t\n",
      "Epoch 10576 \t\t Training Loss: 0.0005809256108477712 \t\n",
      "Epoch 10577 \t\t Training Loss: 0.0005809256108477712 \t\n",
      "Epoch 10578 \t\t Training Loss: 0.0005809256108477712 \t\n",
      "Epoch 10579 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10580 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10581 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10582 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10583 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10584 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10585 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10586 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10587 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10588 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10589 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10590 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10591 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10592 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10593 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10594 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10595 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10596 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10597 \t\t Training Loss: 0.0005809255526401103 \t\n",
      "Epoch 10598 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10599 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10600 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10601 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10602 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10603 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10604 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10605 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10606 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10607 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10608 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10609 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10610 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10611 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10612 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10613 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10614 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10615 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10616 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10617 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10618 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10619 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10620 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10621 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10622 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10623 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10624 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10625 \t\t Training Loss: 0.0005809253198094666 \t\n",
      "Epoch 10626 \t\t Training Loss: 0.0005809253198094666 \t\n",
      "Epoch 10627 \t\t Training Loss: 0.0005809254362247884 \t\n",
      "Epoch 10628 \t\t Training Loss: 0.0005809253198094666 \t\n",
      "Epoch 10629 \t\t Training Loss: 0.0005809253198094666 \t\n",
      "Epoch 10630 \t\t Training Loss: 0.0005809253198094666 \t\n",
      "Epoch 10631 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10632 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10633 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10634 \t\t Training Loss: 0.0005809253198094666 \t\n",
      "Epoch 10635 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10636 \t\t Training Loss: 0.0005809253198094666 \t\n",
      "Epoch 10637 \t\t Training Loss: 0.0005809253198094666 \t\n",
      "Epoch 10638 \t\t Training Loss: 0.0005809253198094666 \t\n",
      "Epoch 10639 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10640 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10641 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10642 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10643 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10644 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10645 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10646 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10647 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10648 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10649 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10650 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10651 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10652 \t\t Training Loss: 0.0005809251451864839 \t\n",
      "Epoch 10653 \t\t Training Loss: 0.0005809251451864839 \t\n",
      "Epoch 10654 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10655 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10656 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10657 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10658 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10659 \t\t Training Loss: 0.0005809252616018057 \t\n",
      "Epoch 10660 \t\t Training Loss: 0.0005809251451864839 \t\n",
      "Epoch 10661 \t\t Training Loss: 0.0005809251451864839 \t\n",
      "Epoch 10662 \t\t Training Loss: 0.0005809251451864839 \t\n",
      "Epoch 10663 \t\t Training Loss: 0.0005809251451864839 \t\n",
      "Epoch 10664 \t\t Training Loss: 0.0005809251451864839 \t\n",
      "Epoch 10665 \t\t Training Loss: 0.0005809251451864839 \t\n",
      "Epoch 10666 \t\t Training Loss: 0.0005809251451864839 \t\n",
      "Epoch 10667 \t\t Training Loss: 0.0005809251451864839 \t\n",
      "Epoch 10668 \t\t Training Loss: 0.0005809251451864839 \t\n",
      "Epoch 10669 \t\t Training Loss: 0.0005809251451864839 \t\n",
      "Epoch 10670 \t\t Training Loss: 0.0005809251451864839 \t\n",
      "Epoch 10671 \t\t Training Loss: 0.0005809251451864839 \t\n",
      "Epoch 10672 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10673 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10674 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10675 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10676 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10677 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10678 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10679 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10680 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10681 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10682 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10683 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10684 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10685 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10686 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10687 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10688 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10689 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10690 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10691 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10692 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10693 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10694 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10695 \t\t Training Loss: 0.0005809249123558402 \t\n",
      "Epoch 10696 \t\t Training Loss: 0.0005809249123558402 \t\n",
      "Epoch 10697 \t\t Training Loss: 0.0005809249123558402 \t\n",
      "Epoch 10698 \t\t Training Loss: 0.0005809249123558402 \t\n",
      "Epoch 10699 \t\t Training Loss: 0.0005809249123558402 \t\n",
      "Epoch 10700 \t\t Training Loss: 0.0005809249123558402 \t\n",
      "Epoch 10701 \t\t Training Loss: 0.0005809249123558402 \t\n",
      "Epoch 10702 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10703 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10704 \t\t Training Loss: 0.0005809249123558402 \t\n",
      "Epoch 10705 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10706 \t\t Training Loss: 0.000580925028771162 \t\n",
      "Epoch 10707 \t\t Training Loss: 0.0005809249123558402 \t\n",
      "Epoch 10708 \t\t Training Loss: 0.0005809249123558402 \t\n",
      "Epoch 10709 \t\t Training Loss: 0.0005809249123558402 \t\n",
      "Epoch 10710 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10711 \t\t Training Loss: 0.0005809248541481793 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10712 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10713 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10714 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10715 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10716 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10717 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10718 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10719 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10720 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10721 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10722 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10723 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10724 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10725 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10726 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10727 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10728 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10729 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10730 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10731 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10732 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10733 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10734 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10735 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10736 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10737 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10738 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10739 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10740 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10741 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10742 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10743 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10744 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10745 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10746 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10747 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10748 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10749 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10750 \t\t Training Loss: 0.0005809248541481793 \t\n",
      "Epoch 10751 \t\t Training Loss: 0.0005809247377328575 \t\n",
      "Epoch 10752 \t\t Training Loss: 0.0005809247377328575 \t\n",
      "Epoch 10753 \t\t Training Loss: 0.0005809247377328575 \t\n",
      "Epoch 10754 \t\t Training Loss: 0.0005809247377328575 \t\n",
      "Epoch 10755 \t\t Training Loss: 0.0005809247377328575 \t\n",
      "Epoch 10756 \t\t Training Loss: 0.0005809247377328575 \t\n",
      "Epoch 10757 \t\t Training Loss: 0.0005809247377328575 \t\n",
      "Epoch 10758 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10759 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10760 \t\t Training Loss: 0.0005809247377328575 \t\n",
      "Epoch 10761 \t\t Training Loss: 0.0005809247377328575 \t\n",
      "Epoch 10762 \t\t Training Loss: 0.0005809247377328575 \t\n",
      "Epoch 10763 \t\t Training Loss: 0.0005809247377328575 \t\n",
      "Epoch 10764 \t\t Training Loss: 0.0005809247377328575 \t\n",
      "Epoch 10765 \t\t Training Loss: 0.0005809247377328575 \t\n",
      "Epoch 10766 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10767 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10768 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10769 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10770 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10771 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10772 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10773 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10774 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10775 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10776 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10777 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10778 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10779 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10780 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10781 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10782 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10783 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10784 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10785 \t\t Training Loss: 0.0005809245631098747 \t\n",
      "Epoch 10786 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10787 \t\t Training Loss: 0.0005809245631098747 \t\n",
      "Epoch 10788 \t\t Training Loss: 0.0005809245631098747 \t\n",
      "Epoch 10789 \t\t Training Loss: 0.0005809245631098747 \t\n",
      "Epoch 10790 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10791 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10792 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10793 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10794 \t\t Training Loss: 0.0005809246213175356 \t\n",
      "Epoch 10795 \t\t Training Loss: 0.0005809245631098747 \t\n",
      "Epoch 10796 \t\t Training Loss: 0.0005809245631098747 \t\n",
      "Epoch 10797 \t\t Training Loss: 0.0005809245631098747 \t\n",
      "Epoch 10798 \t\t Training Loss: 0.0005809245631098747 \t\n",
      "Epoch 10799 \t\t Training Loss: 0.0005809245631098747 \t\n",
      "Epoch 10800 \t\t Training Loss: 0.0005809245631098747 \t\n",
      "Epoch 10801 \t\t Training Loss: 0.0005809245631098747 \t\n",
      "Epoch 10802 \t\t Training Loss: 0.0005809245631098747 \t\n",
      "Epoch 10803 \t\t Training Loss: 0.0005809245631098747 \t\n",
      "Epoch 10804 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10805 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10806 \t\t Training Loss: 0.0005809245631098747 \t\n",
      "Epoch 10807 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10808 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10809 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10810 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10811 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10812 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10813 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10814 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10815 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10816 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10817 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10818 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10819 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10820 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10821 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10822 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10823 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10824 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10825 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10826 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10827 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10828 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10829 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10830 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10831 \t\t Training Loss: 0.0005809243302792311 \t\n",
      "Epoch 10832 \t\t Training Loss: 0.0005809243302792311 \t\n",
      "Epoch 10833 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10834 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10835 \t\t Training Loss: 0.0005809244466945529 \t\n",
      "Epoch 10836 \t\t Training Loss: 0.0005809243302792311 \t\n",
      "Epoch 10837 \t\t Training Loss: 0.0005809243302792311 \t\n",
      "Epoch 10838 \t\t Training Loss: 0.0005809243302792311 \t\n",
      "Epoch 10839 \t\t Training Loss: 0.0005809243302792311 \t\n",
      "Epoch 10840 \t\t Training Loss: 0.0005809243302792311 \t\n",
      "Epoch 10841 \t\t Training Loss: 0.0005809243302792311 \t\n",
      "Epoch 10842 \t\t Training Loss: 0.0005809243302792311 \t\n",
      "Epoch 10843 \t\t Training Loss: 0.0005809243302792311 \t\n",
      "Epoch 10844 \t\t Training Loss: 0.0005809243302792311 \t\n",
      "Epoch 10845 \t\t Training Loss: 0.0005809243302792311 \t\n",
      "Epoch 10846 \t\t Training Loss: 0.0005809242720715702 \t\n",
      "Epoch 10847 \t\t Training Loss: 0.0005809242720715702 \t\n",
      "Epoch 10848 \t\t Training Loss: 0.0005809242720715702 \t\n",
      "Epoch 10849 \t\t Training Loss: 0.0005809243302792311 \t\n",
      "Epoch 10850 \t\t Training Loss: 0.0005809242720715702 \t\n",
      "Epoch 10851 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10852 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10853 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10854 \t\t Training Loss: 0.0005809242720715702 \t\n",
      "Epoch 10855 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10856 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10857 \t\t Training Loss: 0.0005809242720715702 \t\n",
      "Epoch 10858 \t\t Training Loss: 0.0005809242720715702 \t\n",
      "Epoch 10859 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10860 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10861 \t\t Training Loss: 0.0005809242720715702 \t\n",
      "Epoch 10862 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10863 \t\t Training Loss: 0.0005809242720715702 \t\n",
      "Epoch 10864 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10865 \t\t Training Loss: 0.0005809242720715702 \t\n",
      "Epoch 10866 \t\t Training Loss: 0.0005809241556562483 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10867 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10868 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10869 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10870 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10871 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10872 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10873 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10874 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10875 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10876 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10877 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10878 \t\t Training Loss: 0.0005809241556562483 \t\n",
      "Epoch 10879 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10880 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10881 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10882 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10883 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10884 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10885 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10886 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10887 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10888 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10889 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10890 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10891 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10892 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10893 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10894 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10895 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10896 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10897 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10898 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10899 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10900 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10901 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10902 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10903 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10904 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10905 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10906 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10907 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10908 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10909 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10910 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10911 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10912 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10913 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10914 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10915 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10916 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10917 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10918 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10919 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10920 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10921 \t\t Training Loss: 0.0005809240392409265 \t\n",
      "Epoch 10922 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10923 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10924 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10925 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10926 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10927 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10928 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10929 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10930 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10931 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10932 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10933 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10934 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10935 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10936 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10937 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10938 \t\t Training Loss: 0.0005809239228256047 \t\n",
      "Epoch 10939 \t\t Training Loss: 0.0005809239228256047 \t\n",
      "Epoch 10940 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10941 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10942 \t\t Training Loss: 0.0005809239228256047 \t\n",
      "Epoch 10943 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10944 \t\t Training Loss: 0.0005809239810332656 \t\n",
      "Epoch 10945 \t\t Training Loss: 0.0005809239228256047 \t\n",
      "Epoch 10946 \t\t Training Loss: 0.0005809238646179438 \t\n",
      "Epoch 10947 \t\t Training Loss: 0.0005809238646179438 \t\n",
      "Epoch 10948 \t\t Training Loss: 0.0005809238646179438 \t\n",
      "Epoch 10949 \t\t Training Loss: 0.0005809238646179438 \t\n",
      "Epoch 10950 \t\t Training Loss: 0.0005809238646179438 \t\n",
      "Epoch 10951 \t\t Training Loss: 0.0005809238646179438 \t\n",
      "Epoch 10952 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10953 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10954 \t\t Training Loss: 0.0005809238646179438 \t\n",
      "Epoch 10955 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10956 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10957 \t\t Training Loss: 0.0005809238646179438 \t\n",
      "Epoch 10958 \t\t Training Loss: 0.0005809238646179438 \t\n",
      "Epoch 10959 \t\t Training Loss: 0.0005809238646179438 \t\n",
      "Epoch 10960 \t\t Training Loss: 0.0005809238646179438 \t\n",
      "Epoch 10961 \t\t Training Loss: 0.0005809238646179438 \t\n",
      "Epoch 10962 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10963 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10964 \t\t Training Loss: 0.0005809238646179438 \t\n",
      "Epoch 10965 \t\t Training Loss: 0.0005809238646179438 \t\n",
      "Epoch 10966 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10967 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10968 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10969 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10970 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10971 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10972 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10973 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10974 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10975 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10976 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10977 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10978 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10979 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10980 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10981 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10982 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10983 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10984 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10985 \t\t Training Loss: 0.0005809237482026219 \t\n",
      "Epoch 10986 \t\t Training Loss: 0.000580923689994961 \t\n",
      "Epoch 10987 \t\t Training Loss: 0.000580923689994961 \t\n",
      "Epoch 10988 \t\t Training Loss: 0.000580923689994961 \t\n",
      "Epoch 10989 \t\t Training Loss: 0.000580923689994961 \t\n",
      "Epoch 10990 \t\t Training Loss: 0.000580923689994961 \t\n",
      "Epoch 10991 \t\t Training Loss: 0.000580923689994961 \t\n",
      "Epoch 10992 \t\t Training Loss: 0.000580923689994961 \t\n",
      "Epoch 10993 \t\t Training Loss: 0.000580923689994961 \t\n",
      "Epoch 10994 \t\t Training Loss: 0.000580923689994961 \t\n",
      "Epoch 10995 \t\t Training Loss: 0.000580923689994961 \t\n",
      "Epoch 10996 \t\t Training Loss: 0.0005809235735796392 \t\n",
      "Epoch 10997 \t\t Training Loss: 0.000580923689994961 \t\n",
      "Epoch 10998 \t\t Training Loss: 0.0005809235735796392 \t\n",
      "Epoch 10999 \t\t Training Loss: 0.000580923689994961 \t\n",
      "Epoch 11000 \t\t Training Loss: 0.0005809235735796392 \t\n",
      "Epoch 11001 \t\t Training Loss: 0.0005809235735796392 \t\n",
      "Epoch 11002 \t\t Training Loss: 0.0005809235735796392 \t\n",
      "Epoch 11003 \t\t Training Loss: 0.0005809235735796392 \t\n",
      "Epoch 11004 \t\t Training Loss: 0.0005809235735796392 \t\n",
      "Epoch 11005 \t\t Training Loss: 0.0005809235153719783 \t\n",
      "Epoch 11006 \t\t Training Loss: 0.0005809235153719783 \t\n",
      "Epoch 11007 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11008 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11009 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11010 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11011 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11012 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11013 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11014 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11015 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11016 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11017 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11018 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11019 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11020 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11021 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11022 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11023 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11024 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11025 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11026 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11027 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11028 \t\t Training Loss: 0.0005809234571643174 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11029 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11030 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11031 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11032 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11033 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11034 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11035 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11036 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11037 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11038 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11039 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11040 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11041 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11042 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11043 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11044 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11045 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11046 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11047 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11048 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11049 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11050 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11051 \t\t Training Loss: 0.0005809234571643174 \t\n",
      "Epoch 11052 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11053 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11054 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11055 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11056 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11057 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11058 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11059 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11060 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11061 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11062 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11063 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11064 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11065 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11066 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11067 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11068 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11069 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11070 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11071 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11072 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11073 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11074 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11075 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11076 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11077 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11078 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11079 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11080 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11081 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11082 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11083 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11084 \t\t Training Loss: 0.0005809233407489955 \t\n",
      "Epoch 11085 \t\t Training Loss: 0.0005809232825413346 \t\n",
      "Epoch 11086 \t\t Training Loss: 0.0005809232825413346 \t\n",
      "Epoch 11087 \t\t Training Loss: 0.0005809232825413346 \t\n",
      "Epoch 11088 \t\t Training Loss: 0.0005809232825413346 \t\n",
      "Epoch 11089 \t\t Training Loss: 0.0005809232825413346 \t\n",
      "Epoch 11090 \t\t Training Loss: 0.0005809232825413346 \t\n",
      "Epoch 11091 \t\t Training Loss: 0.0005809232825413346 \t\n",
      "Epoch 11092 \t\t Training Loss: 0.0005809232825413346 \t\n",
      "Epoch 11093 \t\t Training Loss: 0.0005809232825413346 \t\n",
      "Epoch 11094 \t\t Training Loss: 0.0005809232825413346 \t\n",
      "Epoch 11095 \t\t Training Loss: 0.0005809232825413346 \t\n",
      "Epoch 11096 \t\t Training Loss: 0.0005809232825413346 \t\n",
      "Epoch 11097 \t\t Training Loss: 0.0005809232825413346 \t\n",
      "Epoch 11098 \t\t Training Loss: 0.0005809232243336737 \t\n",
      "Epoch 11099 \t\t Training Loss: 0.0005809232243336737 \t\n",
      "Epoch 11100 \t\t Training Loss: 0.0005809232243336737 \t\n",
      "Epoch 11101 \t\t Training Loss: 0.0005809232243336737 \t\n",
      "Epoch 11102 \t\t Training Loss: 0.0005809232243336737 \t\n",
      "Epoch 11103 \t\t Training Loss: 0.0005809232243336737 \t\n",
      "Epoch 11104 \t\t Training Loss: 0.0005809232243336737 \t\n",
      "Epoch 11105 \t\t Training Loss: 0.0005809232243336737 \t\n",
      "Epoch 11106 \t\t Training Loss: 0.0005809232243336737 \t\n",
      "Epoch 11107 \t\t Training Loss: 0.0005809231661260128 \t\n",
      "Epoch 11108 \t\t Training Loss: 0.0005809231661260128 \t\n",
      "Epoch 11109 \t\t Training Loss: 0.0005809231661260128 \t\n",
      "Epoch 11110 \t\t Training Loss: 0.0005809231661260128 \t\n",
      "Epoch 11111 \t\t Training Loss: 0.0005809231661260128 \t\n",
      "Epoch 11112 \t\t Training Loss: 0.0005809231661260128 \t\n",
      "Epoch 11113 \t\t Training Loss: 0.0005809231661260128 \t\n",
      "Epoch 11114 \t\t Training Loss: 0.0005809231661260128 \t\n",
      "Epoch 11115 \t\t Training Loss: 0.000580923049710691 \t\n",
      "Epoch 11116 \t\t Training Loss: 0.000580923049710691 \t\n",
      "Epoch 11117 \t\t Training Loss: 0.000580923049710691 \t\n",
      "Epoch 11118 \t\t Training Loss: 0.000580923049710691 \t\n",
      "Epoch 11119 \t\t Training Loss: 0.0005809229915030301 \t\n",
      "Epoch 11120 \t\t Training Loss: 0.0005809229915030301 \t\n",
      "Epoch 11121 \t\t Training Loss: 0.0005809229915030301 \t\n",
      "Epoch 11122 \t\t Training Loss: 0.0005809229332953691 \t\n",
      "Epoch 11123 \t\t Training Loss: 0.0005809229332953691 \t\n",
      "Epoch 11124 \t\t Training Loss: 0.0005809229332953691 \t\n",
      "Epoch 11125 \t\t Training Loss: 0.0005809229332953691 \t\n",
      "Epoch 11126 \t\t Training Loss: 0.0005809228750877082 \t\n",
      "Epoch 11127 \t\t Training Loss: 0.0005809228750877082 \t\n",
      "Epoch 11128 \t\t Training Loss: 0.0005809228168800473 \t\n",
      "Epoch 11129 \t\t Training Loss: 0.0005809228168800473 \t\n",
      "Epoch 11130 \t\t Training Loss: 0.0005809228168800473 \t\n",
      "Epoch 11131 \t\t Training Loss: 0.0005809228168800473 \t\n",
      "Epoch 11132 \t\t Training Loss: 0.0005809228168800473 \t\n",
      "Epoch 11133 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11134 \t\t Training Loss: 0.0005809228168800473 \t\n",
      "Epoch 11135 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11136 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11137 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11138 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11139 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11140 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11141 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11142 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11143 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11144 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11145 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11146 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11147 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11148 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11149 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11150 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11151 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11152 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11153 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11154 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11155 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11156 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11157 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11158 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11159 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11160 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11161 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11162 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11163 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11164 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11165 \t\t Training Loss: 0.0005809227586723864 \t\n",
      "Epoch 11166 \t\t Training Loss: 0.0005809227004647255 \t\n",
      "Epoch 11167 \t\t Training Loss: 0.0005809227004647255 \t\n",
      "Epoch 11168 \t\t Training Loss: 0.0005809227004647255 \t\n",
      "Epoch 11169 \t\t Training Loss: 0.0005809227004647255 \t\n",
      "Epoch 11170 \t\t Training Loss: 0.0005809227004647255 \t\n",
      "Epoch 11171 \t\t Training Loss: 0.0005809227004647255 \t\n",
      "Epoch 11172 \t\t Training Loss: 0.0005809226422570646 \t\n",
      "Epoch 11173 \t\t Training Loss: 0.0005809226422570646 \t\n",
      "Epoch 11174 \t\t Training Loss: 0.0005809226422570646 \t\n",
      "Epoch 11175 \t\t Training Loss: 0.0005809226422570646 \t\n",
      "Epoch 11176 \t\t Training Loss: 0.0005809226422570646 \t\n",
      "Epoch 11177 \t\t Training Loss: 0.0005809226422570646 \t\n",
      "Epoch 11178 \t\t Training Loss: 0.0005809226422570646 \t\n",
      "Epoch 11179 \t\t Training Loss: 0.0005809226422570646 \t\n",
      "Epoch 11180 \t\t Training Loss: 0.0005809225840494037 \t\n",
      "Epoch 11181 \t\t Training Loss: 0.0005809225840494037 \t\n",
      "Epoch 11182 \t\t Training Loss: 0.0005809225840494037 \t\n",
      "Epoch 11183 \t\t Training Loss: 0.0005809225840494037 \t\n",
      "Epoch 11184 \t\t Training Loss: 0.0005809225840494037 \t\n",
      "Epoch 11185 \t\t Training Loss: 0.0005809225840494037 \t\n",
      "Epoch 11186 \t\t Training Loss: 0.0005809225840494037 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11187 \t\t Training Loss: 0.0005809225840494037 \t\n",
      "Epoch 11188 \t\t Training Loss: 0.0005809225258417428 \t\n",
      "Epoch 11189 \t\t Training Loss: 0.0005809225840494037 \t\n",
      "Epoch 11190 \t\t Training Loss: 0.0005809225258417428 \t\n",
      "Epoch 11191 \t\t Training Loss: 0.0005809225258417428 \t\n",
      "Epoch 11192 \t\t Training Loss: 0.0005809225258417428 \t\n",
      "Epoch 11193 \t\t Training Loss: 0.0005809225258417428 \t\n",
      "Epoch 11194 \t\t Training Loss: 0.0005809225258417428 \t\n",
      "Epoch 11195 \t\t Training Loss: 0.0005809225258417428 \t\n",
      "Epoch 11196 \t\t Training Loss: 0.0005809225258417428 \t\n",
      "Epoch 11197 \t\t Training Loss: 0.0005809225258417428 \t\n",
      "Epoch 11198 \t\t Training Loss: 0.0005809224094264209 \t\n",
      "Epoch 11199 \t\t Training Loss: 0.0005809225258417428 \t\n",
      "Epoch 11200 \t\t Training Loss: 0.0005809224094264209 \t\n",
      "Epoch 11201 \t\t Training Loss: 0.0005809224094264209 \t\n",
      "Epoch 11202 \t\t Training Loss: 0.0005809224094264209 \t\n",
      "Epoch 11203 \t\t Training Loss: 0.0005809224094264209 \t\n",
      "Epoch 11204 \t\t Training Loss: 0.0005809224094264209 \t\n",
      "Epoch 11205 \t\t Training Loss: 0.00058092235121876 \t\n",
      "Epoch 11206 \t\t Training Loss: 0.00058092235121876 \t\n",
      "Epoch 11207 \t\t Training Loss: 0.0005809224094264209 \t\n",
      "Epoch 11208 \t\t Training Loss: 0.00058092235121876 \t\n",
      "Epoch 11209 \t\t Training Loss: 0.00058092235121876 \t\n",
      "Epoch 11210 \t\t Training Loss: 0.00058092235121876 \t\n",
      "Epoch 11211 \t\t Training Loss: 0.00058092235121876 \t\n",
      "Epoch 11212 \t\t Training Loss: 0.00058092235121876 \t\n",
      "Epoch 11213 \t\t Training Loss: 0.00058092235121876 \t\n",
      "Epoch 11214 \t\t Training Loss: 0.00058092235121876 \t\n",
      "Epoch 11215 \t\t Training Loss: 0.00058092235121876 \t\n",
      "Epoch 11216 \t\t Training Loss: 0.0005809222930110991 \t\n",
      "Epoch 11217 \t\t Training Loss: 0.0005809222930110991 \t\n",
      "Epoch 11218 \t\t Training Loss: 0.0005809222930110991 \t\n",
      "Epoch 11219 \t\t Training Loss: 0.0005809222930110991 \t\n",
      "Epoch 11220 \t\t Training Loss: 0.0005809222930110991 \t\n",
      "Epoch 11221 \t\t Training Loss: 0.0005809222930110991 \t\n",
      "Epoch 11222 \t\t Training Loss: 0.0005809222930110991 \t\n",
      "Epoch 11223 \t\t Training Loss: 0.0005809222930110991 \t\n",
      "Epoch 11224 \t\t Training Loss: 0.0005809222930110991 \t\n",
      "Epoch 11225 \t\t Training Loss: 0.0005809222930110991 \t\n",
      "Epoch 11226 \t\t Training Loss: 0.0005809222930110991 \t\n",
      "Epoch 11227 \t\t Training Loss: 0.0005809222930110991 \t\n",
      "Epoch 11228 \t\t Training Loss: 0.0005809222930110991 \t\n",
      "Epoch 11229 \t\t Training Loss: 0.0005809222930110991 \t\n",
      "Epoch 11230 \t\t Training Loss: 0.0005809222930110991 \t\n",
      "Epoch 11231 \t\t Training Loss: 0.0005809222348034382 \t\n",
      "Epoch 11232 \t\t Training Loss: 0.0005809222348034382 \t\n",
      "Epoch 11233 \t\t Training Loss: 0.0005809221765957773 \t\n",
      "Epoch 11234 \t\t Training Loss: 0.0005809221765957773 \t\n",
      "Epoch 11235 \t\t Training Loss: 0.0005809221765957773 \t\n",
      "Epoch 11236 \t\t Training Loss: 0.0005809221765957773 \t\n",
      "Epoch 11237 \t\t Training Loss: 0.0005809221183881164 \t\n",
      "Epoch 11238 \t\t Training Loss: 0.0005809221183881164 \t\n",
      "Epoch 11239 \t\t Training Loss: 0.0005809221765957773 \t\n",
      "Epoch 11240 \t\t Training Loss: 0.0005809221765957773 \t\n",
      "Epoch 11241 \t\t Training Loss: 0.0005809221765957773 \t\n",
      "Epoch 11242 \t\t Training Loss: 0.0005809221765957773 \t\n",
      "Epoch 11243 \t\t Training Loss: 0.0005809221765957773 \t\n",
      "Epoch 11244 \t\t Training Loss: 0.0005809221765957773 \t\n",
      "Epoch 11245 \t\t Training Loss: 0.0005809221765957773 \t\n",
      "Epoch 11246 \t\t Training Loss: 0.0005809221183881164 \t\n",
      "Epoch 11247 \t\t Training Loss: 0.0005809221183881164 \t\n",
      "Epoch 11248 \t\t Training Loss: 0.0005809221183881164 \t\n",
      "Epoch 11249 \t\t Training Loss: 0.0005809221183881164 \t\n",
      "Epoch 11250 \t\t Training Loss: 0.0005809221183881164 \t\n",
      "Epoch 11251 \t\t Training Loss: 0.0005809221183881164 \t\n",
      "Epoch 11252 \t\t Training Loss: 0.0005809221183881164 \t\n",
      "Epoch 11253 \t\t Training Loss: 0.0005809221183881164 \t\n",
      "Epoch 11254 \t\t Training Loss: 0.0005809221183881164 \t\n",
      "Epoch 11255 \t\t Training Loss: 0.0005809221183881164 \t\n",
      "Epoch 11256 \t\t Training Loss: 0.0005809220601804554 \t\n",
      "Epoch 11257 \t\t Training Loss: 0.0005809220601804554 \t\n",
      "Epoch 11258 \t\t Training Loss: 0.0005809220019727945 \t\n",
      "Epoch 11259 \t\t Training Loss: 0.0005809220601804554 \t\n",
      "Epoch 11260 \t\t Training Loss: 0.0005809220019727945 \t\n",
      "Epoch 11261 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11262 \t\t Training Loss: 0.0005809220601804554 \t\n",
      "Epoch 11263 \t\t Training Loss: 0.0005809220601804554 \t\n",
      "Epoch 11264 \t\t Training Loss: 0.0005809220019727945 \t\n",
      "Epoch 11265 \t\t Training Loss: 0.0005809220019727945 \t\n",
      "Epoch 11266 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11267 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11268 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11269 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11270 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11271 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11272 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11273 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11274 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11275 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11276 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11277 \t\t Training Loss: 0.0005809218855574727 \t\n",
      "Epoch 11278 \t\t Training Loss: 0.0005809218855574727 \t\n",
      "Epoch 11279 \t\t Training Loss: 0.0005809218855574727 \t\n",
      "Epoch 11280 \t\t Training Loss: 0.0005809218855574727 \t\n",
      "Epoch 11281 \t\t Training Loss: 0.0005809218855574727 \t\n",
      "Epoch 11282 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11283 \t\t Training Loss: 0.0005809218855574727 \t\n",
      "Epoch 11284 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11285 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11286 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11287 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11288 \t\t Training Loss: 0.0005809219437651336 \t\n",
      "Epoch 11289 \t\t Training Loss: 0.0005809218855574727 \t\n",
      "Epoch 11290 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11291 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11292 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11293 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11294 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11295 \t\t Training Loss: 0.0005809218855574727 \t\n",
      "Epoch 11296 \t\t Training Loss: 0.0005809218855574727 \t\n",
      "Epoch 11297 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11298 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11299 \t\t Training Loss: 0.0005809218855574727 \t\n",
      "Epoch 11300 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11301 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11302 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11303 \t\t Training Loss: 0.0005809218855574727 \t\n",
      "Epoch 11304 \t\t Training Loss: 0.0005809218855574727 \t\n",
      "Epoch 11305 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11306 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11307 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11308 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11309 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11310 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11311 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11312 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11313 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11314 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11315 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11316 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11317 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11318 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11319 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11320 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11321 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11322 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11323 \t\t Training Loss: 0.0005809218273498118 \t\n",
      "Epoch 11324 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11325 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11326 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11327 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11328 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11329 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11330 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11331 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11332 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11333 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11334 \t\t Training Loss: 0.000580921652726829 \t\n",
      "Epoch 11335 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11336 \t\t Training Loss: 0.0005809217691421509 \t\n",
      "Epoch 11337 \t\t Training Loss: 0.000580921652726829 \t\n",
      "Epoch 11338 \t\t Training Loss: 0.000580921652726829 \t\n",
      "Epoch 11339 \t\t Training Loss: 0.000580921652726829 \t\n",
      "Epoch 11340 \t\t Training Loss: 0.000580921652726829 \t\n",
      "Epoch 11341 \t\t Training Loss: 0.000580921652726829 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11342 \t\t Training Loss: 0.000580921652726829 \t\n",
      "Epoch 11343 \t\t Training Loss: 0.0005809215945191681 \t\n",
      "Epoch 11344 \t\t Training Loss: 0.0005809215945191681 \t\n",
      "Epoch 11345 \t\t Training Loss: 0.0005809215945191681 \t\n",
      "Epoch 11346 \t\t Training Loss: 0.0005809215945191681 \t\n",
      "Epoch 11347 \t\t Training Loss: 0.0005809215945191681 \t\n",
      "Epoch 11348 \t\t Training Loss: 0.0005809215945191681 \t\n",
      "Epoch 11349 \t\t Training Loss: 0.0005809215945191681 \t\n",
      "Epoch 11350 \t\t Training Loss: 0.0005809215945191681 \t\n",
      "Epoch 11351 \t\t Training Loss: 0.0005809215945191681 \t\n",
      "Epoch 11352 \t\t Training Loss: 0.0005809215363115072 \t\n",
      "Epoch 11353 \t\t Training Loss: 0.0005809215363115072 \t\n",
      "Epoch 11354 \t\t Training Loss: 0.0005809215363115072 \t\n",
      "Epoch 11355 \t\t Training Loss: 0.0005809215363115072 \t\n",
      "Epoch 11356 \t\t Training Loss: 0.0005809215363115072 \t\n",
      "Epoch 11357 \t\t Training Loss: 0.0005809215363115072 \t\n",
      "Epoch 11358 \t\t Training Loss: 0.0005809215363115072 \t\n",
      "Epoch 11359 \t\t Training Loss: 0.0005809215363115072 \t\n",
      "Epoch 11360 \t\t Training Loss: 0.0005809215363115072 \t\n",
      "Epoch 11361 \t\t Training Loss: 0.0005809214781038463 \t\n",
      "Epoch 11362 \t\t Training Loss: 0.0005809214781038463 \t\n",
      "Epoch 11363 \t\t Training Loss: 0.0005809214781038463 \t\n",
      "Epoch 11364 \t\t Training Loss: 0.0005809214781038463 \t\n",
      "Epoch 11365 \t\t Training Loss: 0.0005809214781038463 \t\n",
      "Epoch 11366 \t\t Training Loss: 0.0005809214781038463 \t\n",
      "Epoch 11367 \t\t Training Loss: 0.0005809214781038463 \t\n",
      "Epoch 11368 \t\t Training Loss: 0.0005809214781038463 \t\n",
      "Epoch 11369 \t\t Training Loss: 0.0005809214781038463 \t\n",
      "Epoch 11370 \t\t Training Loss: 0.0005809214781038463 \t\n",
      "Epoch 11371 \t\t Training Loss: 0.0005809214781038463 \t\n",
      "Epoch 11372 \t\t Training Loss: 0.0005809214781038463 \t\n",
      "Epoch 11373 \t\t Training Loss: 0.0005809214781038463 \t\n",
      "Epoch 11374 \t\t Training Loss: 0.0005809214781038463 \t\n",
      "Epoch 11375 \t\t Training Loss: 0.0005809213616885245 \t\n",
      "Epoch 11376 \t\t Training Loss: 0.0005809213616885245 \t\n",
      "Epoch 11377 \t\t Training Loss: 0.0005809213616885245 \t\n",
      "Epoch 11378 \t\t Training Loss: 0.0005809213616885245 \t\n",
      "Epoch 11379 \t\t Training Loss: 0.0005809213616885245 \t\n",
      "Epoch 11380 \t\t Training Loss: 0.0005809213616885245 \t\n",
      "Epoch 11381 \t\t Training Loss: 0.0005809213034808636 \t\n",
      "Epoch 11382 \t\t Training Loss: 0.0005809213034808636 \t\n",
      "Epoch 11383 \t\t Training Loss: 0.0005809213034808636 \t\n",
      "Epoch 11384 \t\t Training Loss: 0.0005809213034808636 \t\n",
      "Epoch 11385 \t\t Training Loss: 0.0005809212452732027 \t\n",
      "Epoch 11386 \t\t Training Loss: 0.0005809212452732027 \t\n",
      "Epoch 11387 \t\t Training Loss: 0.0005809213034808636 \t\n",
      "Epoch 11388 \t\t Training Loss: 0.0005809213034808636 \t\n",
      "Epoch 11389 \t\t Training Loss: 0.0005809212452732027 \t\n",
      "Epoch 11390 \t\t Training Loss: 0.0005809213034808636 \t\n",
      "Epoch 11391 \t\t Training Loss: 0.0005809212452732027 \t\n",
      "Epoch 11392 \t\t Training Loss: 0.0005809212452732027 \t\n",
      "Epoch 11393 \t\t Training Loss: 0.0005809212452732027 \t\n",
      "Epoch 11394 \t\t Training Loss: 0.0005809212452732027 \t\n",
      "Epoch 11395 \t\t Training Loss: 0.0005809212452732027 \t\n",
      "Epoch 11396 \t\t Training Loss: 0.0005809211870655417 \t\n",
      "Epoch 11397 \t\t Training Loss: 0.0005809211870655417 \t\n",
      "Epoch 11398 \t\t Training Loss: 0.0005809211870655417 \t\n",
      "Epoch 11399 \t\t Training Loss: 0.0005809211870655417 \t\n",
      "Epoch 11400 \t\t Training Loss: 0.0005809212452732027 \t\n",
      "Epoch 11401 \t\t Training Loss: 0.0005809212452732027 \t\n",
      "Epoch 11402 \t\t Training Loss: 0.0005809212452732027 \t\n",
      "Epoch 11403 \t\t Training Loss: 0.0005809212452732027 \t\n",
      "Epoch 11404 \t\t Training Loss: 0.0005809211870655417 \t\n",
      "Epoch 11405 \t\t Training Loss: 0.0005809211870655417 \t\n",
      "Epoch 11406 \t\t Training Loss: 0.0005809211870655417 \t\n",
      "Epoch 11407 \t\t Training Loss: 0.0005809211870655417 \t\n",
      "Epoch 11408 \t\t Training Loss: 0.0005809211870655417 \t\n",
      "Epoch 11409 \t\t Training Loss: 0.0005809211870655417 \t\n",
      "Epoch 11410 \t\t Training Loss: 0.0005809211870655417 \t\n",
      "Epoch 11411 \t\t Training Loss: 0.0005809211870655417 \t\n",
      "Epoch 11412 \t\t Training Loss: 0.0005809211288578808 \t\n",
      "Epoch 11413 \t\t Training Loss: 0.0005809211870655417 \t\n",
      "Epoch 11414 \t\t Training Loss: 0.0005809211870655417 \t\n",
      "Epoch 11415 \t\t Training Loss: 0.0005809211870655417 \t\n",
      "Epoch 11416 \t\t Training Loss: 0.0005809211288578808 \t\n",
      "Epoch 11417 \t\t Training Loss: 0.0005809211288578808 \t\n",
      "Epoch 11418 \t\t Training Loss: 0.0005809211288578808 \t\n",
      "Epoch 11419 \t\t Training Loss: 0.0005809211288578808 \t\n",
      "Epoch 11420 \t\t Training Loss: 0.0005809211288578808 \t\n",
      "Epoch 11421 \t\t Training Loss: 0.0005809211288578808 \t\n",
      "Epoch 11422 \t\t Training Loss: 0.0005809211288578808 \t\n",
      "Epoch 11423 \t\t Training Loss: 0.0005809211288578808 \t\n",
      "Epoch 11424 \t\t Training Loss: 0.0005809211288578808 \t\n",
      "Epoch 11425 \t\t Training Loss: 0.0005809210706502199 \t\n",
      "Epoch 11426 \t\t Training Loss: 0.0005809210706502199 \t\n",
      "Epoch 11427 \t\t Training Loss: 0.000580921012442559 \t\n",
      "Epoch 11428 \t\t Training Loss: 0.000580921012442559 \t\n",
      "Epoch 11429 \t\t Training Loss: 0.000580921012442559 \t\n",
      "Epoch 11430 \t\t Training Loss: 0.000580921012442559 \t\n",
      "Epoch 11431 \t\t Training Loss: 0.000580921012442559 \t\n",
      "Epoch 11432 \t\t Training Loss: 0.000580921012442559 \t\n",
      "Epoch 11433 \t\t Training Loss: 0.0005809210706502199 \t\n",
      "Epoch 11434 \t\t Training Loss: 0.000580921012442559 \t\n",
      "Epoch 11435 \t\t Training Loss: 0.000580921012442559 \t\n",
      "Epoch 11436 \t\t Training Loss: 0.000580921012442559 \t\n",
      "Epoch 11437 \t\t Training Loss: 0.000580921012442559 \t\n",
      "Epoch 11438 \t\t Training Loss: 0.000580921012442559 \t\n",
      "Epoch 11439 \t\t Training Loss: 0.000580921012442559 \t\n",
      "Epoch 11440 \t\t Training Loss: 0.000580921012442559 \t\n",
      "Epoch 11441 \t\t Training Loss: 0.0005809209542348981 \t\n",
      "Epoch 11442 \t\t Training Loss: 0.0005809209542348981 \t\n",
      "Epoch 11443 \t\t Training Loss: 0.0005809209542348981 \t\n",
      "Epoch 11444 \t\t Training Loss: 0.0005809209542348981 \t\n",
      "Epoch 11445 \t\t Training Loss: 0.0005809209542348981 \t\n",
      "Epoch 11446 \t\t Training Loss: 0.0005809209542348981 \t\n",
      "Epoch 11447 \t\t Training Loss: 0.0005809209542348981 \t\n",
      "Epoch 11448 \t\t Training Loss: 0.0005809209542348981 \t\n",
      "Epoch 11449 \t\t Training Loss: 0.0005809209542348981 \t\n",
      "Epoch 11450 \t\t Training Loss: 0.0005809209542348981 \t\n",
      "Epoch 11451 \t\t Training Loss: 0.0005809209542348981 \t\n",
      "Epoch 11452 \t\t Training Loss: 0.0005809209542348981 \t\n",
      "Epoch 11453 \t\t Training Loss: 0.0005809208960272372 \t\n",
      "Epoch 11454 \t\t Training Loss: 0.0005809208960272372 \t\n",
      "Epoch 11455 \t\t Training Loss: 0.0005809208960272372 \t\n",
      "Epoch 11456 \t\t Training Loss: 0.0005809208960272372 \t\n",
      "Epoch 11457 \t\t Training Loss: 0.0005809207796119153 \t\n",
      "Epoch 11458 \t\t Training Loss: 0.0005809207796119153 \t\n",
      "Epoch 11459 \t\t Training Loss: 0.0005809207796119153 \t\n",
      "Epoch 11460 \t\t Training Loss: 0.0005809207796119153 \t\n",
      "Epoch 11461 \t\t Training Loss: 0.0005809207796119153 \t\n",
      "Epoch 11462 \t\t Training Loss: 0.0005809207796119153 \t\n",
      "Epoch 11463 \t\t Training Loss: 0.0005809207796119153 \t\n",
      "Epoch 11464 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11465 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11466 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11467 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11468 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11469 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11470 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11471 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11472 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11473 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11474 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11475 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11476 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11477 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11478 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11479 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11480 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11481 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11482 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11483 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11484 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11485 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11486 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11487 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11488 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11489 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11490 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11491 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11492 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11493 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11494 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11495 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11496 \t\t Training Loss: 0.0005809207214042544 \t\n",
      "Epoch 11497 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11498 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11499 \t\t Training Loss: 0.0005809206631965935 \t\n",
      "Epoch 11500 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11501 \t\t Training Loss: 0.0005809206049889326 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11502 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11503 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11504 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11505 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11506 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11507 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11508 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11509 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11510 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11511 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11512 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11513 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11514 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11515 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11516 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11517 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11518 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11519 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11520 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11521 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11522 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11523 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11524 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11525 \t\t Training Loss: 0.0005809206049889326 \t\n",
      "Epoch 11526 \t\t Training Loss: 0.0005809204885736108 \t\n",
      "Epoch 11527 \t\t Training Loss: 0.0005809204885736108 \t\n",
      "Epoch 11528 \t\t Training Loss: 0.0005809204885736108 \t\n",
      "Epoch 11529 \t\t Training Loss: 0.0005809204885736108 \t\n",
      "Epoch 11530 \t\t Training Loss: 0.0005809204885736108 \t\n",
      "Epoch 11531 \t\t Training Loss: 0.0005809204885736108 \t\n",
      "Epoch 11532 \t\t Training Loss: 0.0005809204885736108 \t\n",
      "Epoch 11533 \t\t Training Loss: 0.0005809204885736108 \t\n",
      "Epoch 11534 \t\t Training Loss: 0.0005809204885736108 \t\n",
      "Epoch 11535 \t\t Training Loss: 0.0005809204885736108 \t\n",
      "Epoch 11536 \t\t Training Loss: 0.000580920372158289 \t\n",
      "Epoch 11537 \t\t Training Loss: 0.000580920372158289 \t\n",
      "Epoch 11538 \t\t Training Loss: 0.000580920372158289 \t\n",
      "Epoch 11539 \t\t Training Loss: 0.000580920372158289 \t\n",
      "Epoch 11540 \t\t Training Loss: 0.000580920372158289 \t\n",
      "Epoch 11541 \t\t Training Loss: 0.000580920372158289 \t\n",
      "Epoch 11542 \t\t Training Loss: 0.000580920372158289 \t\n",
      "Epoch 11543 \t\t Training Loss: 0.000580920372158289 \t\n",
      "Epoch 11544 \t\t Training Loss: 0.000580920372158289 \t\n",
      "Epoch 11545 \t\t Training Loss: 0.000580920372158289 \t\n",
      "Epoch 11546 \t\t Training Loss: 0.000580920372158289 \t\n",
      "Epoch 11547 \t\t Training Loss: 0.000580920372158289 \t\n",
      "Epoch 11548 \t\t Training Loss: 0.000580920372158289 \t\n",
      "Epoch 11549 \t\t Training Loss: 0.000580920313950628 \t\n",
      "Epoch 11550 \t\t Training Loss: 0.000580920313950628 \t\n",
      "Epoch 11551 \t\t Training Loss: 0.000580920313950628 \t\n",
      "Epoch 11552 \t\t Training Loss: 0.000580920313950628 \t\n",
      "Epoch 11553 \t\t Training Loss: 0.000580920313950628 \t\n",
      "Epoch 11554 \t\t Training Loss: 0.000580920313950628 \t\n",
      "Epoch 11555 \t\t Training Loss: 0.0005809202557429671 \t\n",
      "Epoch 11556 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11557 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11558 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11559 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11560 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11561 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11562 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11563 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11564 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11565 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11566 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11567 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11568 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11569 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11570 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11571 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11572 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11573 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11574 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11575 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11576 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11577 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11578 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11579 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11580 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11581 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11582 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11583 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11584 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11585 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11586 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11587 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11588 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11589 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11590 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11591 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11592 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11593 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11594 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11595 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11596 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11597 \t\t Training Loss: 0.0005809201975353062 \t\n",
      "Epoch 11598 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11599 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11600 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11601 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11602 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11603 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11604 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11605 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11606 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11607 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11608 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11609 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11610 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11611 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11612 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11613 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11614 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11615 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11616 \t\t Training Loss: 0.0005809200811199844 \t\n",
      "Epoch 11617 \t\t Training Loss: 0.0005809199647046626 \t\n",
      "Epoch 11618 \t\t Training Loss: 0.0005809199647046626 \t\n",
      "Epoch 11619 \t\t Training Loss: 0.0005809199647046626 \t\n",
      "Epoch 11620 \t\t Training Loss: 0.0005809199647046626 \t\n",
      "Epoch 11621 \t\t Training Loss: 0.0005809199647046626 \t\n",
      "Epoch 11622 \t\t Training Loss: 0.0005809199647046626 \t\n",
      "Epoch 11623 \t\t Training Loss: 0.0005809199647046626 \t\n",
      "Epoch 11624 \t\t Training Loss: 0.0005809199647046626 \t\n",
      "Epoch 11625 \t\t Training Loss: 0.0005809199647046626 \t\n",
      "Epoch 11626 \t\t Training Loss: 0.0005809199647046626 \t\n",
      "Epoch 11627 \t\t Training Loss: 0.0005809199647046626 \t\n",
      "Epoch 11628 \t\t Training Loss: 0.0005809199647046626 \t\n",
      "Epoch 11629 \t\t Training Loss: 0.0005809199647046626 \t\n",
      "Epoch 11630 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11631 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11632 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11633 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11634 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11635 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11636 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11637 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11638 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11639 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11640 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11641 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11642 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11643 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11644 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11645 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11646 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11647 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11648 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11649 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11650 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11651 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11652 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11653 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11654 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11655 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11656 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11657 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11658 \t\t Training Loss: 0.0005809199064970016 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11659 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11660 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11661 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11662 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11663 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11664 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11665 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11666 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11667 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11668 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11669 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11670 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11671 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11672 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11673 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11674 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11675 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11676 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11677 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11678 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11679 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11680 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11681 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11682 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11683 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11684 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11685 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11686 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11687 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11688 \t\t Training Loss: 0.0005809199064970016 \t\n",
      "Epoch 11689 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11690 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11691 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11692 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11693 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11694 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11695 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11696 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11697 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11698 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11699 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11700 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11701 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11702 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11703 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11704 \t\t Training Loss: 0.0005809197318740189 \t\n",
      "Epoch 11705 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11706 \t\t Training Loss: 0.0005809197900816798 \t\n",
      "Epoch 11707 \t\t Training Loss: 0.0005809197318740189 \t\n",
      "Epoch 11708 \t\t Training Loss: 0.0005809197318740189 \t\n",
      "Epoch 11709 \t\t Training Loss: 0.0005809197318740189 \t\n",
      "Epoch 11710 \t\t Training Loss: 0.000580919673666358 \t\n",
      "Epoch 11711 \t\t Training Loss: 0.0005809197318740189 \t\n",
      "Epoch 11712 \t\t Training Loss: 0.000580919673666358 \t\n",
      "Epoch 11713 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11714 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11715 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11716 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11717 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11718 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11719 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11720 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11721 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11722 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11723 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11724 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11725 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11726 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11727 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11728 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11729 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11730 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11731 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11732 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11733 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11734 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11735 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11736 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11737 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11738 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11739 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11740 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11741 \t\t Training Loss: 0.0005809196154586971 \t\n",
      "Epoch 11742 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11743 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11744 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11745 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11746 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11747 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11748 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11749 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11750 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11751 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11752 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11753 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11754 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11755 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11756 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11757 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11758 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11759 \t\t Training Loss: 0.0005809194990433753 \t\n",
      "Epoch 11760 \t\t Training Loss: 0.0005809193826280534 \t\n",
      "Epoch 11761 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11762 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11763 \t\t Training Loss: 0.0005809193826280534 \t\n",
      "Epoch 11764 \t\t Training Loss: 0.0005809193826280534 \t\n",
      "Epoch 11765 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11766 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11767 \t\t Training Loss: 0.0005809193826280534 \t\n",
      "Epoch 11768 \t\t Training Loss: 0.0005809193826280534 \t\n",
      "Epoch 11769 \t\t Training Loss: 0.0005809193826280534 \t\n",
      "Epoch 11770 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11771 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11772 \t\t Training Loss: 0.0005809193826280534 \t\n",
      "Epoch 11773 \t\t Training Loss: 0.0005809193826280534 \t\n",
      "Epoch 11774 \t\t Training Loss: 0.0005809193826280534 \t\n",
      "Epoch 11775 \t\t Training Loss: 0.0005809193826280534 \t\n",
      "Epoch 11776 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11777 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11778 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11779 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11780 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11781 \t\t Training Loss: 0.0005809193826280534 \t\n",
      "Epoch 11782 \t\t Training Loss: 0.0005809193826280534 \t\n",
      "Epoch 11783 \t\t Training Loss: 0.0005809193826280534 \t\n",
      "Epoch 11784 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11785 \t\t Training Loss: 0.0005809193826280534 \t\n",
      "Epoch 11786 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11787 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11788 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11789 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11790 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11791 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11792 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11793 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11794 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11795 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11796 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11797 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11798 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11799 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11800 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11801 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11802 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11803 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11804 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11805 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11806 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11807 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11808 \t\t Training Loss: 0.0005809193244203925 \t\n",
      "Epoch 11809 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11810 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11811 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11812 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11813 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11814 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11815 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11816 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11817 \t\t Training Loss: 0.0005809192080050707 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11818 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11819 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11820 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11821 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11822 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11823 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11824 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11825 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11826 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11827 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11828 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11829 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11830 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11831 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11832 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11833 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11834 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11835 \t\t Training Loss: 0.0005809192080050707 \t\n",
      "Epoch 11836 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11837 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11838 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11839 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11840 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11841 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11842 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11843 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11844 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11845 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11846 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11847 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11848 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11849 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11850 \t\t Training Loss: 0.000580919033382088 \t\n",
      "Epoch 11851 \t\t Training Loss: 0.0005809190915897489 \t\n",
      "Epoch 11852 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11853 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11854 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11855 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11856 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11857 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11858 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11859 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11860 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11861 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11862 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11863 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11864 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11865 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11866 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11867 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11868 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11869 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11870 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11871 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11872 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11873 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11874 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11875 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11876 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11877 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11878 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11879 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11880 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11881 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11882 \t\t Training Loss: 0.0005809189169667661 \t\n",
      "Epoch 11883 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11884 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11885 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11886 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11887 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11888 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11889 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11890 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11891 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11892 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11893 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11894 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11895 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11896 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11897 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11898 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11899 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11900 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11901 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11902 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11903 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11904 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11905 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11906 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11907 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11908 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11909 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11910 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11911 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11912 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11913 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11914 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11915 \t\t Training Loss: 0.0005809188005514443 \t\n",
      "Epoch 11916 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11917 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11918 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11919 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11920 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11921 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11922 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11923 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11924 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11925 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11926 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11927 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11928 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11929 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11930 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11931 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11932 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11933 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11934 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11935 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11936 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11937 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11938 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11939 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11940 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11941 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11942 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11943 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11944 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11945 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11946 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11947 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11948 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11949 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11950 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11951 \t\t Training Loss: 0.0005809186841361225 \t\n",
      "Epoch 11952 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11953 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11954 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11955 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11956 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11957 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11958 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11959 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11960 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11961 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11962 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11963 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11964 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11965 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11966 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11967 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11968 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11969 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11970 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11971 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11972 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11973 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11974 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11975 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11976 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11977 \t\t Training Loss: 0.0005809185095131397 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11978 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11979 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11980 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11981 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11982 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11983 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11984 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11985 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11986 \t\t Training Loss: 0.0005809186259284616 \t\n",
      "Epoch 11987 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11988 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11989 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11990 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11991 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11992 \t\t Training Loss: 0.0005809183930978179 \t\n",
      "Epoch 11993 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11994 \t\t Training Loss: 0.0005809183930978179 \t\n",
      "Epoch 11995 \t\t Training Loss: 0.0005809183930978179 \t\n",
      "Epoch 11996 \t\t Training Loss: 0.0005809185095131397 \t\n",
      "Epoch 11997 \t\t Training Loss: 0.0005809183930978179 \t\n",
      "Epoch 11998 \t\t Training Loss: 0.0005809183930978179 \t\n",
      "Epoch 11999 \t\t Training Loss: 0.0005809183930978179 \t\n",
      "Epoch 12000 \t\t Training Loss: 0.0005809183930978179 \t\n",
      "Epoch 12001 \t\t Training Loss: 0.0005809183930978179 \t\n",
      "Epoch 12002 \t\t Training Loss: 0.0005809183930978179 \t\n",
      "Epoch 12003 \t\t Training Loss: 0.0005809183930978179 \t\n",
      "Epoch 12004 \t\t Training Loss: 0.0005809183930978179 \t\n",
      "Epoch 12005 \t\t Training Loss: 0.0005809183930978179 \t\n",
      "Epoch 12006 \t\t Training Loss: 0.0005809183930978179 \t\n",
      "Epoch 12007 \t\t Training Loss: 0.0005809183930978179 \t\n",
      "Epoch 12008 \t\t Training Loss: 0.0005809183930978179 \t\n",
      "Epoch 12009 \t\t Training Loss: 0.0005809183930978179 \t\n",
      "Epoch 12010 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12011 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12012 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12013 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12014 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12015 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12016 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12017 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12018 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12019 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12020 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12021 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12022 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12023 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12024 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12025 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12026 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12027 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12028 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12029 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12030 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12031 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12032 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12033 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12034 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12035 \t\t Training Loss: 0.000580918334890157 \t\n",
      "Epoch 12036 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12037 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12038 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12039 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12040 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12041 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12042 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12043 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12044 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12045 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12046 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12047 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12048 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12049 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12050 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12051 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12052 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12053 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12054 \t\t Training Loss: 0.0005809182184748352 \t\n",
      "Epoch 12055 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12056 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12057 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12058 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12059 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12060 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12061 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12062 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12063 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12064 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12065 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12066 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12067 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12068 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12069 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12070 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12071 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12072 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12073 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12074 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12075 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12076 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12077 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12078 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12079 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12080 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12081 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12082 \t\t Training Loss: 0.0005809179856441915 \t\n",
      "Epoch 12083 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12084 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12085 \t\t Training Loss: 0.0005809180438518524 \t\n",
      "Epoch 12086 \t\t Training Loss: 0.0005809179856441915 \t\n",
      "Epoch 12087 \t\t Training Loss: 0.0005809179856441915 \t\n",
      "Epoch 12088 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12089 \t\t Training Loss: 0.0005809181020595133 \t\n",
      "Epoch 12090 \t\t Training Loss: 0.0005809180438518524 \t\n",
      "Epoch 12091 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12092 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12093 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12094 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12095 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12096 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12097 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12098 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12099 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12100 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12101 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12102 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12103 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12104 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12105 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12106 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12107 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12108 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12109 \t\t Training Loss: 0.0005809179274365306 \t\n",
      "Epoch 12110 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12111 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12112 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12113 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12114 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12115 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12116 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12117 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12118 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12119 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12120 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12121 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12122 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12123 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12124 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12125 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12126 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12127 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12128 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12129 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12130 \t\t Training Loss: 0.0005809177528135478 \t\n",
      "Epoch 12131 \t\t Training Loss: 0.0005809177528135478 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12132 \t\t Training Loss: 0.0005809178110212088 \t\n",
      "Epoch 12133 \t\t Training Loss: 0.0005809177528135478 \t\n",
      "Epoch 12134 \t\t Training Loss: 0.0005809177528135478 \t\n",
      "Epoch 12135 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12136 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12137 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12138 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12139 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12140 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12141 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12142 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12143 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12144 \t\t Training Loss: 0.000580917636398226 \t\n",
      "Epoch 12145 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12146 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12147 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12148 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12149 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12150 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12151 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12152 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12153 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12154 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12155 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12156 \t\t Training Loss: 0.000580917636398226 \t\n",
      "Epoch 12157 \t\t Training Loss: 0.000580917636398226 \t\n",
      "Epoch 12158 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12159 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12160 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12161 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12162 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12163 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12164 \t\t Training Loss: 0.0005809176946058869 \t\n",
      "Epoch 12165 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12166 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12167 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12168 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12169 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12170 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12171 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12172 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12173 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12174 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12175 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12176 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12177 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12178 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12179 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12180 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12181 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12182 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12183 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12184 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12185 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12186 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12187 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12188 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12189 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12190 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12191 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12192 \t\t Training Loss: 0.0005809175199829042 \t\n",
      "Epoch 12193 \t\t Training Loss: 0.0005809174617752433 \t\n",
      "Epoch 12194 \t\t Training Loss: 0.0005809174617752433 \t\n",
      "Epoch 12195 \t\t Training Loss: 0.0005809174617752433 \t\n",
      "Epoch 12196 \t\t Training Loss: 0.0005809174617752433 \t\n",
      "Epoch 12197 \t\t Training Loss: 0.0005809174617752433 \t\n",
      "Epoch 12198 \t\t Training Loss: 0.0005809174617752433 \t\n",
      "Epoch 12199 \t\t Training Loss: 0.0005809174617752433 \t\n",
      "Epoch 12200 \t\t Training Loss: 0.0005809174617752433 \t\n",
      "Epoch 12201 \t\t Training Loss: 0.0005809174617752433 \t\n",
      "Epoch 12202 \t\t Training Loss: 0.0005809174617752433 \t\n",
      "Epoch 12203 \t\t Training Loss: 0.0005809174617752433 \t\n",
      "Epoch 12204 \t\t Training Loss: 0.0005809174617752433 \t\n",
      "Epoch 12205 \t\t Training Loss: 0.0005809174617752433 \t\n",
      "Epoch 12206 \t\t Training Loss: 0.0005809174617752433 \t\n",
      "Epoch 12207 \t\t Training Loss: 0.0005809173453599215 \t\n",
      "Epoch 12208 \t\t Training Loss: 0.0005809173453599215 \t\n",
      "Epoch 12209 \t\t Training Loss: 0.0005809173453599215 \t\n",
      "Epoch 12210 \t\t Training Loss: 0.0005809173453599215 \t\n",
      "Epoch 12211 \t\t Training Loss: 0.0005809173453599215 \t\n",
      "Epoch 12212 \t\t Training Loss: 0.0005809173453599215 \t\n",
      "Epoch 12213 \t\t Training Loss: 0.0005809172871522605 \t\n",
      "Epoch 12214 \t\t Training Loss: 0.0005809172871522605 \t\n",
      "Epoch 12215 \t\t Training Loss: 0.0005809173453599215 \t\n",
      "Epoch 12216 \t\t Training Loss: 0.0005809172871522605 \t\n",
      "Epoch 12217 \t\t Training Loss: 0.0005809173453599215 \t\n",
      "Epoch 12218 \t\t Training Loss: 0.0005809172871522605 \t\n",
      "Epoch 12219 \t\t Training Loss: 0.0005809172871522605 \t\n",
      "Epoch 12220 \t\t Training Loss: 0.0005809172871522605 \t\n",
      "Epoch 12221 \t\t Training Loss: 0.0005809172871522605 \t\n",
      "Epoch 12222 \t\t Training Loss: 0.0005809172871522605 \t\n",
      "Epoch 12223 \t\t Training Loss: 0.0005809172871522605 \t\n",
      "Epoch 12224 \t\t Training Loss: 0.0005809172871522605 \t\n",
      "Epoch 12225 \t\t Training Loss: 0.0005809172871522605 \t\n",
      "Epoch 12226 \t\t Training Loss: 0.0005809172871522605 \t\n",
      "Epoch 12227 \t\t Training Loss: 0.0005809172871522605 \t\n",
      "Epoch 12228 \t\t Training Loss: 0.0005809172871522605 \t\n",
      "Epoch 12229 \t\t Training Loss: 0.0005809172289445996 \t\n",
      "Epoch 12230 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12231 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12232 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12233 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12234 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12235 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12236 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12237 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12238 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12239 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12240 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12241 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12242 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12243 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12244 \t\t Training Loss: 0.0005809172289445996 \t\n",
      "Epoch 12245 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12246 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12247 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12248 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12249 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12250 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12251 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12252 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12253 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12254 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12255 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12256 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12257 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12258 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12259 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12260 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12261 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12262 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12263 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12264 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12265 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12266 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12267 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12268 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12269 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12270 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12271 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12272 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12273 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12274 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12275 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12276 \t\t Training Loss: 0.0005809171125292778 \t\n",
      "Epoch 12277 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12278 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12279 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12280 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12281 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12282 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12283 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12284 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12285 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12286 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12287 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12288 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12289 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12290 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12291 \t\t Training Loss: 0.0005809170543216169 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12292 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12293 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12294 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12295 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12296 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12297 \t\t Training Loss: 0.0005809170543216169 \t\n",
      "Epoch 12298 \t\t Training Loss: 0.000580916996113956 \t\n",
      "Epoch 12299 \t\t Training Loss: 0.0005809169379062951 \t\n",
      "Epoch 12300 \t\t Training Loss: 0.000580916996113956 \t\n",
      "Epoch 12301 \t\t Training Loss: 0.000580916996113956 \t\n",
      "Epoch 12302 \t\t Training Loss: 0.0005809169379062951 \t\n",
      "Epoch 12303 \t\t Training Loss: 0.0005809169379062951 \t\n",
      "Epoch 12304 \t\t Training Loss: 0.0005809169379062951 \t\n",
      "Epoch 12305 \t\t Training Loss: 0.0005809169379062951 \t\n",
      "Epoch 12306 \t\t Training Loss: 0.0005809169379062951 \t\n",
      "Epoch 12307 \t\t Training Loss: 0.0005809169379062951 \t\n",
      "Epoch 12308 \t\t Training Loss: 0.0005809169379062951 \t\n",
      "Epoch 12309 \t\t Training Loss: 0.0005809169379062951 \t\n",
      "Epoch 12310 \t\t Training Loss: 0.0005809169379062951 \t\n",
      "Epoch 12311 \t\t Training Loss: 0.0005809168796986341 \t\n",
      "Epoch 12312 \t\t Training Loss: 0.0005809168796986341 \t\n",
      "Epoch 12313 \t\t Training Loss: 0.0005809168796986341 \t\n",
      "Epoch 12314 \t\t Training Loss: 0.0005809168796986341 \t\n",
      "Epoch 12315 \t\t Training Loss: 0.0005809167632833123 \t\n",
      "Epoch 12316 \t\t Training Loss: 0.0005809168796986341 \t\n",
      "Epoch 12317 \t\t Training Loss: 0.0005809167632833123 \t\n",
      "Epoch 12318 \t\t Training Loss: 0.0005809168796986341 \t\n",
      "Epoch 12319 \t\t Training Loss: 0.0005809167632833123 \t\n",
      "Epoch 12320 \t\t Training Loss: 0.0005809167632833123 \t\n",
      "Epoch 12321 \t\t Training Loss: 0.0005809167632833123 \t\n",
      "Epoch 12322 \t\t Training Loss: 0.0005809167632833123 \t\n",
      "Epoch 12323 \t\t Training Loss: 0.0005809167632833123 \t\n",
      "Epoch 12324 \t\t Training Loss: 0.0005809167632833123 \t\n",
      "Epoch 12325 \t\t Training Loss: 0.0005809167632833123 \t\n",
      "Epoch 12326 \t\t Training Loss: 0.0005809167632833123 \t\n",
      "Epoch 12327 \t\t Training Loss: 0.0005809167632833123 \t\n",
      "Epoch 12328 \t\t Training Loss: 0.0005809167632833123 \t\n",
      "Epoch 12329 \t\t Training Loss: 0.0005809167632833123 \t\n",
      "Epoch 12330 \t\t Training Loss: 0.0005809167050756514 \t\n",
      "Epoch 12331 \t\t Training Loss: 0.0005809167632833123 \t\n",
      "Epoch 12332 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12333 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12334 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12335 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12336 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12337 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12338 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12339 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12340 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12341 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12342 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12343 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12344 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12345 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12346 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12347 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12348 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12349 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12350 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12351 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12352 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12353 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12354 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12355 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12356 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12357 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12358 \t\t Training Loss: 0.0005809166468679905 \t\n",
      "Epoch 12359 \t\t Training Loss: 0.0005809165886603296 \t\n",
      "Epoch 12360 \t\t Training Loss: 0.0005809165886603296 \t\n",
      "Epoch 12361 \t\t Training Loss: 0.0005809165886603296 \t\n",
      "Epoch 12362 \t\t Training Loss: 0.0005809165886603296 \t\n",
      "Epoch 12363 \t\t Training Loss: 0.0005809165886603296 \t\n",
      "Epoch 12364 \t\t Training Loss: 0.0005809165304526687 \t\n",
      "Epoch 12365 \t\t Training Loss: 0.0005809165304526687 \t\n",
      "Epoch 12366 \t\t Training Loss: 0.0005809165304526687 \t\n",
      "Epoch 12367 \t\t Training Loss: 0.0005809165304526687 \t\n",
      "Epoch 12368 \t\t Training Loss: 0.0005809165304526687 \t\n",
      "Epoch 12369 \t\t Training Loss: 0.0005809165304526687 \t\n",
      "Epoch 12370 \t\t Training Loss: 0.0005809165304526687 \t\n",
      "Epoch 12371 \t\t Training Loss: 0.0005809165304526687 \t\n",
      "Epoch 12372 \t\t Training Loss: 0.0005809165304526687 \t\n",
      "Epoch 12373 \t\t Training Loss: 0.0005809164722450078 \t\n",
      "Epoch 12374 \t\t Training Loss: 0.0005809165304526687 \t\n",
      "Epoch 12375 \t\t Training Loss: 0.0005809165304526687 \t\n",
      "Epoch 12376 \t\t Training Loss: 0.0005809165304526687 \t\n",
      "Epoch 12377 \t\t Training Loss: 0.0005809164722450078 \t\n",
      "Epoch 12378 \t\t Training Loss: 0.0005809164722450078 \t\n",
      "Epoch 12379 \t\t Training Loss: 0.0005809164722450078 \t\n",
      "Epoch 12380 \t\t Training Loss: 0.0005809164722450078 \t\n",
      "Epoch 12381 \t\t Training Loss: 0.0005809164722450078 \t\n",
      "Epoch 12382 \t\t Training Loss: 0.0005809164722450078 \t\n",
      "Epoch 12383 \t\t Training Loss: 0.0005809164140373468 \t\n",
      "Epoch 12384 \t\t Training Loss: 0.0005809164140373468 \t\n",
      "Epoch 12385 \t\t Training Loss: 0.0005809164140373468 \t\n",
      "Epoch 12386 \t\t Training Loss: 0.0005809164140373468 \t\n",
      "Epoch 12387 \t\t Training Loss: 0.0005809164140373468 \t\n",
      "Epoch 12388 \t\t Training Loss: 0.0005809164140373468 \t\n",
      "Epoch 12389 \t\t Training Loss: 0.0005809164140373468 \t\n",
      "Epoch 12390 \t\t Training Loss: 0.0005809164140373468 \t\n",
      "Epoch 12391 \t\t Training Loss: 0.0005809164140373468 \t\n",
      "Epoch 12392 \t\t Training Loss: 0.000580916297622025 \t\n",
      "Epoch 12393 \t\t Training Loss: 0.000580916297622025 \t\n",
      "Epoch 12394 \t\t Training Loss: 0.000580916297622025 \t\n",
      "Epoch 12395 \t\t Training Loss: 0.000580916297622025 \t\n",
      "Epoch 12396 \t\t Training Loss: 0.000580916297622025 \t\n",
      "Epoch 12397 \t\t Training Loss: 0.000580916297622025 \t\n",
      "Epoch 12398 \t\t Training Loss: 0.000580916297622025 \t\n",
      "Epoch 12399 \t\t Training Loss: 0.0005809161812067032 \t\n",
      "Epoch 12400 \t\t Training Loss: 0.000580916297622025 \t\n",
      "Epoch 12401 \t\t Training Loss: 0.000580916297622025 \t\n",
      "Epoch 12402 \t\t Training Loss: 0.000580916297622025 \t\n",
      "Epoch 12403 \t\t Training Loss: 0.0005809161812067032 \t\n",
      "Epoch 12404 \t\t Training Loss: 0.0005809161812067032 \t\n",
      "Epoch 12405 \t\t Training Loss: 0.0005809161812067032 \t\n",
      "Epoch 12406 \t\t Training Loss: 0.0005809161812067032 \t\n",
      "Epoch 12407 \t\t Training Loss: 0.0005809161812067032 \t\n",
      "Epoch 12408 \t\t Training Loss: 0.0005809161812067032 \t\n",
      "Epoch 12409 \t\t Training Loss: 0.0005809161812067032 \t\n",
      "Epoch 12410 \t\t Training Loss: 0.0005809161812067032 \t\n",
      "Epoch 12411 \t\t Training Loss: 0.0005809161812067032 \t\n",
      "Epoch 12412 \t\t Training Loss: 0.0005809161812067032 \t\n",
      "Epoch 12413 \t\t Training Loss: 0.0005809161229990423 \t\n",
      "Epoch 12414 \t\t Training Loss: 0.0005809161229990423 \t\n",
      "Epoch 12415 \t\t Training Loss: 0.0005809160647913814 \t\n",
      "Epoch 12416 \t\t Training Loss: 0.0005809160647913814 \t\n",
      "Epoch 12417 \t\t Training Loss: 0.0005809160647913814 \t\n",
      "Epoch 12418 \t\t Training Loss: 0.0005809160647913814 \t\n",
      "Epoch 12419 \t\t Training Loss: 0.0005809160065837204 \t\n",
      "Epoch 12420 \t\t Training Loss: 0.0005809160647913814 \t\n",
      "Epoch 12421 \t\t Training Loss: 0.0005809160647913814 \t\n",
      "Epoch 12422 \t\t Training Loss: 0.0005809160647913814 \t\n",
      "Epoch 12423 \t\t Training Loss: 0.0005809160647913814 \t\n",
      "Epoch 12424 \t\t Training Loss: 0.0005809160647913814 \t\n",
      "Epoch 12425 \t\t Training Loss: 0.0005809160647913814 \t\n",
      "Epoch 12426 \t\t Training Loss: 0.0005809160065837204 \t\n",
      "Epoch 12427 \t\t Training Loss: 0.0005809160647913814 \t\n",
      "Epoch 12428 \t\t Training Loss: 0.0005809160065837204 \t\n",
      "Epoch 12429 \t\t Training Loss: 0.0005809160065837204 \t\n",
      "Epoch 12430 \t\t Training Loss: 0.0005809160065837204 \t\n",
      "Epoch 12431 \t\t Training Loss: 0.0005809160065837204 \t\n",
      "Epoch 12432 \t\t Training Loss: 0.0005809159483760595 \t\n",
      "Epoch 12433 \t\t Training Loss: 0.0005809159483760595 \t\n",
      "Epoch 12434 \t\t Training Loss: 0.0005809159483760595 \t\n",
      "Epoch 12435 \t\t Training Loss: 0.0005809159483760595 \t\n",
      "Epoch 12436 \t\t Training Loss: 0.0005809159483760595 \t\n",
      "Epoch 12437 \t\t Training Loss: 0.0005809159483760595 \t\n",
      "Epoch 12438 \t\t Training Loss: 0.0005809159483760595 \t\n",
      "Epoch 12439 \t\t Training Loss: 0.0005809159483760595 \t\n",
      "Epoch 12440 \t\t Training Loss: 0.0005809159483760595 \t\n",
      "Epoch 12441 \t\t Training Loss: 0.0005809159483760595 \t\n",
      "Epoch 12442 \t\t Training Loss: 0.0005809159483760595 \t\n",
      "Epoch 12443 \t\t Training Loss: 0.0005809158901683986 \t\n",
      "Epoch 12444 \t\t Training Loss: 0.0005809159483760595 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12445 \t\t Training Loss: 0.0005809159483760595 \t\n",
      "Epoch 12446 \t\t Training Loss: 0.0005809158901683986 \t\n",
      "Epoch 12447 \t\t Training Loss: 0.0005809158901683986 \t\n",
      "Epoch 12448 \t\t Training Loss: 0.0005809158901683986 \t\n",
      "Epoch 12449 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12450 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12451 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12452 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12453 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12454 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12455 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12456 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12457 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12458 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12459 \t\t Training Loss: 0.0005809158901683986 \t\n",
      "Epoch 12460 \t\t Training Loss: 0.0005809158901683986 \t\n",
      "Epoch 12461 \t\t Training Loss: 0.0005809158901683986 \t\n",
      "Epoch 12462 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12463 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12464 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12465 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12466 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12467 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12468 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12469 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12470 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12471 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12472 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12473 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12474 \t\t Training Loss: 0.0005809158319607377 \t\n",
      "Epoch 12475 \t\t Training Loss: 0.0005809157737530768 \t\n",
      "Epoch 12476 \t\t Training Loss: 0.0005809157155454159 \t\n",
      "Epoch 12477 \t\t Training Loss: 0.0005809157155454159 \t\n",
      "Epoch 12478 \t\t Training Loss: 0.0005809157155454159 \t\n",
      "Epoch 12479 \t\t Training Loss: 0.0005809157155454159 \t\n",
      "Epoch 12480 \t\t Training Loss: 0.0005809157155454159 \t\n",
      "Epoch 12481 \t\t Training Loss: 0.000580915599130094 \t\n",
      "Epoch 12482 \t\t Training Loss: 0.000580915599130094 \t\n",
      "Epoch 12483 \t\t Training Loss: 0.000580915599130094 \t\n",
      "Epoch 12484 \t\t Training Loss: 0.000580915657337755 \t\n",
      "Epoch 12485 \t\t Training Loss: 0.000580915599130094 \t\n",
      "Epoch 12486 \t\t Training Loss: 0.000580915599130094 \t\n",
      "Epoch 12487 \t\t Training Loss: 0.000580915599130094 \t\n",
      "Epoch 12488 \t\t Training Loss: 0.000580915599130094 \t\n",
      "Epoch 12489 \t\t Training Loss: 0.0005809155409224331 \t\n",
      "Epoch 12490 \t\t Training Loss: 0.000580915599130094 \t\n",
      "Epoch 12491 \t\t Training Loss: 0.0005809155409224331 \t\n",
      "Epoch 12492 \t\t Training Loss: 0.000580915599130094 \t\n",
      "Epoch 12493 \t\t Training Loss: 0.0005809155409224331 \t\n",
      "Epoch 12494 \t\t Training Loss: 0.0005809155409224331 \t\n",
      "Epoch 12495 \t\t Training Loss: 0.0005809155409224331 \t\n",
      "Epoch 12496 \t\t Training Loss: 0.0005809155409224331 \t\n",
      "Epoch 12497 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12498 \t\t Training Loss: 0.0005809155409224331 \t\n",
      "Epoch 12499 \t\t Training Loss: 0.0005809155409224331 \t\n",
      "Epoch 12500 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12501 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12502 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12503 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12504 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12505 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12506 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12507 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12508 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12509 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12510 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12511 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12512 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12513 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12514 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12515 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12516 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12517 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12518 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12519 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12520 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12521 \t\t Training Loss: 0.0005809154827147722 \t\n",
      "Epoch 12522 \t\t Training Loss: 0.0005809154245071113 \t\n",
      "Epoch 12523 \t\t Training Loss: 0.0005809153662994504 \t\n",
      "Epoch 12524 \t\t Training Loss: 0.0005809153662994504 \t\n",
      "Epoch 12525 \t\t Training Loss: 0.0005809153662994504 \t\n",
      "Epoch 12526 \t\t Training Loss: 0.0005809153662994504 \t\n",
      "Epoch 12527 \t\t Training Loss: 0.0005809153662994504 \t\n",
      "Epoch 12528 \t\t Training Loss: 0.0005809153662994504 \t\n",
      "Epoch 12529 \t\t Training Loss: 0.0005809153662994504 \t\n",
      "Epoch 12530 \t\t Training Loss: 0.0005809153662994504 \t\n",
      "Epoch 12531 \t\t Training Loss: 0.0005809153662994504 \t\n",
      "Epoch 12532 \t\t Training Loss: 0.0005809153662994504 \t\n",
      "Epoch 12533 \t\t Training Loss: 0.0005809153662994504 \t\n",
      "Epoch 12534 \t\t Training Loss: 0.0005809153662994504 \t\n",
      "Epoch 12535 \t\t Training Loss: 0.0005809153662994504 \t\n",
      "Epoch 12536 \t\t Training Loss: 0.0005809153080917895 \t\n",
      "Epoch 12537 \t\t Training Loss: 0.0005809153080917895 \t\n",
      "Epoch 12538 \t\t Training Loss: 0.0005809153080917895 \t\n",
      "Epoch 12539 \t\t Training Loss: 0.0005809153080917895 \t\n",
      "Epoch 12540 \t\t Training Loss: 0.0005809153080917895 \t\n",
      "Epoch 12541 \t\t Training Loss: 0.0005809153662994504 \t\n",
      "Epoch 12542 \t\t Training Loss: 0.0005809153080917895 \t\n",
      "Epoch 12543 \t\t Training Loss: 0.0005809153080917895 \t\n",
      "Epoch 12544 \t\t Training Loss: 0.0005809153080917895 \t\n",
      "Epoch 12545 \t\t Training Loss: 0.0005809153662994504 \t\n",
      "Epoch 12546 \t\t Training Loss: 0.0005809153080917895 \t\n",
      "Epoch 12547 \t\t Training Loss: 0.0005809152498841286 \t\n",
      "Epoch 12548 \t\t Training Loss: 0.0005809152498841286 \t\n",
      "Epoch 12549 \t\t Training Loss: 0.0005809152498841286 \t\n",
      "Epoch 12550 \t\t Training Loss: 0.0005809152498841286 \t\n",
      "Epoch 12551 \t\t Training Loss: 0.0005809152498841286 \t\n",
      "Epoch 12552 \t\t Training Loss: 0.0005809152498841286 \t\n",
      "Epoch 12553 \t\t Training Loss: 0.0005809152498841286 \t\n",
      "Epoch 12554 \t\t Training Loss: 0.0005809152498841286 \t\n",
      "Epoch 12555 \t\t Training Loss: 0.0005809151334688067 \t\n",
      "Epoch 12556 \t\t Training Loss: 0.0005809151916764677 \t\n",
      "Epoch 12557 \t\t Training Loss: 0.0005809152498841286 \t\n",
      "Epoch 12558 \t\t Training Loss: 0.0005809152498841286 \t\n",
      "Epoch 12559 \t\t Training Loss: 0.0005809151334688067 \t\n",
      "Epoch 12560 \t\t Training Loss: 0.0005809151334688067 \t\n",
      "Epoch 12561 \t\t Training Loss: 0.0005809151334688067 \t\n",
      "Epoch 12562 \t\t Training Loss: 0.0005809151334688067 \t\n",
      "Epoch 12563 \t\t Training Loss: 0.0005809151334688067 \t\n",
      "Epoch 12564 \t\t Training Loss: 0.0005809151334688067 \t\n",
      "Epoch 12565 \t\t Training Loss: 0.0005809151334688067 \t\n",
      "Epoch 12566 \t\t Training Loss: 0.0005809151334688067 \t\n",
      "Epoch 12567 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12568 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12569 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12570 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12571 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12572 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12573 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12574 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12575 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12576 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12577 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12578 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12579 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12580 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12581 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12582 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12583 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12584 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12585 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12586 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12587 \t\t Training Loss: 0.000580914958845824 \t\n",
      "Epoch 12588 \t\t Training Loss: 0.000580914958845824 \t\n",
      "Epoch 12589 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12590 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12591 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12592 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12593 \t\t Training Loss: 0.000580914958845824 \t\n",
      "Epoch 12594 \t\t Training Loss: 0.000580914958845824 \t\n",
      "Epoch 12595 \t\t Training Loss: 0.0005809150752611458 \t\n",
      "Epoch 12596 \t\t Training Loss: 0.000580914958845824 \t\n",
      "Epoch 12597 \t\t Training Loss: 0.000580914958845824 \t\n",
      "Epoch 12598 \t\t Training Loss: 0.000580914958845824 \t\n",
      "Epoch 12599 \t\t Training Loss: 0.000580914958845824 \t\n",
      "Epoch 12600 \t\t Training Loss: 0.000580914958845824 \t\n",
      "Epoch 12601 \t\t Training Loss: 0.000580914958845824 \t\n",
      "Epoch 12602 \t\t Training Loss: 0.000580914958845824 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12603 \t\t Training Loss: 0.000580914958845824 \t\n",
      "Epoch 12604 \t\t Training Loss: 0.0005809149006381631 \t\n",
      "Epoch 12605 \t\t Training Loss: 0.0005809149006381631 \t\n",
      "Epoch 12606 \t\t Training Loss: 0.0005809149006381631 \t\n",
      "Epoch 12607 \t\t Training Loss: 0.0005809149006381631 \t\n",
      "Epoch 12608 \t\t Training Loss: 0.0005809149006381631 \t\n",
      "Epoch 12609 \t\t Training Loss: 0.0005809149006381631 \t\n",
      "Epoch 12610 \t\t Training Loss: 0.0005809149006381631 \t\n",
      "Epoch 12611 \t\t Training Loss: 0.0005809149006381631 \t\n",
      "Epoch 12612 \t\t Training Loss: 0.0005809149006381631 \t\n",
      "Epoch 12613 \t\t Training Loss: 0.0005809149006381631 \t\n",
      "Epoch 12614 \t\t Training Loss: 0.0005809149006381631 \t\n",
      "Epoch 12615 \t\t Training Loss: 0.0005809149006381631 \t\n",
      "Epoch 12616 \t\t Training Loss: 0.0005809149006381631 \t\n",
      "Epoch 12617 \t\t Training Loss: 0.0005809149006381631 \t\n",
      "Epoch 12618 \t\t Training Loss: 0.0005809149006381631 \t\n",
      "Epoch 12619 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12620 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12621 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12622 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12623 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12624 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12625 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12626 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12627 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12628 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12629 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12630 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12631 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12632 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12633 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12634 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12635 \t\t Training Loss: 0.0005809147842228413 \t\n",
      "Epoch 12636 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12637 \t\t Training Loss: 0.0005809148424305022 \t\n",
      "Epoch 12638 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12639 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12640 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12641 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12642 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12643 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12644 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12645 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12646 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12647 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12648 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12649 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12650 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12651 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12652 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12653 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12654 \t\t Training Loss: 0.0005809146678075194 \t\n",
      "Epoch 12655 \t\t Training Loss: 0.0005809146678075194 \t\n",
      "Epoch 12656 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12657 \t\t Training Loss: 0.0005809146678075194 \t\n",
      "Epoch 12658 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12659 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12660 \t\t Training Loss: 0.0005809147260151803 \t\n",
      "Epoch 12661 \t\t Training Loss: 0.0005809146678075194 \t\n",
      "Epoch 12662 \t\t Training Loss: 0.0005809146678075194 \t\n",
      "Epoch 12663 \t\t Training Loss: 0.0005809146678075194 \t\n",
      "Epoch 12664 \t\t Training Loss: 0.0005809145513921976 \t\n",
      "Epoch 12665 \t\t Training Loss: 0.0005809145513921976 \t\n",
      "Epoch 12666 \t\t Training Loss: 0.0005809146678075194 \t\n",
      "Epoch 12667 \t\t Training Loss: 0.0005809145513921976 \t\n",
      "Epoch 12668 \t\t Training Loss: 0.0005809145513921976 \t\n",
      "Epoch 12669 \t\t Training Loss: 0.0005809145513921976 \t\n",
      "Epoch 12670 \t\t Training Loss: 0.0005809145513921976 \t\n",
      "Epoch 12671 \t\t Training Loss: 0.0005809144931845367 \t\n",
      "Epoch 12672 \t\t Training Loss: 0.0005809144931845367 \t\n",
      "Epoch 12673 \t\t Training Loss: 0.0005809144931845367 \t\n",
      "Epoch 12674 \t\t Training Loss: 0.0005809144931845367 \t\n",
      "Epoch 12675 \t\t Training Loss: 0.0005809144931845367 \t\n",
      "Epoch 12676 \t\t Training Loss: 0.0005809144931845367 \t\n",
      "Epoch 12677 \t\t Training Loss: 0.0005809144931845367 \t\n",
      "Epoch 12678 \t\t Training Loss: 0.0005809144931845367 \t\n",
      "Epoch 12679 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12680 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12681 \t\t Training Loss: 0.0005809144931845367 \t\n",
      "Epoch 12682 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12683 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12684 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12685 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12686 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12687 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12688 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12689 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12690 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12691 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12692 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12693 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12694 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12695 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12696 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12697 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12698 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12699 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12700 \t\t Training Loss: 0.0005809144349768758 \t\n",
      "Epoch 12701 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12702 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12703 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12704 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12705 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12706 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12707 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12708 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12709 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12710 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12711 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12712 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12713 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12714 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12715 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12716 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12717 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12718 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12719 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12720 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12721 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12722 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12723 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12724 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12725 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12726 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12727 \t\t Training Loss: 0.0005809143767692149 \t\n",
      "Epoch 12728 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12729 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12730 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12731 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12732 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12733 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12734 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12735 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12736 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12737 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12738 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12739 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12740 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12741 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12742 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12743 \t\t Training Loss: 0.0005809142021462321 \t\n",
      "Epoch 12744 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12745 \t\t Training Loss: 0.000580914260353893 \t\n",
      "Epoch 12746 \t\t Training Loss: 0.0005809142021462321 \t\n",
      "Epoch 12747 \t\t Training Loss: 0.0005809142021462321 \t\n",
      "Epoch 12748 \t\t Training Loss: 0.0005809142021462321 \t\n",
      "Epoch 12749 \t\t Training Loss: 0.0005809142021462321 \t\n",
      "Epoch 12750 \t\t Training Loss: 0.0005809142021462321 \t\n",
      "Epoch 12751 \t\t Training Loss: 0.0005809142021462321 \t\n",
      "Epoch 12752 \t\t Training Loss: 0.0005809142021462321 \t\n",
      "Epoch 12753 \t\t Training Loss: 0.0005809142021462321 \t\n",
      "Epoch 12754 \t\t Training Loss: 0.0005809142021462321 \t\n",
      "Epoch 12755 \t\t Training Loss: 0.0005809142021462321 \t\n",
      "Epoch 12756 \t\t Training Loss: 0.0005809142021462321 \t\n",
      "Epoch 12757 \t\t Training Loss: 0.0005809142021462321 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12758 \t\t Training Loss: 0.0005809142021462321 \t\n",
      "Epoch 12759 \t\t Training Loss: 0.0005809142021462321 \t\n",
      "Epoch 12760 \t\t Training Loss: 0.0005809141439385712 \t\n",
      "Epoch 12761 \t\t Training Loss: 0.0005809141439385712 \t\n",
      "Epoch 12762 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12763 \t\t Training Loss: 0.0005809141439385712 \t\n",
      "Epoch 12764 \t\t Training Loss: 0.0005809141439385712 \t\n",
      "Epoch 12765 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12766 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12767 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12768 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12769 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12770 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12771 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12772 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12773 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12774 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12775 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12776 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12777 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12778 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12779 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12780 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12781 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12782 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12783 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12784 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12785 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12786 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12787 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12788 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12789 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12790 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12791 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12792 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12793 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12794 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12795 \t\t Training Loss: 0.0005809140857309103 \t\n",
      "Epoch 12796 \t\t Training Loss: 0.0005809140275232494 \t\n",
      "Epoch 12797 \t\t Training Loss: 0.0005809140275232494 \t\n",
      "Epoch 12798 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12799 \t\t Training Loss: 0.0005809140275232494 \t\n",
      "Epoch 12800 \t\t Training Loss: 0.0005809140275232494 \t\n",
      "Epoch 12801 \t\t Training Loss: 0.0005809140275232494 \t\n",
      "Epoch 12802 \t\t Training Loss: 0.0005809140275232494 \t\n",
      "Epoch 12803 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12804 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12805 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12806 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12807 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12808 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12809 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12810 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12811 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12812 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12813 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12814 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12815 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12816 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12817 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12818 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12819 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12820 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12821 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12822 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12823 \t\t Training Loss: 0.0005809139693155885 \t\n",
      "Epoch 12824 \t\t Training Loss: 0.0005809138529002666 \t\n",
      "Epoch 12825 \t\t Training Loss: 0.0005809138529002666 \t\n",
      "Epoch 12826 \t\t Training Loss: 0.0005809138529002666 \t\n",
      "Epoch 12827 \t\t Training Loss: 0.0005809138529002666 \t\n",
      "Epoch 12828 \t\t Training Loss: 0.0005809138529002666 \t\n",
      "Epoch 12829 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12830 \t\t Training Loss: 0.0005809138529002666 \t\n",
      "Epoch 12831 \t\t Training Loss: 0.0005809138529002666 \t\n",
      "Epoch 12832 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12833 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12834 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12835 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12836 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12837 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12838 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12839 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12840 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12841 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12842 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12843 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12844 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12845 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12846 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12847 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12848 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12849 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12850 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12851 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12852 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12853 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12854 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12855 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12856 \t\t Training Loss: 0.0005809137946926057 \t\n",
      "Epoch 12857 \t\t Training Loss: 0.0005809137364849448 \t\n",
      "Epoch 12858 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12859 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12860 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12861 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12862 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12863 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12864 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12865 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12866 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12867 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12868 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12869 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12870 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12871 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12872 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12873 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12874 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12875 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12876 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12877 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12878 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12879 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12880 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12881 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12882 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12883 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12884 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12885 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12886 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12887 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12888 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12889 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12890 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12891 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12892 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12893 \t\t Training Loss: 0.0005809136782772839 \t\n",
      "Epoch 12894 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12895 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12896 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12897 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12898 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12899 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12900 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12901 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12902 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12903 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12904 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12905 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12906 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12907 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12908 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12909 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12910 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12911 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12912 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12913 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12914 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12915 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12916 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12917 \t\t Training Loss: 0.0005809135618619621 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12918 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12919 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12920 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12921 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12922 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12923 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12924 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12925 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12926 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12927 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12928 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12929 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12930 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12931 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12932 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12933 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12934 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12935 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12936 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12937 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12938 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12939 \t\t Training Loss: 0.0005809135618619621 \t\n",
      "Epoch 12940 \t\t Training Loss: 0.0005809134454466403 \t\n",
      "Epoch 12941 \t\t Training Loss: 0.0005809133872389793 \t\n",
      "Epoch 12942 \t\t Training Loss: 0.0005809133872389793 \t\n",
      "Epoch 12943 \t\t Training Loss: 0.0005809133872389793 \t\n",
      "Epoch 12944 \t\t Training Loss: 0.0005809134454466403 \t\n",
      "Epoch 12945 \t\t Training Loss: 0.0005809133872389793 \t\n",
      "Epoch 12946 \t\t Training Loss: 0.0005809134454466403 \t\n",
      "Epoch 12947 \t\t Training Loss: 0.0005809134454466403 \t\n",
      "Epoch 12948 \t\t Training Loss: 0.0005809134454466403 \t\n",
      "Epoch 12949 \t\t Training Loss: 0.0005809134454466403 \t\n",
      "Epoch 12950 \t\t Training Loss: 0.0005809133872389793 \t\n",
      "Epoch 12951 \t\t Training Loss: 0.0005809133872389793 \t\n",
      "Epoch 12952 \t\t Training Loss: 0.0005809133872389793 \t\n",
      "Epoch 12953 \t\t Training Loss: 0.0005809133872389793 \t\n",
      "Epoch 12954 \t\t Training Loss: 0.0005809133872389793 \t\n",
      "Epoch 12955 \t\t Training Loss: 0.0005809133872389793 \t\n",
      "Epoch 12956 \t\t Training Loss: 0.0005809133872389793 \t\n",
      "Epoch 12957 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12958 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12959 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12960 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12961 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12962 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12963 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12964 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12965 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12966 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12967 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12968 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12969 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12970 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12971 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12972 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12973 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12974 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12975 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12976 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12977 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12978 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12979 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12980 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12981 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12982 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12983 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12984 \t\t Training Loss: 0.0005809132708236575 \t\n",
      "Epoch 12985 \t\t Training Loss: 0.0005809131544083357 \t\n",
      "Epoch 12986 \t\t Training Loss: 0.0005809131544083357 \t\n",
      "Epoch 12987 \t\t Training Loss: 0.0005809131544083357 \t\n",
      "Epoch 12988 \t\t Training Loss: 0.0005809131544083357 \t\n",
      "Epoch 12989 \t\t Training Loss: 0.0005809131544083357 \t\n",
      "Epoch 12990 \t\t Training Loss: 0.0005809131544083357 \t\n",
      "Epoch 12991 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 12992 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 12993 \t\t Training Loss: 0.0005809131544083357 \t\n",
      "Epoch 12994 \t\t Training Loss: 0.0005809131544083357 \t\n",
      "Epoch 12995 \t\t Training Loss: 0.0005809131544083357 \t\n",
      "Epoch 12996 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 12997 \t\t Training Loss: 0.0005809131544083357 \t\n",
      "Epoch 12998 \t\t Training Loss: 0.0005809131544083357 \t\n",
      "Epoch 12999 \t\t Training Loss: 0.0005809131544083357 \t\n",
      "Epoch 13000 \t\t Training Loss: 0.0005809131544083357 \t\n",
      "Epoch 13001 \t\t Training Loss: 0.0005809131544083357 \t\n",
      "Epoch 13002 \t\t Training Loss: 0.0005809131544083357 \t\n",
      "Epoch 13003 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13004 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13005 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13006 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13007 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13008 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13009 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13010 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13011 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13012 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13013 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13014 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13015 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13016 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13017 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13018 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13019 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13020 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13021 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13022 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13023 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13024 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13025 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13026 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13027 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13028 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13029 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13030 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13031 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13032 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13033 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13034 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13035 \t\t Training Loss: 0.0005809130962006748 \t\n",
      "Epoch 13036 \t\t Training Loss: 0.0005809129797853529 \t\n",
      "Epoch 13037 \t\t Training Loss: 0.0005809129797853529 \t\n",
      "Epoch 13038 \t\t Training Loss: 0.0005809129797853529 \t\n",
      "Epoch 13039 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13040 \t\t Training Loss: 0.0005809129797853529 \t\n",
      "Epoch 13041 \t\t Training Loss: 0.0005809129797853529 \t\n",
      "Epoch 13042 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13043 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13044 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13045 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13046 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13047 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13048 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13049 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13050 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13051 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13052 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13053 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13054 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13055 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13056 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13057 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13058 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13059 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13060 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13061 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13062 \t\t Training Loss: 0.0005809128051623702 \t\n",
      "Epoch 13063 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13064 \t\t Training Loss: 0.0005809128051623702 \t\n",
      "Epoch 13065 \t\t Training Loss: 0.0005809128051623702 \t\n",
      "Epoch 13066 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13067 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13068 \t\t Training Loss: 0.0005809128633700311 \t\n",
      "Epoch 13069 \t\t Training Loss: 0.0005809128051623702 \t\n",
      "Epoch 13070 \t\t Training Loss: 0.0005809128051623702 \t\n",
      "Epoch 13071 \t\t Training Loss: 0.0005809128051623702 \t\n",
      "Epoch 13072 \t\t Training Loss: 0.0005809128051623702 \t\n",
      "Epoch 13073 \t\t Training Loss: 0.0005809128051623702 \t\n",
      "Epoch 13074 \t\t Training Loss: 0.0005809128051623702 \t\n",
      "Epoch 13075 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13076 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13077 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13078 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13079 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13080 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13081 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13082 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13083 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13084 \t\t Training Loss: 0.0005809126887470484 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13085 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13086 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13087 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13088 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13089 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13090 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13091 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13092 \t\t Training Loss: 0.0005809126887470484 \t\n",
      "Epoch 13093 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13094 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13095 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13096 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13097 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13098 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13099 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13100 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13101 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13102 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13103 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13104 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13105 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13106 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13107 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13108 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13109 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13110 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13111 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13112 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13113 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13114 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13115 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13116 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13117 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13118 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13119 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13120 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13121 \t\t Training Loss: 0.0005809125723317266 \t\n",
      "Epoch 13122 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13123 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13124 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13125 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13126 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13127 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13128 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13129 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13130 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13131 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13132 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13133 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13134 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13135 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13136 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13137 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13138 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13139 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13140 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13141 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13142 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13143 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13144 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13145 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13146 \t\t Training Loss: 0.0005809124559164047 \t\n",
      "Epoch 13147 \t\t Training Loss: 0.0005809123977087438 \t\n",
      "Epoch 13148 \t\t Training Loss: 0.0005809123977087438 \t\n",
      "Epoch 13149 \t\t Training Loss: 0.0005809123977087438 \t\n",
      "Epoch 13150 \t\t Training Loss: 0.0005809123977087438 \t\n",
      "Epoch 13151 \t\t Training Loss: 0.0005809123977087438 \t\n",
      "Epoch 13152 \t\t Training Loss: 0.0005809123977087438 \t\n",
      "Epoch 13153 \t\t Training Loss: 0.0005809123977087438 \t\n",
      "Epoch 13154 \t\t Training Loss: 0.000580912281293422 \t\n",
      "Epoch 13155 \t\t Training Loss: 0.000580912281293422 \t\n",
      "Epoch 13156 \t\t Training Loss: 0.000580912281293422 \t\n",
      "Epoch 13157 \t\t Training Loss: 0.000580912281293422 \t\n",
      "Epoch 13158 \t\t Training Loss: 0.000580912281293422 \t\n",
      "Epoch 13159 \t\t Training Loss: 0.000580912281293422 \t\n",
      "Epoch 13160 \t\t Training Loss: 0.000580912281293422 \t\n",
      "Epoch 13161 \t\t Training Loss: 0.000580912281293422 \t\n",
      "Epoch 13162 \t\t Training Loss: 0.000580912281293422 \t\n",
      "Epoch 13163 \t\t Training Loss: 0.000580912281293422 \t\n",
      "Epoch 13164 \t\t Training Loss: 0.000580912281293422 \t\n",
      "Epoch 13165 \t\t Training Loss: 0.000580912281293422 \t\n",
      "Epoch 13166 \t\t Training Loss: 0.000580912281293422 \t\n",
      "Epoch 13167 \t\t Training Loss: 0.000580912281293422 \t\n",
      "Epoch 13168 \t\t Training Loss: 0.000580912281293422 \t\n",
      "Epoch 13169 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13170 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13171 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13172 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13173 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13174 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13175 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13176 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13177 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13178 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13179 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13180 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13181 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13182 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13183 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13184 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13185 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13186 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13187 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13188 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13189 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13190 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13191 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13192 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13193 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13194 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13195 \t\t Training Loss: 0.0005809121648781002 \t\n",
      "Epoch 13196 \t\t Training Loss: 0.0005809121066704392 \t\n",
      "Epoch 13197 \t\t Training Loss: 0.0005809121066704392 \t\n",
      "Epoch 13198 \t\t Training Loss: 0.0005809121066704392 \t\n",
      "Epoch 13199 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13200 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13201 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13202 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13203 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13204 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13205 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13206 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13207 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13208 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13209 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13210 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13211 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13212 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13213 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13214 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13215 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13216 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13217 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13218 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13219 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13220 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13221 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13222 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13223 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13224 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13225 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13226 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13227 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13228 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13229 \t\t Training Loss: 0.0005809118738397956 \t\n",
      "Epoch 13230 \t\t Training Loss: 0.0005809119902551174 \t\n",
      "Epoch 13231 \t\t Training Loss: 0.0005809118738397956 \t\n",
      "Epoch 13232 \t\t Training Loss: 0.0005809118738397956 \t\n",
      "Epoch 13233 \t\t Training Loss: 0.0005809118738397956 \t\n",
      "Epoch 13234 \t\t Training Loss: 0.0005809118738397956 \t\n",
      "Epoch 13235 \t\t Training Loss: 0.0005809118738397956 \t\n",
      "Epoch 13236 \t\t Training Loss: 0.0005809118738397956 \t\n",
      "Epoch 13237 \t\t Training Loss: 0.0005809118738397956 \t\n",
      "Epoch 13238 \t\t Training Loss: 0.0005809118738397956 \t\n",
      "Epoch 13239 \t\t Training Loss: 0.0005809118738397956 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13240 \t\t Training Loss: 0.0005809118738397956 \t\n",
      "Epoch 13241 \t\t Training Loss: 0.0005809118738397956 \t\n",
      "Epoch 13242 \t\t Training Loss: 0.0005809118738397956 \t\n",
      "Epoch 13243 \t\t Training Loss: 0.0005809118738397956 \t\n",
      "Epoch 13244 \t\t Training Loss: 0.0005809118738397956 \t\n",
      "Epoch 13245 \t\t Training Loss: 0.0005809118738397956 \t\n",
      "Epoch 13246 \t\t Training Loss: 0.0005809118156321347 \t\n",
      "Epoch 13247 \t\t Training Loss: 0.0005809118156321347 \t\n",
      "Epoch 13248 \t\t Training Loss: 0.0005809118156321347 \t\n",
      "Epoch 13249 \t\t Training Loss: 0.0005809118156321347 \t\n",
      "Epoch 13250 \t\t Training Loss: 0.0005809118156321347 \t\n",
      "Epoch 13251 \t\t Training Loss: 0.0005809118156321347 \t\n",
      "Epoch 13252 \t\t Training Loss: 0.0005809118156321347 \t\n",
      "Epoch 13253 \t\t Training Loss: 0.0005809118156321347 \t\n",
      "Epoch 13254 \t\t Training Loss: 0.0005809118156321347 \t\n",
      "Epoch 13255 \t\t Training Loss: 0.0005809118156321347 \t\n",
      "Epoch 13256 \t\t Training Loss: 0.0005809118156321347 \t\n",
      "Epoch 13257 \t\t Training Loss: 0.0005809118156321347 \t\n",
      "Epoch 13258 \t\t Training Loss: 0.0005809118156321347 \t\n",
      "Epoch 13259 \t\t Training Loss: 0.0005809118156321347 \t\n",
      "Epoch 13260 \t\t Training Loss: 0.0005809118156321347 \t\n",
      "Epoch 13261 \t\t Training Loss: 0.0005809118156321347 \t\n",
      "Epoch 13262 \t\t Training Loss: 0.0005809116992168128 \t\n",
      "Epoch 13263 \t\t Training Loss: 0.0005809116992168128 \t\n",
      "Epoch 13264 \t\t Training Loss: 0.0005809116992168128 \t\n",
      "Epoch 13265 \t\t Training Loss: 0.0005809116992168128 \t\n",
      "Epoch 13266 \t\t Training Loss: 0.0005809116992168128 \t\n",
      "Epoch 13267 \t\t Training Loss: 0.0005809116992168128 \t\n",
      "Epoch 13268 \t\t Training Loss: 0.0005809116992168128 \t\n",
      "Epoch 13269 \t\t Training Loss: 0.0005809116992168128 \t\n",
      "Epoch 13270 \t\t Training Loss: 0.0005809116992168128 \t\n",
      "Epoch 13271 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13272 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13273 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13274 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13275 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13276 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13277 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13278 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13279 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13280 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13281 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13282 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13283 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13284 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13285 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13286 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13287 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13288 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13289 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13290 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13291 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13292 \t\t Training Loss: 0.0005809116992168128 \t\n",
      "Epoch 13293 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13294 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13295 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13296 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13297 \t\t Training Loss: 0.000580911582801491 \t\n",
      "Epoch 13298 \t\t Training Loss: 0.0005809115245938301 \t\n",
      "Epoch 13299 \t\t Training Loss: 0.0005809115245938301 \t\n",
      "Epoch 13300 \t\t Training Loss: 0.0005809115245938301 \t\n",
      "Epoch 13301 \t\t Training Loss: 0.0005809115245938301 \t\n",
      "Epoch 13302 \t\t Training Loss: 0.0005809115245938301 \t\n",
      "Epoch 13303 \t\t Training Loss: 0.0005809115245938301 \t\n",
      "Epoch 13304 \t\t Training Loss: 0.0005809115245938301 \t\n",
      "Epoch 13305 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13306 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13307 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13308 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13309 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13310 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13311 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13312 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13313 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13314 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13315 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13316 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13317 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13318 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13319 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13320 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13321 \t\t Training Loss: 0.0005809114663861692 \t\n",
      "Epoch 13322 \t\t Training Loss: 0.0005809114081785083 \t\n",
      "Epoch 13323 \t\t Training Loss: 0.0005809114081785083 \t\n",
      "Epoch 13324 \t\t Training Loss: 0.0005809114081785083 \t\n",
      "Epoch 13325 \t\t Training Loss: 0.0005809114081785083 \t\n",
      "Epoch 13326 \t\t Training Loss: 0.0005809114081785083 \t\n",
      "Epoch 13327 \t\t Training Loss: 0.0005809114081785083 \t\n",
      "Epoch 13328 \t\t Training Loss: 0.0005809113499708474 \t\n",
      "Epoch 13329 \t\t Training Loss: 0.0005809113499708474 \t\n",
      "Epoch 13330 \t\t Training Loss: 0.0005809113499708474 \t\n",
      "Epoch 13331 \t\t Training Loss: 0.0005809113499708474 \t\n",
      "Epoch 13332 \t\t Training Loss: 0.0005809113499708474 \t\n",
      "Epoch 13333 \t\t Training Loss: 0.0005809113499708474 \t\n",
      "Epoch 13334 \t\t Training Loss: 0.0005809113499708474 \t\n",
      "Epoch 13335 \t\t Training Loss: 0.0005809113499708474 \t\n",
      "Epoch 13336 \t\t Training Loss: 0.0005809113499708474 \t\n",
      "Epoch 13337 \t\t Training Loss: 0.0005809113499708474 \t\n",
      "Epoch 13338 \t\t Training Loss: 0.0005809113499708474 \t\n",
      "Epoch 13339 \t\t Training Loss: 0.0005809113499708474 \t\n",
      "Epoch 13340 \t\t Training Loss: 0.0005809113499708474 \t\n",
      "Epoch 13341 \t\t Training Loss: 0.0005809112917631865 \t\n",
      "Epoch 13342 \t\t Training Loss: 0.0005809112917631865 \t\n",
      "Epoch 13343 \t\t Training Loss: 0.0005809112917631865 \t\n",
      "Epoch 13344 \t\t Training Loss: 0.0005809112917631865 \t\n",
      "Epoch 13345 \t\t Training Loss: 0.0005809112917631865 \t\n",
      "Epoch 13346 \t\t Training Loss: 0.0005809112917631865 \t\n",
      "Epoch 13347 \t\t Training Loss: 0.0005809112917631865 \t\n",
      "Epoch 13348 \t\t Training Loss: 0.0005809112917631865 \t\n",
      "Epoch 13349 \t\t Training Loss: 0.0005809112917631865 \t\n",
      "Epoch 13350 \t\t Training Loss: 0.0005809112917631865 \t\n",
      "Epoch 13351 \t\t Training Loss: 0.0005809112335555255 \t\n",
      "Epoch 13352 \t\t Training Loss: 0.0005809112335555255 \t\n",
      "Epoch 13353 \t\t Training Loss: 0.0005809112335555255 \t\n",
      "Epoch 13354 \t\t Training Loss: 0.0005809112335555255 \t\n",
      "Epoch 13355 \t\t Training Loss: 0.0005809112335555255 \t\n",
      "Epoch 13356 \t\t Training Loss: 0.0005809112335555255 \t\n",
      "Epoch 13357 \t\t Training Loss: 0.0005809112335555255 \t\n",
      "Epoch 13358 \t\t Training Loss: 0.0005809112335555255 \t\n",
      "Epoch 13359 \t\t Training Loss: 0.0005809112335555255 \t\n",
      "Epoch 13360 \t\t Training Loss: 0.0005809112335555255 \t\n",
      "Epoch 13361 \t\t Training Loss: 0.0005809112335555255 \t\n",
      "Epoch 13362 \t\t Training Loss: 0.0005809112335555255 \t\n",
      "Epoch 13363 \t\t Training Loss: 0.0005809112335555255 \t\n",
      "Epoch 13364 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13365 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13366 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13367 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13368 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13369 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13370 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13371 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13372 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13373 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13374 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13375 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13376 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13377 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13378 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13379 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13380 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13381 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13382 \t\t Training Loss: 0.0005809111171402037 \t\n",
      "Epoch 13383 \t\t Training Loss: 0.0005809110589325428 \t\n",
      "Epoch 13384 \t\t Training Loss: 0.0005809110589325428 \t\n",
      "Epoch 13385 \t\t Training Loss: 0.0005809110589325428 \t\n",
      "Epoch 13386 \t\t Training Loss: 0.0005809110589325428 \t\n",
      "Epoch 13387 \t\t Training Loss: 0.0005809110589325428 \t\n",
      "Epoch 13388 \t\t Training Loss: 0.0005809110589325428 \t\n",
      "Epoch 13389 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13390 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13391 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13392 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13393 \t\t Training Loss: 0.0005809110589325428 \t\n",
      "Epoch 13394 \t\t Training Loss: 0.0005809110589325428 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13395 \t\t Training Loss: 0.0005809110589325428 \t\n",
      "Epoch 13396 \t\t Training Loss: 0.0005809110589325428 \t\n",
      "Epoch 13397 \t\t Training Loss: 0.0005809110589325428 \t\n",
      "Epoch 13398 \t\t Training Loss: 0.0005809110589325428 \t\n",
      "Epoch 13399 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13400 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13401 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13402 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13403 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13404 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13405 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13406 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13407 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13408 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13409 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13410 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13411 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13412 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13413 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13414 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13415 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13416 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13417 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13418 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13419 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13420 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13421 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13422 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13423 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13424 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13425 \t\t Training Loss: 0.0005809110007248819 \t\n",
      "Epoch 13426 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13427 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13428 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13429 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13430 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13431 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13432 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13433 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13434 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13435 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13436 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13437 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13438 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13439 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13440 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13441 \t\t Training Loss: 0.0005809108843095601 \t\n",
      "Epoch 13442 \t\t Training Loss: 0.0005809108261018991 \t\n",
      "Epoch 13443 \t\t Training Loss: 0.0005809108261018991 \t\n",
      "Epoch 13444 \t\t Training Loss: 0.0005809108261018991 \t\n",
      "Epoch 13445 \t\t Training Loss: 0.0005809107678942382 \t\n",
      "Epoch 13446 \t\t Training Loss: 0.0005809107678942382 \t\n",
      "Epoch 13447 \t\t Training Loss: 0.0005809107678942382 \t\n",
      "Epoch 13448 \t\t Training Loss: 0.0005809107678942382 \t\n",
      "Epoch 13449 \t\t Training Loss: 0.0005809107678942382 \t\n",
      "Epoch 13450 \t\t Training Loss: 0.0005809107678942382 \t\n",
      "Epoch 13451 \t\t Training Loss: 0.0005809107096865773 \t\n",
      "Epoch 13452 \t\t Training Loss: 0.0005809107678942382 \t\n",
      "Epoch 13453 \t\t Training Loss: 0.0005809107678942382 \t\n",
      "Epoch 13454 \t\t Training Loss: 0.0005809107096865773 \t\n",
      "Epoch 13455 \t\t Training Loss: 0.0005809107096865773 \t\n",
      "Epoch 13456 \t\t Training Loss: 0.0005809107096865773 \t\n",
      "Epoch 13457 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13458 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13459 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13460 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13461 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13462 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13463 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13464 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13465 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13466 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13467 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13468 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13469 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13470 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13471 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13472 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13473 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13474 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13475 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13476 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13477 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13478 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13479 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13480 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13481 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13482 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13483 \t\t Training Loss: 0.0005809106514789164 \t\n",
      "Epoch 13484 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13485 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13486 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13487 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13488 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13489 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13490 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13491 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13492 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13493 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13494 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13495 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13496 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13497 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13498 \t\t Training Loss: 0.0005809105350635946 \t\n",
      "Epoch 13499 \t\t Training Loss: 0.0005809104768559337 \t\n",
      "Epoch 13500 \t\t Training Loss: 0.0005809104768559337 \t\n",
      "Epoch 13501 \t\t Training Loss: 0.0005809104768559337 \t\n",
      "Epoch 13502 \t\t Training Loss: 0.0005809104768559337 \t\n",
      "Epoch 13503 \t\t Training Loss: 0.0005809104768559337 \t\n",
      "Epoch 13504 \t\t Training Loss: 0.0005809104768559337 \t\n",
      "Epoch 13505 \t\t Training Loss: 0.0005809104768559337 \t\n",
      "Epoch 13506 \t\t Training Loss: 0.0005809104768559337 \t\n",
      "Epoch 13507 \t\t Training Loss: 0.0005809104768559337 \t\n",
      "Epoch 13508 \t\t Training Loss: 0.0005809104768559337 \t\n",
      "Epoch 13509 \t\t Training Loss: 0.0005809104768559337 \t\n",
      "Epoch 13510 \t\t Training Loss: 0.0005809104768559337 \t\n",
      "Epoch 13511 \t\t Training Loss: 0.0005809104768559337 \t\n",
      "Epoch 13512 \t\t Training Loss: 0.0005809104768559337 \t\n",
      "Epoch 13513 \t\t Training Loss: 0.0005809104186482728 \t\n",
      "Epoch 13514 \t\t Training Loss: 0.0005809104768559337 \t\n",
      "Epoch 13515 \t\t Training Loss: 0.0005809104186482728 \t\n",
      "Epoch 13516 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13517 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13518 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13519 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13520 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13521 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13522 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13523 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13524 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13525 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13526 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13527 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13528 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13529 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13530 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13531 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13532 \t\t Training Loss: 0.0005809103604406118 \t\n",
      "Epoch 13533 \t\t Training Loss: 0.0005809103022329509 \t\n",
      "Epoch 13534 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13535 \t\t Training Loss: 0.0005809103022329509 \t\n",
      "Epoch 13536 \t\t Training Loss: 0.0005809103022329509 \t\n",
      "Epoch 13537 \t\t Training Loss: 0.0005809103022329509 \t\n",
      "Epoch 13538 \t\t Training Loss: 0.0005809103022329509 \t\n",
      "Epoch 13539 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13540 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13541 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13542 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13543 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13544 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13545 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13546 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13547 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13548 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13549 \t\t Training Loss: 0.00058091024402529 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13550 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13551 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13552 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13553 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13554 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13555 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13556 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13557 \t\t Training Loss: 0.00058091024402529 \t\n",
      "Epoch 13558 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13559 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13560 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13561 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13562 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13563 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13564 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13565 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13566 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13567 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13568 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13569 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13570 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13571 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13572 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13573 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13574 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13575 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13576 \t\t Training Loss: 0.0005809101858176291 \t\n",
      "Epoch 13577 \t\t Training Loss: 0.0005809100694023073 \t\n",
      "Epoch 13578 \t\t Training Loss: 0.0005809100694023073 \t\n",
      "Epoch 13579 \t\t Training Loss: 0.0005809100694023073 \t\n",
      "Epoch 13580 \t\t Training Loss: 0.0005809100694023073 \t\n",
      "Epoch 13581 \t\t Training Loss: 0.0005809100694023073 \t\n",
      "Epoch 13582 \t\t Training Loss: 0.0005809100694023073 \t\n",
      "Epoch 13583 \t\t Training Loss: 0.0005809100694023073 \t\n",
      "Epoch 13584 \t\t Training Loss: 0.0005809100694023073 \t\n",
      "Epoch 13585 \t\t Training Loss: 0.0005809100694023073 \t\n",
      "Epoch 13586 \t\t Training Loss: 0.0005809100694023073 \t\n",
      "Epoch 13587 \t\t Training Loss: 0.0005809100694023073 \t\n",
      "Epoch 13588 \t\t Training Loss: 0.0005809100694023073 \t\n",
      "Epoch 13589 \t\t Training Loss: 0.0005809100694023073 \t\n",
      "Epoch 13590 \t\t Training Loss: 0.0005809099529869854 \t\n",
      "Epoch 13591 \t\t Training Loss: 0.0005809099529869854 \t\n",
      "Epoch 13592 \t\t Training Loss: 0.0005809099529869854 \t\n",
      "Epoch 13593 \t\t Training Loss: 0.0005809099529869854 \t\n",
      "Epoch 13594 \t\t Training Loss: 0.0005809099529869854 \t\n",
      "Epoch 13595 \t\t Training Loss: 0.0005809099529869854 \t\n",
      "Epoch 13596 \t\t Training Loss: 0.0005809099529869854 \t\n",
      "Epoch 13597 \t\t Training Loss: 0.0005809099529869854 \t\n",
      "Epoch 13598 \t\t Training Loss: 0.0005809099529869854 \t\n",
      "Epoch 13599 \t\t Training Loss: 0.0005809099529869854 \t\n",
      "Epoch 13600 \t\t Training Loss: 0.0005809098947793245 \t\n",
      "Epoch 13601 \t\t Training Loss: 0.0005809098947793245 \t\n",
      "Epoch 13602 \t\t Training Loss: 0.0005809098947793245 \t\n",
      "Epoch 13603 \t\t Training Loss: 0.0005809098947793245 \t\n",
      "Epoch 13604 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13605 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13606 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13607 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13608 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13609 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13610 \t\t Training Loss: 0.0005809098947793245 \t\n",
      "Epoch 13611 \t\t Training Loss: 0.0005809098947793245 \t\n",
      "Epoch 13612 \t\t Training Loss: 0.0005809098947793245 \t\n",
      "Epoch 13613 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13614 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13615 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13616 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13617 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13618 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13619 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13620 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13621 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13622 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13623 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13624 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13625 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13626 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13627 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13628 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13629 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13630 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13631 \t\t Training Loss: 0.0005809098365716636 \t\n",
      "Epoch 13632 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13633 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13634 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13635 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13636 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13637 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13638 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13639 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13640 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13641 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13642 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13643 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13644 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13645 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13646 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13647 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13648 \t\t Training Loss: 0.0005809097783640027 \t\n",
      "Epoch 13649 \t\t Training Loss: 0.0005809097201563418 \t\n",
      "Epoch 13650 \t\t Training Loss: 0.0005809097201563418 \t\n",
      "Epoch 13651 \t\t Training Loss: 0.0005809097201563418 \t\n",
      "Epoch 13652 \t\t Training Loss: 0.0005809097201563418 \t\n",
      "Epoch 13653 \t\t Training Loss: 0.0005809097201563418 \t\n",
      "Epoch 13654 \t\t Training Loss: 0.0005809097201563418 \t\n",
      "Epoch 13655 \t\t Training Loss: 0.0005809097201563418 \t\n",
      "Epoch 13656 \t\t Training Loss: 0.0005809097201563418 \t\n",
      "Epoch 13657 \t\t Training Loss: 0.00058090960374102 \t\n",
      "Epoch 13658 \t\t Training Loss: 0.00058090960374102 \t\n",
      "Epoch 13659 \t\t Training Loss: 0.00058090960374102 \t\n",
      "Epoch 13660 \t\t Training Loss: 0.00058090960374102 \t\n",
      "Epoch 13661 \t\t Training Loss: 0.00058090960374102 \t\n",
      "Epoch 13662 \t\t Training Loss: 0.00058090960374102 \t\n",
      "Epoch 13663 \t\t Training Loss: 0.00058090960374102 \t\n",
      "Epoch 13664 \t\t Training Loss: 0.00058090960374102 \t\n",
      "Epoch 13665 \t\t Training Loss: 0.00058090960374102 \t\n",
      "Epoch 13666 \t\t Training Loss: 0.000580909545533359 \t\n",
      "Epoch 13667 \t\t Training Loss: 0.0005809094873256981 \t\n",
      "Epoch 13668 \t\t Training Loss: 0.0005809094873256981 \t\n",
      "Epoch 13669 \t\t Training Loss: 0.0005809094873256981 \t\n",
      "Epoch 13670 \t\t Training Loss: 0.0005809094873256981 \t\n",
      "Epoch 13671 \t\t Training Loss: 0.0005809094873256981 \t\n",
      "Epoch 13672 \t\t Training Loss: 0.0005809094873256981 \t\n",
      "Epoch 13673 \t\t Training Loss: 0.0005809094873256981 \t\n",
      "Epoch 13674 \t\t Training Loss: 0.0005809094873256981 \t\n",
      "Epoch 13675 \t\t Training Loss: 0.0005809094873256981 \t\n",
      "Epoch 13676 \t\t Training Loss: 0.000580909545533359 \t\n",
      "Epoch 13677 \t\t Training Loss: 0.0005809094873256981 \t\n",
      "Epoch 13678 \t\t Training Loss: 0.0005809094873256981 \t\n",
      "Epoch 13679 \t\t Training Loss: 0.0005809094873256981 \t\n",
      "Epoch 13680 \t\t Training Loss: 0.0005809094873256981 \t\n",
      "Epoch 13681 \t\t Training Loss: 0.0005809094873256981 \t\n",
      "Epoch 13682 \t\t Training Loss: 0.0005809094873256981 \t\n",
      "Epoch 13683 \t\t Training Loss: 0.0005809094873256981 \t\n",
      "Epoch 13684 \t\t Training Loss: 0.0005809093709103763 \t\n",
      "Epoch 13685 \t\t Training Loss: 0.0005809093709103763 \t\n",
      "Epoch 13686 \t\t Training Loss: 0.0005809093709103763 \t\n",
      "Epoch 13687 \t\t Training Loss: 0.0005809093709103763 \t\n",
      "Epoch 13688 \t\t Training Loss: 0.0005809093709103763 \t\n",
      "Epoch 13689 \t\t Training Loss: 0.0005809093709103763 \t\n",
      "Epoch 13690 \t\t Training Loss: 0.0005809093709103763 \t\n",
      "Epoch 13691 \t\t Training Loss: 0.0005809093709103763 \t\n",
      "Epoch 13692 \t\t Training Loss: 0.0005809093709103763 \t\n",
      "Epoch 13693 \t\t Training Loss: 0.0005809093709103763 \t\n",
      "Epoch 13694 \t\t Training Loss: 0.0005809093709103763 \t\n",
      "Epoch 13695 \t\t Training Loss: 0.0005809093127027154 \t\n",
      "Epoch 13696 \t\t Training Loss: 0.0005809093127027154 \t\n",
      "Epoch 13697 \t\t Training Loss: 0.0005809093127027154 \t\n",
      "Epoch 13698 \t\t Training Loss: 0.0005809093127027154 \t\n",
      "Epoch 13699 \t\t Training Loss: 0.0005809093127027154 \t\n",
      "Epoch 13700 \t\t Training Loss: 0.0005809093127027154 \t\n",
      "Epoch 13701 \t\t Training Loss: 0.0005809093127027154 \t\n",
      "Epoch 13702 \t\t Training Loss: 0.0005809092544950545 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13703 \t\t Training Loss: 0.0005809092544950545 \t\n",
      "Epoch 13704 \t\t Training Loss: 0.0005809092544950545 \t\n",
      "Epoch 13705 \t\t Training Loss: 0.0005809092544950545 \t\n",
      "Epoch 13706 \t\t Training Loss: 0.0005809092544950545 \t\n",
      "Epoch 13707 \t\t Training Loss: 0.0005809091962873936 \t\n",
      "Epoch 13708 \t\t Training Loss: 0.0005809091962873936 \t\n",
      "Epoch 13709 \t\t Training Loss: 0.0005809092544950545 \t\n",
      "Epoch 13710 \t\t Training Loss: 0.0005809091962873936 \t\n",
      "Epoch 13711 \t\t Training Loss: 0.0005809091962873936 \t\n",
      "Epoch 13712 \t\t Training Loss: 0.0005809091962873936 \t\n",
      "Epoch 13713 \t\t Training Loss: 0.0005809091962873936 \t\n",
      "Epoch 13714 \t\t Training Loss: 0.0005809091962873936 \t\n",
      "Epoch 13715 \t\t Training Loss: 0.0005809091962873936 \t\n",
      "Epoch 13716 \t\t Training Loss: 0.0005809091962873936 \t\n",
      "Epoch 13717 \t\t Training Loss: 0.0005809091962873936 \t\n",
      "Epoch 13718 \t\t Training Loss: 0.0005809091962873936 \t\n",
      "Epoch 13719 \t\t Training Loss: 0.0005809091962873936 \t\n",
      "Epoch 13720 \t\t Training Loss: 0.0005809091962873936 \t\n",
      "Epoch 13721 \t\t Training Loss: 0.0005809091962873936 \t\n",
      "Epoch 13722 \t\t Training Loss: 0.0005809091380797327 \t\n",
      "Epoch 13723 \t\t Training Loss: 0.0005809091380797327 \t\n",
      "Epoch 13724 \t\t Training Loss: 0.0005809091380797327 \t\n",
      "Epoch 13725 \t\t Training Loss: 0.0005809091380797327 \t\n",
      "Epoch 13726 \t\t Training Loss: 0.0005809091380797327 \t\n",
      "Epoch 13727 \t\t Training Loss: 0.0005809091380797327 \t\n",
      "Epoch 13728 \t\t Training Loss: 0.0005809091380797327 \t\n",
      "Epoch 13729 \t\t Training Loss: 0.0005809091380797327 \t\n",
      "Epoch 13730 \t\t Training Loss: 0.0005809091380797327 \t\n",
      "Epoch 13731 \t\t Training Loss: 0.0005809091380797327 \t\n",
      "Epoch 13732 \t\t Training Loss: 0.0005809090798720717 \t\n",
      "Epoch 13733 \t\t Training Loss: 0.0005809090798720717 \t\n",
      "Epoch 13734 \t\t Training Loss: 0.0005809090798720717 \t\n",
      "Epoch 13735 \t\t Training Loss: 0.0005809090798720717 \t\n",
      "Epoch 13736 \t\t Training Loss: 0.0005809090798720717 \t\n",
      "Epoch 13737 \t\t Training Loss: 0.0005809090798720717 \t\n",
      "Epoch 13738 \t\t Training Loss: 0.0005809090798720717 \t\n",
      "Epoch 13739 \t\t Training Loss: 0.0005809090798720717 \t\n",
      "Epoch 13740 \t\t Training Loss: 0.0005809090216644108 \t\n",
      "Epoch 13741 \t\t Training Loss: 0.0005809090216644108 \t\n",
      "Epoch 13742 \t\t Training Loss: 0.0005809090216644108 \t\n",
      "Epoch 13743 \t\t Training Loss: 0.0005809090216644108 \t\n",
      "Epoch 13744 \t\t Training Loss: 0.0005809090216644108 \t\n",
      "Epoch 13745 \t\t Training Loss: 0.0005809090216644108 \t\n",
      "Epoch 13746 \t\t Training Loss: 0.0005809090216644108 \t\n",
      "Epoch 13747 \t\t Training Loss: 0.0005809090216644108 \t\n",
      "Epoch 13748 \t\t Training Loss: 0.000580908905249089 \t\n",
      "Epoch 13749 \t\t Training Loss: 0.0005809090216644108 \t\n",
      "Epoch 13750 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13751 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13752 \t\t Training Loss: 0.0005809090216644108 \t\n",
      "Epoch 13753 \t\t Training Loss: 0.0005809090216644108 \t\n",
      "Epoch 13754 \t\t Training Loss: 0.0005809090216644108 \t\n",
      "Epoch 13755 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13756 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13757 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13758 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13759 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13760 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13761 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13762 \t\t Training Loss: 0.000580908905249089 \t\n",
      "Epoch 13763 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13764 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13765 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13766 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13767 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13768 \t\t Training Loss: 0.0005809090216644108 \t\n",
      "Epoch 13769 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13770 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13771 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13772 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13773 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13774 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13775 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13776 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13777 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13778 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13779 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13780 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13781 \t\t Training Loss: 0.0005809089634567499 \t\n",
      "Epoch 13782 \t\t Training Loss: 0.000580908905249089 \t\n",
      "Epoch 13783 \t\t Training Loss: 0.000580908905249089 \t\n",
      "Epoch 13784 \t\t Training Loss: 0.000580908905249089 \t\n",
      "Epoch 13785 \t\t Training Loss: 0.000580908905249089 \t\n",
      "Epoch 13786 \t\t Training Loss: 0.000580908905249089 \t\n",
      "Epoch 13787 \t\t Training Loss: 0.000580908905249089 \t\n",
      "Epoch 13788 \t\t Training Loss: 0.000580908905249089 \t\n",
      "Epoch 13789 \t\t Training Loss: 0.000580908905249089 \t\n",
      "Epoch 13790 \t\t Training Loss: 0.000580908905249089 \t\n",
      "Epoch 13791 \t\t Training Loss: 0.000580908905249089 \t\n",
      "Epoch 13792 \t\t Training Loss: 0.0005809088470414281 \t\n",
      "Epoch 13793 \t\t Training Loss: 0.0005809088470414281 \t\n",
      "Epoch 13794 \t\t Training Loss: 0.0005809088470414281 \t\n",
      "Epoch 13795 \t\t Training Loss: 0.0005809088470414281 \t\n",
      "Epoch 13796 \t\t Training Loss: 0.0005809088470414281 \t\n",
      "Epoch 13797 \t\t Training Loss: 0.0005809088470414281 \t\n",
      "Epoch 13798 \t\t Training Loss: 0.0005809088470414281 \t\n",
      "Epoch 13799 \t\t Training Loss: 0.0005809087888337672 \t\n",
      "Epoch 13800 \t\t Training Loss: 0.0005809087888337672 \t\n",
      "Epoch 13801 \t\t Training Loss: 0.0005809087888337672 \t\n",
      "Epoch 13802 \t\t Training Loss: 0.0005809087888337672 \t\n",
      "Epoch 13803 \t\t Training Loss: 0.0005809087888337672 \t\n",
      "Epoch 13804 \t\t Training Loss: 0.0005809087306261063 \t\n",
      "Epoch 13805 \t\t Training Loss: 0.0005809087306261063 \t\n",
      "Epoch 13806 \t\t Training Loss: 0.0005809087306261063 \t\n",
      "Epoch 13807 \t\t Training Loss: 0.0005809087306261063 \t\n",
      "Epoch 13808 \t\t Training Loss: 0.0005809086724184453 \t\n",
      "Epoch 13809 \t\t Training Loss: 0.0005809087306261063 \t\n",
      "Epoch 13810 \t\t Training Loss: 0.0005809086724184453 \t\n",
      "Epoch 13811 \t\t Training Loss: 0.0005809086724184453 \t\n",
      "Epoch 13812 \t\t Training Loss: 0.0005809086724184453 \t\n",
      "Epoch 13813 \t\t Training Loss: 0.0005809086724184453 \t\n",
      "Epoch 13814 \t\t Training Loss: 0.0005809086724184453 \t\n",
      "Epoch 13815 \t\t Training Loss: 0.0005809086724184453 \t\n",
      "Epoch 13816 \t\t Training Loss: 0.0005809086724184453 \t\n",
      "Epoch 13817 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13818 \t\t Training Loss: 0.0005809086724184453 \t\n",
      "Epoch 13819 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13820 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13821 \t\t Training Loss: 0.0005809086724184453 \t\n",
      "Epoch 13822 \t\t Training Loss: 0.0005809086724184453 \t\n",
      "Epoch 13823 \t\t Training Loss: 0.0005809086724184453 \t\n",
      "Epoch 13824 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13825 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13826 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13827 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13828 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13829 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13830 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13831 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13832 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13833 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13834 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13835 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13836 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13837 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13838 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13839 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13840 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13841 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13842 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13843 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13844 \t\t Training Loss: 0.0005809085560031235 \t\n",
      "Epoch 13845 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13846 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13847 \t\t Training Loss: 0.0005809085560031235 \t\n",
      "Epoch 13848 \t\t Training Loss: 0.0005809085560031235 \t\n",
      "Epoch 13849 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13850 \t\t Training Loss: 0.0005809086142107844 \t\n",
      "Epoch 13851 \t\t Training Loss: 0.0005809085560031235 \t\n",
      "Epoch 13852 \t\t Training Loss: 0.0005809085560031235 \t\n",
      "Epoch 13853 \t\t Training Loss: 0.0005809084977954626 \t\n",
      "Epoch 13854 \t\t Training Loss: 0.0005809084977954626 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13855 \t\t Training Loss: 0.0005809084977954626 \t\n",
      "Epoch 13856 \t\t Training Loss: 0.0005809084977954626 \t\n",
      "Epoch 13857 \t\t Training Loss: 0.0005809084977954626 \t\n",
      "Epoch 13858 \t\t Training Loss: 0.0005809084977954626 \t\n",
      "Epoch 13859 \t\t Training Loss: 0.0005809084977954626 \t\n",
      "Epoch 13860 \t\t Training Loss: 0.0005809084977954626 \t\n",
      "Epoch 13861 \t\t Training Loss: 0.0005809084977954626 \t\n",
      "Epoch 13862 \t\t Training Loss: 0.0005809084977954626 \t\n",
      "Epoch 13863 \t\t Training Loss: 0.0005809084977954626 \t\n",
      "Epoch 13864 \t\t Training Loss: 0.0005809084977954626 \t\n",
      "Epoch 13865 \t\t Training Loss: 0.0005809084977954626 \t\n",
      "Epoch 13866 \t\t Training Loss: 0.0005809084977954626 \t\n",
      "Epoch 13867 \t\t Training Loss: 0.0005809084977954626 \t\n",
      "Epoch 13868 \t\t Training Loss: 0.0005809084977954626 \t\n",
      "Epoch 13869 \t\t Training Loss: 0.0005809084977954626 \t\n",
      "Epoch 13870 \t\t Training Loss: 0.0005809084395878017 \t\n",
      "Epoch 13871 \t\t Training Loss: 0.0005809084395878017 \t\n",
      "Epoch 13872 \t\t Training Loss: 0.0005809084395878017 \t\n",
      "Epoch 13873 \t\t Training Loss: 0.0005809084395878017 \t\n",
      "Epoch 13874 \t\t Training Loss: 0.0005809084395878017 \t\n",
      "Epoch 13875 \t\t Training Loss: 0.0005809084395878017 \t\n",
      "Epoch 13876 \t\t Training Loss: 0.0005809084395878017 \t\n",
      "Epoch 13877 \t\t Training Loss: 0.0005809084395878017 \t\n",
      "Epoch 13878 \t\t Training Loss: 0.0005809084395878017 \t\n",
      "Epoch 13879 \t\t Training Loss: 0.0005809084395878017 \t\n",
      "Epoch 13880 \t\t Training Loss: 0.0005809084395878017 \t\n",
      "Epoch 13881 \t\t Training Loss: 0.0005809084395878017 \t\n",
      "Epoch 13882 \t\t Training Loss: 0.0005809083231724799 \t\n",
      "Epoch 13883 \t\t Training Loss: 0.0005809083231724799 \t\n",
      "Epoch 13884 \t\t Training Loss: 0.0005809083231724799 \t\n",
      "Epoch 13885 \t\t Training Loss: 0.0005809083231724799 \t\n",
      "Epoch 13886 \t\t Training Loss: 0.000580908264964819 \t\n",
      "Epoch 13887 \t\t Training Loss: 0.000580908264964819 \t\n",
      "Epoch 13888 \t\t Training Loss: 0.000580908206757158 \t\n",
      "Epoch 13889 \t\t Training Loss: 0.000580908206757158 \t\n",
      "Epoch 13890 \t\t Training Loss: 0.000580908206757158 \t\n",
      "Epoch 13891 \t\t Training Loss: 0.000580908264964819 \t\n",
      "Epoch 13892 \t\t Training Loss: 0.000580908206757158 \t\n",
      "Epoch 13893 \t\t Training Loss: 0.000580908264964819 \t\n",
      "Epoch 13894 \t\t Training Loss: 0.000580908264964819 \t\n",
      "Epoch 13895 \t\t Training Loss: 0.0005809083231724799 \t\n",
      "Epoch 13896 \t\t Training Loss: 0.0005809083231724799 \t\n",
      "Epoch 13897 \t\t Training Loss: 0.0005809083231724799 \t\n",
      "Epoch 13898 \t\t Training Loss: 0.0005809083231724799 \t\n",
      "Epoch 13899 \t\t Training Loss: 0.0005809083231724799 \t\n",
      "Epoch 13900 \t\t Training Loss: 0.0005809083231724799 \t\n",
      "Epoch 13901 \t\t Training Loss: 0.000580908264964819 \t\n",
      "Epoch 13902 \t\t Training Loss: 0.000580908264964819 \t\n",
      "Epoch 13903 \t\t Training Loss: 0.000580908264964819 \t\n",
      "Epoch 13904 \t\t Training Loss: 0.000580908264964819 \t\n",
      "Epoch 13905 \t\t Training Loss: 0.000580908206757158 \t\n",
      "Epoch 13906 \t\t Training Loss: 0.000580908206757158 \t\n",
      "Epoch 13907 \t\t Training Loss: 0.000580908206757158 \t\n",
      "Epoch 13908 \t\t Training Loss: 0.000580908206757158 \t\n",
      "Epoch 13909 \t\t Training Loss: 0.000580908206757158 \t\n",
      "Epoch 13910 \t\t Training Loss: 0.000580908206757158 \t\n",
      "Epoch 13911 \t\t Training Loss: 0.000580908206757158 \t\n",
      "Epoch 13912 \t\t Training Loss: 0.000580908206757158 \t\n",
      "Epoch 13913 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13914 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13915 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13916 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13917 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13918 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13919 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13920 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13921 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13922 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13923 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13924 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13925 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13926 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13927 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13928 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13929 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13930 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13931 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13932 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13933 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13934 \t\t Training Loss: 0.0005809081485494971 \t\n",
      "Epoch 13935 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13936 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13937 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13938 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13939 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13940 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13941 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13942 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13943 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13944 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13945 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13946 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13947 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13948 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13949 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13950 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13951 \t\t Training Loss: 0.0005809080321341753 \t\n",
      "Epoch 13952 \t\t Training Loss: 0.0005809079157188535 \t\n",
      "Epoch 13953 \t\t Training Loss: 0.0005809079157188535 \t\n",
      "Epoch 13954 \t\t Training Loss: 0.0005809079157188535 \t\n",
      "Epoch 13955 \t\t Training Loss: 0.0005809079157188535 \t\n",
      "Epoch 13956 \t\t Training Loss: 0.0005809079157188535 \t\n",
      "Epoch 13957 \t\t Training Loss: 0.0005809079157188535 \t\n",
      "Epoch 13958 \t\t Training Loss: 0.0005809079157188535 \t\n",
      "Epoch 13959 \t\t Training Loss: 0.0005809079157188535 \t\n",
      "Epoch 13960 \t\t Training Loss: 0.0005809079157188535 \t\n",
      "Epoch 13961 \t\t Training Loss: 0.0005809079157188535 \t\n",
      "Epoch 13962 \t\t Training Loss: 0.0005809079157188535 \t\n",
      "Epoch 13963 \t\t Training Loss: 0.0005809079157188535 \t\n",
      "Epoch 13964 \t\t Training Loss: 0.0005809079157188535 \t\n",
      "Epoch 13965 \t\t Training Loss: 0.0005809079157188535 \t\n",
      "Epoch 13966 \t\t Training Loss: 0.0005809079157188535 \t\n",
      "Epoch 13967 \t\t Training Loss: 0.0005809079157188535 \t\n",
      "Epoch 13968 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13969 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13970 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13971 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13972 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13973 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13974 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13975 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13976 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13977 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13978 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13979 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13980 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13981 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13982 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13983 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13984 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13985 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13986 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13987 \t\t Training Loss: 0.0005809077993035316 \t\n",
      "Epoch 13988 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13989 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13990 \t\t Training Loss: 0.0005809077410958707 \t\n",
      "Epoch 13991 \t\t Training Loss: 0.0005809077410958707 \t\n",
      "Epoch 13992 \t\t Training Loss: 0.0005809077993035316 \t\n",
      "Epoch 13993 \t\t Training Loss: 0.0005809077993035316 \t\n",
      "Epoch 13994 \t\t Training Loss: 0.0005809077993035316 \t\n",
      "Epoch 13995 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13996 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13997 \t\t Training Loss: 0.0005809077410958707 \t\n",
      "Epoch 13998 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 13999 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 14000 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 14001 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 14002 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 14003 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 14004 \t\t Training Loss: 0.0005809078575111926 \t\n",
      "Epoch 14005 \t\t Training Loss: 0.0005809077410958707 \t\n",
      "Epoch 14006 \t\t Training Loss: 0.0005809077410958707 \t\n",
      "Epoch 14007 \t\t Training Loss: 0.0005809077410958707 \t\n",
      "Epoch 14008 \t\t Training Loss: 0.0005809077410958707 \t\n",
      "Epoch 14009 \t\t Training Loss: 0.0005809077410958707 \t\n",
      "Epoch 14010 \t\t Training Loss: 0.0005809077410958707 \t\n",
      "Epoch 14011 \t\t Training Loss: 0.0005809077410958707 \t\n",
      "Epoch 14012 \t\t Training Loss: 0.0005809077410958707 \t\n",
      "Epoch 14013 \t\t Training Loss: 0.0005809076246805489 \t\n",
      "Epoch 14014 \t\t Training Loss: 0.0005809076246805489 \t\n",
      "Epoch 14015 \t\t Training Loss: 0.0005809076246805489 \t\n",
      "Epoch 14016 \t\t Training Loss: 0.0005809076246805489 \t\n",
      "Epoch 14017 \t\t Training Loss: 0.0005809076246805489 \t\n",
      "Epoch 14018 \t\t Training Loss: 0.0005809076246805489 \t\n",
      "Epoch 14019 \t\t Training Loss: 0.0005809076246805489 \t\n",
      "Epoch 14020 \t\t Training Loss: 0.0005809076246805489 \t\n",
      "Epoch 14021 \t\t Training Loss: 0.000580907566472888 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14022 \t\t Training Loss: 0.000580907566472888 \t\n",
      "Epoch 14023 \t\t Training Loss: 0.000580907566472888 \t\n",
      "Epoch 14024 \t\t Training Loss: 0.000580907566472888 \t\n",
      "Epoch 14025 \t\t Training Loss: 0.000580907566472888 \t\n",
      "Epoch 14026 \t\t Training Loss: 0.000580907566472888 \t\n",
      "Epoch 14027 \t\t Training Loss: 0.000580907566472888 \t\n",
      "Epoch 14028 \t\t Training Loss: 0.000580907566472888 \t\n",
      "Epoch 14029 \t\t Training Loss: 0.000580907566472888 \t\n",
      "Epoch 14030 \t\t Training Loss: 0.000580907566472888 \t\n",
      "Epoch 14031 \t\t Training Loss: 0.000580907566472888 \t\n",
      "Epoch 14032 \t\t Training Loss: 0.000580907566472888 \t\n",
      "Epoch 14033 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14034 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14035 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14036 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14037 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14038 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14039 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14040 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14041 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14042 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14043 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14044 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14045 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14046 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14047 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14048 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14049 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14050 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14051 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14052 \t\t Training Loss: 0.0005809075082652271 \t\n",
      "Epoch 14053 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14054 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14055 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14056 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14057 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14058 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14059 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14060 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14061 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14062 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14063 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14064 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14065 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14066 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14067 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14068 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14069 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14070 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14071 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14072 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14073 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14074 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14075 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14076 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14077 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14078 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14079 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14080 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14081 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14082 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14083 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14084 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14085 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14086 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14087 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14088 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14089 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14090 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14091 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14092 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14093 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14094 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14095 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14096 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14097 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14098 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14099 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14100 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14101 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14102 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14103 \t\t Training Loss: 0.0005809074500575662 \t\n",
      "Epoch 14104 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14105 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14106 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14107 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14108 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14109 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14110 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14111 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14112 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14113 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14114 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14115 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14116 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14117 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14118 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14119 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14120 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14121 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14122 \t\t Training Loss: 0.0005809073336422443 \t\n",
      "Epoch 14123 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14124 \t\t Training Loss: 0.0005809072172269225 \t\n",
      "Epoch 14125 \t\t Training Loss: 0.0005809072172269225 \t\n",
      "Epoch 14126 \t\t Training Loss: 0.0005809072172269225 \t\n",
      "Epoch 14127 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14128 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14129 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14130 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14131 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14132 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14133 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14134 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14135 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14136 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14137 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14138 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14139 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14140 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14141 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14142 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14143 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14144 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14145 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14146 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14147 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14148 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14149 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14150 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14151 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14152 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14153 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14154 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14155 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14156 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14157 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14158 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14159 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14160 \t\t Training Loss: 0.0005809070426039398 \t\n",
      "Epoch 14161 \t\t Training Loss: 0.0005809070426039398 \t\n",
      "Epoch 14162 \t\t Training Loss: 0.0005809070426039398 \t\n",
      "Epoch 14163 \t\t Training Loss: 0.0005809070426039398 \t\n",
      "Epoch 14164 \t\t Training Loss: 0.0005809070426039398 \t\n",
      "Epoch 14165 \t\t Training Loss: 0.0005809071590192616 \t\n",
      "Epoch 14166 \t\t Training Loss: 0.0005809070426039398 \t\n",
      "Epoch 14167 \t\t Training Loss: 0.0005809070426039398 \t\n",
      "Epoch 14168 \t\t Training Loss: 0.0005809070426039398 \t\n",
      "Epoch 14169 \t\t Training Loss: 0.0005809070426039398 \t\n",
      "Epoch 14170 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14171 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14172 \t\t Training Loss: 0.0005809070426039398 \t\n",
      "Epoch 14173 \t\t Training Loss: 0.0005809069261886179 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14174 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14175 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14176 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14177 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14178 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14179 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14180 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14181 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14182 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14183 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14184 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14185 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14186 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14187 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14188 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14189 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14190 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14191 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14192 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14193 \t\t Training Loss: 0.0005809069261886179 \t\n",
      "Epoch 14194 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14195 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14196 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14197 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14198 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14199 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14200 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14201 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14202 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14203 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14204 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14205 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14206 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14207 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14208 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14209 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14210 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14211 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14212 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14213 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14214 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14215 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14216 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14217 \t\t Training Loss: 0.000580906867980957 \t\n",
      "Epoch 14218 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14219 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14220 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14221 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14222 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14223 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14224 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14225 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14226 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14227 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14228 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14229 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14230 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14231 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14232 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14233 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14234 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14235 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14236 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14237 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14238 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14239 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14240 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14241 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14242 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14243 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14244 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14245 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14246 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14247 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14248 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14249 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14250 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14251 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14252 \t\t Training Loss: 0.0005809067515656352 \t\n",
      "Epoch 14253 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14254 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14255 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14256 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14257 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14258 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14259 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14260 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14261 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14262 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14263 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14264 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14265 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14266 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14267 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14268 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14269 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14270 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14271 \t\t Training Loss: 0.0005809066351503134 \t\n",
      "Epoch 14272 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14273 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14274 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14275 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14276 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14277 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14278 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14279 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14280 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14281 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14282 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14283 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14284 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14285 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14286 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14287 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14288 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14289 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14290 \t\t Training Loss: 0.0005809063441120088 \t\n",
      "Epoch 14291 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14292 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14293 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14294 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14295 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14296 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14297 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14298 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14299 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14300 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14301 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14302 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14303 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14304 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14305 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14306 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14307 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14308 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14309 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14310 \t\t Training Loss: 0.0005809064605273306 \t\n",
      "Epoch 14311 \t\t Training Loss: 0.0005809063441120088 \t\n",
      "Epoch 14312 \t\t Training Loss: 0.0005809063441120088 \t\n",
      "Epoch 14313 \t\t Training Loss: 0.0005809063441120088 \t\n",
      "Epoch 14314 \t\t Training Loss: 0.0005809063441120088 \t\n",
      "Epoch 14315 \t\t Training Loss: 0.0005809063441120088 \t\n",
      "Epoch 14316 \t\t Training Loss: 0.0005809063441120088 \t\n",
      "Epoch 14317 \t\t Training Loss: 0.0005809063441120088 \t\n",
      "Epoch 14318 \t\t Training Loss: 0.0005809063441120088 \t\n",
      "Epoch 14319 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14320 \t\t Training Loss: 0.0005809063441120088 \t\n",
      "Epoch 14321 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14322 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14323 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14324 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14325 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14326 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14327 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14328 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14329 \t\t Training Loss: 0.000580906227696687 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14330 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14331 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14332 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14333 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14334 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14335 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14336 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14337 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14338 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14339 \t\t Training Loss: 0.000580906227696687 \t\n",
      "Epoch 14340 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14341 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14342 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14343 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14344 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14345 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14346 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14347 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14348 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14349 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14350 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14351 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14352 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14353 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14354 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14355 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14356 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14357 \t\t Training Loss: 0.0005809061694890261 \t\n",
      "Epoch 14358 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14359 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14360 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14361 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14362 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14363 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14364 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14365 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14366 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14367 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14368 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14369 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14370 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14371 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14372 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14373 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14374 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14375 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14376 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14377 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14378 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14379 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14380 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14381 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14382 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14383 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14384 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14385 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14386 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14387 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14388 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14389 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14390 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14391 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14392 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14393 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14394 \t\t Training Loss: 0.0005809059366583824 \t\n",
      "Epoch 14395 \t\t Training Loss: 0.0005809059366583824 \t\n",
      "Epoch 14396 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14397 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14398 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14399 \t\t Training Loss: 0.0005809060530737042 \t\n",
      "Epoch 14400 \t\t Training Loss: 0.0005809059366583824 \t\n",
      "Epoch 14401 \t\t Training Loss: 0.0005809059366583824 \t\n",
      "Epoch 14402 \t\t Training Loss: 0.0005809059366583824 \t\n",
      "Epoch 14403 \t\t Training Loss: 0.0005809059366583824 \t\n",
      "Epoch 14404 \t\t Training Loss: 0.0005809059366583824 \t\n",
      "Epoch 14405 \t\t Training Loss: 0.0005809059366583824 \t\n",
      "Epoch 14406 \t\t Training Loss: 0.0005809059366583824 \t\n",
      "Epoch 14407 \t\t Training Loss: 0.0005809059366583824 \t\n",
      "Epoch 14408 \t\t Training Loss: 0.0005809059366583824 \t\n",
      "Epoch 14409 \t\t Training Loss: 0.0005809059366583824 \t\n",
      "Epoch 14410 \t\t Training Loss: 0.0005809059366583824 \t\n",
      "Epoch 14411 \t\t Training Loss: 0.0005809059366583824 \t\n",
      "Epoch 14412 \t\t Training Loss: 0.0005809059366583824 \t\n",
      "Epoch 14413 \t\t Training Loss: 0.0005809059366583824 \t\n",
      "Epoch 14414 \t\t Training Loss: 0.0005809058784507215 \t\n",
      "Epoch 14415 \t\t Training Loss: 0.0005809058784507215 \t\n",
      "Epoch 14416 \t\t Training Loss: 0.0005809058784507215 \t\n",
      "Epoch 14417 \t\t Training Loss: 0.0005809058784507215 \t\n",
      "Epoch 14418 \t\t Training Loss: 0.0005809058784507215 \t\n",
      "Epoch 14419 \t\t Training Loss: 0.0005809058784507215 \t\n",
      "Epoch 14420 \t\t Training Loss: 0.0005809058784507215 \t\n",
      "Epoch 14421 \t\t Training Loss: 0.0005809058784507215 \t\n",
      "Epoch 14422 \t\t Training Loss: 0.0005809058202430606 \t\n",
      "Epoch 14423 \t\t Training Loss: 0.0005809058202430606 \t\n",
      "Epoch 14424 \t\t Training Loss: 0.0005809058202430606 \t\n",
      "Epoch 14425 \t\t Training Loss: 0.0005809058202430606 \t\n",
      "Epoch 14426 \t\t Training Loss: 0.0005809058202430606 \t\n",
      "Epoch 14427 \t\t Training Loss: 0.0005809058202430606 \t\n",
      "Epoch 14428 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14429 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14430 \t\t Training Loss: 0.0005809058202430606 \t\n",
      "Epoch 14431 \t\t Training Loss: 0.0005809058202430606 \t\n",
      "Epoch 14432 \t\t Training Loss: 0.0005809058202430606 \t\n",
      "Epoch 14433 \t\t Training Loss: 0.0005809058202430606 \t\n",
      "Epoch 14434 \t\t Training Loss: 0.0005809058784507215 \t\n",
      "Epoch 14435 \t\t Training Loss: 0.0005809058202430606 \t\n",
      "Epoch 14436 \t\t Training Loss: 0.0005809058784507215 \t\n",
      "Epoch 14437 \t\t Training Loss: 0.0005809058784507215 \t\n",
      "Epoch 14438 \t\t Training Loss: 0.0005809058202430606 \t\n",
      "Epoch 14439 \t\t Training Loss: 0.0005809058202430606 \t\n",
      "Epoch 14440 \t\t Training Loss: 0.0005809058202430606 \t\n",
      "Epoch 14441 \t\t Training Loss: 0.0005809058202430606 \t\n",
      "Epoch 14442 \t\t Training Loss: 0.0005809058202430606 \t\n",
      "Epoch 14443 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14444 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14445 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14446 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14447 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14448 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14449 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14450 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14451 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14452 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14453 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14454 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14455 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14456 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14457 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14458 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14459 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14460 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14461 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14462 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14463 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14464 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14465 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14466 \t\t Training Loss: 0.0005809057620353997 \t\n",
      "Epoch 14467 \t\t Training Loss: 0.0005809056456200778 \t\n",
      "Epoch 14468 \t\t Training Loss: 0.0005809056456200778 \t\n",
      "Epoch 14469 \t\t Training Loss: 0.0005809056456200778 \t\n",
      "Epoch 14470 \t\t Training Loss: 0.0005809055874124169 \t\n",
      "Epoch 14471 \t\t Training Loss: 0.0005809055874124169 \t\n",
      "Epoch 14472 \t\t Training Loss: 0.0005809055874124169 \t\n",
      "Epoch 14473 \t\t Training Loss: 0.0005809055874124169 \t\n",
      "Epoch 14474 \t\t Training Loss: 0.0005809055874124169 \t\n",
      "Epoch 14475 \t\t Training Loss: 0.0005809055874124169 \t\n",
      "Epoch 14476 \t\t Training Loss: 0.0005809055874124169 \t\n",
      "Epoch 14477 \t\t Training Loss: 0.000580905529204756 \t\n",
      "Epoch 14478 \t\t Training Loss: 0.000580905529204756 \t\n",
      "Epoch 14479 \t\t Training Loss: 0.000580905529204756 \t\n",
      "Epoch 14480 \t\t Training Loss: 0.000580905529204756 \t\n",
      "Epoch 14481 \t\t Training Loss: 0.000580905529204756 \t\n",
      "Epoch 14482 \t\t Training Loss: 0.000580905529204756 \t\n",
      "Epoch 14483 \t\t Training Loss: 0.000580905529204756 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14484 \t\t Training Loss: 0.000580905529204756 \t\n",
      "Epoch 14485 \t\t Training Loss: 0.000580905529204756 \t\n",
      "Epoch 14486 \t\t Training Loss: 0.0005809054709970951 \t\n",
      "Epoch 14487 \t\t Training Loss: 0.000580905529204756 \t\n",
      "Epoch 14488 \t\t Training Loss: 0.000580905529204756 \t\n",
      "Epoch 14489 \t\t Training Loss: 0.000580905529204756 \t\n",
      "Epoch 14490 \t\t Training Loss: 0.0005809054709970951 \t\n",
      "Epoch 14491 \t\t Training Loss: 0.0005809054709970951 \t\n",
      "Epoch 14492 \t\t Training Loss: 0.0005809054709970951 \t\n",
      "Epoch 14493 \t\t Training Loss: 0.0005809054709970951 \t\n",
      "Epoch 14494 \t\t Training Loss: 0.0005809054709970951 \t\n",
      "Epoch 14495 \t\t Training Loss: 0.0005809053545817733 \t\n",
      "Epoch 14496 \t\t Training Loss: 0.0005809054709970951 \t\n",
      "Epoch 14497 \t\t Training Loss: 0.0005809053545817733 \t\n",
      "Epoch 14498 \t\t Training Loss: 0.0005809053545817733 \t\n",
      "Epoch 14499 \t\t Training Loss: 0.0005809053545817733 \t\n",
      "Epoch 14500 \t\t Training Loss: 0.0005809053545817733 \t\n",
      "Epoch 14501 \t\t Training Loss: 0.0005809053545817733 \t\n",
      "Epoch 14502 \t\t Training Loss: 0.0005809053545817733 \t\n",
      "Epoch 14503 \t\t Training Loss: 0.0005809053545817733 \t\n",
      "Epoch 14504 \t\t Training Loss: 0.0005809053545817733 \t\n",
      "Epoch 14505 \t\t Training Loss: 0.0005809053545817733 \t\n",
      "Epoch 14506 \t\t Training Loss: 0.0005809053545817733 \t\n",
      "Epoch 14507 \t\t Training Loss: 0.0005809053545817733 \t\n",
      "Epoch 14508 \t\t Training Loss: 0.0005809053545817733 \t\n",
      "Epoch 14509 \t\t Training Loss: 0.0005809053545817733 \t\n",
      "Epoch 14510 \t\t Training Loss: 0.0005809053545817733 \t\n",
      "Epoch 14511 \t\t Training Loss: 0.0005809052963741124 \t\n",
      "Epoch 14512 \t\t Training Loss: 0.0005809052963741124 \t\n",
      "Epoch 14513 \t\t Training Loss: 0.0005809052963741124 \t\n",
      "Epoch 14514 \t\t Training Loss: 0.0005809052963741124 \t\n",
      "Epoch 14515 \t\t Training Loss: 0.0005809052963741124 \t\n",
      "Epoch 14516 \t\t Training Loss: 0.0005809052963741124 \t\n",
      "Epoch 14517 \t\t Training Loss: 0.0005809052963741124 \t\n",
      "Epoch 14518 \t\t Training Loss: 0.0005809052381664515 \t\n",
      "Epoch 14519 \t\t Training Loss: 0.0005809052381664515 \t\n",
      "Epoch 14520 \t\t Training Loss: 0.0005809052381664515 \t\n",
      "Epoch 14521 \t\t Training Loss: 0.0005809052381664515 \t\n",
      "Epoch 14522 \t\t Training Loss: 0.0005809052381664515 \t\n",
      "Epoch 14523 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14524 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14525 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14526 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14527 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14528 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14529 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14530 \t\t Training Loss: 0.0005809051217511296 \t\n",
      "Epoch 14531 \t\t Training Loss: 0.0005809051217511296 \t\n",
      "Epoch 14532 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14533 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14534 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14535 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14536 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14537 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14538 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14539 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14540 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14541 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14542 \t\t Training Loss: 0.0005809051799587905 \t\n",
      "Epoch 14543 \t\t Training Loss: 0.0005809051217511296 \t\n",
      "Epoch 14544 \t\t Training Loss: 0.0005809051217511296 \t\n",
      "Epoch 14545 \t\t Training Loss: 0.0005809051217511296 \t\n",
      "Epoch 14546 \t\t Training Loss: 0.0005809051217511296 \t\n",
      "Epoch 14547 \t\t Training Loss: 0.0005809051217511296 \t\n",
      "Epoch 14548 \t\t Training Loss: 0.0005809051217511296 \t\n",
      "Epoch 14549 \t\t Training Loss: 0.0005809051217511296 \t\n",
      "Epoch 14550 \t\t Training Loss: 0.0005809051217511296 \t\n",
      "Epoch 14551 \t\t Training Loss: 0.0005809051217511296 \t\n",
      "Epoch 14552 \t\t Training Loss: 0.0005809051217511296 \t\n",
      "Epoch 14553 \t\t Training Loss: 0.0005809051217511296 \t\n",
      "Epoch 14554 \t\t Training Loss: 0.0005809050635434687 \t\n",
      "Epoch 14555 \t\t Training Loss: 0.0005809050053358078 \t\n",
      "Epoch 14556 \t\t Training Loss: 0.0005809050053358078 \t\n",
      "Epoch 14557 \t\t Training Loss: 0.0005809050053358078 \t\n",
      "Epoch 14558 \t\t Training Loss: 0.0005809050053358078 \t\n",
      "Epoch 14559 \t\t Training Loss: 0.0005809050053358078 \t\n",
      "Epoch 14560 \t\t Training Loss: 0.0005809050053358078 \t\n",
      "Epoch 14561 \t\t Training Loss: 0.0005809050053358078 \t\n",
      "Epoch 14562 \t\t Training Loss: 0.0005809050053358078 \t\n",
      "Epoch 14563 \t\t Training Loss: 0.0005809050053358078 \t\n",
      "Epoch 14564 \t\t Training Loss: 0.0005809050053358078 \t\n",
      "Epoch 14565 \t\t Training Loss: 0.0005809050053358078 \t\n",
      "Epoch 14566 \t\t Training Loss: 0.0005809050053358078 \t\n",
      "Epoch 14567 \t\t Training Loss: 0.0005809050053358078 \t\n",
      "Epoch 14568 \t\t Training Loss: 0.0005809049471281469 \t\n",
      "Epoch 14569 \t\t Training Loss: 0.0005809049471281469 \t\n",
      "Epoch 14570 \t\t Training Loss: 0.0005809049471281469 \t\n",
      "Epoch 14571 \t\t Training Loss: 0.0005809049471281469 \t\n",
      "Epoch 14572 \t\t Training Loss: 0.0005809049471281469 \t\n",
      "Epoch 14573 \t\t Training Loss: 0.0005809049471281469 \t\n",
      "Epoch 14574 \t\t Training Loss: 0.000580904888920486 \t\n",
      "Epoch 14575 \t\t Training Loss: 0.0005809049471281469 \t\n",
      "Epoch 14576 \t\t Training Loss: 0.000580904888920486 \t\n",
      "Epoch 14577 \t\t Training Loss: 0.000580904888920486 \t\n",
      "Epoch 14578 \t\t Training Loss: 0.000580904888920486 \t\n",
      "Epoch 14579 \t\t Training Loss: 0.000580904888920486 \t\n",
      "Epoch 14580 \t\t Training Loss: 0.000580904888920486 \t\n",
      "Epoch 14581 \t\t Training Loss: 0.000580904888920486 \t\n",
      "Epoch 14582 \t\t Training Loss: 0.000580904888920486 \t\n",
      "Epoch 14583 \t\t Training Loss: 0.000580904888920486 \t\n",
      "Epoch 14584 \t\t Training Loss: 0.000580904888920486 \t\n",
      "Epoch 14585 \t\t Training Loss: 0.000580904888920486 \t\n",
      "Epoch 14586 \t\t Training Loss: 0.000580904888920486 \t\n",
      "Epoch 14587 \t\t Training Loss: 0.000580904888920486 \t\n",
      "Epoch 14588 \t\t Training Loss: 0.0005809048307128251 \t\n",
      "Epoch 14589 \t\t Training Loss: 0.0005809048307128251 \t\n",
      "Epoch 14590 \t\t Training Loss: 0.0005809047725051641 \t\n",
      "Epoch 14591 \t\t Training Loss: 0.0005809047725051641 \t\n",
      "Epoch 14592 \t\t Training Loss: 0.0005809048307128251 \t\n",
      "Epoch 14593 \t\t Training Loss: 0.0005809047725051641 \t\n",
      "Epoch 14594 \t\t Training Loss: 0.0005809047725051641 \t\n",
      "Epoch 14595 \t\t Training Loss: 0.0005809047725051641 \t\n",
      "Epoch 14596 \t\t Training Loss: 0.0005809047725051641 \t\n",
      "Epoch 14597 \t\t Training Loss: 0.0005809047725051641 \t\n",
      "Epoch 14598 \t\t Training Loss: 0.0005809047725051641 \t\n",
      "Epoch 14599 \t\t Training Loss: 0.0005809047725051641 \t\n",
      "Epoch 14600 \t\t Training Loss: 0.0005809047142975032 \t\n",
      "Epoch 14601 \t\t Training Loss: 0.0005809047725051641 \t\n",
      "Epoch 14602 \t\t Training Loss: 0.0005809047725051641 \t\n",
      "Epoch 14603 \t\t Training Loss: 0.0005809047142975032 \t\n",
      "Epoch 14604 \t\t Training Loss: 0.0005809047725051641 \t\n",
      "Epoch 14605 \t\t Training Loss: 0.0005809047725051641 \t\n",
      "Epoch 14606 \t\t Training Loss: 0.0005809047142975032 \t\n",
      "Epoch 14607 \t\t Training Loss: 0.0005809047142975032 \t\n",
      "Epoch 14608 \t\t Training Loss: 0.0005809047142975032 \t\n",
      "Epoch 14609 \t\t Training Loss: 0.0005809047142975032 \t\n",
      "Epoch 14610 \t\t Training Loss: 0.0005809047142975032 \t\n",
      "Epoch 14611 \t\t Training Loss: 0.0005809047142975032 \t\n",
      "Epoch 14612 \t\t Training Loss: 0.0005809047142975032 \t\n",
      "Epoch 14613 \t\t Training Loss: 0.0005809047142975032 \t\n",
      "Epoch 14614 \t\t Training Loss: 0.0005809047142975032 \t\n",
      "Epoch 14615 \t\t Training Loss: 0.0005809047142975032 \t\n",
      "Epoch 14616 \t\t Training Loss: 0.0005809047142975032 \t\n",
      "Epoch 14617 \t\t Training Loss: 0.0005809046560898423 \t\n",
      "Epoch 14618 \t\t Training Loss: 0.0005809046560898423 \t\n",
      "Epoch 14619 \t\t Training Loss: 0.0005809046560898423 \t\n",
      "Epoch 14620 \t\t Training Loss: 0.0005809046560898423 \t\n",
      "Epoch 14621 \t\t Training Loss: 0.0005809046560898423 \t\n",
      "Epoch 14622 \t\t Training Loss: 0.0005809046560898423 \t\n",
      "Epoch 14623 \t\t Training Loss: 0.0005809046560898423 \t\n",
      "Epoch 14624 \t\t Training Loss: 0.0005809045978821814 \t\n",
      "Epoch 14625 \t\t Training Loss: 0.0005809045978821814 \t\n",
      "Epoch 14626 \t\t Training Loss: 0.0005809045978821814 \t\n",
      "Epoch 14627 \t\t Training Loss: 0.0005809045978821814 \t\n",
      "Epoch 14628 \t\t Training Loss: 0.0005809045978821814 \t\n",
      "Epoch 14629 \t\t Training Loss: 0.0005809045978821814 \t\n",
      "Epoch 14630 \t\t Training Loss: 0.0005809045978821814 \t\n",
      "Epoch 14631 \t\t Training Loss: 0.0005809045978821814 \t\n",
      "Epoch 14632 \t\t Training Loss: 0.0005809045978821814 \t\n",
      "Epoch 14633 \t\t Training Loss: 0.0005809045978821814 \t\n",
      "Epoch 14634 \t\t Training Loss: 0.0005809045978821814 \t\n",
      "Epoch 14635 \t\t Training Loss: 0.0005809045978821814 \t\n",
      "Epoch 14636 \t\t Training Loss: 0.0005809045978821814 \t\n",
      "Epoch 14637 \t\t Training Loss: 0.0005809045978821814 \t\n",
      "Epoch 14638 \t\t Training Loss: 0.0005809045978821814 \t\n",
      "Epoch 14639 \t\t Training Loss: 0.0005809045396745205 \t\n",
      "Epoch 14640 \t\t Training Loss: 0.0005809045396745205 \t\n",
      "Epoch 14641 \t\t Training Loss: 0.0005809045396745205 \t\n",
      "Epoch 14642 \t\t Training Loss: 0.0005809045396745205 \t\n",
      "Epoch 14643 \t\t Training Loss: 0.0005809045396745205 \t\n",
      "Epoch 14644 \t\t Training Loss: 0.0005809045396745205 \t\n",
      "Epoch 14645 \t\t Training Loss: 0.0005809045396745205 \t\n",
      "Epoch 14646 \t\t Training Loss: 0.0005809045396745205 \t\n",
      "Epoch 14647 \t\t Training Loss: 0.0005809045396745205 \t\n",
      "Epoch 14648 \t\t Training Loss: 0.0005809045396745205 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14649 \t\t Training Loss: 0.0005809045396745205 \t\n",
      "Epoch 14650 \t\t Training Loss: 0.0005809045396745205 \t\n",
      "Epoch 14651 \t\t Training Loss: 0.0005809044814668596 \t\n",
      "Epoch 14652 \t\t Training Loss: 0.0005809044814668596 \t\n",
      "Epoch 14653 \t\t Training Loss: 0.0005809044814668596 \t\n",
      "Epoch 14654 \t\t Training Loss: 0.0005809045396745205 \t\n",
      "Epoch 14655 \t\t Training Loss: 0.0005809045396745205 \t\n",
      "Epoch 14656 \t\t Training Loss: 0.0005809045396745205 \t\n",
      "Epoch 14657 \t\t Training Loss: 0.0005809044814668596 \t\n",
      "Epoch 14658 \t\t Training Loss: 0.0005809044814668596 \t\n",
      "Epoch 14659 \t\t Training Loss: 0.0005809044814668596 \t\n",
      "Epoch 14660 \t\t Training Loss: 0.0005809044814668596 \t\n",
      "Epoch 14661 \t\t Training Loss: 0.0005809044814668596 \t\n",
      "Epoch 14662 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14663 \t\t Training Loss: 0.0005809044814668596 \t\n",
      "Epoch 14664 \t\t Training Loss: 0.0005809044814668596 \t\n",
      "Epoch 14665 \t\t Training Loss: 0.0005809044814668596 \t\n",
      "Epoch 14666 \t\t Training Loss: 0.0005809044814668596 \t\n",
      "Epoch 14667 \t\t Training Loss: 0.0005809044814668596 \t\n",
      "Epoch 14668 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14669 \t\t Training Loss: 0.0005809044814668596 \t\n",
      "Epoch 14670 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14671 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14672 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14673 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14674 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14675 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14676 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14677 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14678 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14679 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14680 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14681 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14682 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14683 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14684 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14685 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14686 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14687 \t\t Training Loss: 0.0005809043068438768 \t\n",
      "Epoch 14688 \t\t Training Loss: 0.0005809043068438768 \t\n",
      "Epoch 14689 \t\t Training Loss: 0.0005809043068438768 \t\n",
      "Epoch 14690 \t\t Training Loss: 0.0005809043068438768 \t\n",
      "Epoch 14691 \t\t Training Loss: 0.0005809043650515378 \t\n",
      "Epoch 14692 \t\t Training Loss: 0.0005809043068438768 \t\n",
      "Epoch 14693 \t\t Training Loss: 0.0005809043068438768 \t\n",
      "Epoch 14694 \t\t Training Loss: 0.000580904190428555 \t\n",
      "Epoch 14695 \t\t Training Loss: 0.000580904190428555 \t\n",
      "Epoch 14696 \t\t Training Loss: 0.000580904190428555 \t\n",
      "Epoch 14697 \t\t Training Loss: 0.000580904190428555 \t\n",
      "Epoch 14698 \t\t Training Loss: 0.000580904190428555 \t\n",
      "Epoch 14699 \t\t Training Loss: 0.000580904190428555 \t\n",
      "Epoch 14700 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14701 \t\t Training Loss: 0.000580904190428555 \t\n",
      "Epoch 14702 \t\t Training Loss: 0.000580904190428555 \t\n",
      "Epoch 14703 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14704 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14705 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14706 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14707 \t\t Training Loss: 0.0005809040740132332 \t\n",
      "Epoch 14708 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14709 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14710 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14711 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14712 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14713 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14714 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14715 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14716 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14717 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14718 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14719 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14720 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14721 \t\t Training Loss: 0.0005809040740132332 \t\n",
      "Epoch 14722 \t\t Training Loss: 0.0005809040740132332 \t\n",
      "Epoch 14723 \t\t Training Loss: 0.0005809040740132332 \t\n",
      "Epoch 14724 \t\t Training Loss: 0.0005809040740132332 \t\n",
      "Epoch 14725 \t\t Training Loss: 0.0005809040740132332 \t\n",
      "Epoch 14726 \t\t Training Loss: 0.0005809041322208941 \t\n",
      "Epoch 14727 \t\t Training Loss: 0.0005809040740132332 \t\n",
      "Epoch 14728 \t\t Training Loss: 0.0005809040740132332 \t\n",
      "Epoch 14729 \t\t Training Loss: 0.0005809040740132332 \t\n",
      "Epoch 14730 \t\t Training Loss: 0.0005809040158055723 \t\n",
      "Epoch 14731 \t\t Training Loss: 0.0005809040158055723 \t\n",
      "Epoch 14732 \t\t Training Loss: 0.0005809040158055723 \t\n",
      "Epoch 14733 \t\t Training Loss: 0.0005809040158055723 \t\n",
      "Epoch 14734 \t\t Training Loss: 0.0005809040158055723 \t\n",
      "Epoch 14735 \t\t Training Loss: 0.0005809040158055723 \t\n",
      "Epoch 14736 \t\t Training Loss: 0.0005809040158055723 \t\n",
      "Epoch 14737 \t\t Training Loss: 0.0005809040158055723 \t\n",
      "Epoch 14738 \t\t Training Loss: 0.0005809040158055723 \t\n",
      "Epoch 14739 \t\t Training Loss: 0.0005809040158055723 \t\n",
      "Epoch 14740 \t\t Training Loss: 0.0005809040158055723 \t\n",
      "Epoch 14741 \t\t Training Loss: 0.0005809040158055723 \t\n",
      "Epoch 14742 \t\t Training Loss: 0.0005809040158055723 \t\n",
      "Epoch 14743 \t\t Training Loss: 0.0005809040158055723 \t\n",
      "Epoch 14744 \t\t Training Loss: 0.0005809039575979114 \t\n",
      "Epoch 14745 \t\t Training Loss: 0.0005809039575979114 \t\n",
      "Epoch 14746 \t\t Training Loss: 0.0005809039575979114 \t\n",
      "Epoch 14747 \t\t Training Loss: 0.0005809039575979114 \t\n",
      "Epoch 14748 \t\t Training Loss: 0.0005809039575979114 \t\n",
      "Epoch 14749 \t\t Training Loss: 0.0005809039575979114 \t\n",
      "Epoch 14750 \t\t Training Loss: 0.0005809039575979114 \t\n",
      "Epoch 14751 \t\t Training Loss: 0.0005809039575979114 \t\n",
      "Epoch 14752 \t\t Training Loss: 0.0005809039575979114 \t\n",
      "Epoch 14753 \t\t Training Loss: 0.0005809039575979114 \t\n",
      "Epoch 14754 \t\t Training Loss: 0.0005809039575979114 \t\n",
      "Epoch 14755 \t\t Training Loss: 0.0005809039575979114 \t\n",
      "Epoch 14756 \t\t Training Loss: 0.0005809039575979114 \t\n",
      "Epoch 14757 \t\t Training Loss: 0.0005809039575979114 \t\n",
      "Epoch 14758 \t\t Training Loss: 0.0005809039575979114 \t\n",
      "Epoch 14759 \t\t Training Loss: 0.0005809039575979114 \t\n",
      "Epoch 14760 \t\t Training Loss: 0.0005809038993902504 \t\n",
      "Epoch 14761 \t\t Training Loss: 0.0005809038993902504 \t\n",
      "Epoch 14762 \t\t Training Loss: 0.0005809038993902504 \t\n",
      "Epoch 14763 \t\t Training Loss: 0.0005809038993902504 \t\n",
      "Epoch 14764 \t\t Training Loss: 0.0005809038993902504 \t\n",
      "Epoch 14765 \t\t Training Loss: 0.0005809038993902504 \t\n",
      "Epoch 14766 \t\t Training Loss: 0.0005809038411825895 \t\n",
      "Epoch 14767 \t\t Training Loss: 0.0005809038411825895 \t\n",
      "Epoch 14768 \t\t Training Loss: 0.0005809038411825895 \t\n",
      "Epoch 14769 \t\t Training Loss: 0.0005809038411825895 \t\n",
      "Epoch 14770 \t\t Training Loss: 0.0005809038411825895 \t\n",
      "Epoch 14771 \t\t Training Loss: 0.0005809038411825895 \t\n",
      "Epoch 14772 \t\t Training Loss: 0.0005809038411825895 \t\n",
      "Epoch 14773 \t\t Training Loss: 0.0005809038411825895 \t\n",
      "Epoch 14774 \t\t Training Loss: 0.0005809038411825895 \t\n",
      "Epoch 14775 \t\t Training Loss: 0.0005809038411825895 \t\n",
      "Epoch 14776 \t\t Training Loss: 0.0005809038411825895 \t\n",
      "Epoch 14777 \t\t Training Loss: 0.0005809038411825895 \t\n",
      "Epoch 14778 \t\t Training Loss: 0.0005809037829749286 \t\n",
      "Epoch 14779 \t\t Training Loss: 0.0005809038411825895 \t\n",
      "Epoch 14780 \t\t Training Loss: 0.0005809037829749286 \t\n",
      "Epoch 14781 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14782 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14783 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14784 \t\t Training Loss: 0.0005809037829749286 \t\n",
      "Epoch 14785 \t\t Training Loss: 0.0005809037829749286 \t\n",
      "Epoch 14786 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14787 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14788 \t\t Training Loss: 0.0005809037829749286 \t\n",
      "Epoch 14789 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14790 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14791 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14792 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14793 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14794 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14795 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14796 \t\t Training Loss: 0.0005809036665596068 \t\n",
      "Epoch 14797 \t\t Training Loss: 0.0005809036665596068 \t\n",
      "Epoch 14798 \t\t Training Loss: 0.0005809036665596068 \t\n",
      "Epoch 14799 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14800 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14801 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14802 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14803 \t\t Training Loss: 0.0005809036665596068 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14804 \t\t Training Loss: 0.0005809036665596068 \t\n",
      "Epoch 14805 \t\t Training Loss: 0.0005809036665596068 \t\n",
      "Epoch 14806 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14807 \t\t Training Loss: 0.0005809036665596068 \t\n",
      "Epoch 14808 \t\t Training Loss: 0.0005809036665596068 \t\n",
      "Epoch 14809 \t\t Training Loss: 0.0005809036665596068 \t\n",
      "Epoch 14810 \t\t Training Loss: 0.0005809036665596068 \t\n",
      "Epoch 14811 \t\t Training Loss: 0.0005809036665596068 \t\n",
      "Epoch 14812 \t\t Training Loss: 0.0005809037247672677 \t\n",
      "Epoch 14813 \t\t Training Loss: 0.0005809036665596068 \t\n",
      "Epoch 14814 \t\t Training Loss: 0.0005809036665596068 \t\n",
      "Epoch 14815 \t\t Training Loss: 0.0005809036665596068 \t\n",
      "Epoch 14816 \t\t Training Loss: 0.0005809036665596068 \t\n",
      "Epoch 14817 \t\t Training Loss: 0.0005809036083519459 \t\n",
      "Epoch 14818 \t\t Training Loss: 0.0005809036083519459 \t\n",
      "Epoch 14819 \t\t Training Loss: 0.0005809036083519459 \t\n",
      "Epoch 14820 \t\t Training Loss: 0.0005809036083519459 \t\n",
      "Epoch 14821 \t\t Training Loss: 0.0005809036083519459 \t\n",
      "Epoch 14822 \t\t Training Loss: 0.0005809036083519459 \t\n",
      "Epoch 14823 \t\t Training Loss: 0.0005809036083519459 \t\n",
      "Epoch 14824 \t\t Training Loss: 0.0005809036083519459 \t\n",
      "Epoch 14825 \t\t Training Loss: 0.0005809036083519459 \t\n",
      "Epoch 14826 \t\t Training Loss: 0.000580903550144285 \t\n",
      "Epoch 14827 \t\t Training Loss: 0.000580903550144285 \t\n",
      "Epoch 14828 \t\t Training Loss: 0.000580903550144285 \t\n",
      "Epoch 14829 \t\t Training Loss: 0.000580903491936624 \t\n",
      "Epoch 14830 \t\t Training Loss: 0.000580903491936624 \t\n",
      "Epoch 14831 \t\t Training Loss: 0.000580903491936624 \t\n",
      "Epoch 14832 \t\t Training Loss: 0.0005809034337289631 \t\n",
      "Epoch 14833 \t\t Training Loss: 0.000580903491936624 \t\n",
      "Epoch 14834 \t\t Training Loss: 0.000580903491936624 \t\n",
      "Epoch 14835 \t\t Training Loss: 0.000580903491936624 \t\n",
      "Epoch 14836 \t\t Training Loss: 0.000580903491936624 \t\n",
      "Epoch 14837 \t\t Training Loss: 0.000580903491936624 \t\n",
      "Epoch 14838 \t\t Training Loss: 0.0005809034337289631 \t\n",
      "Epoch 14839 \t\t Training Loss: 0.0005809034337289631 \t\n",
      "Epoch 14840 \t\t Training Loss: 0.0005809033755213022 \t\n",
      "Epoch 14841 \t\t Training Loss: 0.0005809034337289631 \t\n",
      "Epoch 14842 \t\t Training Loss: 0.0005809034337289631 \t\n",
      "Epoch 14843 \t\t Training Loss: 0.0005809034337289631 \t\n",
      "Epoch 14844 \t\t Training Loss: 0.0005809034337289631 \t\n",
      "Epoch 14845 \t\t Training Loss: 0.0005809034337289631 \t\n",
      "Epoch 14846 \t\t Training Loss: 0.0005809033755213022 \t\n",
      "Epoch 14847 \t\t Training Loss: 0.0005809033755213022 \t\n",
      "Epoch 14848 \t\t Training Loss: 0.0005809034337289631 \t\n",
      "Epoch 14849 \t\t Training Loss: 0.0005809033173136413 \t\n",
      "Epoch 14850 \t\t Training Loss: 0.0005809033173136413 \t\n",
      "Epoch 14851 \t\t Training Loss: 0.0005809033173136413 \t\n",
      "Epoch 14852 \t\t Training Loss: 0.0005809033173136413 \t\n",
      "Epoch 14853 \t\t Training Loss: 0.0005809033173136413 \t\n",
      "Epoch 14854 \t\t Training Loss: 0.0005809033173136413 \t\n",
      "Epoch 14855 \t\t Training Loss: 0.0005809033173136413 \t\n",
      "Epoch 14856 \t\t Training Loss: 0.0005809033173136413 \t\n",
      "Epoch 14857 \t\t Training Loss: 0.0005809033173136413 \t\n",
      "Epoch 14858 \t\t Training Loss: 0.0005809033173136413 \t\n",
      "Epoch 14859 \t\t Training Loss: 0.0005809033173136413 \t\n",
      "Epoch 14860 \t\t Training Loss: 0.0005809033173136413 \t\n",
      "Epoch 14861 \t\t Training Loss: 0.0005809032591059804 \t\n",
      "Epoch 14862 \t\t Training Loss: 0.0005809032591059804 \t\n",
      "Epoch 14863 \t\t Training Loss: 0.0005809032591059804 \t\n",
      "Epoch 14864 \t\t Training Loss: 0.0005809032591059804 \t\n",
      "Epoch 14865 \t\t Training Loss: 0.0005809031426906586 \t\n",
      "Epoch 14866 \t\t Training Loss: 0.0005809031426906586 \t\n",
      "Epoch 14867 \t\t Training Loss: 0.0005809031426906586 \t\n",
      "Epoch 14868 \t\t Training Loss: 0.0005809031426906586 \t\n",
      "Epoch 14869 \t\t Training Loss: 0.0005809031426906586 \t\n",
      "Epoch 14870 \t\t Training Loss: 0.0005809031426906586 \t\n",
      "Epoch 14871 \t\t Training Loss: 0.0005809031426906586 \t\n",
      "Epoch 14872 \t\t Training Loss: 0.0005809031426906586 \t\n",
      "Epoch 14873 \t\t Training Loss: 0.0005809031426906586 \t\n",
      "Epoch 14874 \t\t Training Loss: 0.0005809031426906586 \t\n",
      "Epoch 14875 \t\t Training Loss: 0.0005809031426906586 \t\n",
      "Epoch 14876 \t\t Training Loss: 0.0005809031426906586 \t\n",
      "Epoch 14877 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14878 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14879 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14880 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14881 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14882 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14883 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14884 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14885 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14886 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14887 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14888 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14889 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14890 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14891 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14892 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14893 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14894 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14895 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14896 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14897 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14898 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14899 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14900 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14901 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14902 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14903 \t\t Training Loss: 0.0005809029680676758 \t\n",
      "Epoch 14904 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14905 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14906 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14907 \t\t Training Loss: 0.0005809030262753367 \t\n",
      "Epoch 14908 \t\t Training Loss: 0.0005809029680676758 \t\n",
      "Epoch 14909 \t\t Training Loss: 0.0005809029098600149 \t\n",
      "Epoch 14910 \t\t Training Loss: 0.0005809029098600149 \t\n",
      "Epoch 14911 \t\t Training Loss: 0.0005809029098600149 \t\n",
      "Epoch 14912 \t\t Training Loss: 0.0005809029098600149 \t\n",
      "Epoch 14913 \t\t Training Loss: 0.0005809029098600149 \t\n",
      "Epoch 14914 \t\t Training Loss: 0.0005809029098600149 \t\n",
      "Epoch 14915 \t\t Training Loss: 0.0005809029098600149 \t\n",
      "Epoch 14916 \t\t Training Loss: 0.0005809029098600149 \t\n",
      "Epoch 14917 \t\t Training Loss: 0.0005809029098600149 \t\n",
      "Epoch 14918 \t\t Training Loss: 0.0005809029098600149 \t\n",
      "Epoch 14919 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14920 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14921 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14922 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14923 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14924 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14925 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14926 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14927 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14928 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14929 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14930 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14931 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14932 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14933 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14934 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14935 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14936 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14937 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14938 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14939 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14940 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14941 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14942 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14943 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14944 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14945 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14946 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14947 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14948 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14949 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14950 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14951 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14952 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14953 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14954 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14955 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14956 \t\t Training Loss: 0.000580902851652354 \t\n",
      "Epoch 14957 \t\t Training Loss: 0.0005809027934446931 \t\n",
      "Epoch 14958 \t\t Training Loss: 0.0005809027934446931 \t\n",
      "Epoch 14959 \t\t Training Loss: 0.0005809027934446931 \t\n",
      "Epoch 14960 \t\t Training Loss: 0.0005809027934446931 \t\n",
      "Epoch 14961 \t\t Training Loss: 0.0005809027934446931 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14962 \t\t Training Loss: 0.0005809027934446931 \t\n",
      "Epoch 14963 \t\t Training Loss: 0.0005809027934446931 \t\n",
      "Epoch 14964 \t\t Training Loss: 0.0005809027934446931 \t\n",
      "Epoch 14965 \t\t Training Loss: 0.0005809027934446931 \t\n",
      "Epoch 14966 \t\t Training Loss: 0.0005809027934446931 \t\n",
      "Epoch 14967 \t\t Training Loss: 0.0005809027934446931 \t\n",
      "Epoch 14968 \t\t Training Loss: 0.0005809027352370322 \t\n",
      "Epoch 14969 \t\t Training Loss: 0.0005809027352370322 \t\n",
      "Epoch 14970 \t\t Training Loss: 0.0005809027352370322 \t\n",
      "Epoch 14971 \t\t Training Loss: 0.0005809027352370322 \t\n",
      "Epoch 14972 \t\t Training Loss: 0.0005809027352370322 \t\n",
      "Epoch 14973 \t\t Training Loss: 0.0005809027352370322 \t\n",
      "Epoch 14974 \t\t Training Loss: 0.0005809027352370322 \t\n",
      "Epoch 14975 \t\t Training Loss: 0.0005809027352370322 \t\n",
      "Epoch 14976 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14977 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14978 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14979 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14980 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14981 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14982 \t\t Training Loss: 0.0005809026188217103 \t\n",
      "Epoch 14983 \t\t Training Loss: 0.0005809026188217103 \t\n",
      "Epoch 14984 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14985 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14986 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14987 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14988 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14989 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14990 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14991 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14992 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14993 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14994 \t\t Training Loss: 0.0005809026770293713 \t\n",
      "Epoch 14995 \t\t Training Loss: 0.0005809026188217103 \t\n",
      "Epoch 14996 \t\t Training Loss: 0.0005809026188217103 \t\n",
      "Epoch 14997 \t\t Training Loss: 0.0005809026188217103 \t\n",
      "Epoch 14998 \t\t Training Loss: 0.0005809026188217103 \t\n",
      "Epoch 14999 \t\t Training Loss: 0.0005809026188217103 \t\n",
      "Epoch 15000 \t\t Training Loss: 0.0005809026188217103 \t\n",
      "Epoch 15001 \t\t Training Loss: 0.0005809026188217103 \t\n",
      "Epoch 15002 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15003 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15004 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15005 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15006 \t\t Training Loss: 0.0005809025024063885 \t\n",
      "Epoch 15007 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15008 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15009 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15010 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15011 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15012 \t\t Training Loss: 0.0005809026188217103 \t\n",
      "Epoch 15013 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15014 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15015 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15016 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15017 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15018 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15019 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15020 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15021 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15022 \t\t Training Loss: 0.0005809025024063885 \t\n",
      "Epoch 15023 \t\t Training Loss: 0.0005809025024063885 \t\n",
      "Epoch 15024 \t\t Training Loss: 0.0005809025606140494 \t\n",
      "Epoch 15025 \t\t Training Loss: 0.0005809025024063885 \t\n",
      "Epoch 15026 \t\t Training Loss: 0.0005809025024063885 \t\n",
      "Epoch 15027 \t\t Training Loss: 0.0005809025024063885 \t\n",
      "Epoch 15028 \t\t Training Loss: 0.0005809025024063885 \t\n",
      "Epoch 15029 \t\t Training Loss: 0.0005809025024063885 \t\n",
      "Epoch 15030 \t\t Training Loss: 0.0005809023859910667 \t\n",
      "Epoch 15031 \t\t Training Loss: 0.0005809023859910667 \t\n",
      "Epoch 15032 \t\t Training Loss: 0.0005809023859910667 \t\n",
      "Epoch 15033 \t\t Training Loss: 0.0005809023859910667 \t\n",
      "Epoch 15034 \t\t Training Loss: 0.0005809023859910667 \t\n",
      "Epoch 15035 \t\t Training Loss: 0.0005809023859910667 \t\n",
      "Epoch 15036 \t\t Training Loss: 0.0005809023859910667 \t\n",
      "Epoch 15037 \t\t Training Loss: 0.0005809023859910667 \t\n",
      "Epoch 15038 \t\t Training Loss: 0.0005809023859910667 \t\n",
      "Epoch 15039 \t\t Training Loss: 0.0005809023277834058 \t\n",
      "Epoch 15040 \t\t Training Loss: 0.0005809023277834058 \t\n",
      "Epoch 15041 \t\t Training Loss: 0.0005809022695757449 \t\n",
      "Epoch 15042 \t\t Training Loss: 0.0005809023277834058 \t\n",
      "Epoch 15043 \t\t Training Loss: 0.0005809022695757449 \t\n",
      "Epoch 15044 \t\t Training Loss: 0.0005809022695757449 \t\n",
      "Epoch 15045 \t\t Training Loss: 0.0005809023277834058 \t\n",
      "Epoch 15046 \t\t Training Loss: 0.0005809022695757449 \t\n",
      "Epoch 15047 \t\t Training Loss: 0.0005809023277834058 \t\n",
      "Epoch 15048 \t\t Training Loss: 0.0005809023277834058 \t\n",
      "Epoch 15049 \t\t Training Loss: 0.0005809023277834058 \t\n",
      "Epoch 15050 \t\t Training Loss: 0.0005809022695757449 \t\n",
      "Epoch 15051 \t\t Training Loss: 0.0005809022695757449 \t\n",
      "Epoch 15052 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15053 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15054 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15055 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15056 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15057 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15058 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15059 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15060 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15061 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15062 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15063 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15064 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15065 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15066 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15067 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15068 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15069 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15070 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15071 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15072 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15073 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15074 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15075 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15076 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15077 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15078 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15079 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15080 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15081 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15082 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15083 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15084 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15085 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15086 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15087 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15088 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15089 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15090 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15091 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15092 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15093 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15094 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15095 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15096 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15097 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15098 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15099 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15100 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15101 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15102 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15103 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15104 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15105 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15106 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15107 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15108 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15109 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15110 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15111 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15112 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15113 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15114 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15115 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15116 \t\t Training Loss: 0.000580902211368084 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15117 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15118 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15119 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15120 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15121 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15122 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15123 \t\t Training Loss: 0.0005809020949527621 \t\n",
      "Epoch 15124 \t\t Training Loss: 0.000580902211368084 \t\n",
      "Epoch 15125 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15126 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15127 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15128 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15129 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15130 \t\t Training Loss: 0.0005809020949527621 \t\n",
      "Epoch 15131 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15132 \t\t Training Loss: 0.0005809020949527621 \t\n",
      "Epoch 15133 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15134 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15135 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15136 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15137 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15138 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15139 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15140 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15141 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15142 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15143 \t\t Training Loss: 0.0005809019785374403 \t\n",
      "Epoch 15144 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15145 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15146 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15147 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15148 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15149 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15150 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15151 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15152 \t\t Training Loss: 0.0005809019785374403 \t\n",
      "Epoch 15153 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15154 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15155 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15156 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15157 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15158 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15159 \t\t Training Loss: 0.0005809019785374403 \t\n",
      "Epoch 15160 \t\t Training Loss: 0.0005809020367451012 \t\n",
      "Epoch 15161 \t\t Training Loss: 0.0005809019785374403 \t\n",
      "Epoch 15162 \t\t Training Loss: 0.0005809019785374403 \t\n",
      "Epoch 15163 \t\t Training Loss: 0.0005809019785374403 \t\n",
      "Epoch 15164 \t\t Training Loss: 0.0005809019785374403 \t\n",
      "Epoch 15165 \t\t Training Loss: 0.0005809019785374403 \t\n",
      "Epoch 15166 \t\t Training Loss: 0.0005809019785374403 \t\n",
      "Epoch 15167 \t\t Training Loss: 0.0005809019785374403 \t\n",
      "Epoch 15168 \t\t Training Loss: 0.0005809019785374403 \t\n",
      "Epoch 15169 \t\t Training Loss: 0.0005809019785374403 \t\n",
      "Epoch 15170 \t\t Training Loss: 0.0005809019785374403 \t\n",
      "Epoch 15171 \t\t Training Loss: 0.0005809019785374403 \t\n",
      "Epoch 15172 \t\t Training Loss: 0.0005809019785374403 \t\n",
      "Epoch 15173 \t\t Training Loss: 0.0005809019203297794 \t\n",
      "Epoch 15174 \t\t Training Loss: 0.0005809019785374403 \t\n",
      "Epoch 15175 \t\t Training Loss: 0.0005809019203297794 \t\n",
      "Epoch 15176 \t\t Training Loss: 0.0005809019203297794 \t\n",
      "Epoch 15177 \t\t Training Loss: 0.0005809019203297794 \t\n",
      "Epoch 15178 \t\t Training Loss: 0.0005809019203297794 \t\n",
      "Epoch 15179 \t\t Training Loss: 0.0005809019203297794 \t\n",
      "Epoch 15180 \t\t Training Loss: 0.0005809019203297794 \t\n",
      "Epoch 15181 \t\t Training Loss: 0.0005809019203297794 \t\n",
      "Epoch 15182 \t\t Training Loss: 0.0005809019203297794 \t\n",
      "Epoch 15183 \t\t Training Loss: 0.0005809019203297794 \t\n",
      "Epoch 15184 \t\t Training Loss: 0.0005809019203297794 \t\n",
      "Epoch 15185 \t\t Training Loss: 0.0005809019203297794 \t\n",
      "Epoch 15186 \t\t Training Loss: 0.0005809019203297794 \t\n",
      "Epoch 15187 \t\t Training Loss: 0.0005809019203297794 \t\n",
      "Epoch 15188 \t\t Training Loss: 0.0005809019203297794 \t\n",
      "Epoch 15189 \t\t Training Loss: 0.0005809018039144576 \t\n",
      "Epoch 15190 \t\t Training Loss: 0.0005809018039144576 \t\n",
      "Epoch 15191 \t\t Training Loss: 0.0005809018039144576 \t\n",
      "Epoch 15192 \t\t Training Loss: 0.0005809018039144576 \t\n",
      "Epoch 15193 \t\t Training Loss: 0.0005809018039144576 \t\n",
      "Epoch 15194 \t\t Training Loss: 0.0005809018039144576 \t\n",
      "Epoch 15195 \t\t Training Loss: 0.0005809018039144576 \t\n",
      "Epoch 15196 \t\t Training Loss: 0.0005809018039144576 \t\n",
      "Epoch 15197 \t\t Training Loss: 0.0005809018039144576 \t\n",
      "Epoch 15198 \t\t Training Loss: 0.0005809016874991357 \t\n",
      "Epoch 15199 \t\t Training Loss: 0.0005809016874991357 \t\n",
      "Epoch 15200 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15201 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15202 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15203 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15204 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15205 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15206 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15207 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15208 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15209 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15210 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15211 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15212 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15213 \t\t Training Loss: 0.0005809015710838139 \t\n",
      "Epoch 15214 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15215 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15216 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15217 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15218 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15219 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15220 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15221 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15222 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15223 \t\t Training Loss: 0.0005809016292914748 \t\n",
      "Epoch 15224 \t\t Training Loss: 0.0005809015710838139 \t\n",
      "Epoch 15225 \t\t Training Loss: 0.0005809015710838139 \t\n",
      "Epoch 15226 \t\t Training Loss: 0.0005809015710838139 \t\n",
      "Epoch 15227 \t\t Training Loss: 0.0005809015710838139 \t\n",
      "Epoch 15228 \t\t Training Loss: 0.0005809015710838139 \t\n",
      "Epoch 15229 \t\t Training Loss: 0.0005809015710838139 \t\n",
      "Epoch 15230 \t\t Training Loss: 0.0005809015710838139 \t\n",
      "Epoch 15231 \t\t Training Loss: 0.0005809015710838139 \t\n",
      "Epoch 15232 \t\t Training Loss: 0.0005809015710838139 \t\n",
      "Epoch 15233 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15234 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15235 \t\t Training Loss: 0.0005809015710838139 \t\n",
      "Epoch 15236 \t\t Training Loss: 0.0005809015710838139 \t\n",
      "Epoch 15237 \t\t Training Loss: 0.0005809015710838139 \t\n",
      "Epoch 15238 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15239 \t\t Training Loss: 0.0005809015710838139 \t\n",
      "Epoch 15240 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15241 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15242 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15243 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15244 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15245 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15246 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15247 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15248 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15249 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15250 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15251 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15252 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15253 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15254 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15255 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15256 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15257 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15258 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15259 \t\t Training Loss: 0.000580901512876153 \t\n",
      "Epoch 15260 \t\t Training Loss: 0.0005809013964608312 \t\n",
      "Epoch 15261 \t\t Training Loss: 0.0005809013964608312 \t\n",
      "Epoch 15262 \t\t Training Loss: 0.0005809013964608312 \t\n",
      "Epoch 15263 \t\t Training Loss: 0.0005809013964608312 \t\n",
      "Epoch 15264 \t\t Training Loss: 0.0005809013964608312 \t\n",
      "Epoch 15265 \t\t Training Loss: 0.0005809013964608312 \t\n",
      "Epoch 15266 \t\t Training Loss: 0.0005809013964608312 \t\n",
      "Epoch 15267 \t\t Training Loss: 0.0005809012800455093 \t\n",
      "Epoch 15268 \t\t Training Loss: 0.0005809012800455093 \t\n",
      "Epoch 15269 \t\t Training Loss: 0.0005809012800455093 \t\n",
      "Epoch 15270 \t\t Training Loss: 0.0005809012800455093 \t\n",
      "Epoch 15271 \t\t Training Loss: 0.0005809012800455093 \t\n",
      "Epoch 15272 \t\t Training Loss: 0.0005809012800455093 \t\n",
      "Epoch 15273 \t\t Training Loss: 0.0005809012800455093 \t\n",
      "Epoch 15274 \t\t Training Loss: 0.0005809012218378484 \t\n",
      "Epoch 15275 \t\t Training Loss: 0.0005809012218378484 \t\n",
      "Epoch 15276 \t\t Training Loss: 0.0005809012800455093 \t\n",
      "Epoch 15277 \t\t Training Loss: 0.0005809012218378484 \t\n",
      "Epoch 15278 \t\t Training Loss: 0.0005809012218378484 \t\n",
      "Epoch 15279 \t\t Training Loss: 0.0005809012218378484 \t\n",
      "Epoch 15280 \t\t Training Loss: 0.0005809012218378484 \t\n",
      "Epoch 15281 \t\t Training Loss: 0.0005809012218378484 \t\n",
      "Epoch 15282 \t\t Training Loss: 0.0005809012218378484 \t\n",
      "Epoch 15283 \t\t Training Loss: 0.0005809012218378484 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15284 \t\t Training Loss: 0.0005809012218378484 \t\n",
      "Epoch 15285 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15286 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15287 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15288 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15289 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15290 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15291 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15292 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15293 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15294 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15295 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15296 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15297 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15298 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15299 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15300 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15301 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15302 \t\t Training Loss: 0.0005809011054225266 \t\n",
      "Epoch 15303 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15304 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15305 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15306 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15307 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15308 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15309 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15310 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15311 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15312 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15313 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15314 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15315 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15316 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15317 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15318 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15319 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15320 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15321 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15322 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15323 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15324 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15325 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15326 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15327 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15328 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15329 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15330 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15331 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15332 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15333 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15334 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15335 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15336 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15337 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15338 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15339 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15340 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15341 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15342 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15343 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15344 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15345 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15346 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15347 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15348 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15349 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15350 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15351 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15352 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15353 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15354 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15355 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15356 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15357 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15358 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15359 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15360 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15361 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15362 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15363 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15364 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15365 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15366 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15367 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15368 \t\t Training Loss: 0.0005809009890072048 \t\n",
      "Epoch 15369 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15370 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15371 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15372 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15373 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15374 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15375 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15376 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15377 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15378 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15379 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15380 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15381 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15382 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15383 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15384 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15385 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15386 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15387 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15388 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15389 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15390 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15391 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15392 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15393 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15394 \t\t Training Loss: 0.0005809009307995439 \t\n",
      "Epoch 15395 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15396 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15397 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15398 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15399 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15400 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15401 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15402 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15403 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15404 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15405 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15406 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15407 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15408 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15409 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15410 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15411 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15412 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15413 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15414 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15415 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15416 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15417 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15418 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15419 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15420 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15421 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15422 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15423 \t\t Training Loss: 0.000580900814384222 \t\n",
      "Epoch 15424 \t\t Training Loss: 0.0005809006979689002 \t\n",
      "Epoch 15425 \t\t Training Loss: 0.0005809006979689002 \t\n",
      "Epoch 15426 \t\t Training Loss: 0.0005809006979689002 \t\n",
      "Epoch 15427 \t\t Training Loss: 0.0005809006979689002 \t\n",
      "Epoch 15428 \t\t Training Loss: 0.0005809006979689002 \t\n",
      "Epoch 15429 \t\t Training Loss: 0.0005809006979689002 \t\n",
      "Epoch 15430 \t\t Training Loss: 0.0005809006979689002 \t\n",
      "Epoch 15431 \t\t Training Loss: 0.0005809006979689002 \t\n",
      "Epoch 15432 \t\t Training Loss: 0.0005809006979689002 \t\n",
      "Epoch 15433 \t\t Training Loss: 0.0005809006979689002 \t\n",
      "Epoch 15434 \t\t Training Loss: 0.0005809006979689002 \t\n",
      "Epoch 15435 \t\t Training Loss: 0.0005809006979689002 \t\n",
      "Epoch 15436 \t\t Training Loss: 0.0005809006979689002 \t\n",
      "Epoch 15437 \t\t Training Loss: 0.0005809006979689002 \t\n",
      "Epoch 15438 \t\t Training Loss: 0.0005809006979689002 \t\n",
      "Epoch 15439 \t\t Training Loss: 0.0005809006397612393 \t\n",
      "Epoch 15440 \t\t Training Loss: 0.0005809006397612393 \t\n",
      "Epoch 15441 \t\t Training Loss: 0.0005809006397612393 \t\n",
      "Epoch 15442 \t\t Training Loss: 0.0005809006397612393 \t\n",
      "Epoch 15443 \t\t Training Loss: 0.0005809006397612393 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15444 \t\t Training Loss: 0.0005809006397612393 \t\n",
      "Epoch 15445 \t\t Training Loss: 0.0005809006397612393 \t\n",
      "Epoch 15446 \t\t Training Loss: 0.0005809006397612393 \t\n",
      "Epoch 15447 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15448 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15449 \t\t Training Loss: 0.0005809006397612393 \t\n",
      "Epoch 15450 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15451 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15452 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15453 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15454 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15455 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15456 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15457 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15458 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15459 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15460 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15461 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15462 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15463 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15464 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15465 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15466 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15467 \t\t Training Loss: 0.0005809005233459175 \t\n",
      "Epoch 15468 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15469 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15470 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15471 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15472 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15473 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15474 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15475 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15476 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15477 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15478 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15479 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15480 \t\t Training Loss: 0.0005809003487229347 \t\n",
      "Epoch 15481 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15482 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15483 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15484 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15485 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15486 \t\t Training Loss: 0.0005809003487229347 \t\n",
      "Epoch 15487 \t\t Training Loss: 0.0005809004069305956 \t\n",
      "Epoch 15488 \t\t Training Loss: 0.0005809003487229347 \t\n",
      "Epoch 15489 \t\t Training Loss: 0.0005809003487229347 \t\n",
      "Epoch 15490 \t\t Training Loss: 0.0005809002905152738 \t\n",
      "Epoch 15491 \t\t Training Loss: 0.0005809002905152738 \t\n",
      "Epoch 15492 \t\t Training Loss: 0.0005809002905152738 \t\n",
      "Epoch 15493 \t\t Training Loss: 0.0005809002905152738 \t\n",
      "Epoch 15494 \t\t Training Loss: 0.0005809002905152738 \t\n",
      "Epoch 15495 \t\t Training Loss: 0.0005809002905152738 \t\n",
      "Epoch 15496 \t\t Training Loss: 0.0005809002905152738 \t\n",
      "Epoch 15497 \t\t Training Loss: 0.0005809002905152738 \t\n",
      "Epoch 15498 \t\t Training Loss: 0.0005809002905152738 \t\n",
      "Epoch 15499 \t\t Training Loss: 0.0005809002905152738 \t\n",
      "Epoch 15500 \t\t Training Loss: 0.0005809002905152738 \t\n",
      "Epoch 15501 \t\t Training Loss: 0.0005809002905152738 \t\n",
      "Epoch 15502 \t\t Training Loss: 0.0005809002905152738 \t\n",
      "Epoch 15503 \t\t Training Loss: 0.0005809002905152738 \t\n",
      "Epoch 15504 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15505 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15506 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15507 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15508 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15509 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15510 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15511 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15512 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15513 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15514 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15515 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15516 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15517 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15518 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15519 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15520 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15521 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15522 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15523 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15524 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15525 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15526 \t\t Training Loss: 0.0005809001158922911 \t\n",
      "Epoch 15527 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15528 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15529 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15530 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15531 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15532 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15533 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15534 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15535 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15536 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15537 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15538 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15539 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15540 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15541 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15542 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15543 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15544 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15545 \t\t Training Loss: 0.0005808999412693083 \t\n",
      "Epoch 15546 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15547 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15548 \t\t Training Loss: 0.0005808999994769692 \t\n",
      "Epoch 15549 \t\t Training Loss: 0.0005808999412693083 \t\n",
      "Epoch 15550 \t\t Training Loss: 0.0005808999412693083 \t\n",
      "Epoch 15551 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15552 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15553 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15554 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15555 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15556 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15557 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15558 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15559 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15560 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15561 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15562 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15563 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15564 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15565 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15566 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15567 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15568 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15569 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15570 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15571 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15572 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15573 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15574 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15575 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15576 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15577 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15578 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15579 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15580 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15581 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15582 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15583 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15584 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15585 \t\t Training Loss: 0.0005808997084386647 \t\n",
      "Epoch 15586 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15587 \t\t Training Loss: 0.0005808997084386647 \t\n",
      "Epoch 15588 \t\t Training Loss: 0.0005808997084386647 \t\n",
      "Epoch 15589 \t\t Training Loss: 0.0005808997084386647 \t\n",
      "Epoch 15590 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15591 \t\t Training Loss: 0.0005808998248539865 \t\n",
      "Epoch 15592 \t\t Training Loss: 0.0005808997084386647 \t\n",
      "Epoch 15593 \t\t Training Loss: 0.0005808997084386647 \t\n",
      "Epoch 15594 \t\t Training Loss: 0.0005808997084386647 \t\n",
      "Epoch 15595 \t\t Training Loss: 0.0005808997084386647 \t\n",
      "Epoch 15596 \t\t Training Loss: 0.0005808997084386647 \t\n",
      "Epoch 15597 \t\t Training Loss: 0.0005808997084386647 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15598 \t\t Training Loss: 0.0005808997084386647 \t\n",
      "Epoch 15599 \t\t Training Loss: 0.0005808995920233428 \t\n",
      "Epoch 15600 \t\t Training Loss: 0.0005808995920233428 \t\n",
      "Epoch 15601 \t\t Training Loss: 0.0005808995920233428 \t\n",
      "Epoch 15602 \t\t Training Loss: 0.0005808995920233428 \t\n",
      "Epoch 15603 \t\t Training Loss: 0.0005808995920233428 \t\n",
      "Epoch 15604 \t\t Training Loss: 0.0005808995920233428 \t\n",
      "Epoch 15605 \t\t Training Loss: 0.0005808995920233428 \t\n",
      "Epoch 15606 \t\t Training Loss: 0.0005808995920233428 \t\n",
      "Epoch 15607 \t\t Training Loss: 0.0005808995920233428 \t\n",
      "Epoch 15608 \t\t Training Loss: 0.0005808995920233428 \t\n",
      "Epoch 15609 \t\t Training Loss: 0.0005808995920233428 \t\n",
      "Epoch 15610 \t\t Training Loss: 0.0005808995920233428 \t\n",
      "Epoch 15611 \t\t Training Loss: 0.0005808995920233428 \t\n",
      "Epoch 15612 \t\t Training Loss: 0.0005808995920233428 \t\n",
      "Epoch 15613 \t\t Training Loss: 0.0005808995920233428 \t\n",
      "Epoch 15614 \t\t Training Loss: 0.0005808995920233428 \t\n",
      "Epoch 15615 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15616 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15617 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15618 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15619 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15620 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15621 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15622 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15623 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15624 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15625 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15626 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15627 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15628 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15629 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15630 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15631 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15632 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15633 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15634 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15635 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15636 \t\t Training Loss: 0.0005808995338156819 \t\n",
      "Epoch 15637 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15638 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15639 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15640 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15641 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15642 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15643 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15644 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15645 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15646 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15647 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15648 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15649 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15650 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15651 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15652 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15653 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15654 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15655 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15656 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15657 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15658 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15659 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15660 \t\t Training Loss: 0.0005808994174003601 \t\n",
      "Epoch 15661 \t\t Training Loss: 0.0005808993591926992 \t\n",
      "Epoch 15662 \t\t Training Loss: 0.0005808993591926992 \t\n",
      "Epoch 15663 \t\t Training Loss: 0.0005808993591926992 \t\n",
      "Epoch 15664 \t\t Training Loss: 0.0005808993591926992 \t\n",
      "Epoch 15665 \t\t Training Loss: 0.0005808993591926992 \t\n",
      "Epoch 15666 \t\t Training Loss: 0.0005808993591926992 \t\n",
      "Epoch 15667 \t\t Training Loss: 0.0005808993591926992 \t\n",
      "Epoch 15668 \t\t Training Loss: 0.0005808993591926992 \t\n",
      "Epoch 15669 \t\t Training Loss: 0.0005808993591926992 \t\n",
      "Epoch 15670 \t\t Training Loss: 0.0005808993591926992 \t\n",
      "Epoch 15671 \t\t Training Loss: 0.0005808993591926992 \t\n",
      "Epoch 15672 \t\t Training Loss: 0.0005808993591926992 \t\n",
      "Epoch 15673 \t\t Training Loss: 0.0005808993009850383 \t\n",
      "Epoch 15674 \t\t Training Loss: 0.0005808993591926992 \t\n",
      "Epoch 15675 \t\t Training Loss: 0.0005808993591926992 \t\n",
      "Epoch 15676 \t\t Training Loss: 0.0005808992427773774 \t\n",
      "Epoch 15677 \t\t Training Loss: 0.0005808992427773774 \t\n",
      "Epoch 15678 \t\t Training Loss: 0.0005808992427773774 \t\n",
      "Epoch 15679 \t\t Training Loss: 0.0005808992427773774 \t\n",
      "Epoch 15680 \t\t Training Loss: 0.0005808993009850383 \t\n",
      "Epoch 15681 \t\t Training Loss: 0.0005808993009850383 \t\n",
      "Epoch 15682 \t\t Training Loss: 0.0005808993009850383 \t\n",
      "Epoch 15683 \t\t Training Loss: 0.0005808993009850383 \t\n",
      "Epoch 15684 \t\t Training Loss: 0.0005808993009850383 \t\n",
      "Epoch 15685 \t\t Training Loss: 0.0005808993591926992 \t\n",
      "Epoch 15686 \t\t Training Loss: 0.0005808993009850383 \t\n",
      "Epoch 15687 \t\t Training Loss: 0.0005808993009850383 \t\n",
      "Epoch 15688 \t\t Training Loss: 0.0005808992427773774 \t\n",
      "Epoch 15689 \t\t Training Loss: 0.0005808992427773774 \t\n",
      "Epoch 15690 \t\t Training Loss: 0.0005808992427773774 \t\n",
      "Epoch 15691 \t\t Training Loss: 0.0005808992427773774 \t\n",
      "Epoch 15692 \t\t Training Loss: 0.0005808992427773774 \t\n",
      "Epoch 15693 \t\t Training Loss: 0.0005808991845697165 \t\n",
      "Epoch 15694 \t\t Training Loss: 0.0005808991845697165 \t\n",
      "Epoch 15695 \t\t Training Loss: 0.0005808991845697165 \t\n",
      "Epoch 15696 \t\t Training Loss: 0.0005808991845697165 \t\n",
      "Epoch 15697 \t\t Training Loss: 0.0005808991263620555 \t\n",
      "Epoch 15698 \t\t Training Loss: 0.0005808991845697165 \t\n",
      "Epoch 15699 \t\t Training Loss: 0.0005808991845697165 \t\n",
      "Epoch 15700 \t\t Training Loss: 0.0005808991263620555 \t\n",
      "Epoch 15701 \t\t Training Loss: 0.0005808991263620555 \t\n",
      "Epoch 15702 \t\t Training Loss: 0.0005808991263620555 \t\n",
      "Epoch 15703 \t\t Training Loss: 0.0005808991263620555 \t\n",
      "Epoch 15704 \t\t Training Loss: 0.0005808991845697165 \t\n",
      "Epoch 15705 \t\t Training Loss: 0.0005808991263620555 \t\n",
      "Epoch 15706 \t\t Training Loss: 0.0005808990099467337 \t\n",
      "Epoch 15707 \t\t Training Loss: 0.0005808990099467337 \t\n",
      "Epoch 15708 \t\t Training Loss: 0.0005808990099467337 \t\n",
      "Epoch 15709 \t\t Training Loss: 0.0005808990099467337 \t\n",
      "Epoch 15710 \t\t Training Loss: 0.0005808990099467337 \t\n",
      "Epoch 15711 \t\t Training Loss: 0.0005808990099467337 \t\n",
      "Epoch 15712 \t\t Training Loss: 0.0005808990099467337 \t\n",
      "Epoch 15713 \t\t Training Loss: 0.0005808990099467337 \t\n",
      "Epoch 15714 \t\t Training Loss: 0.0005808990099467337 \t\n",
      "Epoch 15715 \t\t Training Loss: 0.0005808990099467337 \t\n",
      "Epoch 15716 \t\t Training Loss: 0.0005808989517390728 \t\n",
      "Epoch 15717 \t\t Training Loss: 0.0005808990099467337 \t\n",
      "Epoch 15718 \t\t Training Loss: 0.0005808989517390728 \t\n",
      "Epoch 15719 \t\t Training Loss: 0.0005808990099467337 \t\n",
      "Epoch 15720 \t\t Training Loss: 0.0005808989517390728 \t\n",
      "Epoch 15721 \t\t Training Loss: 0.0005808990099467337 \t\n",
      "Epoch 15722 \t\t Training Loss: 0.0005808989517390728 \t\n",
      "Epoch 15723 \t\t Training Loss: 0.0005808989517390728 \t\n",
      "Epoch 15724 \t\t Training Loss: 0.0005808989517390728 \t\n",
      "Epoch 15725 \t\t Training Loss: 0.0005808989517390728 \t\n",
      "Epoch 15726 \t\t Training Loss: 0.0005808989517390728 \t\n",
      "Epoch 15727 \t\t Training Loss: 0.0005808989517390728 \t\n",
      "Epoch 15728 \t\t Training Loss: 0.0005808989517390728 \t\n",
      "Epoch 15729 \t\t Training Loss: 0.0005808989517390728 \t\n",
      "Epoch 15730 \t\t Training Loss: 0.0005808989517390728 \t\n",
      "Epoch 15731 \t\t Training Loss: 0.0005808989517390728 \t\n",
      "Epoch 15732 \t\t Training Loss: 0.0005808989517390728 \t\n",
      "Epoch 15733 \t\t Training Loss: 0.0005808989517390728 \t\n",
      "Epoch 15734 \t\t Training Loss: 0.0005808989517390728 \t\n",
      "Epoch 15735 \t\t Training Loss: 0.0005808988935314119 \t\n",
      "Epoch 15736 \t\t Training Loss: 0.0005808988935314119 \t\n",
      "Epoch 15737 \t\t Training Loss: 0.0005808988935314119 \t\n",
      "Epoch 15738 \t\t Training Loss: 0.0005808988935314119 \t\n",
      "Epoch 15739 \t\t Training Loss: 0.0005808988935314119 \t\n",
      "Epoch 15740 \t\t Training Loss: 0.0005808988935314119 \t\n",
      "Epoch 15741 \t\t Training Loss: 0.0005808988935314119 \t\n",
      "Epoch 15742 \t\t Training Loss: 0.0005808988935314119 \t\n",
      "Epoch 15743 \t\t Training Loss: 0.0005808988935314119 \t\n",
      "Epoch 15744 \t\t Training Loss: 0.0005808988935314119 \t\n",
      "Epoch 15745 \t\t Training Loss: 0.000580898835323751 \t\n",
      "Epoch 15746 \t\t Training Loss: 0.000580898835323751 \t\n",
      "Epoch 15747 \t\t Training Loss: 0.000580898835323751 \t\n",
      "Epoch 15748 \t\t Training Loss: 0.0005808987771160901 \t\n",
      "Epoch 15749 \t\t Training Loss: 0.000580898835323751 \t\n",
      "Epoch 15750 \t\t Training Loss: 0.000580898835323751 \t\n",
      "Epoch 15751 \t\t Training Loss: 0.0005808987771160901 \t\n",
      "Epoch 15752 \t\t Training Loss: 0.0005808987771160901 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15753 \t\t Training Loss: 0.0005808987771160901 \t\n",
      "Epoch 15754 \t\t Training Loss: 0.0005808987771160901 \t\n",
      "Epoch 15755 \t\t Training Loss: 0.0005808987771160901 \t\n",
      "Epoch 15756 \t\t Training Loss: 0.0005808987771160901 \t\n",
      "Epoch 15757 \t\t Training Loss: 0.0005808987771160901 \t\n",
      "Epoch 15758 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15759 \t\t Training Loss: 0.0005808987771160901 \t\n",
      "Epoch 15760 \t\t Training Loss: 0.0005808987771160901 \t\n",
      "Epoch 15761 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15762 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15763 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15764 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15765 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15766 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15767 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15768 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15769 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15770 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15771 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15772 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15773 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15774 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15775 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15776 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15777 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15778 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15779 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15780 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15781 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15782 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15783 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15784 \t\t Training Loss: 0.0005808986607007682 \t\n",
      "Epoch 15785 \t\t Training Loss: 0.0005808986607007682 \t\n",
      "Epoch 15786 \t\t Training Loss: 0.0005808986607007682 \t\n",
      "Epoch 15787 \t\t Training Loss: 0.0005808986607007682 \t\n",
      "Epoch 15788 \t\t Training Loss: 0.0005808986607007682 \t\n",
      "Epoch 15789 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15790 \t\t Training Loss: 0.0005808986607007682 \t\n",
      "Epoch 15791 \t\t Training Loss: 0.0005808987189084291 \t\n",
      "Epoch 15792 \t\t Training Loss: 0.0005808986607007682 \t\n",
      "Epoch 15793 \t\t Training Loss: 0.0005808986607007682 \t\n",
      "Epoch 15794 \t\t Training Loss: 0.0005808986024931073 \t\n",
      "Epoch 15795 \t\t Training Loss: 0.0005808986024931073 \t\n",
      "Epoch 15796 \t\t Training Loss: 0.0005808986024931073 \t\n",
      "Epoch 15797 \t\t Training Loss: 0.0005808986024931073 \t\n",
      "Epoch 15798 \t\t Training Loss: 0.0005808986024931073 \t\n",
      "Epoch 15799 \t\t Training Loss: 0.0005808986024931073 \t\n",
      "Epoch 15800 \t\t Training Loss: 0.0005808986024931073 \t\n",
      "Epoch 15801 \t\t Training Loss: 0.0005808985442854464 \t\n",
      "Epoch 15802 \t\t Training Loss: 0.0005808986024931073 \t\n",
      "Epoch 15803 \t\t Training Loss: 0.0005808985442854464 \t\n",
      "Epoch 15804 \t\t Training Loss: 0.0005808985442854464 \t\n",
      "Epoch 15805 \t\t Training Loss: 0.0005808985442854464 \t\n",
      "Epoch 15806 \t\t Training Loss: 0.0005808985442854464 \t\n",
      "Epoch 15807 \t\t Training Loss: 0.0005808985442854464 \t\n",
      "Epoch 15808 \t\t Training Loss: 0.0005808985442854464 \t\n",
      "Epoch 15809 \t\t Training Loss: 0.0005808985442854464 \t\n",
      "Epoch 15810 \t\t Training Loss: 0.0005808985442854464 \t\n",
      "Epoch 15811 \t\t Training Loss: 0.0005808984860777855 \t\n",
      "Epoch 15812 \t\t Training Loss: 0.0005808984860777855 \t\n",
      "Epoch 15813 \t\t Training Loss: 0.0005808984860777855 \t\n",
      "Epoch 15814 \t\t Training Loss: 0.0005808984860777855 \t\n",
      "Epoch 15815 \t\t Training Loss: 0.0005808984860777855 \t\n",
      "Epoch 15816 \t\t Training Loss: 0.0005808984278701246 \t\n",
      "Epoch 15817 \t\t Training Loss: 0.0005808984278701246 \t\n",
      "Epoch 15818 \t\t Training Loss: 0.0005808984278701246 \t\n",
      "Epoch 15819 \t\t Training Loss: 0.0005808984278701246 \t\n",
      "Epoch 15820 \t\t Training Loss: 0.0005808984278701246 \t\n",
      "Epoch 15821 \t\t Training Loss: 0.0005808983696624637 \t\n",
      "Epoch 15822 \t\t Training Loss: 0.0005808983696624637 \t\n",
      "Epoch 15823 \t\t Training Loss: 0.0005808983114548028 \t\n",
      "Epoch 15824 \t\t Training Loss: 0.0005808983696624637 \t\n",
      "Epoch 15825 \t\t Training Loss: 0.0005808983114548028 \t\n",
      "Epoch 15826 \t\t Training Loss: 0.0005808983114548028 \t\n",
      "Epoch 15827 \t\t Training Loss: 0.0005808983114548028 \t\n",
      "Epoch 15828 \t\t Training Loss: 0.0005808983114548028 \t\n",
      "Epoch 15829 \t\t Training Loss: 0.0005808983114548028 \t\n",
      "Epoch 15830 \t\t Training Loss: 0.0005808983114548028 \t\n",
      "Epoch 15831 \t\t Training Loss: 0.0005808983114548028 \t\n",
      "Epoch 15832 \t\t Training Loss: 0.0005808983114548028 \t\n",
      "Epoch 15833 \t\t Training Loss: 0.0005808983114548028 \t\n",
      "Epoch 15834 \t\t Training Loss: 0.0005808983114548028 \t\n",
      "Epoch 15835 \t\t Training Loss: 0.0005808983114548028 \t\n",
      "Epoch 15836 \t\t Training Loss: 0.0005808983114548028 \t\n",
      "Epoch 15837 \t\t Training Loss: 0.0005808983114548028 \t\n",
      "Epoch 15838 \t\t Training Loss: 0.0005808982532471418 \t\n",
      "Epoch 15839 \t\t Training Loss: 0.0005808982532471418 \t\n",
      "Epoch 15840 \t\t Training Loss: 0.0005808982532471418 \t\n",
      "Epoch 15841 \t\t Training Loss: 0.0005808983114548028 \t\n",
      "Epoch 15842 \t\t Training Loss: 0.0005808983114548028 \t\n",
      "Epoch 15843 \t\t Training Loss: 0.0005808982532471418 \t\n",
      "Epoch 15844 \t\t Training Loss: 0.0005808982532471418 \t\n",
      "Epoch 15845 \t\t Training Loss: 0.0005808982532471418 \t\n",
      "Epoch 15846 \t\t Training Loss: 0.0005808982532471418 \t\n",
      "Epoch 15847 \t\t Training Loss: 0.0005808982532471418 \t\n",
      "Epoch 15848 \t\t Training Loss: 0.0005808981950394809 \t\n",
      "Epoch 15849 \t\t Training Loss: 0.0005808981950394809 \t\n",
      "Epoch 15850 \t\t Training Loss: 0.00058089813683182 \t\n",
      "Epoch 15851 \t\t Training Loss: 0.0005808981950394809 \t\n",
      "Epoch 15852 \t\t Training Loss: 0.0005808981950394809 \t\n",
      "Epoch 15853 \t\t Training Loss: 0.0005808981950394809 \t\n",
      "Epoch 15854 \t\t Training Loss: 0.00058089813683182 \t\n",
      "Epoch 15855 \t\t Training Loss: 0.00058089813683182 \t\n",
      "Epoch 15856 \t\t Training Loss: 0.0005808981950394809 \t\n",
      "Epoch 15857 \t\t Training Loss: 0.00058089813683182 \t\n",
      "Epoch 15858 \t\t Training Loss: 0.00058089813683182 \t\n",
      "Epoch 15859 \t\t Training Loss: 0.00058089813683182 \t\n",
      "Epoch 15860 \t\t Training Loss: 0.00058089813683182 \t\n",
      "Epoch 15861 \t\t Training Loss: 0.00058089813683182 \t\n",
      "Epoch 15862 \t\t Training Loss: 0.00058089813683182 \t\n",
      "Epoch 15863 \t\t Training Loss: 0.00058089813683182 \t\n",
      "Epoch 15864 \t\t Training Loss: 0.00058089813683182 \t\n",
      "Epoch 15865 \t\t Training Loss: 0.00058089813683182 \t\n",
      "Epoch 15866 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15867 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15868 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15869 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15870 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15871 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15872 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15873 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15874 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15875 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15876 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15877 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15878 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15879 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15880 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15881 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15882 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15883 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15884 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15885 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15886 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15887 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15888 \t\t Training Loss: 0.0005808979622088373 \t\n",
      "Epoch 15889 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15890 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15891 \t\t Training Loss: 0.0005808980786241591 \t\n",
      "Epoch 15892 \t\t Training Loss: 0.0005808979622088373 \t\n",
      "Epoch 15893 \t\t Training Loss: 0.0005808979040011764 \t\n",
      "Epoch 15894 \t\t Training Loss: 0.0005808979040011764 \t\n",
      "Epoch 15895 \t\t Training Loss: 0.0005808979040011764 \t\n",
      "Epoch 15896 \t\t Training Loss: 0.0005808979040011764 \t\n",
      "Epoch 15897 \t\t Training Loss: 0.0005808979040011764 \t\n",
      "Epoch 15898 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15899 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15900 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15901 \t\t Training Loss: 0.0005808979040011764 \t\n",
      "Epoch 15902 \t\t Training Loss: 0.0005808979040011764 \t\n",
      "Epoch 15903 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15904 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15905 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15906 \t\t Training Loss: 0.0005808979040011764 \t\n",
      "Epoch 15907 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15908 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15909 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15910 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15911 \t\t Training Loss: 0.0005808979040011764 \t\n",
      "Epoch 15912 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15913 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15914 \t\t Training Loss: 0.0005808978457935154 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15915 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15916 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15917 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15918 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15919 \t\t Training Loss: 0.0005808977875858545 \t\n",
      "Epoch 15920 \t\t Training Loss: 0.0005808977875858545 \t\n",
      "Epoch 15921 \t\t Training Loss: 0.0005808977875858545 \t\n",
      "Epoch 15922 \t\t Training Loss: 0.0005808977875858545 \t\n",
      "Epoch 15923 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15924 \t\t Training Loss: 0.0005808978457935154 \t\n",
      "Epoch 15925 \t\t Training Loss: 0.0005808977875858545 \t\n",
      "Epoch 15926 \t\t Training Loss: 0.0005808977875858545 \t\n",
      "Epoch 15927 \t\t Training Loss: 0.0005808977875858545 \t\n",
      "Epoch 15928 \t\t Training Loss: 0.0005808977293781936 \t\n",
      "Epoch 15929 \t\t Training Loss: 0.0005808977293781936 \t\n",
      "Epoch 15930 \t\t Training Loss: 0.0005808977875858545 \t\n",
      "Epoch 15931 \t\t Training Loss: 0.0005808977293781936 \t\n",
      "Epoch 15932 \t\t Training Loss: 0.0005808977875858545 \t\n",
      "Epoch 15933 \t\t Training Loss: 0.0005808977875858545 \t\n",
      "Epoch 15934 \t\t Training Loss: 0.0005808977875858545 \t\n",
      "Epoch 15935 \t\t Training Loss: 0.0005808977293781936 \t\n",
      "Epoch 15936 \t\t Training Loss: 0.0005808977293781936 \t\n",
      "Epoch 15937 \t\t Training Loss: 0.0005808977293781936 \t\n",
      "Epoch 15938 \t\t Training Loss: 0.0005808977293781936 \t\n",
      "Epoch 15939 \t\t Training Loss: 0.0005808977293781936 \t\n",
      "Epoch 15940 \t\t Training Loss: 0.0005808977293781936 \t\n",
      "Epoch 15941 \t\t Training Loss: 0.0005808977293781936 \t\n",
      "Epoch 15942 \t\t Training Loss: 0.0005808977293781936 \t\n",
      "Epoch 15943 \t\t Training Loss: 0.0005808977293781936 \t\n",
      "Epoch 15944 \t\t Training Loss: 0.0005808977293781936 \t\n",
      "Epoch 15945 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15946 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15947 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15948 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15949 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15950 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15951 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15952 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15953 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15954 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15955 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15956 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15957 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15958 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15959 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15960 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15961 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15962 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15963 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15964 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15965 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15966 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15967 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15968 \t\t Training Loss: 0.0005808976711705327 \t\n",
      "Epoch 15969 \t\t Training Loss: 0.0005808976129628718 \t\n",
      "Epoch 15970 \t\t Training Loss: 0.0005808976129628718 \t\n",
      "Epoch 15971 \t\t Training Loss: 0.0005808976129628718 \t\n",
      "Epoch 15972 \t\t Training Loss: 0.0005808976129628718 \t\n",
      "Epoch 15973 \t\t Training Loss: 0.0005808975547552109 \t\n",
      "Epoch 15974 \t\t Training Loss: 0.0005808975547552109 \t\n",
      "Epoch 15975 \t\t Training Loss: 0.0005808975547552109 \t\n",
      "Epoch 15976 \t\t Training Loss: 0.0005808975547552109 \t\n",
      "Epoch 15977 \t\t Training Loss: 0.0005808975547552109 \t\n",
      "Epoch 15978 \t\t Training Loss: 0.0005808975547552109 \t\n",
      "Epoch 15979 \t\t Training Loss: 0.0005808975547552109 \t\n",
      "Epoch 15980 \t\t Training Loss: 0.0005808975547552109 \t\n",
      "Epoch 15981 \t\t Training Loss: 0.0005808975547552109 \t\n",
      "Epoch 15982 \t\t Training Loss: 0.0005808975547552109 \t\n",
      "Epoch 15983 \t\t Training Loss: 0.0005808975547552109 \t\n",
      "Epoch 15984 \t\t Training Loss: 0.0005808975547552109 \t\n",
      "Epoch 15985 \t\t Training Loss: 0.0005808975547552109 \t\n",
      "Epoch 15986 \t\t Training Loss: 0.0005808975547552109 \t\n",
      "Epoch 15987 \t\t Training Loss: 0.0005808975547552109 \t\n",
      "Epoch 15988 \t\t Training Loss: 0.0005808975547552109 \t\n",
      "Epoch 15989 \t\t Training Loss: 0.00058089749654755 \t\n",
      "Epoch 15990 \t\t Training Loss: 0.00058089749654755 \t\n",
      "Epoch 15991 \t\t Training Loss: 0.00058089749654755 \t\n",
      "Epoch 15992 \t\t Training Loss: 0.00058089749654755 \t\n",
      "Epoch 15993 \t\t Training Loss: 0.00058089749654755 \t\n",
      "Epoch 15994 \t\t Training Loss: 0.00058089749654755 \t\n",
      "Epoch 15995 \t\t Training Loss: 0.000580897438339889 \t\n",
      "Epoch 15996 \t\t Training Loss: 0.000580897438339889 \t\n",
      "Epoch 15997 \t\t Training Loss: 0.00058089749654755 \t\n",
      "Epoch 15998 \t\t Training Loss: 0.000580897438339889 \t\n",
      "Epoch 15999 \t\t Training Loss: 0.000580897438339889 \t\n",
      "Epoch 16000 \t\t Training Loss: 0.0005808973801322281 \t\n",
      "Epoch 16001 \t\t Training Loss: 0.0005808973801322281 \t\n",
      "Epoch 16002 \t\t Training Loss: 0.0005808973801322281 \t\n",
      "Epoch 16003 \t\t Training Loss: 0.0005808973801322281 \t\n",
      "Epoch 16004 \t\t Training Loss: 0.0005808973801322281 \t\n",
      "Epoch 16005 \t\t Training Loss: 0.0005808973801322281 \t\n",
      "Epoch 16006 \t\t Training Loss: 0.0005808973801322281 \t\n",
      "Epoch 16007 \t\t Training Loss: 0.0005808973801322281 \t\n",
      "Epoch 16008 \t\t Training Loss: 0.0005808973801322281 \t\n",
      "Epoch 16009 \t\t Training Loss: 0.0005808973801322281 \t\n",
      "Epoch 16010 \t\t Training Loss: 0.0005808973801322281 \t\n",
      "Epoch 16011 \t\t Training Loss: 0.0005808973801322281 \t\n",
      "Epoch 16012 \t\t Training Loss: 0.0005808973801322281 \t\n",
      "Epoch 16013 \t\t Training Loss: 0.0005808973801322281 \t\n",
      "Epoch 16014 \t\t Training Loss: 0.0005808972637169063 \t\n",
      "Epoch 16015 \t\t Training Loss: 0.0005808972637169063 \t\n",
      "Epoch 16016 \t\t Training Loss: 0.0005808972637169063 \t\n",
      "Epoch 16017 \t\t Training Loss: 0.0005808972637169063 \t\n",
      "Epoch 16018 \t\t Training Loss: 0.0005808972637169063 \t\n",
      "Epoch 16019 \t\t Training Loss: 0.0005808972637169063 \t\n",
      "Epoch 16020 \t\t Training Loss: 0.0005808972637169063 \t\n",
      "Epoch 16021 \t\t Training Loss: 0.0005808972637169063 \t\n",
      "Epoch 16022 \t\t Training Loss: 0.0005808972637169063 \t\n",
      "Epoch 16023 \t\t Training Loss: 0.0005808972637169063 \t\n",
      "Epoch 16024 \t\t Training Loss: 0.0005808972637169063 \t\n",
      "Epoch 16025 \t\t Training Loss: 0.0005808972637169063 \t\n",
      "Epoch 16026 \t\t Training Loss: 0.0005808972637169063 \t\n",
      "Epoch 16027 \t\t Training Loss: 0.0005808972637169063 \t\n",
      "Epoch 16028 \t\t Training Loss: 0.0005808972637169063 \t\n",
      "Epoch 16029 \t\t Training Loss: 0.0005808972055092454 \t\n",
      "Epoch 16030 \t\t Training Loss: 0.0005808972055092454 \t\n",
      "Epoch 16031 \t\t Training Loss: 0.0005808972055092454 \t\n",
      "Epoch 16032 \t\t Training Loss: 0.0005808972055092454 \t\n",
      "Epoch 16033 \t\t Training Loss: 0.0005808972055092454 \t\n",
      "Epoch 16034 \t\t Training Loss: 0.0005808972055092454 \t\n",
      "Epoch 16035 \t\t Training Loss: 0.0005808972055092454 \t\n",
      "Epoch 16036 \t\t Training Loss: 0.0005808972055092454 \t\n",
      "Epoch 16037 \t\t Training Loss: 0.0005808972055092454 \t\n",
      "Epoch 16038 \t\t Training Loss: 0.0005808972055092454 \t\n",
      "Epoch 16039 \t\t Training Loss: 0.0005808972055092454 \t\n",
      "Epoch 16040 \t\t Training Loss: 0.0005808971473015845 \t\n",
      "Epoch 16041 \t\t Training Loss: 0.0005808971473015845 \t\n",
      "Epoch 16042 \t\t Training Loss: 0.0005808971473015845 \t\n",
      "Epoch 16043 \t\t Training Loss: 0.0005808971473015845 \t\n",
      "Epoch 16044 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16045 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16046 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16047 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16048 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16049 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16050 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16051 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16052 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16053 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16054 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16055 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16056 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16057 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16058 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16059 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16060 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16061 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16062 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16063 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16064 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16065 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16066 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16067 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16068 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16069 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16070 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16071 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16072 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16073 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16074 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16075 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16076 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16077 \t\t Training Loss: 0.0005808969726786017 \t\n",
      "Epoch 16078 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16079 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16080 \t\t Training Loss: 0.0005808970308862627 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16081 \t\t Training Loss: 0.0005808969726786017 \t\n",
      "Epoch 16082 \t\t Training Loss: 0.0005808969726786017 \t\n",
      "Epoch 16083 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16084 \t\t Training Loss: 0.0005808969726786017 \t\n",
      "Epoch 16085 \t\t Training Loss: 0.0005808970308862627 \t\n",
      "Epoch 16086 \t\t Training Loss: 0.0005808969726786017 \t\n",
      "Epoch 16087 \t\t Training Loss: 0.0005808969144709408 \t\n",
      "Epoch 16088 \t\t Training Loss: 0.0005808969144709408 \t\n",
      "Epoch 16089 \t\t Training Loss: 0.0005808969144709408 \t\n",
      "Epoch 16090 \t\t Training Loss: 0.0005808969144709408 \t\n",
      "Epoch 16091 \t\t Training Loss: 0.0005808969144709408 \t\n",
      "Epoch 16092 \t\t Training Loss: 0.000580896798055619 \t\n",
      "Epoch 16093 \t\t Training Loss: 0.0005808969144709408 \t\n",
      "Epoch 16094 \t\t Training Loss: 0.0005808969144709408 \t\n",
      "Epoch 16095 \t\t Training Loss: 0.0005808969144709408 \t\n",
      "Epoch 16096 \t\t Training Loss: 0.000580896798055619 \t\n",
      "Epoch 16097 \t\t Training Loss: 0.000580896798055619 \t\n",
      "Epoch 16098 \t\t Training Loss: 0.000580896798055619 \t\n",
      "Epoch 16099 \t\t Training Loss: 0.000580896798055619 \t\n",
      "Epoch 16100 \t\t Training Loss: 0.000580896798055619 \t\n",
      "Epoch 16101 \t\t Training Loss: 0.000580896798055619 \t\n",
      "Epoch 16102 \t\t Training Loss: 0.0005808967398479581 \t\n",
      "Epoch 16103 \t\t Training Loss: 0.0005808967398479581 \t\n",
      "Epoch 16104 \t\t Training Loss: 0.0005808967398479581 \t\n",
      "Epoch 16105 \t\t Training Loss: 0.0005808967398479581 \t\n",
      "Epoch 16106 \t\t Training Loss: 0.0005808967398479581 \t\n",
      "Epoch 16107 \t\t Training Loss: 0.0005808967398479581 \t\n",
      "Epoch 16108 \t\t Training Loss: 0.0005808967398479581 \t\n",
      "Epoch 16109 \t\t Training Loss: 0.0005808967398479581 \t\n",
      "Epoch 16110 \t\t Training Loss: 0.0005808967398479581 \t\n",
      "Epoch 16111 \t\t Training Loss: 0.0005808967398479581 \t\n",
      "Epoch 16112 \t\t Training Loss: 0.0005808967398479581 \t\n",
      "Epoch 16113 \t\t Training Loss: 0.0005808967398479581 \t\n",
      "Epoch 16114 \t\t Training Loss: 0.0005808967398479581 \t\n",
      "Epoch 16115 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16116 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16117 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16118 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16119 \t\t Training Loss: 0.0005808967398479581 \t\n",
      "Epoch 16120 \t\t Training Loss: 0.0005808967398479581 \t\n",
      "Epoch 16121 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16122 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16123 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16124 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16125 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16126 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16127 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16128 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16129 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16130 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16131 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16132 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16133 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16134 \t\t Training Loss: 0.0005808965652249753 \t\n",
      "Epoch 16135 \t\t Training Loss: 0.0005808965652249753 \t\n",
      "Epoch 16136 \t\t Training Loss: 0.0005808966234326363 \t\n",
      "Epoch 16137 \t\t Training Loss: 0.0005808965652249753 \t\n",
      "Epoch 16138 \t\t Training Loss: 0.0005808965652249753 \t\n",
      "Epoch 16139 \t\t Training Loss: 0.0005808965652249753 \t\n",
      "Epoch 16140 \t\t Training Loss: 0.0005808965652249753 \t\n",
      "Epoch 16141 \t\t Training Loss: 0.0005808965652249753 \t\n",
      "Epoch 16142 \t\t Training Loss: 0.0005808965652249753 \t\n",
      "Epoch 16143 \t\t Training Loss: 0.0005808965652249753 \t\n",
      "Epoch 16144 \t\t Training Loss: 0.0005808965652249753 \t\n",
      "Epoch 16145 \t\t Training Loss: 0.0005808965652249753 \t\n",
      "Epoch 16146 \t\t Training Loss: 0.0005808965652249753 \t\n",
      "Epoch 16147 \t\t Training Loss: 0.0005808965652249753 \t\n",
      "Epoch 16148 \t\t Training Loss: 0.0005808965070173144 \t\n",
      "Epoch 16149 \t\t Training Loss: 0.0005808965070173144 \t\n",
      "Epoch 16150 \t\t Training Loss: 0.0005808964488096535 \t\n",
      "Epoch 16151 \t\t Training Loss: 0.0005808964488096535 \t\n",
      "Epoch 16152 \t\t Training Loss: 0.0005808964488096535 \t\n",
      "Epoch 16153 \t\t Training Loss: 0.0005808964488096535 \t\n",
      "Epoch 16154 \t\t Training Loss: 0.0005808964488096535 \t\n",
      "Epoch 16155 \t\t Training Loss: 0.0005808964488096535 \t\n",
      "Epoch 16156 \t\t Training Loss: 0.0005808964488096535 \t\n",
      "Epoch 16157 \t\t Training Loss: 0.0005808964488096535 \t\n",
      "Epoch 16158 \t\t Training Loss: 0.0005808964488096535 \t\n",
      "Epoch 16159 \t\t Training Loss: 0.0005808964488096535 \t\n",
      "Epoch 16160 \t\t Training Loss: 0.0005808964488096535 \t\n",
      "Epoch 16161 \t\t Training Loss: 0.0005808963906019926 \t\n",
      "Epoch 16162 \t\t Training Loss: 0.0005808963906019926 \t\n",
      "Epoch 16163 \t\t Training Loss: 0.0005808964488096535 \t\n",
      "Epoch 16164 \t\t Training Loss: 0.0005808963906019926 \t\n",
      "Epoch 16165 \t\t Training Loss: 0.0005808964488096535 \t\n",
      "Epoch 16166 \t\t Training Loss: 0.0005808964488096535 \t\n",
      "Epoch 16167 \t\t Training Loss: 0.0005808964488096535 \t\n",
      "Epoch 16168 \t\t Training Loss: 0.0005808964488096535 \t\n",
      "Epoch 16169 \t\t Training Loss: 0.0005808963906019926 \t\n",
      "Epoch 16170 \t\t Training Loss: 0.0005808963906019926 \t\n",
      "Epoch 16171 \t\t Training Loss: 0.0005808963906019926 \t\n",
      "Epoch 16172 \t\t Training Loss: 0.0005808963323943317 \t\n",
      "Epoch 16173 \t\t Training Loss: 0.0005808963906019926 \t\n",
      "Epoch 16174 \t\t Training Loss: 0.0005808963906019926 \t\n",
      "Epoch 16175 \t\t Training Loss: 0.0005808963906019926 \t\n",
      "Epoch 16176 \t\t Training Loss: 0.0005808963323943317 \t\n",
      "Epoch 16177 \t\t Training Loss: 0.0005808963323943317 \t\n",
      "Epoch 16178 \t\t Training Loss: 0.0005808963323943317 \t\n",
      "Epoch 16179 \t\t Training Loss: 0.0005808963323943317 \t\n",
      "Epoch 16180 \t\t Training Loss: 0.0005808963323943317 \t\n",
      "Epoch 16181 \t\t Training Loss: 0.0005808963323943317 \t\n",
      "Epoch 16182 \t\t Training Loss: 0.0005808963323943317 \t\n",
      "Epoch 16183 \t\t Training Loss: 0.0005808963323943317 \t\n",
      "Epoch 16184 \t\t Training Loss: 0.0005808963323943317 \t\n",
      "Epoch 16185 \t\t Training Loss: 0.0005808963323943317 \t\n",
      "Epoch 16186 \t\t Training Loss: 0.0005808963323943317 \t\n",
      "Epoch 16187 \t\t Training Loss: 0.0005808963323943317 \t\n",
      "Epoch 16188 \t\t Training Loss: 0.0005808962741866708 \t\n",
      "Epoch 16189 \t\t Training Loss: 0.000580896157771349 \t\n",
      "Epoch 16190 \t\t Training Loss: 0.0005808962741866708 \t\n",
      "Epoch 16191 \t\t Training Loss: 0.000580896157771349 \t\n",
      "Epoch 16192 \t\t Training Loss: 0.000580896157771349 \t\n",
      "Epoch 16193 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16194 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16195 \t\t Training Loss: 0.000580896157771349 \t\n",
      "Epoch 16196 \t\t Training Loss: 0.000580896157771349 \t\n",
      "Epoch 16197 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16198 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16199 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16200 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16201 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16202 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16203 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16204 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16205 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16206 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16207 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16208 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16209 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16210 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16211 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16212 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16213 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16214 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16215 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16216 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16217 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16218 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16219 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16220 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16221 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16222 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16223 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16224 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16225 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16226 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16227 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16228 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16229 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16230 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16231 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16232 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16233 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16234 \t\t Training Loss: 0.000580896099563688 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16235 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16236 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16237 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16238 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16239 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16240 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16241 \t\t Training Loss: 0.0005808960413560271 \t\n",
      "Epoch 16242 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16243 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16244 \t\t Training Loss: 0.000580896099563688 \t\n",
      "Epoch 16245 \t\t Training Loss: 0.0005808960413560271 \t\n",
      "Epoch 16246 \t\t Training Loss: 0.0005808960413560271 \t\n",
      "Epoch 16247 \t\t Training Loss: 0.0005808960413560271 \t\n",
      "Epoch 16248 \t\t Training Loss: 0.0005808960413560271 \t\n",
      "Epoch 16249 \t\t Training Loss: 0.0005808960413560271 \t\n",
      "Epoch 16250 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16251 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16252 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16253 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16254 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16255 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16256 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16257 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16258 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16259 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16260 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16261 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16262 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16263 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16264 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16265 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16266 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16267 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16268 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16269 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16270 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16271 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16272 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16273 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16274 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16275 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16276 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16277 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16278 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16279 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16280 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16281 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16282 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16283 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16284 \t\t Training Loss: 0.0005808959831483662 \t\n",
      "Epoch 16285 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16286 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16287 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16288 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16289 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16290 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16291 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16292 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16293 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16294 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16295 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16296 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16297 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16298 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16299 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16300 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16301 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16302 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16303 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16304 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16305 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16306 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16307 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16308 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16309 \t\t Training Loss: 0.0005808958667330444 \t\n",
      "Epoch 16310 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16311 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16312 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16313 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16314 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16315 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16316 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16317 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16318 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16319 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16320 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16321 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16322 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16323 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16324 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16325 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16326 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16327 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16328 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16329 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16330 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16331 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16332 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16333 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16334 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16335 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16336 \t\t Training Loss: 0.0005808956921100616 \t\n",
      "Epoch 16337 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16338 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16339 \t\t Training Loss: 0.0005808957503177226 \t\n",
      "Epoch 16340 \t\t Training Loss: 0.0005808956921100616 \t\n",
      "Epoch 16341 \t\t Training Loss: 0.0005808956921100616 \t\n",
      "Epoch 16342 \t\t Training Loss: 0.0005808956921100616 \t\n",
      "Epoch 16343 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16344 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16345 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16346 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16347 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16348 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16349 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16350 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16351 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16352 \t\t Training Loss: 0.0005808956921100616 \t\n",
      "Epoch 16353 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16354 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16355 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16356 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16357 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16358 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16359 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16360 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16361 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16362 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16363 \t\t Training Loss: 0.0005808956921100616 \t\n",
      "Epoch 16364 \t\t Training Loss: 0.0005808956921100616 \t\n",
      "Epoch 16365 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16366 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16367 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16368 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16369 \t\t Training Loss: 0.0005808956921100616 \t\n",
      "Epoch 16370 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16371 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16372 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16373 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16374 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16375 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16376 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16377 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16378 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16379 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16380 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16381 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16382 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16383 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16384 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16385 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16386 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16387 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16388 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16389 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16390 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16391 \t\t Training Loss: 0.000580895459279418 \t\n",
      "Epoch 16392 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16393 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16394 \t\t Training Loss: 0.0005808955756947398 \t\n",
      "Epoch 16395 \t\t Training Loss: 0.000580895459279418 \t\n",
      "Epoch 16396 \t\t Training Loss: 0.0005808953428640962 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16397 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16398 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16399 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16400 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16401 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16402 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16403 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16404 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16405 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16406 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16407 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16408 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16409 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16410 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16411 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16412 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16413 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16414 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16415 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16416 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16417 \t\t Training Loss: 0.0005808953428640962 \t\n",
      "Epoch 16418 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16419 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16420 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16421 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16422 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16423 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16424 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16425 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16426 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16427 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16428 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16429 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16430 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16431 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16432 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16433 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16434 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16435 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16436 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16437 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16438 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16439 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16440 \t\t Training Loss: 0.0005808952846564353 \t\n",
      "Epoch 16441 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16442 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16443 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16444 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16445 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16446 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16447 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16448 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16449 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16450 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16451 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16452 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16453 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16454 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16455 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16456 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16457 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16458 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16459 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16460 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16461 \t\t Training Loss: 0.0005808951682411134 \t\n",
      "Epoch 16462 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16463 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16464 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16465 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16466 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16467 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16468 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16469 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16470 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16471 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16472 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16473 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16474 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16475 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16476 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16477 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16478 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16479 \t\t Training Loss: 0.0005808950518257916 \t\n",
      "Epoch 16480 \t\t Training Loss: 0.0005808949936181307 \t\n",
      "Epoch 16481 \t\t Training Loss: 0.0005808949936181307 \t\n",
      "Epoch 16482 \t\t Training Loss: 0.0005808949936181307 \t\n",
      "Epoch 16483 \t\t Training Loss: 0.0005808949936181307 \t\n",
      "Epoch 16484 \t\t Training Loss: 0.0005808949936181307 \t\n",
      "Epoch 16485 \t\t Training Loss: 0.0005808949936181307 \t\n",
      "Epoch 16486 \t\t Training Loss: 0.0005808949936181307 \t\n",
      "Epoch 16487 \t\t Training Loss: 0.0005808949936181307 \t\n",
      "Epoch 16488 \t\t Training Loss: 0.0005808949936181307 \t\n",
      "Epoch 16489 \t\t Training Loss: 0.0005808949936181307 \t\n",
      "Epoch 16490 \t\t Training Loss: 0.0005808949936181307 \t\n",
      "Epoch 16491 \t\t Training Loss: 0.0005808949936181307 \t\n",
      "Epoch 16492 \t\t Training Loss: 0.0005808949936181307 \t\n",
      "Epoch 16493 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16494 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16495 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16496 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16497 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16498 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16499 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16500 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16501 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16502 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16503 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16504 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16505 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16506 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16507 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16508 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16509 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16510 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16511 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16512 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16513 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16514 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16515 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16516 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16517 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16518 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16519 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16520 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16521 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16522 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16523 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16524 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16525 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16526 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16527 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16528 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16529 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16530 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16531 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16532 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16533 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16534 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16535 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16536 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16537 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16538 \t\t Training Loss: 0.0005808948772028089 \t\n",
      "Epoch 16539 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16540 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16541 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16542 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16543 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16544 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16545 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16546 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16547 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16548 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16549 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16550 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16551 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16552 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16553 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16554 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16555 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16556 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16557 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16558 \t\t Training Loss: 0.000580894760787487 \t\n",
      "Epoch 16559 \t\t Training Loss: 0.0005808947025798261 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16560 \t\t Training Loss: 0.0005808945861645043 \t\n",
      "Epoch 16561 \t\t Training Loss: 0.0005808945861645043 \t\n",
      "Epoch 16562 \t\t Training Loss: 0.0005808945861645043 \t\n",
      "Epoch 16563 \t\t Training Loss: 0.0005808945861645043 \t\n",
      "Epoch 16564 \t\t Training Loss: 0.0005808945861645043 \t\n",
      "Epoch 16565 \t\t Training Loss: 0.0005808945861645043 \t\n",
      "Epoch 16566 \t\t Training Loss: 0.0005808945861645043 \t\n",
      "Epoch 16567 \t\t Training Loss: 0.0005808945861645043 \t\n",
      "Epoch 16568 \t\t Training Loss: 0.0005808945861645043 \t\n",
      "Epoch 16569 \t\t Training Loss: 0.0005808945861645043 \t\n",
      "Epoch 16570 \t\t Training Loss: 0.0005808945861645043 \t\n",
      "Epoch 16571 \t\t Training Loss: 0.0005808945861645043 \t\n",
      "Epoch 16572 \t\t Training Loss: 0.0005808945861645043 \t\n",
      "Epoch 16573 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16574 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16575 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16576 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16577 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16578 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16579 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16580 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16581 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16582 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16583 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16584 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16585 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16586 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16587 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16588 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16589 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16590 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16591 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16592 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16593 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16594 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16595 \t\t Training Loss: 0.0005808944697491825 \t\n",
      "Epoch 16596 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16597 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16598 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16599 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16600 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16601 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16602 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16603 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16604 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16605 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16606 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16607 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16608 \t\t Training Loss: 0.0005808944115415215 \t\n",
      "Epoch 16609 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16610 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16611 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16612 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16613 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16614 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16615 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16616 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16617 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16618 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16619 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16620 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16621 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16622 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16623 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16624 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16625 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16626 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16627 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16628 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16629 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16630 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16631 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16632 \t\t Training Loss: 0.0005808942951261997 \t\n",
      "Epoch 16633 \t\t Training Loss: 0.0005808941787108779 \t\n",
      "Epoch 16634 \t\t Training Loss: 0.0005808941787108779 \t\n",
      "Epoch 16635 \t\t Training Loss: 0.0005808941787108779 \t\n",
      "Epoch 16636 \t\t Training Loss: 0.0005808941787108779 \t\n",
      "Epoch 16637 \t\t Training Loss: 0.0005808941787108779 \t\n",
      "Epoch 16638 \t\t Training Loss: 0.0005808941787108779 \t\n",
      "Epoch 16639 \t\t Training Loss: 0.0005808941787108779 \t\n",
      "Epoch 16640 \t\t Training Loss: 0.0005808941787108779 \t\n",
      "Epoch 16641 \t\t Training Loss: 0.0005808941787108779 \t\n",
      "Epoch 16642 \t\t Training Loss: 0.0005808941787108779 \t\n",
      "Epoch 16643 \t\t Training Loss: 0.0005808941787108779 \t\n",
      "Epoch 16644 \t\t Training Loss: 0.0005808941787108779 \t\n",
      "Epoch 16645 \t\t Training Loss: 0.0005808941787108779 \t\n",
      "Epoch 16646 \t\t Training Loss: 0.0005808941787108779 \t\n",
      "Epoch 16647 \t\t Training Loss: 0.0005808941787108779 \t\n",
      "Epoch 16648 \t\t Training Loss: 0.0005808941787108779 \t\n",
      "Epoch 16649 \t\t Training Loss: 0.000580894120503217 \t\n",
      "Epoch 16650 \t\t Training Loss: 0.000580894120503217 \t\n",
      "Epoch 16651 \t\t Training Loss: 0.000580894120503217 \t\n",
      "Epoch 16652 \t\t Training Loss: 0.000580894120503217 \t\n",
      "Epoch 16653 \t\t Training Loss: 0.000580894120503217 \t\n",
      "Epoch 16654 \t\t Training Loss: 0.000580894120503217 \t\n",
      "Epoch 16655 \t\t Training Loss: 0.000580894120503217 \t\n",
      "Epoch 16656 \t\t Training Loss: 0.000580894120503217 \t\n",
      "Epoch 16657 \t\t Training Loss: 0.000580894120503217 \t\n",
      "Epoch 16658 \t\t Training Loss: 0.000580894120503217 \t\n",
      "Epoch 16659 \t\t Training Loss: 0.000580894120503217 \t\n",
      "Epoch 16660 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16661 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16662 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16663 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16664 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16665 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16666 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16667 \t\t Training Loss: 0.000580894120503217 \t\n",
      "Epoch 16668 \t\t Training Loss: 0.000580894120503217 \t\n",
      "Epoch 16669 \t\t Training Loss: 0.000580894120503217 \t\n",
      "Epoch 16670 \t\t Training Loss: 0.000580894120503217 \t\n",
      "Epoch 16671 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16672 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16673 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16674 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16675 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16676 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16677 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16678 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16679 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16680 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16681 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16682 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16683 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16684 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16685 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16686 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16687 \t\t Training Loss: 0.0005808940622955561 \t\n",
      "Epoch 16688 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16689 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16690 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16691 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16692 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16693 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16694 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16695 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16696 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16697 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16698 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16699 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16700 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16701 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16702 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16703 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16704 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16705 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16706 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16707 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16708 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16709 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16710 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16711 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16712 \t\t Training Loss: 0.0005808938876725733 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16713 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16714 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16715 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16716 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16717 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16718 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16719 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16720 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16721 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16722 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16723 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16724 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16725 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16726 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16727 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16728 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16729 \t\t Training Loss: 0.0005808937712572515 \t\n",
      "Epoch 16730 \t\t Training Loss: 0.0005808937712572515 \t\n",
      "Epoch 16731 \t\t Training Loss: 0.0005808937712572515 \t\n",
      "Epoch 16732 \t\t Training Loss: 0.0005808937712572515 \t\n",
      "Epoch 16733 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16734 \t\t Training Loss: 0.0005808938876725733 \t\n",
      "Epoch 16735 \t\t Training Loss: 0.0005808937712572515 \t\n",
      "Epoch 16736 \t\t Training Loss: 0.0005808937712572515 \t\n",
      "Epoch 16737 \t\t Training Loss: 0.0005808937712572515 \t\n",
      "Epoch 16738 \t\t Training Loss: 0.0005808937712572515 \t\n",
      "Epoch 16739 \t\t Training Loss: 0.0005808937712572515 \t\n",
      "Epoch 16740 \t\t Training Loss: 0.0005808937712572515 \t\n",
      "Epoch 16741 \t\t Training Loss: 0.0005808937712572515 \t\n",
      "Epoch 16742 \t\t Training Loss: 0.0005808937712572515 \t\n",
      "Epoch 16743 \t\t Training Loss: 0.0005808937130495906 \t\n",
      "Epoch 16744 \t\t Training Loss: 0.0005808937130495906 \t\n",
      "Epoch 16745 \t\t Training Loss: 0.0005808937130495906 \t\n",
      "Epoch 16746 \t\t Training Loss: 0.0005808937130495906 \t\n",
      "Epoch 16747 \t\t Training Loss: 0.0005808937130495906 \t\n",
      "Epoch 16748 \t\t Training Loss: 0.0005808937130495906 \t\n",
      "Epoch 16749 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16750 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16751 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16752 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16753 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16754 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16755 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16756 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16757 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16758 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16759 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16760 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16761 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16762 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16763 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16764 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16765 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16766 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16767 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16768 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16769 \t\t Training Loss: 0.0005808935966342688 \t\n",
      "Epoch 16770 \t\t Training Loss: 0.0005808935966342688 \t\n",
      "Epoch 16771 \t\t Training Loss: 0.0005808935966342688 \t\n",
      "Epoch 16772 \t\t Training Loss: 0.0005808935966342688 \t\n",
      "Epoch 16773 \t\t Training Loss: 0.0005808935966342688 \t\n",
      "Epoch 16774 \t\t Training Loss: 0.0005808935966342688 \t\n",
      "Epoch 16775 \t\t Training Loss: 0.0005808935966342688 \t\n",
      "Epoch 16776 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16777 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16778 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16779 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16780 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16781 \t\t Training Loss: 0.0005808936548419297 \t\n",
      "Epoch 16782 \t\t Training Loss: 0.0005808935966342688 \t\n",
      "Epoch 16783 \t\t Training Loss: 0.0005808935966342688 \t\n",
      "Epoch 16784 \t\t Training Loss: 0.0005808935966342688 \t\n",
      "Epoch 16785 \t\t Training Loss: 0.0005808935966342688 \t\n",
      "Epoch 16786 \t\t Training Loss: 0.0005808935966342688 \t\n",
      "Epoch 16787 \t\t Training Loss: 0.0005808935966342688 \t\n",
      "Epoch 16788 \t\t Training Loss: 0.0005808935966342688 \t\n",
      "Epoch 16789 \t\t Training Loss: 0.0005808935966342688 \t\n",
      "Epoch 16790 \t\t Training Loss: 0.0005808934802189469 \t\n",
      "Epoch 16791 \t\t Training Loss: 0.0005808934802189469 \t\n",
      "Epoch 16792 \t\t Training Loss: 0.0005808934802189469 \t\n",
      "Epoch 16793 \t\t Training Loss: 0.0005808934802189469 \t\n",
      "Epoch 16794 \t\t Training Loss: 0.0005808934802189469 \t\n",
      "Epoch 16795 \t\t Training Loss: 0.0005808934802189469 \t\n",
      "Epoch 16796 \t\t Training Loss: 0.0005808934802189469 \t\n",
      "Epoch 16797 \t\t Training Loss: 0.0005808934802189469 \t\n",
      "Epoch 16798 \t\t Training Loss: 0.0005808934802189469 \t\n",
      "Epoch 16799 \t\t Training Loss: 0.0005808934802189469 \t\n",
      "Epoch 16800 \t\t Training Loss: 0.0005808934802189469 \t\n",
      "Epoch 16801 \t\t Training Loss: 0.0005808933638036251 \t\n",
      "Epoch 16802 \t\t Training Loss: 0.0005808934802189469 \t\n",
      "Epoch 16803 \t\t Training Loss: 0.0005808934802189469 \t\n",
      "Epoch 16804 \t\t Training Loss: 0.0005808933638036251 \t\n",
      "Epoch 16805 \t\t Training Loss: 0.0005808933638036251 \t\n",
      "Epoch 16806 \t\t Training Loss: 0.0005808933638036251 \t\n",
      "Epoch 16807 \t\t Training Loss: 0.0005808933638036251 \t\n",
      "Epoch 16808 \t\t Training Loss: 0.0005808933638036251 \t\n",
      "Epoch 16809 \t\t Training Loss: 0.0005808933638036251 \t\n",
      "Epoch 16810 \t\t Training Loss: 0.0005808933638036251 \t\n",
      "Epoch 16811 \t\t Training Loss: 0.0005808933638036251 \t\n",
      "Epoch 16812 \t\t Training Loss: 0.0005808933638036251 \t\n",
      "Epoch 16813 \t\t Training Loss: 0.0005808933638036251 \t\n",
      "Epoch 16814 \t\t Training Loss: 0.0005808933055959642 \t\n",
      "Epoch 16815 \t\t Training Loss: 0.0005808933055959642 \t\n",
      "Epoch 16816 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16817 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16818 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16819 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16820 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16821 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16822 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16823 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16824 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16825 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16826 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16827 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16828 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16829 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16830 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16831 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16832 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16833 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16834 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16835 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16836 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16837 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16838 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16839 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16840 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16841 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16842 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16843 \t\t Training Loss: 0.0005808931309729815 \t\n",
      "Epoch 16844 \t\t Training Loss: 0.0005808931309729815 \t\n",
      "Epoch 16845 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16846 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16847 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16848 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16849 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16850 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16851 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16852 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16853 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16854 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16855 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16856 \t\t Training Loss: 0.0005808931891806424 \t\n",
      "Epoch 16857 \t\t Training Loss: 0.0005808931309729815 \t\n",
      "Epoch 16858 \t\t Training Loss: 0.0005808931309729815 \t\n",
      "Epoch 16859 \t\t Training Loss: 0.0005808931309729815 \t\n",
      "Epoch 16860 \t\t Training Loss: 0.0005808931309729815 \t\n",
      "Epoch 16861 \t\t Training Loss: 0.0005808931309729815 \t\n",
      "Epoch 16862 \t\t Training Loss: 0.0005808931309729815 \t\n",
      "Epoch 16863 \t\t Training Loss: 0.0005808930727653205 \t\n",
      "Epoch 16864 \t\t Training Loss: 0.0005808930727653205 \t\n",
      "Epoch 16865 \t\t Training Loss: 0.0005808930727653205 \t\n",
      "Epoch 16866 \t\t Training Loss: 0.0005808930727653205 \t\n",
      "Epoch 16867 \t\t Training Loss: 0.0005808930727653205 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16868 \t\t Training Loss: 0.0005808930727653205 \t\n",
      "Epoch 16869 \t\t Training Loss: 0.0005808930727653205 \t\n",
      "Epoch 16870 \t\t Training Loss: 0.0005808930727653205 \t\n",
      "Epoch 16871 \t\t Training Loss: 0.0005808930727653205 \t\n",
      "Epoch 16872 \t\t Training Loss: 0.0005808930727653205 \t\n",
      "Epoch 16873 \t\t Training Loss: 0.0005808930727653205 \t\n",
      "Epoch 16874 \t\t Training Loss: 0.0005808930145576596 \t\n",
      "Epoch 16875 \t\t Training Loss: 0.0005808930145576596 \t\n",
      "Epoch 16876 \t\t Training Loss: 0.0005808930145576596 \t\n",
      "Epoch 16877 \t\t Training Loss: 0.0005808930145576596 \t\n",
      "Epoch 16878 \t\t Training Loss: 0.0005808930145576596 \t\n",
      "Epoch 16879 \t\t Training Loss: 0.0005808930145576596 \t\n",
      "Epoch 16880 \t\t Training Loss: 0.0005808929563499987 \t\n",
      "Epoch 16881 \t\t Training Loss: 0.0005808929563499987 \t\n",
      "Epoch 16882 \t\t Training Loss: 0.0005808929563499987 \t\n",
      "Epoch 16883 \t\t Training Loss: 0.0005808929563499987 \t\n",
      "Epoch 16884 \t\t Training Loss: 0.0005808929563499987 \t\n",
      "Epoch 16885 \t\t Training Loss: 0.0005808929563499987 \t\n",
      "Epoch 16886 \t\t Training Loss: 0.0005808929563499987 \t\n",
      "Epoch 16887 \t\t Training Loss: 0.0005808929563499987 \t\n",
      "Epoch 16888 \t\t Training Loss: 0.0005808928981423378 \t\n",
      "Epoch 16889 \t\t Training Loss: 0.0005808928981423378 \t\n",
      "Epoch 16890 \t\t Training Loss: 0.0005808928981423378 \t\n",
      "Epoch 16891 \t\t Training Loss: 0.0005808928981423378 \t\n",
      "Epoch 16892 \t\t Training Loss: 0.0005808928981423378 \t\n",
      "Epoch 16893 \t\t Training Loss: 0.0005808928981423378 \t\n",
      "Epoch 16894 \t\t Training Loss: 0.0005808928981423378 \t\n",
      "Epoch 16895 \t\t Training Loss: 0.0005808928981423378 \t\n",
      "Epoch 16896 \t\t Training Loss: 0.0005808928981423378 \t\n",
      "Epoch 16897 \t\t Training Loss: 0.000580892781727016 \t\n",
      "Epoch 16898 \t\t Training Loss: 0.000580892781727016 \t\n",
      "Epoch 16899 \t\t Training Loss: 0.000580892781727016 \t\n",
      "Epoch 16900 \t\t Training Loss: 0.000580892781727016 \t\n",
      "Epoch 16901 \t\t Training Loss: 0.000580892781727016 \t\n",
      "Epoch 16902 \t\t Training Loss: 0.000580892781727016 \t\n",
      "Epoch 16903 \t\t Training Loss: 0.000580892781727016 \t\n",
      "Epoch 16904 \t\t Training Loss: 0.000580892781727016 \t\n",
      "Epoch 16905 \t\t Training Loss: 0.000580892781727016 \t\n",
      "Epoch 16906 \t\t Training Loss: 0.000580892781727016 \t\n",
      "Epoch 16907 \t\t Training Loss: 0.000580892781727016 \t\n",
      "Epoch 16908 \t\t Training Loss: 0.000580892781727016 \t\n",
      "Epoch 16909 \t\t Training Loss: 0.000580892781727016 \t\n",
      "Epoch 16910 \t\t Training Loss: 0.0005808927235193551 \t\n",
      "Epoch 16911 \t\t Training Loss: 0.0005808927235193551 \t\n",
      "Epoch 16912 \t\t Training Loss: 0.0005808927235193551 \t\n",
      "Epoch 16913 \t\t Training Loss: 0.0005808927235193551 \t\n",
      "Epoch 16914 \t\t Training Loss: 0.0005808927235193551 \t\n",
      "Epoch 16915 \t\t Training Loss: 0.0005808927235193551 \t\n",
      "Epoch 16916 \t\t Training Loss: 0.0005808927235193551 \t\n",
      "Epoch 16917 \t\t Training Loss: 0.0005808927235193551 \t\n",
      "Epoch 16918 \t\t Training Loss: 0.0005808927235193551 \t\n",
      "Epoch 16919 \t\t Training Loss: 0.0005808927235193551 \t\n",
      "Epoch 16920 \t\t Training Loss: 0.0005808927235193551 \t\n",
      "Epoch 16921 \t\t Training Loss: 0.0005808926653116941 \t\n",
      "Epoch 16922 \t\t Training Loss: 0.0005808926653116941 \t\n",
      "Epoch 16923 \t\t Training Loss: 0.0005808926653116941 \t\n",
      "Epoch 16924 \t\t Training Loss: 0.0005808926653116941 \t\n",
      "Epoch 16925 \t\t Training Loss: 0.0005808926653116941 \t\n",
      "Epoch 16926 \t\t Training Loss: 0.0005808926653116941 \t\n",
      "Epoch 16927 \t\t Training Loss: 0.0005808926653116941 \t\n",
      "Epoch 16928 \t\t Training Loss: 0.0005808926653116941 \t\n",
      "Epoch 16929 \t\t Training Loss: 0.0005808926653116941 \t\n",
      "Epoch 16930 \t\t Training Loss: 0.0005808926071040332 \t\n",
      "Epoch 16931 \t\t Training Loss: 0.0005808926071040332 \t\n",
      "Epoch 16932 \t\t Training Loss: 0.0005808926071040332 \t\n",
      "Epoch 16933 \t\t Training Loss: 0.0005808926071040332 \t\n",
      "Epoch 16934 \t\t Training Loss: 0.0005808926071040332 \t\n",
      "Epoch 16935 \t\t Training Loss: 0.0005808926071040332 \t\n",
      "Epoch 16936 \t\t Training Loss: 0.0005808926653116941 \t\n",
      "Epoch 16937 \t\t Training Loss: 0.0005808926071040332 \t\n",
      "Epoch 16938 \t\t Training Loss: 0.0005808926071040332 \t\n",
      "Epoch 16939 \t\t Training Loss: 0.0005808926071040332 \t\n",
      "Epoch 16940 \t\t Training Loss: 0.0005808926071040332 \t\n",
      "Epoch 16941 \t\t Training Loss: 0.0005808926071040332 \t\n",
      "Epoch 16942 \t\t Training Loss: 0.0005808926071040332 \t\n",
      "Epoch 16943 \t\t Training Loss: 0.0005808926071040332 \t\n",
      "Epoch 16944 \t\t Training Loss: 0.0005808926071040332 \t\n",
      "Epoch 16945 \t\t Training Loss: 0.0005808926071040332 \t\n",
      "Epoch 16946 \t\t Training Loss: 0.0005808925488963723 \t\n",
      "Epoch 16947 \t\t Training Loss: 0.0005808926071040332 \t\n",
      "Epoch 16948 \t\t Training Loss: 0.0005808925488963723 \t\n",
      "Epoch 16949 \t\t Training Loss: 0.0005808925488963723 \t\n",
      "Epoch 16950 \t\t Training Loss: 0.0005808925488963723 \t\n",
      "Epoch 16951 \t\t Training Loss: 0.0005808925488963723 \t\n",
      "Epoch 16952 \t\t Training Loss: 0.0005808925488963723 \t\n",
      "Epoch 16953 \t\t Training Loss: 0.0005808925488963723 \t\n",
      "Epoch 16954 \t\t Training Loss: 0.0005808925488963723 \t\n",
      "Epoch 16955 \t\t Training Loss: 0.0005808925488963723 \t\n",
      "Epoch 16956 \t\t Training Loss: 0.0005808925488963723 \t\n",
      "Epoch 16957 \t\t Training Loss: 0.0005808924906887114 \t\n",
      "Epoch 16958 \t\t Training Loss: 0.0005808924906887114 \t\n",
      "Epoch 16959 \t\t Training Loss: 0.0005808924906887114 \t\n",
      "Epoch 16960 \t\t Training Loss: 0.0005808924906887114 \t\n",
      "Epoch 16961 \t\t Training Loss: 0.0005808924906887114 \t\n",
      "Epoch 16962 \t\t Training Loss: 0.0005808924906887114 \t\n",
      "Epoch 16963 \t\t Training Loss: 0.0005808924906887114 \t\n",
      "Epoch 16964 \t\t Training Loss: 0.0005808924906887114 \t\n",
      "Epoch 16965 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16966 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16967 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16968 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16969 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16970 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16971 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16972 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16973 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16974 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16975 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16976 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16977 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16978 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16979 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16980 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16981 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16982 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16983 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16984 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16985 \t\t Training Loss: 0.0005808924324810505 \t\n",
      "Epoch 16986 \t\t Training Loss: 0.0005808923742733896 \t\n",
      "Epoch 16987 \t\t Training Loss: 0.0005808923742733896 \t\n",
      "Epoch 16988 \t\t Training Loss: 0.0005808923742733896 \t\n",
      "Epoch 16989 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 16990 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 16991 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 16992 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 16993 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 16994 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 16995 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 16996 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 16997 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 16998 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 16999 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17000 \t\t Training Loss: 0.0005808923742733896 \t\n",
      "Epoch 17001 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17002 \t\t Training Loss: 0.0005808923742733896 \t\n",
      "Epoch 17003 \t\t Training Loss: 0.0005808923742733896 \t\n",
      "Epoch 17004 \t\t Training Loss: 0.0005808923742733896 \t\n",
      "Epoch 17005 \t\t Training Loss: 0.0005808923742733896 \t\n",
      "Epoch 17006 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17007 \t\t Training Loss: 0.0005808923742733896 \t\n",
      "Epoch 17008 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17009 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17010 \t\t Training Loss: 0.0005808923742733896 \t\n",
      "Epoch 17011 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17012 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17013 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17014 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17015 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17016 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17017 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17018 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17019 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17020 \t\t Training Loss: 0.0005808923160657287 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17021 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17022 \t\t Training Loss: 0.0005808921996504068 \t\n",
      "Epoch 17023 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17024 \t\t Training Loss: 0.0005808923160657287 \t\n",
      "Epoch 17025 \t\t Training Loss: 0.0005808921996504068 \t\n",
      "Epoch 17026 \t\t Training Loss: 0.0005808921996504068 \t\n",
      "Epoch 17027 \t\t Training Loss: 0.0005808921996504068 \t\n",
      "Epoch 17028 \t\t Training Loss: 0.0005808921996504068 \t\n",
      "Epoch 17029 \t\t Training Loss: 0.0005808921996504068 \t\n",
      "Epoch 17030 \t\t Training Loss: 0.0005808921996504068 \t\n",
      "Epoch 17031 \t\t Training Loss: 0.0005808921996504068 \t\n",
      "Epoch 17032 \t\t Training Loss: 0.0005808921996504068 \t\n",
      "Epoch 17033 \t\t Training Loss: 0.0005808921996504068 \t\n",
      "Epoch 17034 \t\t Training Loss: 0.0005808921414427459 \t\n",
      "Epoch 17035 \t\t Training Loss: 0.0005808921996504068 \t\n",
      "Epoch 17036 \t\t Training Loss: 0.0005808921996504068 \t\n",
      "Epoch 17037 \t\t Training Loss: 0.0005808921996504068 \t\n",
      "Epoch 17038 \t\t Training Loss: 0.0005808921996504068 \t\n",
      "Epoch 17039 \t\t Training Loss: 0.0005808921414427459 \t\n",
      "Epoch 17040 \t\t Training Loss: 0.0005808921414427459 \t\n",
      "Epoch 17041 \t\t Training Loss: 0.0005808921414427459 \t\n",
      "Epoch 17042 \t\t Training Loss: 0.0005808921414427459 \t\n",
      "Epoch 17043 \t\t Training Loss: 0.0005808921414427459 \t\n",
      "Epoch 17044 \t\t Training Loss: 0.0005808921414427459 \t\n",
      "Epoch 17045 \t\t Training Loss: 0.0005808921414427459 \t\n",
      "Epoch 17046 \t\t Training Loss: 0.0005808921414427459 \t\n",
      "Epoch 17047 \t\t Training Loss: 0.000580892083235085 \t\n",
      "Epoch 17048 \t\t Training Loss: 0.000580892083235085 \t\n",
      "Epoch 17049 \t\t Training Loss: 0.0005808921414427459 \t\n",
      "Epoch 17050 \t\t Training Loss: 0.000580892083235085 \t\n",
      "Epoch 17051 \t\t Training Loss: 0.000580892083235085 \t\n",
      "Epoch 17052 \t\t Training Loss: 0.000580892083235085 \t\n",
      "Epoch 17053 \t\t Training Loss: 0.000580892083235085 \t\n",
      "Epoch 17054 \t\t Training Loss: 0.000580892083235085 \t\n",
      "Epoch 17055 \t\t Training Loss: 0.0005808920250274241 \t\n",
      "Epoch 17056 \t\t Training Loss: 0.0005808919668197632 \t\n",
      "Epoch 17057 \t\t Training Loss: 0.0005808919668197632 \t\n",
      "Epoch 17058 \t\t Training Loss: 0.0005808919668197632 \t\n",
      "Epoch 17059 \t\t Training Loss: 0.0005808919668197632 \t\n",
      "Epoch 17060 \t\t Training Loss: 0.0005808919668197632 \t\n",
      "Epoch 17061 \t\t Training Loss: 0.0005808919668197632 \t\n",
      "Epoch 17062 \t\t Training Loss: 0.0005808920250274241 \t\n",
      "Epoch 17063 \t\t Training Loss: 0.0005808920250274241 \t\n",
      "Epoch 17064 \t\t Training Loss: 0.0005808920250274241 \t\n",
      "Epoch 17065 \t\t Training Loss: 0.0005808919668197632 \t\n",
      "Epoch 17066 \t\t Training Loss: 0.0005808919668197632 \t\n",
      "Epoch 17067 \t\t Training Loss: 0.0005808919668197632 \t\n",
      "Epoch 17068 \t\t Training Loss: 0.0005808919668197632 \t\n",
      "Epoch 17069 \t\t Training Loss: 0.0005808919668197632 \t\n",
      "Epoch 17070 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17071 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17072 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17073 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17074 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17075 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17076 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17077 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17078 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17079 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17080 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17081 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17082 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17083 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17084 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17085 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17086 \t\t Training Loss: 0.0005808919086121023 \t\n",
      "Epoch 17087 \t\t Training Loss: 0.0005808918504044414 \t\n",
      "Epoch 17088 \t\t Training Loss: 0.0005808918504044414 \t\n",
      "Epoch 17089 \t\t Training Loss: 0.0005808918504044414 \t\n",
      "Epoch 17090 \t\t Training Loss: 0.0005808918504044414 \t\n",
      "Epoch 17091 \t\t Training Loss: 0.0005808918504044414 \t\n",
      "Epoch 17092 \t\t Training Loss: 0.0005808918504044414 \t\n",
      "Epoch 17093 \t\t Training Loss: 0.0005808918504044414 \t\n",
      "Epoch 17094 \t\t Training Loss: 0.0005808918504044414 \t\n",
      "Epoch 17095 \t\t Training Loss: 0.0005808917921967804 \t\n",
      "Epoch 17096 \t\t Training Loss: 0.0005808917921967804 \t\n",
      "Epoch 17097 \t\t Training Loss: 0.0005808917921967804 \t\n",
      "Epoch 17098 \t\t Training Loss: 0.0005808917921967804 \t\n",
      "Epoch 17099 \t\t Training Loss: 0.0005808917921967804 \t\n",
      "Epoch 17100 \t\t Training Loss: 0.0005808917339891195 \t\n",
      "Epoch 17101 \t\t Training Loss: 0.0005808917339891195 \t\n",
      "Epoch 17102 \t\t Training Loss: 0.0005808917339891195 \t\n",
      "Epoch 17103 \t\t Training Loss: 0.0005808917339891195 \t\n",
      "Epoch 17104 \t\t Training Loss: 0.0005808917339891195 \t\n",
      "Epoch 17105 \t\t Training Loss: 0.0005808917339891195 \t\n",
      "Epoch 17106 \t\t Training Loss: 0.0005808917339891195 \t\n",
      "Epoch 17107 \t\t Training Loss: 0.0005808917339891195 \t\n",
      "Epoch 17108 \t\t Training Loss: 0.0005808917339891195 \t\n",
      "Epoch 17109 \t\t Training Loss: 0.0005808917339891195 \t\n",
      "Epoch 17110 \t\t Training Loss: 0.0005808916757814586 \t\n",
      "Epoch 17111 \t\t Training Loss: 0.0005808916757814586 \t\n",
      "Epoch 17112 \t\t Training Loss: 0.0005808916757814586 \t\n",
      "Epoch 17113 \t\t Training Loss: 0.0005808916757814586 \t\n",
      "Epoch 17114 \t\t Training Loss: 0.0005808916757814586 \t\n",
      "Epoch 17115 \t\t Training Loss: 0.0005808916757814586 \t\n",
      "Epoch 17116 \t\t Training Loss: 0.0005808916175737977 \t\n",
      "Epoch 17117 \t\t Training Loss: 0.0005808916175737977 \t\n",
      "Epoch 17118 \t\t Training Loss: 0.0005808916175737977 \t\n",
      "Epoch 17119 \t\t Training Loss: 0.0005808916175737977 \t\n",
      "Epoch 17120 \t\t Training Loss: 0.0005808916175737977 \t\n",
      "Epoch 17121 \t\t Training Loss: 0.0005808916175737977 \t\n",
      "Epoch 17122 \t\t Training Loss: 0.0005808916175737977 \t\n",
      "Epoch 17123 \t\t Training Loss: 0.0005808915593661368 \t\n",
      "Epoch 17124 \t\t Training Loss: 0.0005808915593661368 \t\n",
      "Epoch 17125 \t\t Training Loss: 0.0005808915593661368 \t\n",
      "Epoch 17126 \t\t Training Loss: 0.0005808915593661368 \t\n",
      "Epoch 17127 \t\t Training Loss: 0.0005808916175737977 \t\n",
      "Epoch 17128 \t\t Training Loss: 0.0005808916175737977 \t\n",
      "Epoch 17129 \t\t Training Loss: 0.0005808916175737977 \t\n",
      "Epoch 17130 \t\t Training Loss: 0.0005808916175737977 \t\n",
      "Epoch 17131 \t\t Training Loss: 0.0005808915593661368 \t\n",
      "Epoch 17132 \t\t Training Loss: 0.0005808916175737977 \t\n",
      "Epoch 17133 \t\t Training Loss: 0.0005808915593661368 \t\n",
      "Epoch 17134 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17135 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17136 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17137 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17138 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17139 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17140 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17141 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17142 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17143 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17144 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17145 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17146 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17147 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17148 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17149 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17150 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17151 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17152 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17153 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17154 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17155 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17156 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17157 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17158 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17159 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17160 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17161 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17162 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17163 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17164 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17165 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17166 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17167 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17168 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17169 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17170 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17171 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17172 \t\t Training Loss: 0.0005808915011584759 \t\n",
      "Epoch 17173 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17174 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17175 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17176 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17177 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17178 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17179 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17180 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17181 \t\t Training Loss: 0.000580891442950815 \t\n",
      "Epoch 17182 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17183 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17184 \t\t Training Loss: 0.000580891384743154 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17185 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17186 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17187 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17188 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17189 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17190 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17191 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17192 \t\t Training Loss: 0.000580891384743154 \t\n",
      "Epoch 17193 \t\t Training Loss: 0.0005808913265354931 \t\n",
      "Epoch 17194 \t\t Training Loss: 0.0005808912683278322 \t\n",
      "Epoch 17195 \t\t Training Loss: 0.0005808912683278322 \t\n",
      "Epoch 17196 \t\t Training Loss: 0.0005808912683278322 \t\n",
      "Epoch 17197 \t\t Training Loss: 0.0005808912683278322 \t\n",
      "Epoch 17198 \t\t Training Loss: 0.0005808912683278322 \t\n",
      "Epoch 17199 \t\t Training Loss: 0.0005808912683278322 \t\n",
      "Epoch 17200 \t\t Training Loss: 0.0005808912683278322 \t\n",
      "Epoch 17201 \t\t Training Loss: 0.0005808912683278322 \t\n",
      "Epoch 17202 \t\t Training Loss: 0.0005808912683278322 \t\n",
      "Epoch 17203 \t\t Training Loss: 0.0005808912683278322 \t\n",
      "Epoch 17204 \t\t Training Loss: 0.0005808912683278322 \t\n",
      "Epoch 17205 \t\t Training Loss: 0.0005808912683278322 \t\n",
      "Epoch 17206 \t\t Training Loss: 0.0005808912683278322 \t\n",
      "Epoch 17207 \t\t Training Loss: 0.0005808911519125104 \t\n",
      "Epoch 17208 \t\t Training Loss: 0.0005808911519125104 \t\n",
      "Epoch 17209 \t\t Training Loss: 0.0005808911519125104 \t\n",
      "Epoch 17210 \t\t Training Loss: 0.0005808911519125104 \t\n",
      "Epoch 17211 \t\t Training Loss: 0.0005808912683278322 \t\n",
      "Epoch 17212 \t\t Training Loss: 0.0005808911519125104 \t\n",
      "Epoch 17213 \t\t Training Loss: 0.0005808911519125104 \t\n",
      "Epoch 17214 \t\t Training Loss: 0.0005808911519125104 \t\n",
      "Epoch 17215 \t\t Training Loss: 0.0005808911519125104 \t\n",
      "Epoch 17216 \t\t Training Loss: 0.0005808911519125104 \t\n",
      "Epoch 17217 \t\t Training Loss: 0.0005808911519125104 \t\n",
      "Epoch 17218 \t\t Training Loss: 0.0005808911519125104 \t\n",
      "Epoch 17219 \t\t Training Loss: 0.0005808911519125104 \t\n",
      "Epoch 17220 \t\t Training Loss: 0.0005808911519125104 \t\n",
      "Epoch 17221 \t\t Training Loss: 0.0005808911519125104 \t\n",
      "Epoch 17222 \t\t Training Loss: 0.0005808911519125104 \t\n",
      "Epoch 17223 \t\t Training Loss: 0.0005808911519125104 \t\n",
      "Epoch 17224 \t\t Training Loss: 0.0005808910937048495 \t\n",
      "Epoch 17225 \t\t Training Loss: 0.0005808910937048495 \t\n",
      "Epoch 17226 \t\t Training Loss: 0.0005808910937048495 \t\n",
      "Epoch 17227 \t\t Training Loss: 0.0005808910937048495 \t\n",
      "Epoch 17228 \t\t Training Loss: 0.0005808910354971886 \t\n",
      "Epoch 17229 \t\t Training Loss: 0.0005808910937048495 \t\n",
      "Epoch 17230 \t\t Training Loss: 0.0005808910354971886 \t\n",
      "Epoch 17231 \t\t Training Loss: 0.0005808910354971886 \t\n",
      "Epoch 17232 \t\t Training Loss: 0.0005808910354971886 \t\n",
      "Epoch 17233 \t\t Training Loss: 0.0005808910354971886 \t\n",
      "Epoch 17234 \t\t Training Loss: 0.0005808910354971886 \t\n",
      "Epoch 17235 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17236 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17237 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17238 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17239 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17240 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17241 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17242 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17243 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17244 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17245 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17246 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17247 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17248 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17249 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17250 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17251 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17252 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17253 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17254 \t\t Training Loss: 0.0005808909772895277 \t\n",
      "Epoch 17255 \t\t Training Loss: 0.0005808909190818667 \t\n",
      "Epoch 17256 \t\t Training Loss: 0.0005808909190818667 \t\n",
      "Epoch 17257 \t\t Training Loss: 0.0005808909190818667 \t\n",
      "Epoch 17258 \t\t Training Loss: 0.0005808909190818667 \t\n",
      "Epoch 17259 \t\t Training Loss: 0.0005808909190818667 \t\n",
      "Epoch 17260 \t\t Training Loss: 0.0005808909190818667 \t\n",
      "Epoch 17261 \t\t Training Loss: 0.0005808909190818667 \t\n",
      "Epoch 17262 \t\t Training Loss: 0.0005808909190818667 \t\n",
      "Epoch 17263 \t\t Training Loss: 0.0005808909190818667 \t\n",
      "Epoch 17264 \t\t Training Loss: 0.0005808909190818667 \t\n",
      "Epoch 17265 \t\t Training Loss: 0.0005808908608742058 \t\n",
      "Epoch 17266 \t\t Training Loss: 0.0005808908608742058 \t\n",
      "Epoch 17267 \t\t Training Loss: 0.0005808908608742058 \t\n",
      "Epoch 17268 \t\t Training Loss: 0.0005808908608742058 \t\n",
      "Epoch 17269 \t\t Training Loss: 0.0005808909190818667 \t\n",
      "Epoch 17270 \t\t Training Loss: 0.0005808908608742058 \t\n",
      "Epoch 17271 \t\t Training Loss: 0.0005808908608742058 \t\n",
      "Epoch 17272 \t\t Training Loss: 0.0005808908608742058 \t\n",
      "Epoch 17273 \t\t Training Loss: 0.0005808908608742058 \t\n",
      "Epoch 17274 \t\t Training Loss: 0.0005808908026665449 \t\n",
      "Epoch 17275 \t\t Training Loss: 0.0005808908026665449 \t\n",
      "Epoch 17276 \t\t Training Loss: 0.0005808908026665449 \t\n",
      "Epoch 17277 \t\t Training Loss: 0.0005808908026665449 \t\n",
      "Epoch 17278 \t\t Training Loss: 0.0005808908026665449 \t\n",
      "Epoch 17279 \t\t Training Loss: 0.0005808908026665449 \t\n",
      "Epoch 17280 \t\t Training Loss: 0.0005808908026665449 \t\n",
      "Epoch 17281 \t\t Training Loss: 0.0005808908026665449 \t\n",
      "Epoch 17282 \t\t Training Loss: 0.0005808908026665449 \t\n",
      "Epoch 17283 \t\t Training Loss: 0.000580890744458884 \t\n",
      "Epoch 17284 \t\t Training Loss: 0.000580890744458884 \t\n",
      "Epoch 17285 \t\t Training Loss: 0.000580890744458884 \t\n",
      "Epoch 17286 \t\t Training Loss: 0.000580890744458884 \t\n",
      "Epoch 17287 \t\t Training Loss: 0.000580890744458884 \t\n",
      "Epoch 17288 \t\t Training Loss: 0.0005808906862512231 \t\n",
      "Epoch 17289 \t\t Training Loss: 0.000580890744458884 \t\n",
      "Epoch 17290 \t\t Training Loss: 0.0005808906862512231 \t\n",
      "Epoch 17291 \t\t Training Loss: 0.000580890744458884 \t\n",
      "Epoch 17292 \t\t Training Loss: 0.000580890744458884 \t\n",
      "Epoch 17293 \t\t Training Loss: 0.0005808906862512231 \t\n",
      "Epoch 17294 \t\t Training Loss: 0.0005808906862512231 \t\n",
      "Epoch 17295 \t\t Training Loss: 0.0005808906862512231 \t\n",
      "Epoch 17296 \t\t Training Loss: 0.0005808906862512231 \t\n",
      "Epoch 17297 \t\t Training Loss: 0.0005808906862512231 \t\n",
      "Epoch 17298 \t\t Training Loss: 0.0005808906862512231 \t\n",
      "Epoch 17299 \t\t Training Loss: 0.0005808906862512231 \t\n",
      "Epoch 17300 \t\t Training Loss: 0.0005808906862512231 \t\n",
      "Epoch 17301 \t\t Training Loss: 0.0005808906862512231 \t\n",
      "Epoch 17302 \t\t Training Loss: 0.0005808906862512231 \t\n",
      "Epoch 17303 \t\t Training Loss: 0.0005808905698359013 \t\n",
      "Epoch 17304 \t\t Training Loss: 0.0005808905698359013 \t\n",
      "Epoch 17305 \t\t Training Loss: 0.0005808905698359013 \t\n",
      "Epoch 17306 \t\t Training Loss: 0.0005808905698359013 \t\n",
      "Epoch 17307 \t\t Training Loss: 0.0005808905698359013 \t\n",
      "Epoch 17308 \t\t Training Loss: 0.0005808905698359013 \t\n",
      "Epoch 17309 \t\t Training Loss: 0.0005808905698359013 \t\n",
      "Epoch 17310 \t\t Training Loss: 0.0005808905116282403 \t\n",
      "Epoch 17311 \t\t Training Loss: 0.0005808905698359013 \t\n",
      "Epoch 17312 \t\t Training Loss: 0.0005808905116282403 \t\n",
      "Epoch 17313 \t\t Training Loss: 0.0005808905116282403 \t\n",
      "Epoch 17314 \t\t Training Loss: 0.0005808904534205794 \t\n",
      "Epoch 17315 \t\t Training Loss: 0.0005808904534205794 \t\n",
      "Epoch 17316 \t\t Training Loss: 0.0005808904534205794 \t\n",
      "Epoch 17317 \t\t Training Loss: 0.0005808904534205794 \t\n",
      "Epoch 17318 \t\t Training Loss: 0.0005808904534205794 \t\n",
      "Epoch 17319 \t\t Training Loss: 0.0005808904534205794 \t\n",
      "Epoch 17320 \t\t Training Loss: 0.0005808903952129185 \t\n",
      "Epoch 17321 \t\t Training Loss: 0.0005808903952129185 \t\n",
      "Epoch 17322 \t\t Training Loss: 0.0005808903952129185 \t\n",
      "Epoch 17323 \t\t Training Loss: 0.0005808903952129185 \t\n",
      "Epoch 17324 \t\t Training Loss: 0.0005808903952129185 \t\n",
      "Epoch 17325 \t\t Training Loss: 0.0005808903952129185 \t\n",
      "Epoch 17326 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17327 \t\t Training Loss: 0.0005808903952129185 \t\n",
      "Epoch 17328 \t\t Training Loss: 0.0005808903952129185 \t\n",
      "Epoch 17329 \t\t Training Loss: 0.0005808903952129185 \t\n",
      "Epoch 17330 \t\t Training Loss: 0.0005808903952129185 \t\n",
      "Epoch 17331 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17332 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17333 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17334 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17335 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17336 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17337 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17338 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17339 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17340 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17341 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17342 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17343 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17344 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17345 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17346 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17347 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17348 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17349 \t\t Training Loss: 0.0005808903370052576 \t\n",
      "Epoch 17350 \t\t Training Loss: 0.0005808902205899358 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17351 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17352 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17353 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17354 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17355 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17356 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17357 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17358 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17359 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17360 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17361 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17362 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17363 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17364 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17365 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17366 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17367 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17368 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17369 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17370 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17371 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17372 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17373 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17374 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17375 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17376 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17377 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17378 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17379 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17380 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17381 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17382 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17383 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17384 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17385 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17386 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17387 \t\t Training Loss: 0.0005808902205899358 \t\n",
      "Epoch 17388 \t\t Training Loss: 0.0005808901623822749 \t\n",
      "Epoch 17389 \t\t Training Loss: 0.0005808901623822749 \t\n",
      "Epoch 17390 \t\t Training Loss: 0.0005808901623822749 \t\n",
      "Epoch 17391 \t\t Training Loss: 0.0005808901623822749 \t\n",
      "Epoch 17392 \t\t Training Loss: 0.000580890104174614 \t\n",
      "Epoch 17393 \t\t Training Loss: 0.000580890104174614 \t\n",
      "Epoch 17394 \t\t Training Loss: 0.000580890104174614 \t\n",
      "Epoch 17395 \t\t Training Loss: 0.000580890104174614 \t\n",
      "Epoch 17396 \t\t Training Loss: 0.000580890104174614 \t\n",
      "Epoch 17397 \t\t Training Loss: 0.000580890104174614 \t\n",
      "Epoch 17398 \t\t Training Loss: 0.000580890104174614 \t\n",
      "Epoch 17399 \t\t Training Loss: 0.000580890104174614 \t\n",
      "Epoch 17400 \t\t Training Loss: 0.000580890104174614 \t\n",
      "Epoch 17401 \t\t Training Loss: 0.000580890104174614 \t\n",
      "Epoch 17402 \t\t Training Loss: 0.000580890104174614 \t\n",
      "Epoch 17403 \t\t Training Loss: 0.000580890104174614 \t\n",
      "Epoch 17404 \t\t Training Loss: 0.000580890104174614 \t\n",
      "Epoch 17405 \t\t Training Loss: 0.000580890045966953 \t\n",
      "Epoch 17406 \t\t Training Loss: 0.000580890045966953 \t\n",
      "Epoch 17407 \t\t Training Loss: 0.000580890045966953 \t\n",
      "Epoch 17408 \t\t Training Loss: 0.000580890045966953 \t\n",
      "Epoch 17409 \t\t Training Loss: 0.000580890045966953 \t\n",
      "Epoch 17410 \t\t Training Loss: 0.000580890045966953 \t\n",
      "Epoch 17411 \t\t Training Loss: 0.000580890045966953 \t\n",
      "Epoch 17412 \t\t Training Loss: 0.000580890045966953 \t\n",
      "Epoch 17413 \t\t Training Loss: 0.000580890045966953 \t\n",
      "Epoch 17414 \t\t Training Loss: 0.000580890045966953 \t\n",
      "Epoch 17415 \t\t Training Loss: 0.0005808899295516312 \t\n",
      "Epoch 17416 \t\t Training Loss: 0.0005808899295516312 \t\n",
      "Epoch 17417 \t\t Training Loss: 0.0005808899295516312 \t\n",
      "Epoch 17418 \t\t Training Loss: 0.0005808899295516312 \t\n",
      "Epoch 17419 \t\t Training Loss: 0.0005808899295516312 \t\n",
      "Epoch 17420 \t\t Training Loss: 0.0005808899295516312 \t\n",
      "Epoch 17421 \t\t Training Loss: 0.0005808899295516312 \t\n",
      "Epoch 17422 \t\t Training Loss: 0.0005808899295516312 \t\n",
      "Epoch 17423 \t\t Training Loss: 0.0005808899295516312 \t\n",
      "Epoch 17424 \t\t Training Loss: 0.0005808899295516312 \t\n",
      "Epoch 17425 \t\t Training Loss: 0.0005808899295516312 \t\n",
      "Epoch 17426 \t\t Training Loss: 0.0005808899295516312 \t\n",
      "Epoch 17427 \t\t Training Loss: 0.0005808899295516312 \t\n",
      "Epoch 17428 \t\t Training Loss: 0.0005808899295516312 \t\n",
      "Epoch 17429 \t\t Training Loss: 0.0005808898713439703 \t\n",
      "Epoch 17430 \t\t Training Loss: 0.0005808898713439703 \t\n",
      "Epoch 17431 \t\t Training Loss: 0.0005808898713439703 \t\n",
      "Epoch 17432 \t\t Training Loss: 0.0005808898713439703 \t\n",
      "Epoch 17433 \t\t Training Loss: 0.0005808898713439703 \t\n",
      "Epoch 17434 \t\t Training Loss: 0.0005808898713439703 \t\n",
      "Epoch 17435 \t\t Training Loss: 0.0005808898131363094 \t\n",
      "Epoch 17436 \t\t Training Loss: 0.0005808898131363094 \t\n",
      "Epoch 17437 \t\t Training Loss: 0.0005808898131363094 \t\n",
      "Epoch 17438 \t\t Training Loss: 0.0005808898131363094 \t\n",
      "Epoch 17439 \t\t Training Loss: 0.0005808898131363094 \t\n",
      "Epoch 17440 \t\t Training Loss: 0.0005808898131363094 \t\n",
      "Epoch 17441 \t\t Training Loss: 0.0005808898131363094 \t\n",
      "Epoch 17442 \t\t Training Loss: 0.0005808898131363094 \t\n",
      "Epoch 17443 \t\t Training Loss: 0.0005808898713439703 \t\n",
      "Epoch 17444 \t\t Training Loss: 0.0005808898131363094 \t\n",
      "Epoch 17445 \t\t Training Loss: 0.0005808898131363094 \t\n",
      "Epoch 17446 \t\t Training Loss: 0.0005808898131363094 \t\n",
      "Epoch 17447 \t\t Training Loss: 0.0005808898131363094 \t\n",
      "Epoch 17448 \t\t Training Loss: 0.0005808898131363094 \t\n",
      "Epoch 17449 \t\t Training Loss: 0.0005808898131363094 \t\n",
      "Epoch 17450 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17451 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17452 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17453 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17454 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17455 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17456 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17457 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17458 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17459 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17460 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17461 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17462 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17463 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17464 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17465 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17466 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17467 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17468 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17469 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17470 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17471 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17472 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17473 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17474 \t\t Training Loss: 0.0005808897549286485 \t\n",
      "Epoch 17475 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17476 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17477 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17478 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17479 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17480 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17481 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17482 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17483 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17484 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17485 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17486 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17487 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17488 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17489 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17490 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17491 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17492 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17493 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17494 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17495 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17496 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17497 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17498 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17499 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17500 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17501 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17502 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17503 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17504 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17505 \t\t Training Loss: 0.0005808896385133266 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17506 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17507 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17508 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17509 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17510 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17511 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17512 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17513 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17514 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17515 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17516 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17517 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17518 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17519 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17520 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17521 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17522 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17523 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17524 \t\t Training Loss: 0.0005808896385133266 \t\n",
      "Epoch 17525 \t\t Training Loss: 0.0005808894638903439 \t\n",
      "Epoch 17526 \t\t Training Loss: 0.0005808894638903439 \t\n",
      "Epoch 17527 \t\t Training Loss: 0.0005808894638903439 \t\n",
      "Epoch 17528 \t\t Training Loss: 0.0005808894638903439 \t\n",
      "Epoch 17529 \t\t Training Loss: 0.0005808894638903439 \t\n",
      "Epoch 17530 \t\t Training Loss: 0.0005808894638903439 \t\n",
      "Epoch 17531 \t\t Training Loss: 0.0005808894638903439 \t\n",
      "Epoch 17532 \t\t Training Loss: 0.0005808894638903439 \t\n",
      "Epoch 17533 \t\t Training Loss: 0.0005808894638903439 \t\n",
      "Epoch 17534 \t\t Training Loss: 0.0005808894638903439 \t\n",
      "Epoch 17535 \t\t Training Loss: 0.0005808894638903439 \t\n",
      "Epoch 17536 \t\t Training Loss: 0.0005808894638903439 \t\n",
      "Epoch 17537 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17538 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17539 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17540 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17541 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17542 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17543 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17544 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17545 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17546 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17547 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17548 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17549 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17550 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17551 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17552 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17553 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17554 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17555 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17556 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17557 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17558 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17559 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17560 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17561 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17562 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17563 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17564 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17565 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17566 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17567 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17568 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17569 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17570 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17571 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17572 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17573 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17574 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17575 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17576 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17577 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17578 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17579 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17580 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17581 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17582 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17583 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17584 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17585 \t\t Training Loss: 0.0005808892310597003 \t\n",
      "Epoch 17586 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17587 \t\t Training Loss: 0.0005808893474750221 \t\n",
      "Epoch 17588 \t\t Training Loss: 0.0005808892310597003 \t\n",
      "Epoch 17589 \t\t Training Loss: 0.0005808892310597003 \t\n",
      "Epoch 17590 \t\t Training Loss: 0.0005808892310597003 \t\n",
      "Epoch 17591 \t\t Training Loss: 0.0005808892310597003 \t\n",
      "Epoch 17592 \t\t Training Loss: 0.0005808892310597003 \t\n",
      "Epoch 17593 \t\t Training Loss: 0.0005808892310597003 \t\n",
      "Epoch 17594 \t\t Training Loss: 0.0005808892310597003 \t\n",
      "Epoch 17595 \t\t Training Loss: 0.0005808892310597003 \t\n",
      "Epoch 17596 \t\t Training Loss: 0.0005808892310597003 \t\n",
      "Epoch 17597 \t\t Training Loss: 0.0005808892310597003 \t\n",
      "Epoch 17598 \t\t Training Loss: 0.0005808892310597003 \t\n",
      "Epoch 17599 \t\t Training Loss: 0.0005808892310597003 \t\n",
      "Epoch 17600 \t\t Training Loss: 0.0005808892310597003 \t\n",
      "Epoch 17601 \t\t Training Loss: 0.0005808892310597003 \t\n",
      "Epoch 17602 \t\t Training Loss: 0.0005808892310597003 \t\n",
      "Epoch 17603 \t\t Training Loss: 0.0005808891146443784 \t\n",
      "Epoch 17604 \t\t Training Loss: 0.0005808891146443784 \t\n",
      "Epoch 17605 \t\t Training Loss: 0.0005808891146443784 \t\n",
      "Epoch 17606 \t\t Training Loss: 0.0005808891146443784 \t\n",
      "Epoch 17607 \t\t Training Loss: 0.0005808891146443784 \t\n",
      "Epoch 17608 \t\t Training Loss: 0.0005808890564367175 \t\n",
      "Epoch 17609 \t\t Training Loss: 0.0005808890564367175 \t\n",
      "Epoch 17610 \t\t Training Loss: 0.0005808890564367175 \t\n",
      "Epoch 17611 \t\t Training Loss: 0.0005808891146443784 \t\n",
      "Epoch 17612 \t\t Training Loss: 0.0005808890564367175 \t\n",
      "Epoch 17613 \t\t Training Loss: 0.0005808890564367175 \t\n",
      "Epoch 17614 \t\t Training Loss: 0.0005808890564367175 \t\n",
      "Epoch 17615 \t\t Training Loss: 0.0005808890564367175 \t\n",
      "Epoch 17616 \t\t Training Loss: 0.0005808890564367175 \t\n",
      "Epoch 17617 \t\t Training Loss: 0.0005808890564367175 \t\n",
      "Epoch 17618 \t\t Training Loss: 0.0005808890564367175 \t\n",
      "Epoch 17619 \t\t Training Loss: 0.0005808890564367175 \t\n",
      "Epoch 17620 \t\t Training Loss: 0.0005808890564367175 \t\n",
      "Epoch 17621 \t\t Training Loss: 0.0005808890564367175 \t\n",
      "Epoch 17622 \t\t Training Loss: 0.0005808890564367175 \t\n",
      "Epoch 17623 \t\t Training Loss: 0.0005808890564367175 \t\n",
      "Epoch 17624 \t\t Training Loss: 0.0005808889400213957 \t\n",
      "Epoch 17625 \t\t Training Loss: 0.0005808889400213957 \t\n",
      "Epoch 17626 \t\t Training Loss: 0.0005808889400213957 \t\n",
      "Epoch 17627 \t\t Training Loss: 0.0005808889400213957 \t\n",
      "Epoch 17628 \t\t Training Loss: 0.0005808889400213957 \t\n",
      "Epoch 17629 \t\t Training Loss: 0.0005808889400213957 \t\n",
      "Epoch 17630 \t\t Training Loss: 0.0005808889400213957 \t\n",
      "Epoch 17631 \t\t Training Loss: 0.0005808889400213957 \t\n",
      "Epoch 17632 \t\t Training Loss: 0.0005808889400213957 \t\n",
      "Epoch 17633 \t\t Training Loss: 0.0005808889400213957 \t\n",
      "Epoch 17634 \t\t Training Loss: 0.0005808889400213957 \t\n",
      "Epoch 17635 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17636 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17637 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17638 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17639 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17640 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17641 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17642 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17643 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17644 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17645 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17646 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17647 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17648 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17649 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17650 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17651 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17652 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17653 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17654 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17655 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17656 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17657 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17658 \t\t Training Loss: 0.0005808887653984129 \t\n",
      "Epoch 17659 \t\t Training Loss: 0.0005808887653984129 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17660 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17661 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17662 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17663 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17664 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17665 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17666 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17667 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17668 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17669 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17670 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17671 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17672 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17673 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17674 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17675 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17676 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17677 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17678 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17679 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17680 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17681 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17682 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17683 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17684 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17685 \t\t Training Loss: 0.0005808886489830911 \t\n",
      "Epoch 17686 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17687 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17688 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17689 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17690 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17691 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17692 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17693 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17694 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17695 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17696 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17697 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17698 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17699 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17700 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17701 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17702 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17703 \t\t Training Loss: 0.0005808884743601084 \t\n",
      "Epoch 17704 \t\t Training Loss: 0.0005808885325677693 \t\n",
      "Epoch 17705 \t\t Training Loss: 0.0005808884743601084 \t\n",
      "Epoch 17706 \t\t Training Loss: 0.0005808884743601084 \t\n",
      "Epoch 17707 \t\t Training Loss: 0.0005808884743601084 \t\n",
      "Epoch 17708 \t\t Training Loss: 0.0005808884743601084 \t\n",
      "Epoch 17709 \t\t Training Loss: 0.0005808884743601084 \t\n",
      "Epoch 17710 \t\t Training Loss: 0.0005808884743601084 \t\n",
      "Epoch 17711 \t\t Training Loss: 0.0005808884743601084 \t\n",
      "Epoch 17712 \t\t Training Loss: 0.0005808884743601084 \t\n",
      "Epoch 17713 \t\t Training Loss: 0.0005808884743601084 \t\n",
      "Epoch 17714 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17715 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17716 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17717 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17718 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17719 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17720 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17721 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17722 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17723 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17724 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17725 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17726 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17727 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17728 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17729 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17730 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17731 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17732 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17733 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17734 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17735 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17736 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17737 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17738 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17739 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17740 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17741 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17742 \t\t Training Loss: 0.0005808883579447865 \t\n",
      "Epoch 17743 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17744 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17745 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17746 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17747 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17748 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17749 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17750 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17751 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17752 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17753 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17754 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17755 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17756 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17757 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17758 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17759 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17760 \t\t Training Loss: 0.0005808882415294647 \t\n",
      "Epoch 17761 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17762 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17763 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17764 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17765 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17766 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17767 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17768 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17769 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17770 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17771 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17772 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17773 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17774 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17775 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17776 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17777 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17778 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17779 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17780 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17781 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17782 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17783 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17784 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17785 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17786 \t\t Training Loss: 0.0005808881251141429 \t\n",
      "Epoch 17787 \t\t Training Loss: 0.0005808881833218038 \t\n",
      "Epoch 17788 \t\t Training Loss: 0.0005808881251141429 \t\n",
      "Epoch 17789 \t\t Training Loss: 0.0005808881251141429 \t\n",
      "Epoch 17790 \t\t Training Loss: 0.0005808881251141429 \t\n",
      "Epoch 17791 \t\t Training Loss: 0.0005808881251141429 \t\n",
      "Epoch 17792 \t\t Training Loss: 0.0005808881251141429 \t\n",
      "Epoch 17793 \t\t Training Loss: 0.0005808881251141429 \t\n",
      "Epoch 17794 \t\t Training Loss: 0.0005808881251141429 \t\n",
      "Epoch 17795 \t\t Training Loss: 0.0005808881251141429 \t\n",
      "Epoch 17796 \t\t Training Loss: 0.0005808881251141429 \t\n",
      "Epoch 17797 \t\t Training Loss: 0.0005808881251141429 \t\n",
      "Epoch 17798 \t\t Training Loss: 0.0005808881251141429 \t\n",
      "Epoch 17799 \t\t Training Loss: 0.0005808881251141429 \t\n",
      "Epoch 17800 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17801 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17802 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17803 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17804 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17805 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17806 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17807 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17808 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17809 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17810 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17811 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17812 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17813 \t\t Training Loss: 0.0005808879504911602 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17814 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17815 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17816 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17817 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17818 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17819 \t\t Training Loss: 0.0005808881251141429 \t\n",
      "Epoch 17820 \t\t Training Loss: 0.0005808881251141429 \t\n",
      "Epoch 17821 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17822 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17823 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17824 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17825 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17826 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17827 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17828 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17829 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17830 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17831 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17832 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17833 \t\t Training Loss: 0.000580888066906482 \t\n",
      "Epoch 17834 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17835 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17836 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17837 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17838 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17839 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17840 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17841 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17842 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17843 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17844 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17845 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17846 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17847 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17848 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17849 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17850 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17851 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17852 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17853 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17854 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17855 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17856 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17857 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17858 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17859 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17860 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17861 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17862 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17863 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17864 \t\t Training Loss: 0.0005808878922834992 \t\n",
      "Epoch 17865 \t\t Training Loss: 0.0005808878922834992 \t\n",
      "Epoch 17866 \t\t Training Loss: 0.0005808878922834992 \t\n",
      "Epoch 17867 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17868 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17869 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17870 \t\t Training Loss: 0.0005808879504911602 \t\n",
      "Epoch 17871 \t\t Training Loss: 0.0005808878922834992 \t\n",
      "Epoch 17872 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17873 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17874 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17875 \t\t Training Loss: 0.0005808878922834992 \t\n",
      "Epoch 17876 \t\t Training Loss: 0.0005808878922834992 \t\n",
      "Epoch 17877 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17878 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17879 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17880 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17881 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17882 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17883 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17884 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17885 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17886 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17887 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17888 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17889 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17890 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17891 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17892 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17893 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17894 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17895 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17896 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17897 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17898 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17899 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17900 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17901 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17902 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17903 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17904 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17905 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17906 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17907 \t\t Training Loss: 0.0005808876594528556 \t\n",
      "Epoch 17908 \t\t Training Loss: 0.0005808876594528556 \t\n",
      "Epoch 17909 \t\t Training Loss: 0.0005808876594528556 \t\n",
      "Epoch 17910 \t\t Training Loss: 0.0005808878340758383 \t\n",
      "Epoch 17911 \t\t Training Loss: 0.0005808876594528556 \t\n",
      "Epoch 17912 \t\t Training Loss: 0.0005808876594528556 \t\n",
      "Epoch 17913 \t\t Training Loss: 0.0005808876594528556 \t\n",
      "Epoch 17914 \t\t Training Loss: 0.0005808876594528556 \t\n",
      "Epoch 17915 \t\t Training Loss: 0.0005808876594528556 \t\n",
      "Epoch 17916 \t\t Training Loss: 0.0005808876594528556 \t\n",
      "Epoch 17917 \t\t Training Loss: 0.0005808876594528556 \t\n",
      "Epoch 17918 \t\t Training Loss: 0.0005808876594528556 \t\n",
      "Epoch 17919 \t\t Training Loss: 0.0005808876594528556 \t\n",
      "Epoch 17920 \t\t Training Loss: 0.0005808876594528556 \t\n",
      "Epoch 17921 \t\t Training Loss: 0.0005808876594528556 \t\n",
      "Epoch 17922 \t\t Training Loss: 0.0005808876594528556 \t\n",
      "Epoch 17923 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17924 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17925 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17926 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17927 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17928 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17929 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17930 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17931 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17932 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17933 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17934 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17935 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17936 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17937 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17938 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17939 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17940 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17941 \t\t Training Loss: 0.0005808874848298728 \t\n",
      "Epoch 17942 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17943 \t\t Training Loss: 0.0005808874848298728 \t\n",
      "Epoch 17944 \t\t Training Loss: 0.0005808874848298728 \t\n",
      "Epoch 17945 \t\t Training Loss: 0.0005808875430375338 \t\n",
      "Epoch 17946 \t\t Training Loss: 0.0005808874848298728 \t\n",
      "Epoch 17947 \t\t Training Loss: 0.0005808874848298728 \t\n",
      "Epoch 17948 \t\t Training Loss: 0.0005808874848298728 \t\n",
      "Epoch 17949 \t\t Training Loss: 0.0005808874848298728 \t\n",
      "Epoch 17950 \t\t Training Loss: 0.0005808874848298728 \t\n",
      "Epoch 17951 \t\t Training Loss: 0.0005808874848298728 \t\n",
      "Epoch 17952 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17953 \t\t Training Loss: 0.0005808874848298728 \t\n",
      "Epoch 17954 \t\t Training Loss: 0.0005808874848298728 \t\n",
      "Epoch 17955 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17956 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17957 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17958 \t\t Training Loss: 0.0005808874848298728 \t\n",
      "Epoch 17959 \t\t Training Loss: 0.0005808874848298728 \t\n",
      "Epoch 17960 \t\t Training Loss: 0.0005808874848298728 \t\n",
      "Epoch 17961 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17962 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17963 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17964 \t\t Training Loss: 0.0005808874848298728 \t\n",
      "Epoch 17965 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17966 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17967 \t\t Training Loss: 0.0005808874266222119 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17968 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17969 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17970 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17971 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17972 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17973 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17974 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17975 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17976 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17977 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17978 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17979 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17980 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17981 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17982 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17983 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17984 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17985 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17986 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17987 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17988 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17989 \t\t Training Loss: 0.000580887368414551 \t\n",
      "Epoch 17990 \t\t Training Loss: 0.000580887368414551 \t\n",
      "Epoch 17991 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17992 \t\t Training Loss: 0.0005808874266222119 \t\n",
      "Epoch 17993 \t\t Training Loss: 0.000580887368414551 \t\n",
      "Epoch 17994 \t\t Training Loss: 0.000580887368414551 \t\n",
      "Epoch 17995 \t\t Training Loss: 0.000580887368414551 \t\n",
      "Epoch 17996 \t\t Training Loss: 0.000580887368414551 \t\n",
      "Epoch 17997 \t\t Training Loss: 0.000580887368414551 \t\n",
      "Epoch 17998 \t\t Training Loss: 0.000580887368414551 \t\n",
      "Epoch 17999 \t\t Training Loss: 0.000580887368414551 \t\n",
      "Epoch 18000 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18001 \t\t Training Loss: 0.000580887368414551 \t\n",
      "Epoch 18002 \t\t Training Loss: 0.000580887368414551 \t\n",
      "Epoch 18003 \t\t Training Loss: 0.000580887368414551 \t\n",
      "Epoch 18004 \t\t Training Loss: 0.000580887368414551 \t\n",
      "Epoch 18005 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18006 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18007 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18008 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18009 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18010 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18011 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18012 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18013 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18014 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18015 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18016 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18017 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18018 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18019 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18020 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18021 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18022 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18023 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18024 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18025 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18026 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18027 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18028 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18029 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18030 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18031 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18032 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18033 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18034 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18035 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18036 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18037 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18038 \t\t Training Loss: 0.0005808872519992292 \t\n",
      "Epoch 18039 \t\t Training Loss: 0.0005808871355839074 \t\n",
      "Epoch 18040 \t\t Training Loss: 0.0005808871355839074 \t\n",
      "Epoch 18041 \t\t Training Loss: 0.0005808871355839074 \t\n",
      "Epoch 18042 \t\t Training Loss: 0.0005808871355839074 \t\n",
      "Epoch 18043 \t\t Training Loss: 0.0005808871355839074 \t\n",
      "Epoch 18044 \t\t Training Loss: 0.0005808871355839074 \t\n",
      "Epoch 18045 \t\t Training Loss: 0.0005808871355839074 \t\n",
      "Epoch 18046 \t\t Training Loss: 0.0005808871355839074 \t\n",
      "Epoch 18047 \t\t Training Loss: 0.0005808870773762465 \t\n",
      "Epoch 18048 \t\t Training Loss: 0.0005808871355839074 \t\n",
      "Epoch 18049 \t\t Training Loss: 0.0005808871355839074 \t\n",
      "Epoch 18050 \t\t Training Loss: 0.0005808871355839074 \t\n",
      "Epoch 18051 \t\t Training Loss: 0.0005808870773762465 \t\n",
      "Epoch 18052 \t\t Training Loss: 0.0005808871355839074 \t\n",
      "Epoch 18053 \t\t Training Loss: 0.0005808871355839074 \t\n",
      "Epoch 18054 \t\t Training Loss: 0.0005808870773762465 \t\n",
      "Epoch 18055 \t\t Training Loss: 0.0005808870773762465 \t\n",
      "Epoch 18056 \t\t Training Loss: 0.0005808870773762465 \t\n",
      "Epoch 18057 \t\t Training Loss: 0.0005808870773762465 \t\n",
      "Epoch 18058 \t\t Training Loss: 0.0005808870773762465 \t\n",
      "Epoch 18059 \t\t Training Loss: 0.0005808870773762465 \t\n",
      "Epoch 18060 \t\t Training Loss: 0.0005808870773762465 \t\n",
      "Epoch 18061 \t\t Training Loss: 0.0005808870773762465 \t\n",
      "Epoch 18062 \t\t Training Loss: 0.0005808870773762465 \t\n",
      "Epoch 18063 \t\t Training Loss: 0.0005808870773762465 \t\n",
      "Epoch 18064 \t\t Training Loss: 0.0005808870191685855 \t\n",
      "Epoch 18065 \t\t Training Loss: 0.0005808870191685855 \t\n",
      "Epoch 18066 \t\t Training Loss: 0.0005808870191685855 \t\n",
      "Epoch 18067 \t\t Training Loss: 0.0005808870191685855 \t\n",
      "Epoch 18068 \t\t Training Loss: 0.0005808870191685855 \t\n",
      "Epoch 18069 \t\t Training Loss: 0.0005808870191685855 \t\n",
      "Epoch 18070 \t\t Training Loss: 0.0005808870191685855 \t\n",
      "Epoch 18071 \t\t Training Loss: 0.0005808870191685855 \t\n",
      "Epoch 18072 \t\t Training Loss: 0.0005808870191685855 \t\n",
      "Epoch 18073 \t\t Training Loss: 0.0005808870191685855 \t\n",
      "Epoch 18074 \t\t Training Loss: 0.0005808870191685855 \t\n",
      "Epoch 18075 \t\t Training Loss: 0.0005808870191685855 \t\n",
      "Epoch 18076 \t\t Training Loss: 0.0005808870191685855 \t\n",
      "Epoch 18077 \t\t Training Loss: 0.0005808869609609246 \t\n",
      "Epoch 18078 \t\t Training Loss: 0.0005808869609609246 \t\n",
      "Epoch 18079 \t\t Training Loss: 0.0005808869609609246 \t\n",
      "Epoch 18080 \t\t Training Loss: 0.0005808869027532637 \t\n",
      "Epoch 18081 \t\t Training Loss: 0.0005808869027532637 \t\n",
      "Epoch 18082 \t\t Training Loss: 0.0005808869027532637 \t\n",
      "Epoch 18083 \t\t Training Loss: 0.0005808869027532637 \t\n",
      "Epoch 18084 \t\t Training Loss: 0.0005808869027532637 \t\n",
      "Epoch 18085 \t\t Training Loss: 0.0005808869027532637 \t\n",
      "Epoch 18086 \t\t Training Loss: 0.0005808869027532637 \t\n",
      "Epoch 18087 \t\t Training Loss: 0.0005808869027532637 \t\n",
      "Epoch 18088 \t\t Training Loss: 0.0005808869027532637 \t\n",
      "Epoch 18089 \t\t Training Loss: 0.0005808869027532637 \t\n",
      "Epoch 18090 \t\t Training Loss: 0.0005808869027532637 \t\n",
      "Epoch 18091 \t\t Training Loss: 0.0005808869027532637 \t\n",
      "Epoch 18092 \t\t Training Loss: 0.0005808869027532637 \t\n",
      "Epoch 18093 \t\t Training Loss: 0.0005808869027532637 \t\n",
      "Epoch 18094 \t\t Training Loss: 0.0005808868445456028 \t\n",
      "Epoch 18095 \t\t Training Loss: 0.0005808868445456028 \t\n",
      "Epoch 18096 \t\t Training Loss: 0.0005808869027532637 \t\n",
      "Epoch 18097 \t\t Training Loss: 0.0005808869027532637 \t\n",
      "Epoch 18098 \t\t Training Loss: 0.0005808868445456028 \t\n",
      "Epoch 18099 \t\t Training Loss: 0.0005808868445456028 \t\n",
      "Epoch 18100 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18101 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18102 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18103 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18104 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18105 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18106 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18107 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18108 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18109 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18110 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18111 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18112 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18113 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18114 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18115 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18116 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18117 \t\t Training Loss: 0.0005808867863379419 \t\n",
      "Epoch 18118 \t\t Training Loss: 0.000580886728130281 \t\n",
      "Epoch 18119 \t\t Training Loss: 0.0005808866699226201 \t\n",
      "Epoch 18120 \t\t Training Loss: 0.0005808866699226201 \t\n",
      "Epoch 18121 \t\t Training Loss: 0.0005808866699226201 \t\n",
      "Epoch 18122 \t\t Training Loss: 0.0005808866699226201 \t\n",
      "Epoch 18123 \t\t Training Loss: 0.0005808866699226201 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18124 \t\t Training Loss: 0.0005808866699226201 \t\n",
      "Epoch 18125 \t\t Training Loss: 0.0005808866699226201 \t\n",
      "Epoch 18126 \t\t Training Loss: 0.0005808866699226201 \t\n",
      "Epoch 18127 \t\t Training Loss: 0.0005808866699226201 \t\n",
      "Epoch 18128 \t\t Training Loss: 0.0005808866699226201 \t\n",
      "Epoch 18129 \t\t Training Loss: 0.0005808866699226201 \t\n",
      "Epoch 18130 \t\t Training Loss: 0.0005808866699226201 \t\n",
      "Epoch 18131 \t\t Training Loss: 0.0005808866699226201 \t\n",
      "Epoch 18132 \t\t Training Loss: 0.0005808866699226201 \t\n",
      "Epoch 18133 \t\t Training Loss: 0.0005808865535072982 \t\n",
      "Epoch 18134 \t\t Training Loss: 0.0005808865535072982 \t\n",
      "Epoch 18135 \t\t Training Loss: 0.0005808866117149591 \t\n",
      "Epoch 18136 \t\t Training Loss: 0.0005808866117149591 \t\n",
      "Epoch 18137 \t\t Training Loss: 0.0005808866117149591 \t\n",
      "Epoch 18138 \t\t Training Loss: 0.0005808866117149591 \t\n",
      "Epoch 18139 \t\t Training Loss: 0.0005808865535072982 \t\n",
      "Epoch 18140 \t\t Training Loss: 0.0005808865535072982 \t\n",
      "Epoch 18141 \t\t Training Loss: 0.0005808865535072982 \t\n",
      "Epoch 18142 \t\t Training Loss: 0.0005808865535072982 \t\n",
      "Epoch 18143 \t\t Training Loss: 0.0005808865535072982 \t\n",
      "Epoch 18144 \t\t Training Loss: 0.0005808865535072982 \t\n",
      "Epoch 18145 \t\t Training Loss: 0.0005808865535072982 \t\n",
      "Epoch 18146 \t\t Training Loss: 0.0005808865535072982 \t\n",
      "Epoch 18147 \t\t Training Loss: 0.0005808865535072982 \t\n",
      "Epoch 18148 \t\t Training Loss: 0.0005808865535072982 \t\n",
      "Epoch 18149 \t\t Training Loss: 0.0005808865535072982 \t\n",
      "Epoch 18150 \t\t Training Loss: 0.0005808864952996373 \t\n",
      "Epoch 18151 \t\t Training Loss: 0.0005808864952996373 \t\n",
      "Epoch 18152 \t\t Training Loss: 0.0005808864952996373 \t\n",
      "Epoch 18153 \t\t Training Loss: 0.0005808864952996373 \t\n",
      "Epoch 18154 \t\t Training Loss: 0.0005808864952996373 \t\n",
      "Epoch 18155 \t\t Training Loss: 0.0005808864952996373 \t\n",
      "Epoch 18156 \t\t Training Loss: 0.0005808864952996373 \t\n",
      "Epoch 18157 \t\t Training Loss: 0.0005808864370919764 \t\n",
      "Epoch 18158 \t\t Training Loss: 0.0005808864370919764 \t\n",
      "Epoch 18159 \t\t Training Loss: 0.0005808864370919764 \t\n",
      "Epoch 18160 \t\t Training Loss: 0.0005808864370919764 \t\n",
      "Epoch 18161 \t\t Training Loss: 0.0005808864370919764 \t\n",
      "Epoch 18162 \t\t Training Loss: 0.0005808864370919764 \t\n",
      "Epoch 18163 \t\t Training Loss: 0.0005808864370919764 \t\n",
      "Epoch 18164 \t\t Training Loss: 0.0005808864370919764 \t\n",
      "Epoch 18165 \t\t Training Loss: 0.0005808864370919764 \t\n",
      "Epoch 18166 \t\t Training Loss: 0.0005808863788843155 \t\n",
      "Epoch 18167 \t\t Training Loss: 0.0005808863788843155 \t\n",
      "Epoch 18168 \t\t Training Loss: 0.0005808863788843155 \t\n",
      "Epoch 18169 \t\t Training Loss: 0.0005808864370919764 \t\n",
      "Epoch 18170 \t\t Training Loss: 0.0005808864370919764 \t\n",
      "Epoch 18171 \t\t Training Loss: 0.0005808863788843155 \t\n",
      "Epoch 18172 \t\t Training Loss: 0.0005808863788843155 \t\n",
      "Epoch 18173 \t\t Training Loss: 0.0005808863788843155 \t\n",
      "Epoch 18174 \t\t Training Loss: 0.0005808863788843155 \t\n",
      "Epoch 18175 \t\t Training Loss: 0.0005808863788843155 \t\n",
      "Epoch 18176 \t\t Training Loss: 0.0005808863788843155 \t\n",
      "Epoch 18177 \t\t Training Loss: 0.0005808863788843155 \t\n",
      "Epoch 18178 \t\t Training Loss: 0.0005808863788843155 \t\n",
      "Epoch 18179 \t\t Training Loss: 0.0005808863788843155 \t\n",
      "Epoch 18180 \t\t Training Loss: 0.0005808863206766546 \t\n",
      "Epoch 18181 \t\t Training Loss: 0.0005808863206766546 \t\n",
      "Epoch 18182 \t\t Training Loss: 0.0005808863206766546 \t\n",
      "Epoch 18183 \t\t Training Loss: 0.0005808863206766546 \t\n",
      "Epoch 18184 \t\t Training Loss: 0.0005808863206766546 \t\n",
      "Epoch 18185 \t\t Training Loss: 0.0005808863206766546 \t\n",
      "Epoch 18186 \t\t Training Loss: 0.0005808863206766546 \t\n",
      "Epoch 18187 \t\t Training Loss: 0.0005808863206766546 \t\n",
      "Epoch 18188 \t\t Training Loss: 0.0005808863206766546 \t\n",
      "Epoch 18189 \t\t Training Loss: 0.0005808863206766546 \t\n",
      "Epoch 18190 \t\t Training Loss: 0.0005808862624689937 \t\n",
      "Epoch 18191 \t\t Training Loss: 0.0005808863206766546 \t\n",
      "Epoch 18192 \t\t Training Loss: 0.0005808863206766546 \t\n",
      "Epoch 18193 \t\t Training Loss: 0.0005808862624689937 \t\n",
      "Epoch 18194 \t\t Training Loss: 0.0005808863206766546 \t\n",
      "Epoch 18195 \t\t Training Loss: 0.0005808863206766546 \t\n",
      "Epoch 18196 \t\t Training Loss: 0.0005808862624689937 \t\n",
      "Epoch 18197 \t\t Training Loss: 0.0005808862624689937 \t\n",
      "Epoch 18198 \t\t Training Loss: 0.0005808862624689937 \t\n",
      "Epoch 18199 \t\t Training Loss: 0.0005808862624689937 \t\n",
      "Epoch 18200 \t\t Training Loss: 0.0005808862624689937 \t\n",
      "Epoch 18201 \t\t Training Loss: 0.0005808862624689937 \t\n",
      "Epoch 18202 \t\t Training Loss: 0.0005808862042613328 \t\n",
      "Epoch 18203 \t\t Training Loss: 0.0005808862624689937 \t\n",
      "Epoch 18204 \t\t Training Loss: 0.0005808862042613328 \t\n",
      "Epoch 18205 \t\t Training Loss: 0.0005808862042613328 \t\n",
      "Epoch 18206 \t\t Training Loss: 0.0005808862042613328 \t\n",
      "Epoch 18207 \t\t Training Loss: 0.0005808862042613328 \t\n",
      "Epoch 18208 \t\t Training Loss: 0.0005808862042613328 \t\n",
      "Epoch 18209 \t\t Training Loss: 0.0005808862042613328 \t\n",
      "Epoch 18210 \t\t Training Loss: 0.0005808862042613328 \t\n",
      "Epoch 18211 \t\t Training Loss: 0.0005808861460536718 \t\n",
      "Epoch 18212 \t\t Training Loss: 0.0005808861460536718 \t\n",
      "Epoch 18213 \t\t Training Loss: 0.0005808861460536718 \t\n",
      "Epoch 18214 \t\t Training Loss: 0.0005808861460536718 \t\n",
      "Epoch 18215 \t\t Training Loss: 0.0005808860878460109 \t\n",
      "Epoch 18216 \t\t Training Loss: 0.0005808861460536718 \t\n",
      "Epoch 18217 \t\t Training Loss: 0.0005808860878460109 \t\n",
      "Epoch 18218 \t\t Training Loss: 0.0005808860878460109 \t\n",
      "Epoch 18219 \t\t Training Loss: 0.0005808860878460109 \t\n",
      "Epoch 18220 \t\t Training Loss: 0.0005808860878460109 \t\n",
      "Epoch 18221 \t\t Training Loss: 0.00058088602963835 \t\n",
      "Epoch 18222 \t\t Training Loss: 0.00058088602963835 \t\n",
      "Epoch 18223 \t\t Training Loss: 0.00058088602963835 \t\n",
      "Epoch 18224 \t\t Training Loss: 0.00058088602963835 \t\n",
      "Epoch 18225 \t\t Training Loss: 0.00058088602963835 \t\n",
      "Epoch 18226 \t\t Training Loss: 0.00058088602963835 \t\n",
      "Epoch 18227 \t\t Training Loss: 0.00058088602963835 \t\n",
      "Epoch 18228 \t\t Training Loss: 0.00058088602963835 \t\n",
      "Epoch 18229 \t\t Training Loss: 0.00058088602963835 \t\n",
      "Epoch 18230 \t\t Training Loss: 0.00058088602963835 \t\n",
      "Epoch 18231 \t\t Training Loss: 0.00058088602963835 \t\n",
      "Epoch 18232 \t\t Training Loss: 0.0005808859714306891 \t\n",
      "Epoch 18233 \t\t Training Loss: 0.0005808859714306891 \t\n",
      "Epoch 18234 \t\t Training Loss: 0.0005808859714306891 \t\n",
      "Epoch 18235 \t\t Training Loss: 0.0005808859714306891 \t\n",
      "Epoch 18236 \t\t Training Loss: 0.0005808859714306891 \t\n",
      "Epoch 18237 \t\t Training Loss: 0.0005808859714306891 \t\n",
      "Epoch 18238 \t\t Training Loss: 0.0005808859714306891 \t\n",
      "Epoch 18239 \t\t Training Loss: 0.0005808859714306891 \t\n",
      "Epoch 18240 \t\t Training Loss: 0.0005808859714306891 \t\n",
      "Epoch 18241 \t\t Training Loss: 0.0005808859714306891 \t\n",
      "Epoch 18242 \t\t Training Loss: 0.0005808859714306891 \t\n",
      "Epoch 18243 \t\t Training Loss: 0.0005808859714306891 \t\n",
      "Epoch 18244 \t\t Training Loss: 0.0005808859714306891 \t\n",
      "Epoch 18245 \t\t Training Loss: 0.0005808859714306891 \t\n",
      "Epoch 18246 \t\t Training Loss: 0.0005808859714306891 \t\n",
      "Epoch 18247 \t\t Training Loss: 0.0005808859132230282 \t\n",
      "Epoch 18248 \t\t Training Loss: 0.0005808859132230282 \t\n",
      "Epoch 18249 \t\t Training Loss: 0.0005808859132230282 \t\n",
      "Epoch 18250 \t\t Training Loss: 0.0005808859132230282 \t\n",
      "Epoch 18251 \t\t Training Loss: 0.0005808859132230282 \t\n",
      "Epoch 18252 \t\t Training Loss: 0.0005808859132230282 \t\n",
      "Epoch 18253 \t\t Training Loss: 0.0005808859132230282 \t\n",
      "Epoch 18254 \t\t Training Loss: 0.0005808859132230282 \t\n",
      "Epoch 18255 \t\t Training Loss: 0.0005808859132230282 \t\n",
      "Epoch 18256 \t\t Training Loss: 0.0005808859132230282 \t\n",
      "Epoch 18257 \t\t Training Loss: 0.0005808859132230282 \t\n",
      "Epoch 18258 \t\t Training Loss: 0.0005808858550153673 \t\n",
      "Epoch 18259 \t\t Training Loss: 0.0005808858550153673 \t\n",
      "Epoch 18260 \t\t Training Loss: 0.0005808858550153673 \t\n",
      "Epoch 18261 \t\t Training Loss: 0.0005808858550153673 \t\n",
      "Epoch 18262 \t\t Training Loss: 0.0005808858550153673 \t\n",
      "Epoch 18263 \t\t Training Loss: 0.0005808858550153673 \t\n",
      "Epoch 18264 \t\t Training Loss: 0.0005808858550153673 \t\n",
      "Epoch 18265 \t\t Training Loss: 0.0005808857968077064 \t\n",
      "Epoch 18266 \t\t Training Loss: 0.0005808857968077064 \t\n",
      "Epoch 18267 \t\t Training Loss: 0.0005808857968077064 \t\n",
      "Epoch 18268 \t\t Training Loss: 0.0005808857968077064 \t\n",
      "Epoch 18269 \t\t Training Loss: 0.0005808857968077064 \t\n",
      "Epoch 18270 \t\t Training Loss: 0.0005808857968077064 \t\n",
      "Epoch 18271 \t\t Training Loss: 0.0005808857968077064 \t\n",
      "Epoch 18272 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18273 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18274 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18275 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18276 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18277 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18278 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18279 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18280 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18281 \t\t Training Loss: 0.0005808857386000454 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18282 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18283 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18284 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18285 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18286 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18287 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18288 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18289 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18290 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18291 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18292 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18293 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18294 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18295 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18296 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18297 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18298 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18299 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18300 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18301 \t\t Training Loss: 0.0005808857386000454 \t\n",
      "Epoch 18302 \t\t Training Loss: 0.0005808856803923845 \t\n",
      "Epoch 18303 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18304 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18305 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18306 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18307 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18308 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18309 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18310 \t\t Training Loss: 0.0005808856803923845 \t\n",
      "Epoch 18311 \t\t Training Loss: 0.0005808856803923845 \t\n",
      "Epoch 18312 \t\t Training Loss: 0.0005808856803923845 \t\n",
      "Epoch 18313 \t\t Training Loss: 0.0005808856803923845 \t\n",
      "Epoch 18314 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18315 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18316 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18317 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18318 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18319 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18320 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18321 \t\t Training Loss: 0.0005808856803923845 \t\n",
      "Epoch 18322 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18323 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18324 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18325 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18326 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18327 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18328 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18329 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18330 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18331 \t\t Training Loss: 0.0005808856221847236 \t\n",
      "Epoch 18332 \t\t Training Loss: 0.0005808855057694018 \t\n",
      "Epoch 18333 \t\t Training Loss: 0.0005808855057694018 \t\n",
      "Epoch 18334 \t\t Training Loss: 0.0005808855057694018 \t\n",
      "Epoch 18335 \t\t Training Loss: 0.0005808855057694018 \t\n",
      "Epoch 18336 \t\t Training Loss: 0.0005808855057694018 \t\n",
      "Epoch 18337 \t\t Training Loss: 0.0005808855057694018 \t\n",
      "Epoch 18338 \t\t Training Loss: 0.0005808855057694018 \t\n",
      "Epoch 18339 \t\t Training Loss: 0.0005808855057694018 \t\n",
      "Epoch 18340 \t\t Training Loss: 0.0005808855057694018 \t\n",
      "Epoch 18341 \t\t Training Loss: 0.0005808855057694018 \t\n",
      "Epoch 18342 \t\t Training Loss: 0.0005808854475617409 \t\n",
      "Epoch 18343 \t\t Training Loss: 0.0005808855057694018 \t\n",
      "Epoch 18344 \t\t Training Loss: 0.0005808854475617409 \t\n",
      "Epoch 18345 \t\t Training Loss: 0.0005808854475617409 \t\n",
      "Epoch 18346 \t\t Training Loss: 0.0005808855057694018 \t\n",
      "Epoch 18347 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18348 \t\t Training Loss: 0.0005808854475617409 \t\n",
      "Epoch 18349 \t\t Training Loss: 0.0005808854475617409 \t\n",
      "Epoch 18350 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18351 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18352 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18353 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18354 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18355 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18356 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18357 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18358 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18359 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18360 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18361 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18362 \t\t Training Loss: 0.000580885331146419 \t\n",
      "Epoch 18363 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18364 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18365 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18366 \t\t Training Loss: 0.000580885331146419 \t\n",
      "Epoch 18367 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18368 \t\t Training Loss: 0.000580885331146419 \t\n",
      "Epoch 18369 \t\t Training Loss: 0.000580885331146419 \t\n",
      "Epoch 18370 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18371 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18372 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18373 \t\t Training Loss: 0.000580885331146419 \t\n",
      "Epoch 18374 \t\t Training Loss: 0.000580885331146419 \t\n",
      "Epoch 18375 \t\t Training Loss: 0.000580885331146419 \t\n",
      "Epoch 18376 \t\t Training Loss: 0.00058088538935408 \t\n",
      "Epoch 18377 \t\t Training Loss: 0.000580885331146419 \t\n",
      "Epoch 18378 \t\t Training Loss: 0.000580885331146419 \t\n",
      "Epoch 18379 \t\t Training Loss: 0.000580885331146419 \t\n",
      "Epoch 18380 \t\t Training Loss: 0.0005808852729387581 \t\n",
      "Epoch 18381 \t\t Training Loss: 0.000580885331146419 \t\n",
      "Epoch 18382 \t\t Training Loss: 0.000580885331146419 \t\n",
      "Epoch 18383 \t\t Training Loss: 0.0005808852729387581 \t\n",
      "Epoch 18384 \t\t Training Loss: 0.0005808852729387581 \t\n",
      "Epoch 18385 \t\t Training Loss: 0.0005808852729387581 \t\n",
      "Epoch 18386 \t\t Training Loss: 0.0005808852729387581 \t\n",
      "Epoch 18387 \t\t Training Loss: 0.0005808852729387581 \t\n",
      "Epoch 18388 \t\t Training Loss: 0.0005808852147310972 \t\n",
      "Epoch 18389 \t\t Training Loss: 0.0005808852147310972 \t\n",
      "Epoch 18390 \t\t Training Loss: 0.0005808852147310972 \t\n",
      "Epoch 18391 \t\t Training Loss: 0.0005808852147310972 \t\n",
      "Epoch 18392 \t\t Training Loss: 0.0005808852147310972 \t\n",
      "Epoch 18393 \t\t Training Loss: 0.0005808852147310972 \t\n",
      "Epoch 18394 \t\t Training Loss: 0.0005808852147310972 \t\n",
      "Epoch 18395 \t\t Training Loss: 0.0005808852147310972 \t\n",
      "Epoch 18396 \t\t Training Loss: 0.0005808851565234363 \t\n",
      "Epoch 18397 \t\t Training Loss: 0.0005808852147310972 \t\n",
      "Epoch 18398 \t\t Training Loss: 0.0005808851565234363 \t\n",
      "Epoch 18399 \t\t Training Loss: 0.0005808851565234363 \t\n",
      "Epoch 18400 \t\t Training Loss: 0.0005808851565234363 \t\n",
      "Epoch 18401 \t\t Training Loss: 0.0005808851565234363 \t\n",
      "Epoch 18402 \t\t Training Loss: 0.0005808851565234363 \t\n",
      "Epoch 18403 \t\t Training Loss: 0.0005808851565234363 \t\n",
      "Epoch 18404 \t\t Training Loss: 0.0005808850983157754 \t\n",
      "Epoch 18405 \t\t Training Loss: 0.0005808850983157754 \t\n",
      "Epoch 18406 \t\t Training Loss: 0.0005808850983157754 \t\n",
      "Epoch 18407 \t\t Training Loss: 0.0005808850983157754 \t\n",
      "Epoch 18408 \t\t Training Loss: 0.0005808850983157754 \t\n",
      "Epoch 18409 \t\t Training Loss: 0.0005808850983157754 \t\n",
      "Epoch 18410 \t\t Training Loss: 0.0005808850983157754 \t\n",
      "Epoch 18411 \t\t Training Loss: 0.0005808850983157754 \t\n",
      "Epoch 18412 \t\t Training Loss: 0.0005808850401081145 \t\n",
      "Epoch 18413 \t\t Training Loss: 0.0005808850401081145 \t\n",
      "Epoch 18414 \t\t Training Loss: 0.0005808850401081145 \t\n",
      "Epoch 18415 \t\t Training Loss: 0.0005808850401081145 \t\n",
      "Epoch 18416 \t\t Training Loss: 0.0005808850401081145 \t\n",
      "Epoch 18417 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18418 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18419 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18420 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18421 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18422 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18423 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18424 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18425 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18426 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18427 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18428 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18429 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18430 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18431 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18432 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18433 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18434 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18435 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18436 \t\t Training Loss: 0.0005808849236927927 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18437 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18438 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18439 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18440 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18441 \t\t Training Loss: 0.0005808848654851317 \t\n",
      "Epoch 18442 \t\t Training Loss: 0.0005808849236927927 \t\n",
      "Epoch 18443 \t\t Training Loss: 0.0005808848654851317 \t\n",
      "Epoch 18444 \t\t Training Loss: 0.0005808848654851317 \t\n",
      "Epoch 18445 \t\t Training Loss: 0.0005808848654851317 \t\n",
      "Epoch 18446 \t\t Training Loss: 0.0005808848654851317 \t\n",
      "Epoch 18447 \t\t Training Loss: 0.0005808848654851317 \t\n",
      "Epoch 18448 \t\t Training Loss: 0.0005808848072774708 \t\n",
      "Epoch 18449 \t\t Training Loss: 0.0005808848072774708 \t\n",
      "Epoch 18450 \t\t Training Loss: 0.0005808848072774708 \t\n",
      "Epoch 18451 \t\t Training Loss: 0.0005808848072774708 \t\n",
      "Epoch 18452 \t\t Training Loss: 0.0005808848072774708 \t\n",
      "Epoch 18453 \t\t Training Loss: 0.0005808848072774708 \t\n",
      "Epoch 18454 \t\t Training Loss: 0.0005808847490698099 \t\n",
      "Epoch 18455 \t\t Training Loss: 0.0005808847490698099 \t\n",
      "Epoch 18456 \t\t Training Loss: 0.0005808847490698099 \t\n",
      "Epoch 18457 \t\t Training Loss: 0.0005808848072774708 \t\n",
      "Epoch 18458 \t\t Training Loss: 0.0005808848072774708 \t\n",
      "Epoch 18459 \t\t Training Loss: 0.0005808847490698099 \t\n",
      "Epoch 18460 \t\t Training Loss: 0.0005808847490698099 \t\n",
      "Epoch 18461 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18462 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18463 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18464 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18465 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18466 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18467 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18468 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18469 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18470 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18471 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18472 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18473 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18474 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18475 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18476 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18477 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18478 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18479 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18480 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18481 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18482 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18483 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18484 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18485 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18486 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18487 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18488 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18489 \t\t Training Loss: 0.0005808846326544881 \t\n",
      "Epoch 18490 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18491 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18492 \t\t Training Loss: 0.0005808846326544881 \t\n",
      "Epoch 18493 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18494 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18495 \t\t Training Loss: 0.0005808846326544881 \t\n",
      "Epoch 18496 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18497 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18498 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18499 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18500 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18501 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18502 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18503 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18504 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18505 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18506 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18507 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18508 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18509 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18510 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18511 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18512 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18513 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18514 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18515 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18516 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18517 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18518 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18519 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18520 \t\t Training Loss: 0.000580884690862149 \t\n",
      "Epoch 18521 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18522 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18523 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18524 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18525 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18526 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18527 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18528 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18529 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18530 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18531 \t\t Training Loss: 0.0005808845162391663 \t\n",
      "Epoch 18532 \t\t Training Loss: 0.0005808845162391663 \t\n",
      "Epoch 18533 \t\t Training Loss: 0.0005808845162391663 \t\n",
      "Epoch 18534 \t\t Training Loss: 0.0005808845162391663 \t\n",
      "Epoch 18535 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18536 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18537 \t\t Training Loss: 0.0005808845744468272 \t\n",
      "Epoch 18538 \t\t Training Loss: 0.0005808845162391663 \t\n",
      "Epoch 18539 \t\t Training Loss: 0.0005808845162391663 \t\n",
      "Epoch 18540 \t\t Training Loss: 0.0005808845162391663 \t\n",
      "Epoch 18541 \t\t Training Loss: 0.0005808845162391663 \t\n",
      "Epoch 18542 \t\t Training Loss: 0.0005808845162391663 \t\n",
      "Epoch 18543 \t\t Training Loss: 0.0005808845162391663 \t\n",
      "Epoch 18544 \t\t Training Loss: 0.0005808845162391663 \t\n",
      "Epoch 18545 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18546 \t\t Training Loss: 0.0005808845162391663 \t\n",
      "Epoch 18547 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18548 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18549 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18550 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18551 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18552 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18553 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18554 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18555 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18556 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18557 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18558 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18559 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18560 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18561 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18562 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18563 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18564 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18565 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18566 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18567 \t\t Training Loss: 0.0005808844580315053 \t\n",
      "Epoch 18568 \t\t Training Loss: 0.0005808843998238444 \t\n",
      "Epoch 18569 \t\t Training Loss: 0.0005808843416161835 \t\n",
      "Epoch 18570 \t\t Training Loss: 0.0005808843416161835 \t\n",
      "Epoch 18571 \t\t Training Loss: 0.0005808842834085226 \t\n",
      "Epoch 18572 \t\t Training Loss: 0.0005808842834085226 \t\n",
      "Epoch 18573 \t\t Training Loss: 0.0005808842834085226 \t\n",
      "Epoch 18574 \t\t Training Loss: 0.0005808842834085226 \t\n",
      "Epoch 18575 \t\t Training Loss: 0.0005808842834085226 \t\n",
      "Epoch 18576 \t\t Training Loss: 0.0005808842834085226 \t\n",
      "Epoch 18577 \t\t Training Loss: 0.0005808842834085226 \t\n",
      "Epoch 18578 \t\t Training Loss: 0.0005808842834085226 \t\n",
      "Epoch 18579 \t\t Training Loss: 0.0005808842834085226 \t\n",
      "Epoch 18580 \t\t Training Loss: 0.0005808842834085226 \t\n",
      "Epoch 18581 \t\t Training Loss: 0.0005808842834085226 \t\n",
      "Epoch 18582 \t\t Training Loss: 0.0005808842834085226 \t\n",
      "Epoch 18583 \t\t Training Loss: 0.0005808842834085226 \t\n",
      "Epoch 18584 \t\t Training Loss: 0.0005808842834085226 \t\n",
      "Epoch 18585 \t\t Training Loss: 0.0005808842834085226 \t\n",
      "Epoch 18586 \t\t Training Loss: 0.0005808842834085226 \t\n",
      "Epoch 18587 \t\t Training Loss: 0.0005808842252008617 \t\n",
      "Epoch 18588 \t\t Training Loss: 0.0005808842252008617 \t\n",
      "Epoch 18589 \t\t Training Loss: 0.0005808841669932008 \t\n",
      "Epoch 18590 \t\t Training Loss: 0.0005808842252008617 \t\n",
      "Epoch 18591 \t\t Training Loss: 0.0005808842252008617 \t\n",
      "Epoch 18592 \t\t Training Loss: 0.0005808842252008617 \t\n",
      "Epoch 18593 \t\t Training Loss: 0.0005808842252008617 \t\n",
      "Epoch 18594 \t\t Training Loss: 0.0005808842252008617 \t\n",
      "Epoch 18595 \t\t Training Loss: 0.0005808841669932008 \t\n",
      "Epoch 18596 \t\t Training Loss: 0.0005808841669932008 \t\n",
      "Epoch 18597 \t\t Training Loss: 0.0005808841669932008 \t\n",
      "Epoch 18598 \t\t Training Loss: 0.0005808841669932008 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18599 \t\t Training Loss: 0.0005808841669932008 \t\n",
      "Epoch 18600 \t\t Training Loss: 0.0005808841669932008 \t\n",
      "Epoch 18601 \t\t Training Loss: 0.0005808841669932008 \t\n",
      "Epoch 18602 \t\t Training Loss: 0.0005808841669932008 \t\n",
      "Epoch 18603 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18604 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18605 \t\t Training Loss: 0.0005808841669932008 \t\n",
      "Epoch 18606 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18607 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18608 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18609 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18610 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18611 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18612 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18613 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18614 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18615 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18616 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18617 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18618 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18619 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18620 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18621 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18622 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18623 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18624 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18625 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18626 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18627 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18628 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18629 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18630 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18631 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18632 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18633 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18634 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18635 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18636 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18637 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18638 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18639 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18640 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18641 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18642 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18643 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18644 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18645 \t\t Training Loss: 0.0005808841087855399 \t\n",
      "Epoch 18646 \t\t Training Loss: 0.000580883992370218 \t\n",
      "Epoch 18647 \t\t Training Loss: 0.0005808839341625571 \t\n",
      "Epoch 18648 \t\t Training Loss: 0.0005808839341625571 \t\n",
      "Epoch 18649 \t\t Training Loss: 0.0005808839341625571 \t\n",
      "Epoch 18650 \t\t Training Loss: 0.0005808839341625571 \t\n",
      "Epoch 18651 \t\t Training Loss: 0.0005808839341625571 \t\n",
      "Epoch 18652 \t\t Training Loss: 0.0005808839341625571 \t\n",
      "Epoch 18653 \t\t Training Loss: 0.0005808839341625571 \t\n",
      "Epoch 18654 \t\t Training Loss: 0.0005808839341625571 \t\n",
      "Epoch 18655 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18656 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18657 \t\t Training Loss: 0.0005808839341625571 \t\n",
      "Epoch 18658 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18659 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18660 \t\t Training Loss: 0.0005808839341625571 \t\n",
      "Epoch 18661 \t\t Training Loss: 0.0005808839341625571 \t\n",
      "Epoch 18662 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18663 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18664 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18665 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18666 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18667 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18668 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18669 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18670 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18671 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18672 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18673 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18674 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18675 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18676 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18677 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18678 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18679 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18680 \t\t Training Loss: 0.0005808838177472353 \t\n",
      "Epoch 18681 \t\t Training Loss: 0.0005808838177472353 \t\n",
      "Epoch 18682 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18683 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18684 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18685 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18686 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18687 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18688 \t\t Training Loss: 0.0005808838759548962 \t\n",
      "Epoch 18689 \t\t Training Loss: 0.0005808838177472353 \t\n",
      "Epoch 18690 \t\t Training Loss: 0.0005808838177472353 \t\n",
      "Epoch 18691 \t\t Training Loss: 0.0005808838177472353 \t\n",
      "Epoch 18692 \t\t Training Loss: 0.0005808838177472353 \t\n",
      "Epoch 18693 \t\t Training Loss: 0.0005808838177472353 \t\n",
      "Epoch 18694 \t\t Training Loss: 0.0005808838177472353 \t\n",
      "Epoch 18695 \t\t Training Loss: 0.0005808838177472353 \t\n",
      "Epoch 18696 \t\t Training Loss: 0.0005808838177472353 \t\n",
      "Epoch 18697 \t\t Training Loss: 0.0005808838177472353 \t\n",
      "Epoch 18698 \t\t Training Loss: 0.0005808838177472353 \t\n",
      "Epoch 18699 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18700 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18701 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18702 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18703 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18704 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18705 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18706 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18707 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18708 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18709 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18710 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18711 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18712 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18713 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18714 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18715 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18716 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18717 \t\t Training Loss: 0.0005808837013319135 \t\n",
      "Epoch 18718 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18719 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18720 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18721 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18722 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18723 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18724 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18725 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18726 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18727 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18728 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18729 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18730 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18731 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18732 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18733 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18734 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18735 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18736 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18737 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18738 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18739 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18740 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18741 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18742 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18743 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18744 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18745 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18746 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18747 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18748 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18749 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18750 \t\t Training Loss: 0.0005808835267089307 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18751 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18752 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18753 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18754 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18755 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18756 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18757 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18758 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18759 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18760 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18761 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18762 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18763 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18764 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18765 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18766 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18767 \t\t Training Loss: 0.0005808835267089307 \t\n",
      "Epoch 18768 \t\t Training Loss: 0.0005808834102936089 \t\n",
      "Epoch 18769 \t\t Training Loss: 0.0005808834102936089 \t\n",
      "Epoch 18770 \t\t Training Loss: 0.0005808834102936089 \t\n",
      "Epoch 18771 \t\t Training Loss: 0.0005808834102936089 \t\n",
      "Epoch 18772 \t\t Training Loss: 0.0005808834102936089 \t\n",
      "Epoch 18773 \t\t Training Loss: 0.0005808834102936089 \t\n",
      "Epoch 18774 \t\t Training Loss: 0.0005808834102936089 \t\n",
      "Epoch 18775 \t\t Training Loss: 0.0005808834102936089 \t\n",
      "Epoch 18776 \t\t Training Loss: 0.0005808834102936089 \t\n",
      "Epoch 18777 \t\t Training Loss: 0.0005808832938782871 \t\n",
      "Epoch 18778 \t\t Training Loss: 0.0005808834102936089 \t\n",
      "Epoch 18779 \t\t Training Loss: 0.0005808834102936089 \t\n",
      "Epoch 18780 \t\t Training Loss: 0.0005808832938782871 \t\n",
      "Epoch 18781 \t\t Training Loss: 0.0005808832938782871 \t\n",
      "Epoch 18782 \t\t Training Loss: 0.0005808832938782871 \t\n",
      "Epoch 18783 \t\t Training Loss: 0.0005808832938782871 \t\n",
      "Epoch 18784 \t\t Training Loss: 0.0005808832938782871 \t\n",
      "Epoch 18785 \t\t Training Loss: 0.0005808832938782871 \t\n",
      "Epoch 18786 \t\t Training Loss: 0.0005808832938782871 \t\n",
      "Epoch 18787 \t\t Training Loss: 0.0005808832938782871 \t\n",
      "Epoch 18788 \t\t Training Loss: 0.0005808832938782871 \t\n",
      "Epoch 18789 \t\t Training Loss: 0.0005808832938782871 \t\n",
      "Epoch 18790 \t\t Training Loss: 0.0005808832938782871 \t\n",
      "Epoch 18791 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18792 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18793 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18794 \t\t Training Loss: 0.0005808832938782871 \t\n",
      "Epoch 18795 \t\t Training Loss: 0.0005808832938782871 \t\n",
      "Epoch 18796 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18797 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18798 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18799 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18800 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18801 \t\t Training Loss: 0.0005808832356706262 \t\n",
      "Epoch 18802 \t\t Training Loss: 0.0005808832356706262 \t\n",
      "Epoch 18803 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18804 \t\t Training Loss: 0.0005808832356706262 \t\n",
      "Epoch 18805 \t\t Training Loss: 0.0005808832938782871 \t\n",
      "Epoch 18806 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18807 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18808 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18809 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18810 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18811 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18812 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18813 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18814 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18815 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18816 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18817 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18818 \t\t Training Loss: 0.0005808831774629653 \t\n",
      "Epoch 18819 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18820 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18821 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18822 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18823 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18824 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18825 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18826 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18827 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18828 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18829 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18830 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18831 \t\t Training Loss: 0.0005808830028399825 \t\n",
      "Epoch 18832 \t\t Training Loss: 0.0005808831192553043 \t\n",
      "Epoch 18833 \t\t Training Loss: 0.0005808830028399825 \t\n",
      "Epoch 18834 \t\t Training Loss: 0.0005808830028399825 \t\n",
      "Epoch 18835 \t\t Training Loss: 0.0005808830028399825 \t\n",
      "Epoch 18836 \t\t Training Loss: 0.0005808830028399825 \t\n",
      "Epoch 18837 \t\t Training Loss: 0.0005808830028399825 \t\n",
      "Epoch 18838 \t\t Training Loss: 0.0005808830028399825 \t\n",
      "Epoch 18839 \t\t Training Loss: 0.0005808830028399825 \t\n",
      "Epoch 18840 \t\t Training Loss: 0.0005808830028399825 \t\n",
      "Epoch 18841 \t\t Training Loss: 0.0005808830028399825 \t\n",
      "Epoch 18842 \t\t Training Loss: 0.0005808830028399825 \t\n",
      "Epoch 18843 \t\t Training Loss: 0.0005808830028399825 \t\n",
      "Epoch 18844 \t\t Training Loss: 0.0005808830028399825 \t\n",
      "Epoch 18845 \t\t Training Loss: 0.0005808830028399825 \t\n",
      "Epoch 18846 \t\t Training Loss: 0.0005808830028399825 \t\n",
      "Epoch 18847 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18848 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18849 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18850 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18851 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18852 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18853 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18854 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18855 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18856 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18857 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18858 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18859 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18860 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18861 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18862 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18863 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18864 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18865 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18866 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18867 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18868 \t\t Training Loss: 0.0005808828864246607 \t\n",
      "Epoch 18869 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18870 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18871 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18872 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18873 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18874 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18875 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18876 \t\t Training Loss: 0.0005808827118016779 \t\n",
      "Epoch 18877 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18878 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18879 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18880 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18881 \t\t Training Loss: 0.0005808827118016779 \t\n",
      "Epoch 18882 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18883 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18884 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18885 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18886 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18887 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18888 \t\t Training Loss: 0.0005808828282169998 \t\n",
      "Epoch 18889 \t\t Training Loss: 0.0005808827118016779 \t\n",
      "Epoch 18890 \t\t Training Loss: 0.0005808827118016779 \t\n",
      "Epoch 18891 \t\t Training Loss: 0.0005808827118016779 \t\n",
      "Epoch 18892 \t\t Training Loss: 0.0005808827118016779 \t\n",
      "Epoch 18893 \t\t Training Loss: 0.0005808827118016779 \t\n",
      "Epoch 18894 \t\t Training Loss: 0.0005808827118016779 \t\n",
      "Epoch 18895 \t\t Training Loss: 0.0005808827118016779 \t\n",
      "Epoch 18896 \t\t Training Loss: 0.0005808827118016779 \t\n",
      "Epoch 18897 \t\t Training Loss: 0.0005808827118016779 \t\n",
      "Epoch 18898 \t\t Training Loss: 0.0005808827118016779 \t\n",
      "Epoch 18899 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18900 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18901 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18902 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18903 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18904 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18905 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18906 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18907 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18908 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18909 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18910 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18911 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18912 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18913 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18914 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18915 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18916 \t\t Training Loss: 0.0005808825953863561 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18917 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18918 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18919 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18920 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18921 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18922 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18923 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18924 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18925 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18926 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18927 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18928 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18929 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18930 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18931 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18932 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18933 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18934 \t\t Training Loss: 0.0005808825371786952 \t\n",
      "Epoch 18935 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18936 \t\t Training Loss: 0.0005808825953863561 \t\n",
      "Epoch 18937 \t\t Training Loss: 0.0005808825371786952 \t\n",
      "Epoch 18938 \t\t Training Loss: 0.0005808825371786952 \t\n",
      "Epoch 18939 \t\t Training Loss: 0.0005808825371786952 \t\n",
      "Epoch 18940 \t\t Training Loss: 0.0005808825371786952 \t\n",
      "Epoch 18941 \t\t Training Loss: 0.0005808825371786952 \t\n",
      "Epoch 18942 \t\t Training Loss: 0.0005808825371786952 \t\n",
      "Epoch 18943 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18944 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18945 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18946 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18947 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18948 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18949 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18950 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18951 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18952 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18953 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18954 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18955 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18956 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18957 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18958 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18959 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18960 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18961 \t\t Training Loss: 0.0005808824207633734 \t\n",
      "Epoch 18962 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18963 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18964 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18965 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18966 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18967 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18968 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18969 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18970 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18971 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18972 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18973 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18974 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18975 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18976 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18977 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18978 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18979 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18980 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18981 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18982 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18983 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18984 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18985 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18986 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18987 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18988 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18989 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18990 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18991 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18992 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18993 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18994 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18995 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18996 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 18997 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 18998 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 18999 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19000 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19001 \t\t Training Loss: 0.0005808823043480515 \t\n",
      "Epoch 19002 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19003 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19004 \t\t Training Loss: 0.0005808822461403906 \t\n",
      "Epoch 19005 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19006 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19007 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19008 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19009 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19010 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19011 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19012 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19013 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19014 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19015 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19016 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19017 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19018 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19019 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19020 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19021 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19022 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19023 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19024 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19025 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19026 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19027 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19028 \t\t Training Loss: 0.0005808821297250688 \t\n",
      "Epoch 19029 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19030 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19031 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19032 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19033 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19034 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19035 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19036 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19037 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19038 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19039 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19040 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19041 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19042 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19043 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19044 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19045 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19046 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19047 \t\t Training Loss: 0.0005808819551020861 \t\n",
      "Epoch 19048 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19049 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19050 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19051 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19052 \t\t Training Loss: 0.0005808819551020861 \t\n",
      "Epoch 19053 \t\t Training Loss: 0.0005808819551020861 \t\n",
      "Epoch 19054 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19055 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19056 \t\t Training Loss: 0.000580882013309747 \t\n",
      "Epoch 19057 \t\t Training Loss: 0.0005808819551020861 \t\n",
      "Epoch 19058 \t\t Training Loss: 0.0005808819551020861 \t\n",
      "Epoch 19059 \t\t Training Loss: 0.0005808819551020861 \t\n",
      "Epoch 19060 \t\t Training Loss: 0.0005808819551020861 \t\n",
      "Epoch 19061 \t\t Training Loss: 0.0005808819551020861 \t\n",
      "Epoch 19062 \t\t Training Loss: 0.0005808819551020861 \t\n",
      "Epoch 19063 \t\t Training Loss: 0.0005808818968944252 \t\n",
      "Epoch 19064 \t\t Training Loss: 0.0005808818968944252 \t\n",
      "Epoch 19065 \t\t Training Loss: 0.0005808818968944252 \t\n",
      "Epoch 19066 \t\t Training Loss: 0.0005808818968944252 \t\n",
      "Epoch 19067 \t\t Training Loss: 0.0005808818968944252 \t\n",
      "Epoch 19068 \t\t Training Loss: 0.0005808818968944252 \t\n",
      "Epoch 19069 \t\t Training Loss: 0.0005808818968944252 \t\n",
      "Epoch 19070 \t\t Training Loss: 0.0005808818968944252 \t\n",
      "Epoch 19071 \t\t Training Loss: 0.0005808818968944252 \t\n",
      "Epoch 19072 \t\t Training Loss: 0.0005808818968944252 \t\n",
      "Epoch 19073 \t\t Training Loss: 0.0005808818968944252 \t\n",
      "Epoch 19074 \t\t Training Loss: 0.0005808818968944252 \t\n",
      "Epoch 19075 \t\t Training Loss: 0.0005808818968944252 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19076 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19077 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19078 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19079 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19080 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19081 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19082 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19083 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19084 \t\t Training Loss: 0.0005808818968944252 \t\n",
      "Epoch 19085 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19086 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19087 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19088 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19089 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19090 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19091 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19092 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19093 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19094 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19095 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19096 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19097 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19098 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19099 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19100 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19101 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19102 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19103 \t\t Training Loss: 0.0005808818386867642 \t\n",
      "Epoch 19104 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19105 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19106 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19107 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19108 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19109 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19110 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19111 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19112 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19113 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19114 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19115 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19116 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19117 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19118 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19119 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19120 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19121 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19122 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19123 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19124 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19125 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19126 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19127 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19128 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19129 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19130 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19131 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19132 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19133 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19134 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19135 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19136 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19137 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19138 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19139 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19140 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19141 \t\t Training Loss: 0.0005808816058561206 \t\n",
      "Epoch 19142 \t\t Training Loss: 0.0005808816058561206 \t\n",
      "Epoch 19143 \t\t Training Loss: 0.0005808816058561206 \t\n",
      "Epoch 19144 \t\t Training Loss: 0.0005808817222714424 \t\n",
      "Epoch 19145 \t\t Training Loss: 0.0005808816058561206 \t\n",
      "Epoch 19146 \t\t Training Loss: 0.0005808816058561206 \t\n",
      "Epoch 19147 \t\t Training Loss: 0.0005808816058561206 \t\n",
      "Epoch 19148 \t\t Training Loss: 0.0005808816058561206 \t\n",
      "Epoch 19149 \t\t Training Loss: 0.0005808816058561206 \t\n",
      "Epoch 19150 \t\t Training Loss: 0.0005808816058561206 \t\n",
      "Epoch 19151 \t\t Training Loss: 0.0005808816058561206 \t\n",
      "Epoch 19152 \t\t Training Loss: 0.0005808816058561206 \t\n",
      "Epoch 19153 \t\t Training Loss: 0.0005808815476484597 \t\n",
      "Epoch 19154 \t\t Training Loss: 0.0005808815476484597 \t\n",
      "Epoch 19155 \t\t Training Loss: 0.0005808815476484597 \t\n",
      "Epoch 19156 \t\t Training Loss: 0.0005808815476484597 \t\n",
      "Epoch 19157 \t\t Training Loss: 0.0005808815476484597 \t\n",
      "Epoch 19158 \t\t Training Loss: 0.0005808815476484597 \t\n",
      "Epoch 19159 \t\t Training Loss: 0.0005808814894407988 \t\n",
      "Epoch 19160 \t\t Training Loss: 0.0005808814894407988 \t\n",
      "Epoch 19161 \t\t Training Loss: 0.0005808814894407988 \t\n",
      "Epoch 19162 \t\t Training Loss: 0.0005808814894407988 \t\n",
      "Epoch 19163 \t\t Training Loss: 0.0005808814894407988 \t\n",
      "Epoch 19164 \t\t Training Loss: 0.0005808814894407988 \t\n",
      "Epoch 19165 \t\t Training Loss: 0.0005808814894407988 \t\n",
      "Epoch 19166 \t\t Training Loss: 0.0005808814894407988 \t\n",
      "Epoch 19167 \t\t Training Loss: 0.0005808814894407988 \t\n",
      "Epoch 19168 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19169 \t\t Training Loss: 0.0005808814894407988 \t\n",
      "Epoch 19170 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19171 \t\t Training Loss: 0.0005808814894407988 \t\n",
      "Epoch 19172 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19173 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19174 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19175 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19176 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19177 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19178 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19179 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19180 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19181 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19182 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19183 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19184 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19185 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19186 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19187 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19188 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19189 \t\t Training Loss: 0.0005808812566101551 \t\n",
      "Epoch 19190 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19191 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19192 \t\t Training Loss: 0.0005808812566101551 \t\n",
      "Epoch 19193 \t\t Training Loss: 0.000580881314817816 \t\n",
      "Epoch 19194 \t\t Training Loss: 0.0005808812566101551 \t\n",
      "Epoch 19195 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19196 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19197 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19198 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19199 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19200 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19201 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19202 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19203 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19204 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19205 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19206 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19207 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19208 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19209 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19210 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19211 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19212 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19213 \t\t Training Loss: 0.0005808810237795115 \t\n",
      "Epoch 19214 \t\t Training Loss: 0.0005808810237795115 \t\n",
      "Epoch 19215 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19216 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19217 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19218 \t\t Training Loss: 0.0005808810237795115 \t\n",
      "Epoch 19219 \t\t Training Loss: 0.0005808810237795115 \t\n",
      "Epoch 19220 \t\t Training Loss: 0.0005808810237795115 \t\n",
      "Epoch 19221 \t\t Training Loss: 0.0005808810237795115 \t\n",
      "Epoch 19222 \t\t Training Loss: 0.0005808810237795115 \t\n",
      "Epoch 19223 \t\t Training Loss: 0.0005808810237795115 \t\n",
      "Epoch 19224 \t\t Training Loss: 0.0005808810237795115 \t\n",
      "Epoch 19225 \t\t Training Loss: 0.0005808810237795115 \t\n",
      "Epoch 19226 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19227 \t\t Training Loss: 0.0005808811401948333 \t\n",
      "Epoch 19228 \t\t Training Loss: 0.0005808810237795115 \t\n",
      "Epoch 19229 \t\t Training Loss: 0.0005808810237795115 \t\n",
      "Epoch 19230 \t\t Training Loss: 0.0005808810237795115 \t\n",
      "Epoch 19231 \t\t Training Loss: 0.0005808810237795115 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19232 \t\t Training Loss: 0.0005808810237795115 \t\n",
      "Epoch 19233 \t\t Training Loss: 0.0005808810237795115 \t\n",
      "Epoch 19234 \t\t Training Loss: 0.0005808810237795115 \t\n",
      "Epoch 19235 \t\t Training Loss: 0.0005808809655718505 \t\n",
      "Epoch 19236 \t\t Training Loss: 0.0005808809655718505 \t\n",
      "Epoch 19237 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19238 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19239 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19240 \t\t Training Loss: 0.0005808809655718505 \t\n",
      "Epoch 19241 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19242 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19243 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19244 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19245 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19246 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19247 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19248 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19249 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19250 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19251 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19252 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19253 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19254 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19255 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19256 \t\t Training Loss: 0.0005808809073641896 \t\n",
      "Epoch 19257 \t\t Training Loss: 0.0005808808491565287 \t\n",
      "Epoch 19258 \t\t Training Loss: 0.0005808808491565287 \t\n",
      "Epoch 19259 \t\t Training Loss: 0.0005808808491565287 \t\n",
      "Epoch 19260 \t\t Training Loss: 0.0005808808491565287 \t\n",
      "Epoch 19261 \t\t Training Loss: 0.0005808808491565287 \t\n",
      "Epoch 19262 \t\t Training Loss: 0.0005808808491565287 \t\n",
      "Epoch 19263 \t\t Training Loss: 0.0005808808491565287 \t\n",
      "Epoch 19264 \t\t Training Loss: 0.0005808808491565287 \t\n",
      "Epoch 19265 \t\t Training Loss: 0.0005808808491565287 \t\n",
      "Epoch 19266 \t\t Training Loss: 0.0005808808491565287 \t\n",
      "Epoch 19267 \t\t Training Loss: 0.0005808807909488678 \t\n",
      "Epoch 19268 \t\t Training Loss: 0.0005808808491565287 \t\n",
      "Epoch 19269 \t\t Training Loss: 0.0005808808491565287 \t\n",
      "Epoch 19270 \t\t Training Loss: 0.0005808807909488678 \t\n",
      "Epoch 19271 \t\t Training Loss: 0.0005808807909488678 \t\n",
      "Epoch 19272 \t\t Training Loss: 0.0005808807909488678 \t\n",
      "Epoch 19273 \t\t Training Loss: 0.0005808807909488678 \t\n",
      "Epoch 19274 \t\t Training Loss: 0.0005808807909488678 \t\n",
      "Epoch 19275 \t\t Training Loss: 0.0005808807909488678 \t\n",
      "Epoch 19276 \t\t Training Loss: 0.0005808807909488678 \t\n",
      "Epoch 19277 \t\t Training Loss: 0.0005808807327412069 \t\n",
      "Epoch 19278 \t\t Training Loss: 0.0005808807327412069 \t\n",
      "Epoch 19279 \t\t Training Loss: 0.0005808807327412069 \t\n",
      "Epoch 19280 \t\t Training Loss: 0.0005808807327412069 \t\n",
      "Epoch 19281 \t\t Training Loss: 0.0005808807327412069 \t\n",
      "Epoch 19282 \t\t Training Loss: 0.000580880674533546 \t\n",
      "Epoch 19283 \t\t Training Loss: 0.000580880674533546 \t\n",
      "Epoch 19284 \t\t Training Loss: 0.000580880674533546 \t\n",
      "Epoch 19285 \t\t Training Loss: 0.000580880674533546 \t\n",
      "Epoch 19286 \t\t Training Loss: 0.000580880674533546 \t\n",
      "Epoch 19287 \t\t Training Loss: 0.000580880674533546 \t\n",
      "Epoch 19288 \t\t Training Loss: 0.0005808806163258851 \t\n",
      "Epoch 19289 \t\t Training Loss: 0.0005808805581182241 \t\n",
      "Epoch 19290 \t\t Training Loss: 0.0005808805581182241 \t\n",
      "Epoch 19291 \t\t Training Loss: 0.0005808805581182241 \t\n",
      "Epoch 19292 \t\t Training Loss: 0.0005808805581182241 \t\n",
      "Epoch 19293 \t\t Training Loss: 0.0005808805581182241 \t\n",
      "Epoch 19294 \t\t Training Loss: 0.0005808804999105632 \t\n",
      "Epoch 19295 \t\t Training Loss: 0.0005808804999105632 \t\n",
      "Epoch 19296 \t\t Training Loss: 0.0005808804999105632 \t\n",
      "Epoch 19297 \t\t Training Loss: 0.0005808804999105632 \t\n",
      "Epoch 19298 \t\t Training Loss: 0.0005808804999105632 \t\n",
      "Epoch 19299 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19300 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19301 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19302 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19303 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19304 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19305 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19306 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19307 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19308 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19309 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19310 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19311 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19312 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19313 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19314 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19315 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19316 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19317 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19318 \t\t Training Loss: 0.0005808804417029023 \t\n",
      "Epoch 19319 \t\t Training Loss: 0.0005808803834952414 \t\n",
      "Epoch 19320 \t\t Training Loss: 0.0005808803834952414 \t\n",
      "Epoch 19321 \t\t Training Loss: 0.0005808803834952414 \t\n",
      "Epoch 19322 \t\t Training Loss: 0.0005808803252875805 \t\n",
      "Epoch 19323 \t\t Training Loss: 0.0005808803252875805 \t\n",
      "Epoch 19324 \t\t Training Loss: 0.0005808803252875805 \t\n",
      "Epoch 19325 \t\t Training Loss: 0.0005808803252875805 \t\n",
      "Epoch 19326 \t\t Training Loss: 0.0005808803252875805 \t\n",
      "Epoch 19327 \t\t Training Loss: 0.0005808802670799196 \t\n",
      "Epoch 19328 \t\t Training Loss: 0.0005808802670799196 \t\n",
      "Epoch 19329 \t\t Training Loss: 0.0005808802670799196 \t\n",
      "Epoch 19330 \t\t Training Loss: 0.0005808802670799196 \t\n",
      "Epoch 19331 \t\t Training Loss: 0.0005808802670799196 \t\n",
      "Epoch 19332 \t\t Training Loss: 0.0005808802670799196 \t\n",
      "Epoch 19333 \t\t Training Loss: 0.0005808802670799196 \t\n",
      "Epoch 19334 \t\t Training Loss: 0.0005808802670799196 \t\n",
      "Epoch 19335 \t\t Training Loss: 0.0005808802670799196 \t\n",
      "Epoch 19336 \t\t Training Loss: 0.0005808802088722587 \t\n",
      "Epoch 19337 \t\t Training Loss: 0.0005808802088722587 \t\n",
      "Epoch 19338 \t\t Training Loss: 0.0005808802088722587 \t\n",
      "Epoch 19339 \t\t Training Loss: 0.0005808802088722587 \t\n",
      "Epoch 19340 \t\t Training Loss: 0.0005808802088722587 \t\n",
      "Epoch 19341 \t\t Training Loss: 0.0005808802088722587 \t\n",
      "Epoch 19342 \t\t Training Loss: 0.0005808802088722587 \t\n",
      "Epoch 19343 \t\t Training Loss: 0.0005808802088722587 \t\n",
      "Epoch 19344 \t\t Training Loss: 0.0005808802088722587 \t\n",
      "Epoch 19345 \t\t Training Loss: 0.0005808802088722587 \t\n",
      "Epoch 19346 \t\t Training Loss: 0.0005808802088722587 \t\n",
      "Epoch 19347 \t\t Training Loss: 0.0005808802088722587 \t\n",
      "Epoch 19348 \t\t Training Loss: 0.0005808802088722587 \t\n",
      "Epoch 19349 \t\t Training Loss: 0.0005808802088722587 \t\n",
      "Epoch 19350 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19351 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19352 \t\t Training Loss: 0.0005808801506645977 \t\n",
      "Epoch 19353 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19354 \t\t Training Loss: 0.0005808800342492759 \t\n",
      "Epoch 19355 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19356 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19357 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19358 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19359 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19360 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19361 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19362 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19363 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19364 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19365 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19366 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19367 \t\t Training Loss: 0.0005808800342492759 \t\n",
      "Epoch 19368 \t\t Training Loss: 0.0005808800342492759 \t\n",
      "Epoch 19369 \t\t Training Loss: 0.0005808800342492759 \t\n",
      "Epoch 19370 \t\t Training Loss: 0.0005808800342492759 \t\n",
      "Epoch 19371 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19372 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19373 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19374 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19375 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19376 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19377 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19378 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19379 \t\t Training Loss: 0.0005808800924569368 \t\n",
      "Epoch 19380 \t\t Training Loss: 0.000580879976041615 \t\n",
      "Epoch 19381 \t\t Training Loss: 0.000580879976041615 \t\n",
      "Epoch 19382 \t\t Training Loss: 0.000580879976041615 \t\n",
      "Epoch 19383 \t\t Training Loss: 0.0005808799178339541 \t\n",
      "Epoch 19384 \t\t Training Loss: 0.000580879976041615 \t\n",
      "Epoch 19385 \t\t Training Loss: 0.000580879976041615 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19386 \t\t Training Loss: 0.0005808799178339541 \t\n",
      "Epoch 19387 \t\t Training Loss: 0.000580879976041615 \t\n",
      "Epoch 19388 \t\t Training Loss: 0.0005808799178339541 \t\n",
      "Epoch 19389 \t\t Training Loss: 0.0005808799178339541 \t\n",
      "Epoch 19390 \t\t Training Loss: 0.0005808799178339541 \t\n",
      "Epoch 19391 \t\t Training Loss: 0.0005808799178339541 \t\n",
      "Epoch 19392 \t\t Training Loss: 0.0005808799178339541 \t\n",
      "Epoch 19393 \t\t Training Loss: 0.0005808799178339541 \t\n",
      "Epoch 19394 \t\t Training Loss: 0.0005808799178339541 \t\n",
      "Epoch 19395 \t\t Training Loss: 0.0005808799178339541 \t\n",
      "Epoch 19396 \t\t Training Loss: 0.0005808799178339541 \t\n",
      "Epoch 19397 \t\t Training Loss: 0.0005808799178339541 \t\n",
      "Epoch 19398 \t\t Training Loss: 0.0005808798596262932 \t\n",
      "Epoch 19399 \t\t Training Loss: 0.0005808798596262932 \t\n",
      "Epoch 19400 \t\t Training Loss: 0.0005808798596262932 \t\n",
      "Epoch 19401 \t\t Training Loss: 0.0005808798596262932 \t\n",
      "Epoch 19402 \t\t Training Loss: 0.0005808798596262932 \t\n",
      "Epoch 19403 \t\t Training Loss: 0.0005808798596262932 \t\n",
      "Epoch 19404 \t\t Training Loss: 0.0005808798596262932 \t\n",
      "Epoch 19405 \t\t Training Loss: 0.0005808798596262932 \t\n",
      "Epoch 19406 \t\t Training Loss: 0.0005808798014186323 \t\n",
      "Epoch 19407 \t\t Training Loss: 0.0005808798014186323 \t\n",
      "Epoch 19408 \t\t Training Loss: 0.0005808798014186323 \t\n",
      "Epoch 19409 \t\t Training Loss: 0.0005808798596262932 \t\n",
      "Epoch 19410 \t\t Training Loss: 0.0005808798014186323 \t\n",
      "Epoch 19411 \t\t Training Loss: 0.0005808798014186323 \t\n",
      "Epoch 19412 \t\t Training Loss: 0.0005808798014186323 \t\n",
      "Epoch 19413 \t\t Training Loss: 0.0005808798014186323 \t\n",
      "Epoch 19414 \t\t Training Loss: 0.0005808798014186323 \t\n",
      "Epoch 19415 \t\t Training Loss: 0.0005808798014186323 \t\n",
      "Epoch 19416 \t\t Training Loss: 0.0005808798014186323 \t\n",
      "Epoch 19417 \t\t Training Loss: 0.0005808798014186323 \t\n",
      "Epoch 19418 \t\t Training Loss: 0.0005808797432109714 \t\n",
      "Epoch 19419 \t\t Training Loss: 0.0005808798014186323 \t\n",
      "Epoch 19420 \t\t Training Loss: 0.0005808798014186323 \t\n",
      "Epoch 19421 \t\t Training Loss: 0.0005808797432109714 \t\n",
      "Epoch 19422 \t\t Training Loss: 0.0005808798014186323 \t\n",
      "Epoch 19423 \t\t Training Loss: 0.0005808798014186323 \t\n",
      "Epoch 19424 \t\t Training Loss: 0.0005808797432109714 \t\n",
      "Epoch 19425 \t\t Training Loss: 0.0005808797432109714 \t\n",
      "Epoch 19426 \t\t Training Loss: 0.0005808797432109714 \t\n",
      "Epoch 19427 \t\t Training Loss: 0.0005808797432109714 \t\n",
      "Epoch 19428 \t\t Training Loss: 0.0005808796850033104 \t\n",
      "Epoch 19429 \t\t Training Loss: 0.0005808796850033104 \t\n",
      "Epoch 19430 \t\t Training Loss: 0.0005808796850033104 \t\n",
      "Epoch 19431 \t\t Training Loss: 0.0005808796850033104 \t\n",
      "Epoch 19432 \t\t Training Loss: 0.0005808796850033104 \t\n",
      "Epoch 19433 \t\t Training Loss: 0.0005808796850033104 \t\n",
      "Epoch 19434 \t\t Training Loss: 0.0005808796850033104 \t\n",
      "Epoch 19435 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19436 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19437 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19438 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19439 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19440 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19441 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19442 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19443 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19444 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19445 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19446 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19447 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19448 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19449 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19450 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19451 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19452 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19453 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19454 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19455 \t\t Training Loss: 0.0005808796267956495 \t\n",
      "Epoch 19456 \t\t Training Loss: 0.0005808795103803277 \t\n",
      "Epoch 19457 \t\t Training Loss: 0.0005808795103803277 \t\n",
      "Epoch 19458 \t\t Training Loss: 0.0005808795103803277 \t\n",
      "Epoch 19459 \t\t Training Loss: 0.0005808795103803277 \t\n",
      "Epoch 19460 \t\t Training Loss: 0.0005808795103803277 \t\n",
      "Epoch 19461 \t\t Training Loss: 0.0005808795103803277 \t\n",
      "Epoch 19462 \t\t Training Loss: 0.0005808795103803277 \t\n",
      "Epoch 19463 \t\t Training Loss: 0.0005808795103803277 \t\n",
      "Epoch 19464 \t\t Training Loss: 0.0005808794521726668 \t\n",
      "Epoch 19465 \t\t Training Loss: 0.0005808794521726668 \t\n",
      "Epoch 19466 \t\t Training Loss: 0.0005808794521726668 \t\n",
      "Epoch 19467 \t\t Training Loss: 0.0005808794521726668 \t\n",
      "Epoch 19468 \t\t Training Loss: 0.0005808794521726668 \t\n",
      "Epoch 19469 \t\t Training Loss: 0.0005808794521726668 \t\n",
      "Epoch 19470 \t\t Training Loss: 0.0005808794521726668 \t\n",
      "Epoch 19471 \t\t Training Loss: 0.0005808794521726668 \t\n",
      "Epoch 19472 \t\t Training Loss: 0.0005808794521726668 \t\n",
      "Epoch 19473 \t\t Training Loss: 0.0005808794521726668 \t\n",
      "Epoch 19474 \t\t Training Loss: 0.0005808794521726668 \t\n",
      "Epoch 19475 \t\t Training Loss: 0.0005808794521726668 \t\n",
      "Epoch 19476 \t\t Training Loss: 0.0005808793939650059 \t\n",
      "Epoch 19477 \t\t Training Loss: 0.0005808793939650059 \t\n",
      "Epoch 19478 \t\t Training Loss: 0.0005808793939650059 \t\n",
      "Epoch 19479 \t\t Training Loss: 0.0005808793939650059 \t\n",
      "Epoch 19480 \t\t Training Loss: 0.0005808793939650059 \t\n",
      "Epoch 19481 \t\t Training Loss: 0.0005808793939650059 \t\n",
      "Epoch 19482 \t\t Training Loss: 0.0005808793939650059 \t\n",
      "Epoch 19483 \t\t Training Loss: 0.0005808793939650059 \t\n",
      "Epoch 19484 \t\t Training Loss: 0.0005808794521726668 \t\n",
      "Epoch 19485 \t\t Training Loss: 0.0005808793939650059 \t\n",
      "Epoch 19486 \t\t Training Loss: 0.0005808793939650059 \t\n",
      "Epoch 19487 \t\t Training Loss: 0.0005808793939650059 \t\n",
      "Epoch 19488 \t\t Training Loss: 0.0005808793939650059 \t\n",
      "Epoch 19489 \t\t Training Loss: 0.0005808793939650059 \t\n",
      "Epoch 19490 \t\t Training Loss: 0.0005808793939650059 \t\n",
      "Epoch 19491 \t\t Training Loss: 0.0005808793939650059 \t\n",
      "Epoch 19492 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19493 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19494 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19495 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19496 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19497 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19498 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19499 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19500 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19501 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19502 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19503 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19504 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19505 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19506 \t\t Training Loss: 0.000580879277549684 \t\n",
      "Epoch 19507 \t\t Training Loss: 0.000580879277549684 \t\n",
      "Epoch 19508 \t\t Training Loss: 0.000580879277549684 \t\n",
      "Epoch 19509 \t\t Training Loss: 0.000580879277549684 \t\n",
      "Epoch 19510 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19511 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19512 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19513 \t\t Training Loss: 0.000580879277549684 \t\n",
      "Epoch 19514 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19515 \t\t Training Loss: 0.000580879277549684 \t\n",
      "Epoch 19516 \t\t Training Loss: 0.000580879277549684 \t\n",
      "Epoch 19517 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19518 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19519 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19520 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19521 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19522 \t\t Training Loss: 0.000580879277549684 \t\n",
      "Epoch 19523 \t\t Training Loss: 0.000580879335757345 \t\n",
      "Epoch 19524 \t\t Training Loss: 0.000580879277549684 \t\n",
      "Epoch 19525 \t\t Training Loss: 0.0005808792193420231 \t\n",
      "Epoch 19526 \t\t Training Loss: 0.000580879277549684 \t\n",
      "Epoch 19527 \t\t Training Loss: 0.0005808792193420231 \t\n",
      "Epoch 19528 \t\t Training Loss: 0.0005808792193420231 \t\n",
      "Epoch 19529 \t\t Training Loss: 0.0005808792193420231 \t\n",
      "Epoch 19530 \t\t Training Loss: 0.0005808792193420231 \t\n",
      "Epoch 19531 \t\t Training Loss: 0.0005808792193420231 \t\n",
      "Epoch 19532 \t\t Training Loss: 0.0005808792193420231 \t\n",
      "Epoch 19533 \t\t Training Loss: 0.0005808792193420231 \t\n",
      "Epoch 19534 \t\t Training Loss: 0.0005808792193420231 \t\n",
      "Epoch 19535 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19536 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19537 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19538 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19539 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19540 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19541 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19542 \t\t Training Loss: 0.0005808792193420231 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19543 \t\t Training Loss: 0.0005808792193420231 \t\n",
      "Epoch 19544 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19545 \t\t Training Loss: 0.0005808792193420231 \t\n",
      "Epoch 19546 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19547 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19548 \t\t Training Loss: 0.0005808792193420231 \t\n",
      "Epoch 19549 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19550 \t\t Training Loss: 0.0005808792193420231 \t\n",
      "Epoch 19551 \t\t Training Loss: 0.0005808792193420231 \t\n",
      "Epoch 19552 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19553 \t\t Training Loss: 0.0005808792193420231 \t\n",
      "Epoch 19554 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19555 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19556 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19557 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19558 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19559 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19560 \t\t Training Loss: 0.0005808791611343622 \t\n",
      "Epoch 19561 \t\t Training Loss: 0.0005808790447190404 \t\n",
      "Epoch 19562 \t\t Training Loss: 0.0005808790447190404 \t\n",
      "Epoch 19563 \t\t Training Loss: 0.0005808790447190404 \t\n",
      "Epoch 19564 \t\t Training Loss: 0.0005808790447190404 \t\n",
      "Epoch 19565 \t\t Training Loss: 0.0005808790447190404 \t\n",
      "Epoch 19566 \t\t Training Loss: 0.0005808790447190404 \t\n",
      "Epoch 19567 \t\t Training Loss: 0.0005808790447190404 \t\n",
      "Epoch 19568 \t\t Training Loss: 0.0005808790447190404 \t\n",
      "Epoch 19569 \t\t Training Loss: 0.0005808790447190404 \t\n",
      "Epoch 19570 \t\t Training Loss: 0.0005808789283037186 \t\n",
      "Epoch 19571 \t\t Training Loss: 0.0005808790447190404 \t\n",
      "Epoch 19572 \t\t Training Loss: 0.0005808789283037186 \t\n",
      "Epoch 19573 \t\t Training Loss: 0.0005808789283037186 \t\n",
      "Epoch 19574 \t\t Training Loss: 0.0005808789283037186 \t\n",
      "Epoch 19575 \t\t Training Loss: 0.0005808789283037186 \t\n",
      "Epoch 19576 \t\t Training Loss: 0.0005808789283037186 \t\n",
      "Epoch 19577 \t\t Training Loss: 0.0005808789283037186 \t\n",
      "Epoch 19578 \t\t Training Loss: 0.0005808789283037186 \t\n",
      "Epoch 19579 \t\t Training Loss: 0.0005808789283037186 \t\n",
      "Epoch 19580 \t\t Training Loss: 0.0005808789283037186 \t\n",
      "Epoch 19581 \t\t Training Loss: 0.0005808789283037186 \t\n",
      "Epoch 19582 \t\t Training Loss: 0.0005808789283037186 \t\n",
      "Epoch 19583 \t\t Training Loss: 0.0005808789283037186 \t\n",
      "Epoch 19584 \t\t Training Loss: 0.0005808789283037186 \t\n",
      "Epoch 19585 \t\t Training Loss: 0.0005808788700960577 \t\n",
      "Epoch 19586 \t\t Training Loss: 0.0005808788700960577 \t\n",
      "Epoch 19587 \t\t Training Loss: 0.0005808788700960577 \t\n",
      "Epoch 19588 \t\t Training Loss: 0.0005808788700960577 \t\n",
      "Epoch 19589 \t\t Training Loss: 0.0005808788700960577 \t\n",
      "Epoch 19590 \t\t Training Loss: 0.0005808788700960577 \t\n",
      "Epoch 19591 \t\t Training Loss: 0.0005808789283037186 \t\n",
      "Epoch 19592 \t\t Training Loss: 0.0005808788700960577 \t\n",
      "Epoch 19593 \t\t Training Loss: 0.0005808788700960577 \t\n",
      "Epoch 19594 \t\t Training Loss: 0.0005808788700960577 \t\n",
      "Epoch 19595 \t\t Training Loss: 0.0005808788700960577 \t\n",
      "Epoch 19596 \t\t Training Loss: 0.0005808788700960577 \t\n",
      "Epoch 19597 \t\t Training Loss: 0.0005808788700960577 \t\n",
      "Epoch 19598 \t\t Training Loss: 0.0005808788700960577 \t\n",
      "Epoch 19599 \t\t Training Loss: 0.0005808788700960577 \t\n",
      "Epoch 19600 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19601 \t\t Training Loss: 0.0005808788700960577 \t\n",
      "Epoch 19602 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19603 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19604 \t\t Training Loss: 0.0005808788700960577 \t\n",
      "Epoch 19605 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19606 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19607 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19608 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19609 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19610 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19611 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19612 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19613 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19614 \t\t Training Loss: 0.0005808786954730749 \t\n",
      "Epoch 19615 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19616 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19617 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19618 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19619 \t\t Training Loss: 0.0005808786954730749 \t\n",
      "Epoch 19620 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19621 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19622 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19623 \t\t Training Loss: 0.0005808787536807358 \t\n",
      "Epoch 19624 \t\t Training Loss: 0.0005808786954730749 \t\n",
      "Epoch 19625 \t\t Training Loss: 0.0005808786954730749 \t\n",
      "Epoch 19626 \t\t Training Loss: 0.0005808786954730749 \t\n",
      "Epoch 19627 \t\t Training Loss: 0.0005808786954730749 \t\n",
      "Epoch 19628 \t\t Training Loss: 0.0005808786954730749 \t\n",
      "Epoch 19629 \t\t Training Loss: 0.000580878637265414 \t\n",
      "Epoch 19630 \t\t Training Loss: 0.000580878637265414 \t\n",
      "Epoch 19631 \t\t Training Loss: 0.000580878637265414 \t\n",
      "Epoch 19632 \t\t Training Loss: 0.000580878637265414 \t\n",
      "Epoch 19633 \t\t Training Loss: 0.000580878637265414 \t\n",
      "Epoch 19634 \t\t Training Loss: 0.000580878637265414 \t\n",
      "Epoch 19635 \t\t Training Loss: 0.000580878637265414 \t\n",
      "Epoch 19636 \t\t Training Loss: 0.000580878637265414 \t\n",
      "Epoch 19637 \t\t Training Loss: 0.000580878637265414 \t\n",
      "Epoch 19638 \t\t Training Loss: 0.0005808785790577531 \t\n",
      "Epoch 19639 \t\t Training Loss: 0.0005808785790577531 \t\n",
      "Epoch 19640 \t\t Training Loss: 0.0005808785790577531 \t\n",
      "Epoch 19641 \t\t Training Loss: 0.0005808785790577531 \t\n",
      "Epoch 19642 \t\t Training Loss: 0.0005808785790577531 \t\n",
      "Epoch 19643 \t\t Training Loss: 0.0005808785790577531 \t\n",
      "Epoch 19644 \t\t Training Loss: 0.000580878637265414 \t\n",
      "Epoch 19645 \t\t Training Loss: 0.000580878637265414 \t\n",
      "Epoch 19646 \t\t Training Loss: 0.0005808785790577531 \t\n",
      "Epoch 19647 \t\t Training Loss: 0.0005808785790577531 \t\n",
      "Epoch 19648 \t\t Training Loss: 0.0005808785790577531 \t\n",
      "Epoch 19649 \t\t Training Loss: 0.0005808785208500922 \t\n",
      "Epoch 19650 \t\t Training Loss: 0.0005808785208500922 \t\n",
      "Epoch 19651 \t\t Training Loss: 0.0005808785208500922 \t\n",
      "Epoch 19652 \t\t Training Loss: 0.0005808785208500922 \t\n",
      "Epoch 19653 \t\t Training Loss: 0.0005808785208500922 \t\n",
      "Epoch 19654 \t\t Training Loss: 0.0005808784626424313 \t\n",
      "Epoch 19655 \t\t Training Loss: 0.0005808784626424313 \t\n",
      "Epoch 19656 \t\t Training Loss: 0.0005808783462271094 \t\n",
      "Epoch 19657 \t\t Training Loss: 0.0005808784626424313 \t\n",
      "Epoch 19658 \t\t Training Loss: 0.0005808783462271094 \t\n",
      "Epoch 19659 \t\t Training Loss: 0.0005808783462271094 \t\n",
      "Epoch 19660 \t\t Training Loss: 0.0005808783462271094 \t\n",
      "Epoch 19661 \t\t Training Loss: 0.0005808783462271094 \t\n",
      "Epoch 19662 \t\t Training Loss: 0.0005808783462271094 \t\n",
      "Epoch 19663 \t\t Training Loss: 0.0005808783462271094 \t\n",
      "Epoch 19664 \t\t Training Loss: 0.0005808783462271094 \t\n",
      "Epoch 19665 \t\t Training Loss: 0.0005808783462271094 \t\n",
      "Epoch 19666 \t\t Training Loss: 0.0005808783462271094 \t\n",
      "Epoch 19667 \t\t Training Loss: 0.0005808783462271094 \t\n",
      "Epoch 19668 \t\t Training Loss: 0.0005808783462271094 \t\n",
      "Epoch 19669 \t\t Training Loss: 0.0005808782880194485 \t\n",
      "Epoch 19670 \t\t Training Loss: 0.0005808782880194485 \t\n",
      "Epoch 19671 \t\t Training Loss: 0.0005808782880194485 \t\n",
      "Epoch 19672 \t\t Training Loss: 0.0005808782880194485 \t\n",
      "Epoch 19673 \t\t Training Loss: 0.0005808782880194485 \t\n",
      "Epoch 19674 \t\t Training Loss: 0.0005808782880194485 \t\n",
      "Epoch 19675 \t\t Training Loss: 0.0005808782880194485 \t\n",
      "Epoch 19676 \t\t Training Loss: 0.0005808782880194485 \t\n",
      "Epoch 19677 \t\t Training Loss: 0.0005808782880194485 \t\n",
      "Epoch 19678 \t\t Training Loss: 0.0005808782880194485 \t\n",
      "Epoch 19679 \t\t Training Loss: 0.0005808782880194485 \t\n",
      "Epoch 19680 \t\t Training Loss: 0.0005808782880194485 \t\n",
      "Epoch 19681 \t\t Training Loss: 0.0005808782298117876 \t\n",
      "Epoch 19682 \t\t Training Loss: 0.0005808782880194485 \t\n",
      "Epoch 19683 \t\t Training Loss: 0.0005808782880194485 \t\n",
      "Epoch 19684 \t\t Training Loss: 0.0005808782298117876 \t\n",
      "Epoch 19685 \t\t Training Loss: 0.0005808782298117876 \t\n",
      "Epoch 19686 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19687 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19688 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19689 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19690 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19691 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19692 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19693 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19694 \t\t Training Loss: 0.0005808781716041267 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19695 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19696 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19697 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19698 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19699 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19700 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19701 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19702 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19703 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19704 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19705 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19706 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19707 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19708 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19709 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19710 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19711 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19712 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19713 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19714 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19715 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19716 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19717 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19718 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19719 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19720 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19721 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19722 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19723 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19724 \t\t Training Loss: 0.0005808781716041267 \t\n",
      "Epoch 19725 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19726 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19727 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19728 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19729 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19730 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19731 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19732 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19733 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19734 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19735 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19736 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19737 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19738 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19739 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19740 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19741 \t\t Training Loss: 0.0005808780551888049 \t\n",
      "Epoch 19742 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19743 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19744 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19745 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19746 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19747 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19748 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19749 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19750 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19751 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19752 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19753 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19754 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19755 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19756 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19757 \t\t Training Loss: 0.000580877938773483 \t\n",
      "Epoch 19758 \t\t Training Loss: 0.000580877938773483 \t\n",
      "Epoch 19759 \t\t Training Loss: 0.000580877938773483 \t\n",
      "Epoch 19760 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19761 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19762 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19763 \t\t Training Loss: 0.000580877938773483 \t\n",
      "Epoch 19764 \t\t Training Loss: 0.000580877938773483 \t\n",
      "Epoch 19765 \t\t Training Loss: 0.000580877938773483 \t\n",
      "Epoch 19766 \t\t Training Loss: 0.000580877996981144 \t\n",
      "Epoch 19767 \t\t Training Loss: 0.000580877938773483 \t\n",
      "Epoch 19768 \t\t Training Loss: 0.000580877938773483 \t\n",
      "Epoch 19769 \t\t Training Loss: 0.000580877938773483 \t\n",
      "Epoch 19770 \t\t Training Loss: 0.000580877938773483 \t\n",
      "Epoch 19771 \t\t Training Loss: 0.000580877938773483 \t\n",
      "Epoch 19772 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19773 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19774 \t\t Training Loss: 0.000580877938773483 \t\n",
      "Epoch 19775 \t\t Training Loss: 0.000580877938773483 \t\n",
      "Epoch 19776 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19777 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19778 \t\t Training Loss: 0.000580877938773483 \t\n",
      "Epoch 19779 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19780 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19781 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19782 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19783 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19784 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19785 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19786 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19787 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19788 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19789 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19790 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19791 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19792 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19793 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19794 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19795 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19796 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19797 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19798 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19799 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19800 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19801 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19802 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19803 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19804 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19805 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19806 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19807 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19808 \t\t Training Loss: 0.0005808778805658221 \t\n",
      "Epoch 19809 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19810 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19811 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19812 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19813 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19814 \t\t Training Loss: 0.0005808777059428394 \t\n",
      "Epoch 19815 \t\t Training Loss: 0.0005808777059428394 \t\n",
      "Epoch 19816 \t\t Training Loss: 0.0005808777059428394 \t\n",
      "Epoch 19817 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19818 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19819 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19820 \t\t Training Loss: 0.0005808777059428394 \t\n",
      "Epoch 19821 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19822 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19823 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19824 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19825 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19826 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19827 \t\t Training Loss: 0.0005808777059428394 \t\n",
      "Epoch 19828 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19829 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19830 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19831 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19832 \t\t Training Loss: 0.0005808777059428394 \t\n",
      "Epoch 19833 \t\t Training Loss: 0.0005808777641505003 \t\n",
      "Epoch 19834 \t\t Training Loss: 0.0005808777059428394 \t\n",
      "Epoch 19835 \t\t Training Loss: 0.0005808777059428394 \t\n",
      "Epoch 19836 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19837 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19838 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19839 \t\t Training Loss: 0.0005808777059428394 \t\n",
      "Epoch 19840 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19841 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19842 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19843 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19844 \t\t Training Loss: 0.0005808777059428394 \t\n",
      "Epoch 19845 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19846 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19847 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19848 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19849 \t\t Training Loss: 0.0005808777059428394 \t\n",
      "Epoch 19850 \t\t Training Loss: 0.0005808777059428394 \t\n",
      "Epoch 19851 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19852 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19853 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19854 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19855 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19856 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19857 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19858 \t\t Training Loss: 0.0005808776477351785 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19859 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19860 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19861 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19862 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19863 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19864 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19865 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19866 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19867 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19868 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19869 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19870 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19871 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19872 \t\t Training Loss: 0.0005808775895275176 \t\n",
      "Epoch 19873 \t\t Training Loss: 0.0005808775895275176 \t\n",
      "Epoch 19874 \t\t Training Loss: 0.0005808775895275176 \t\n",
      "Epoch 19875 \t\t Training Loss: 0.0005808776477351785 \t\n",
      "Epoch 19876 \t\t Training Loss: 0.0005808775895275176 \t\n",
      "Epoch 19877 \t\t Training Loss: 0.0005808775895275176 \t\n",
      "Epoch 19878 \t\t Training Loss: 0.0005808775895275176 \t\n",
      "Epoch 19879 \t\t Training Loss: 0.0005808775895275176 \t\n",
      "Epoch 19880 \t\t Training Loss: 0.0005808775895275176 \t\n",
      "Epoch 19881 \t\t Training Loss: 0.0005808775895275176 \t\n",
      "Epoch 19882 \t\t Training Loss: 0.0005808775895275176 \t\n",
      "Epoch 19883 \t\t Training Loss: 0.0005808775895275176 \t\n",
      "Epoch 19884 \t\t Training Loss: 0.0005808775895275176 \t\n",
      "Epoch 19885 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19886 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19887 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19888 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19889 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19890 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19891 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19892 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19893 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19894 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19895 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19896 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19897 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19898 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19899 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19900 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19901 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19902 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19903 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19904 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19905 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19906 \t\t Training Loss: 0.0005808774731121957 \t\n",
      "Epoch 19907 \t\t Training Loss: 0.0005808773566968739 \t\n",
      "Epoch 19908 \t\t Training Loss: 0.0005808773566968739 \t\n",
      "Epoch 19909 \t\t Training Loss: 0.0005808773566968739 \t\n",
      "Epoch 19910 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19911 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19912 \t\t Training Loss: 0.0005808773566968739 \t\n",
      "Epoch 19913 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19914 \t\t Training Loss: 0.0005808773566968739 \t\n",
      "Epoch 19915 \t\t Training Loss: 0.0005808773566968739 \t\n",
      "Epoch 19916 \t\t Training Loss: 0.0005808773566968739 \t\n",
      "Epoch 19917 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19918 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19919 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19920 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19921 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19922 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19923 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19924 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19925 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19926 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19927 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19928 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19929 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19930 \t\t Training Loss: 0.000580877298489213 \t\n",
      "Epoch 19931 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19932 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19933 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19934 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19935 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19936 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19937 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19938 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19939 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19940 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19941 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19942 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19943 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19944 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19945 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19946 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19947 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19948 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19949 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19950 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19951 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19952 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19953 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19954 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19955 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19956 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19957 \t\t Training Loss: 0.0005808771820738912 \t\n",
      "Epoch 19958 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19959 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19960 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19961 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19962 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19963 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19964 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19965 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19966 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19967 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19968 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19969 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19970 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19971 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19972 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19973 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19974 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19975 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19976 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19977 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19978 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19979 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19980 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19981 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19982 \t\t Training Loss: 0.0005808770656585693 \t\n",
      "Epoch 19983 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19984 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19985 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19986 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19987 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19988 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19989 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19990 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19991 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19992 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19993 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19994 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19995 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19996 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19997 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19998 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 19999 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 20000 \t\t Training Loss: 0.0005808769492432475 \t\n",
      "Epoch 20001 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20002 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20003 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20004 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20005 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20006 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20007 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20008 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20009 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20010 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20011 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20012 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20013 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20014 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20015 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20016 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20017 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20018 \t\t Training Loss: 0.0005808768910355866 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20019 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20020 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20021 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20022 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20023 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20024 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20025 \t\t Training Loss: 0.0005808768910355866 \t\n",
      "Epoch 20026 \t\t Training Loss: 0.0005808767746202648 \t\n",
      "Epoch 20027 \t\t Training Loss: 0.0005808767746202648 \t\n",
      "Epoch 20028 \t\t Training Loss: 0.0005808767746202648 \t\n",
      "Epoch 20029 \t\t Training Loss: 0.0005808767746202648 \t\n",
      "Epoch 20030 \t\t Training Loss: 0.0005808767746202648 \t\n",
      "Epoch 20031 \t\t Training Loss: 0.0005808767746202648 \t\n",
      "Epoch 20032 \t\t Training Loss: 0.0005808767746202648 \t\n",
      "Epoch 20033 \t\t Training Loss: 0.0005808767746202648 \t\n",
      "Epoch 20034 \t\t Training Loss: 0.0005808767746202648 \t\n",
      "Epoch 20035 \t\t Training Loss: 0.0005808767746202648 \t\n",
      "Epoch 20036 \t\t Training Loss: 0.0005808767746202648 \t\n",
      "Epoch 20037 \t\t Training Loss: 0.0005808767746202648 \t\n",
      "Epoch 20038 \t\t Training Loss: 0.0005808767746202648 \t\n",
      "Epoch 20039 \t\t Training Loss: 0.0005808766582049429 \t\n",
      "Epoch 20040 \t\t Training Loss: 0.0005808766582049429 \t\n",
      "Epoch 20041 \t\t Training Loss: 0.0005808766582049429 \t\n",
      "Epoch 20042 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20043 \t\t Training Loss: 0.0005808766582049429 \t\n",
      "Epoch 20044 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20045 \t\t Training Loss: 0.0005808766582049429 \t\n",
      "Epoch 20046 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20047 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20048 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20049 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20050 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20051 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20052 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20053 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20054 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20055 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20056 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20057 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20058 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20059 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20060 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20061 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20062 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20063 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20064 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20065 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20066 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20067 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20068 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20069 \t\t Training Loss: 0.000580876599997282 \t\n",
      "Epoch 20070 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20071 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20072 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20073 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20074 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20075 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20076 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20077 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20078 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20079 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20080 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20081 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20082 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20083 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20084 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20085 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20086 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20087 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20088 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20089 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20090 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20091 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20092 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20093 \t\t Training Loss: 0.0005808764835819602 \t\n",
      "Epoch 20094 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20095 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20096 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20097 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20098 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20099 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20100 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20101 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20102 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20103 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20104 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20105 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20106 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20107 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20108 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20109 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20110 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20111 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20112 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20113 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20114 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20115 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20116 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20117 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20118 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20119 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20120 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20121 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20122 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20123 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20124 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20125 \t\t Training Loss: 0.0005808763671666384 \t\n",
      "Epoch 20126 \t\t Training Loss: 0.0005808763089589775 \t\n",
      "Epoch 20127 \t\t Training Loss: 0.0005808763089589775 \t\n",
      "Epoch 20128 \t\t Training Loss: 0.0005808763089589775 \t\n",
      "Epoch 20129 \t\t Training Loss: 0.0005808763089589775 \t\n",
      "Epoch 20130 \t\t Training Loss: 0.0005808763089589775 \t\n",
      "Epoch 20131 \t\t Training Loss: 0.0005808761925436556 \t\n",
      "Epoch 20132 \t\t Training Loss: 0.0005808761925436556 \t\n",
      "Epoch 20133 \t\t Training Loss: 0.0005808763089589775 \t\n",
      "Epoch 20134 \t\t Training Loss: 0.0005808761925436556 \t\n",
      "Epoch 20135 \t\t Training Loss: 0.0005808761925436556 \t\n",
      "Epoch 20136 \t\t Training Loss: 0.0005808761925436556 \t\n",
      "Epoch 20137 \t\t Training Loss: 0.0005808761925436556 \t\n",
      "Epoch 20138 \t\t Training Loss: 0.0005808761925436556 \t\n",
      "Epoch 20139 \t\t Training Loss: 0.0005808761925436556 \t\n",
      "Epoch 20140 \t\t Training Loss: 0.0005808761925436556 \t\n",
      "Epoch 20141 \t\t Training Loss: 0.0005808761925436556 \t\n",
      "Epoch 20142 \t\t Training Loss: 0.0005808761925436556 \t\n",
      "Epoch 20143 \t\t Training Loss: 0.0005808761925436556 \t\n",
      "Epoch 20144 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20145 \t\t Training Loss: 0.0005808761925436556 \t\n",
      "Epoch 20146 \t\t Training Loss: 0.0005808761925436556 \t\n",
      "Epoch 20147 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20148 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20149 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20150 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20151 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20152 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20153 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20154 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20155 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20156 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20157 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20158 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20159 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20160 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20161 \t\t Training Loss: 0.0005808760179206729 \t\n",
      "Epoch 20162 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20163 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20164 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20165 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20166 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20167 \t\t Training Loss: 0.0005808760179206729 \t\n",
      "Epoch 20168 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20169 \t\t Training Loss: 0.0005808760179206729 \t\n",
      "Epoch 20170 \t\t Training Loss: 0.0005808760179206729 \t\n",
      "Epoch 20171 \t\t Training Loss: 0.0005808760179206729 \t\n",
      "Epoch 20172 \t\t Training Loss: 0.0005808760179206729 \t\n",
      "Epoch 20173 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20174 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20175 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20176 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20177 \t\t Training Loss: 0.0005808760761283338 \t\n",
      "Epoch 20178 \t\t Training Loss: 0.0005808760179206729 \t\n",
      "Epoch 20179 \t\t Training Loss: 0.0005808760179206729 \t\n",
      "Epoch 20180 \t\t Training Loss: 0.0005808760179206729 \t\n",
      "Epoch 20181 \t\t Training Loss: 0.0005808760179206729 \t\n",
      "Epoch 20182 \t\t Training Loss: 0.0005808760179206729 \t\n",
      "Epoch 20183 \t\t Training Loss: 0.0005808760179206729 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20184 \t\t Training Loss: 0.0005808760179206729 \t\n",
      "Epoch 20185 \t\t Training Loss: 0.0005808760179206729 \t\n",
      "Epoch 20186 \t\t Training Loss: 0.000580875959713012 \t\n",
      "Epoch 20187 \t\t Training Loss: 0.000580875959713012 \t\n",
      "Epoch 20188 \t\t Training Loss: 0.000580875959713012 \t\n",
      "Epoch 20189 \t\t Training Loss: 0.000580875959713012 \t\n",
      "Epoch 20190 \t\t Training Loss: 0.000580875959713012 \t\n",
      "Epoch 20191 \t\t Training Loss: 0.000580875959713012 \t\n",
      "Epoch 20192 \t\t Training Loss: 0.000580875959713012 \t\n",
      "Epoch 20193 \t\t Training Loss: 0.000580875959713012 \t\n",
      "Epoch 20194 \t\t Training Loss: 0.000580875959713012 \t\n",
      "Epoch 20195 \t\t Training Loss: 0.000580875959713012 \t\n",
      "Epoch 20196 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20197 \t\t Training Loss: 0.0005808759015053511 \t\n",
      "Epoch 20198 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20199 \t\t Training Loss: 0.0005808759015053511 \t\n",
      "Epoch 20200 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20201 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20202 \t\t Training Loss: 0.0005808759015053511 \t\n",
      "Epoch 20203 \t\t Training Loss: 0.0005808759015053511 \t\n",
      "Epoch 20204 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20205 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20206 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20207 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20208 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20209 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20210 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20211 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20212 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20213 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20214 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20215 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20216 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20217 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20218 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20219 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20220 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20221 \t\t Training Loss: 0.0005808757850900292 \t\n",
      "Epoch 20222 \t\t Training Loss: 0.0005808757268823683 \t\n",
      "Epoch 20223 \t\t Training Loss: 0.0005808757268823683 \t\n",
      "Epoch 20224 \t\t Training Loss: 0.0005808757268823683 \t\n",
      "Epoch 20225 \t\t Training Loss: 0.0005808757268823683 \t\n",
      "Epoch 20226 \t\t Training Loss: 0.0005808757268823683 \t\n",
      "Epoch 20227 \t\t Training Loss: 0.0005808757268823683 \t\n",
      "Epoch 20228 \t\t Training Loss: 0.0005808757268823683 \t\n",
      "Epoch 20229 \t\t Training Loss: 0.0005808756686747074 \t\n",
      "Epoch 20230 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20231 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20232 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20233 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20234 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20235 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20236 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20237 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20238 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20239 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20240 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20241 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20242 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20243 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20244 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20245 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20246 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20247 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20248 \t\t Training Loss: 0.0005808756104670465 \t\n",
      "Epoch 20249 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20250 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20251 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20252 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20253 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20254 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20255 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20256 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20257 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20258 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20259 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20260 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20261 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20262 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20263 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20264 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20265 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20266 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20267 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20268 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20269 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20270 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20271 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20272 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20273 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20274 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20275 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20276 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20277 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20278 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20279 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20280 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20281 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20282 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20283 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20284 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20285 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20286 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20287 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20288 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20289 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20290 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20291 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20292 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20293 \t\t Training Loss: 0.0005808754940517247 \t\n",
      "Epoch 20294 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20295 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20296 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20297 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20298 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20299 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20300 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20301 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20302 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20303 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20304 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20305 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20306 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20307 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20308 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20309 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20310 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20311 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20312 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20313 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20314 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20315 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20316 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20317 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20318 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20319 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20320 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20321 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20322 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20323 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20324 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20325 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20326 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20327 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20328 \t\t Training Loss: 0.0005808753776364028 \t\n",
      "Epoch 20329 \t\t Training Loss: 0.0005808753194287419 \t\n",
      "Epoch 20330 \t\t Training Loss: 0.0005808753194287419 \t\n",
      "Epoch 20331 \t\t Training Loss: 0.0005808753194287419 \t\n",
      "Epoch 20332 \t\t Training Loss: 0.0005808753194287419 \t\n",
      "Epoch 20333 \t\t Training Loss: 0.0005808753194287419 \t\n",
      "Epoch 20334 \t\t Training Loss: 0.0005808753194287419 \t\n",
      "Epoch 20335 \t\t Training Loss: 0.0005808753194287419 \t\n",
      "Epoch 20336 \t\t Training Loss: 0.0005808753194287419 \t\n",
      "Epoch 20337 \t\t Training Loss: 0.000580875261221081 \t\n",
      "Epoch 20338 \t\t Training Loss: 0.000580875261221081 \t\n",
      "Epoch 20339 \t\t Training Loss: 0.000580875261221081 \t\n",
      "Epoch 20340 \t\t Training Loss: 0.000580875261221081 \t\n",
      "Epoch 20341 \t\t Training Loss: 0.000580875261221081 \t\n",
      "Epoch 20342 \t\t Training Loss: 0.000580875261221081 \t\n",
      "Epoch 20343 \t\t Training Loss: 0.000580875261221081 \t\n",
      "Epoch 20344 \t\t Training Loss: 0.000580875261221081 \t\n",
      "Epoch 20345 \t\t Training Loss: 0.000580875261221081 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20346 \t\t Training Loss: 0.0005808752030134201 \t\n",
      "Epoch 20347 \t\t Training Loss: 0.0005808752030134201 \t\n",
      "Epoch 20348 \t\t Training Loss: 0.0005808752030134201 \t\n",
      "Epoch 20349 \t\t Training Loss: 0.0005808750865980983 \t\n",
      "Epoch 20350 \t\t Training Loss: 0.0005808750865980983 \t\n",
      "Epoch 20351 \t\t Training Loss: 0.0005808752030134201 \t\n",
      "Epoch 20352 \t\t Training Loss: 0.0005808752030134201 \t\n",
      "Epoch 20353 \t\t Training Loss: 0.0005808752030134201 \t\n",
      "Epoch 20354 \t\t Training Loss: 0.0005808750865980983 \t\n",
      "Epoch 20355 \t\t Training Loss: 0.0005808750865980983 \t\n",
      "Epoch 20356 \t\t Training Loss: 0.0005808750865980983 \t\n",
      "Epoch 20357 \t\t Training Loss: 0.0005808750865980983 \t\n",
      "Epoch 20358 \t\t Training Loss: 0.0005808750865980983 \t\n",
      "Epoch 20359 \t\t Training Loss: 0.0005808750865980983 \t\n",
      "Epoch 20360 \t\t Training Loss: 0.0005808750865980983 \t\n",
      "Epoch 20361 \t\t Training Loss: 0.0005808750865980983 \t\n",
      "Epoch 20362 \t\t Training Loss: 0.0005808750865980983 \t\n",
      "Epoch 20363 \t\t Training Loss: 0.0005808750865980983 \t\n",
      "Epoch 20364 \t\t Training Loss: 0.0005808750865980983 \t\n",
      "Epoch 20365 \t\t Training Loss: 0.0005808750865980983 \t\n",
      "Epoch 20366 \t\t Training Loss: 0.0005808750865980983 \t\n",
      "Epoch 20367 \t\t Training Loss: 0.0005808750865980983 \t\n",
      "Epoch 20368 \t\t Training Loss: 0.0005808750283904374 \t\n",
      "Epoch 20369 \t\t Training Loss: 0.0005808750283904374 \t\n",
      "Epoch 20370 \t\t Training Loss: 0.0005808750283904374 \t\n",
      "Epoch 20371 \t\t Training Loss: 0.0005808750283904374 \t\n",
      "Epoch 20372 \t\t Training Loss: 0.0005808750283904374 \t\n",
      "Epoch 20373 \t\t Training Loss: 0.0005808750283904374 \t\n",
      "Epoch 20374 \t\t Training Loss: 0.0005808750283904374 \t\n",
      "Epoch 20375 \t\t Training Loss: 0.0005808750283904374 \t\n",
      "Epoch 20376 \t\t Training Loss: 0.0005808750283904374 \t\n",
      "Epoch 20377 \t\t Training Loss: 0.0005808749119751155 \t\n",
      "Epoch 20378 \t\t Training Loss: 0.0005808749701827765 \t\n",
      "Epoch 20379 \t\t Training Loss: 0.0005808749119751155 \t\n",
      "Epoch 20380 \t\t Training Loss: 0.0005808749119751155 \t\n",
      "Epoch 20381 \t\t Training Loss: 0.0005808749119751155 \t\n",
      "Epoch 20382 \t\t Training Loss: 0.0005808749119751155 \t\n",
      "Epoch 20383 \t\t Training Loss: 0.0005808749119751155 \t\n",
      "Epoch 20384 \t\t Training Loss: 0.0005808749119751155 \t\n",
      "Epoch 20385 \t\t Training Loss: 0.0005808749119751155 \t\n",
      "Epoch 20386 \t\t Training Loss: 0.0005808748537674546 \t\n",
      "Epoch 20387 \t\t Training Loss: 0.0005808747955597937 \t\n",
      "Epoch 20388 \t\t Training Loss: 0.0005808747955597937 \t\n",
      "Epoch 20389 \t\t Training Loss: 0.0005808747955597937 \t\n",
      "Epoch 20390 \t\t Training Loss: 0.0005808747955597937 \t\n",
      "Epoch 20391 \t\t Training Loss: 0.0005808747955597937 \t\n",
      "Epoch 20392 \t\t Training Loss: 0.0005808747955597937 \t\n",
      "Epoch 20393 \t\t Training Loss: 0.0005808747955597937 \t\n",
      "Epoch 20394 \t\t Training Loss: 0.0005808747955597937 \t\n",
      "Epoch 20395 \t\t Training Loss: 0.0005808747955597937 \t\n",
      "Epoch 20396 \t\t Training Loss: 0.0005808747955597937 \t\n",
      "Epoch 20397 \t\t Training Loss: 0.0005808747373521328 \t\n",
      "Epoch 20398 \t\t Training Loss: 0.0005808747955597937 \t\n",
      "Epoch 20399 \t\t Training Loss: 0.0005808747373521328 \t\n",
      "Epoch 20400 \t\t Training Loss: 0.0005808747373521328 \t\n",
      "Epoch 20401 \t\t Training Loss: 0.0005808747373521328 \t\n",
      "Epoch 20402 \t\t Training Loss: 0.0005808747373521328 \t\n",
      "Epoch 20403 \t\t Training Loss: 0.0005808747373521328 \t\n",
      "Epoch 20404 \t\t Training Loss: 0.0005808747373521328 \t\n",
      "Epoch 20405 \t\t Training Loss: 0.0005808747373521328 \t\n",
      "Epoch 20406 \t\t Training Loss: 0.0005808747373521328 \t\n",
      "Epoch 20407 \t\t Training Loss: 0.0005808747373521328 \t\n",
      "Epoch 20408 \t\t Training Loss: 0.0005808747373521328 \t\n",
      "Epoch 20409 \t\t Training Loss: 0.0005808747373521328 \t\n",
      "Epoch 20410 \t\t Training Loss: 0.0005808747373521328 \t\n",
      "Epoch 20411 \t\t Training Loss: 0.0005808747373521328 \t\n",
      "Epoch 20412 \t\t Training Loss: 0.0005808747373521328 \t\n",
      "Epoch 20413 \t\t Training Loss: 0.0005808746791444719 \t\n",
      "Epoch 20414 \t\t Training Loss: 0.0005808746791444719 \t\n",
      "Epoch 20415 \t\t Training Loss: 0.0005808746791444719 \t\n",
      "Epoch 20416 \t\t Training Loss: 0.0005808746791444719 \t\n",
      "Epoch 20417 \t\t Training Loss: 0.0005808746791444719 \t\n",
      "Epoch 20418 \t\t Training Loss: 0.0005808746791444719 \t\n",
      "Epoch 20419 \t\t Training Loss: 0.0005808746791444719 \t\n",
      "Epoch 20420 \t\t Training Loss: 0.0005808746791444719 \t\n",
      "Epoch 20421 \t\t Training Loss: 0.0005808746791444719 \t\n",
      "Epoch 20422 \t\t Training Loss: 0.0005808746791444719 \t\n",
      "Epoch 20423 \t\t Training Loss: 0.0005808746791444719 \t\n",
      "Epoch 20424 \t\t Training Loss: 0.0005808746791444719 \t\n",
      "Epoch 20425 \t\t Training Loss: 0.0005808746791444719 \t\n",
      "Epoch 20426 \t\t Training Loss: 0.0005808746791444719 \t\n",
      "Epoch 20427 \t\t Training Loss: 0.0005808746791444719 \t\n",
      "Epoch 20428 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20429 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20430 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20431 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20432 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20433 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20434 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20435 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20436 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20437 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20438 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20439 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20440 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20441 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20442 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20443 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20444 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20445 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20446 \t\t Training Loss: 0.0005808744463138282 \t\n",
      "Epoch 20447 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20448 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20449 \t\t Training Loss: 0.0005808744463138282 \t\n",
      "Epoch 20450 \t\t Training Loss: 0.0005808744463138282 \t\n",
      "Epoch 20451 \t\t Training Loss: 0.0005808745045214891 \t\n",
      "Epoch 20452 \t\t Training Loss: 0.0005808743881061673 \t\n",
      "Epoch 20453 \t\t Training Loss: 0.0005808743881061673 \t\n",
      "Epoch 20454 \t\t Training Loss: 0.0005808743881061673 \t\n",
      "Epoch 20455 \t\t Training Loss: 0.0005808743881061673 \t\n",
      "Epoch 20456 \t\t Training Loss: 0.0005808743881061673 \t\n",
      "Epoch 20457 \t\t Training Loss: 0.0005808743881061673 \t\n",
      "Epoch 20458 \t\t Training Loss: 0.0005808743881061673 \t\n",
      "Epoch 20459 \t\t Training Loss: 0.0005808743298985064 \t\n",
      "Epoch 20460 \t\t Training Loss: 0.0005808743298985064 \t\n",
      "Epoch 20461 \t\t Training Loss: 0.0005808743298985064 \t\n",
      "Epoch 20462 \t\t Training Loss: 0.0005808743298985064 \t\n",
      "Epoch 20463 \t\t Training Loss: 0.0005808743298985064 \t\n",
      "Epoch 20464 \t\t Training Loss: 0.0005808743298985064 \t\n",
      "Epoch 20465 \t\t Training Loss: 0.0005808743298985064 \t\n",
      "Epoch 20466 \t\t Training Loss: 0.0005808743298985064 \t\n",
      "Epoch 20467 \t\t Training Loss: 0.0005808743298985064 \t\n",
      "Epoch 20468 \t\t Training Loss: 0.0005808743298985064 \t\n",
      "Epoch 20469 \t\t Training Loss: 0.0005808742716908455 \t\n",
      "Epoch 20470 \t\t Training Loss: 0.0005808742716908455 \t\n",
      "Epoch 20471 \t\t Training Loss: 0.0005808742716908455 \t\n",
      "Epoch 20472 \t\t Training Loss: 0.0005808742716908455 \t\n",
      "Epoch 20473 \t\t Training Loss: 0.0005808742716908455 \t\n",
      "Epoch 20474 \t\t Training Loss: 0.0005808742716908455 \t\n",
      "Epoch 20475 \t\t Training Loss: 0.0005808742716908455 \t\n",
      "Epoch 20476 \t\t Training Loss: 0.0005808742716908455 \t\n",
      "Epoch 20477 \t\t Training Loss: 0.0005808742716908455 \t\n",
      "Epoch 20478 \t\t Training Loss: 0.0005808742716908455 \t\n",
      "Epoch 20479 \t\t Training Loss: 0.0005808742716908455 \t\n",
      "Epoch 20480 \t\t Training Loss: 0.0005808742134831846 \t\n",
      "Epoch 20481 \t\t Training Loss: 0.0005808742134831846 \t\n",
      "Epoch 20482 \t\t Training Loss: 0.0005808742134831846 \t\n",
      "Epoch 20483 \t\t Training Loss: 0.0005808742134831846 \t\n",
      "Epoch 20484 \t\t Training Loss: 0.0005808742134831846 \t\n",
      "Epoch 20485 \t\t Training Loss: 0.0005808742134831846 \t\n",
      "Epoch 20486 \t\t Training Loss: 0.0005808742134831846 \t\n",
      "Epoch 20487 \t\t Training Loss: 0.0005808742134831846 \t\n",
      "Epoch 20488 \t\t Training Loss: 0.0005808742134831846 \t\n",
      "Epoch 20489 \t\t Training Loss: 0.0005808742134831846 \t\n",
      "Epoch 20490 \t\t Training Loss: 0.0005808742134831846 \t\n",
      "Epoch 20491 \t\t Training Loss: 0.0005808742716908455 \t\n",
      "Epoch 20492 \t\t Training Loss: 0.0005808742134831846 \t\n",
      "Epoch 20493 \t\t Training Loss: 0.0005808742134831846 \t\n",
      "Epoch 20494 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20495 \t\t Training Loss: 0.0005808741552755237 \t\n",
      "Epoch 20496 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20497 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20498 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20499 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20500 \t\t Training Loss: 0.0005808740970678627 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20501 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20502 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20503 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20504 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20505 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20506 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20507 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20508 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20509 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20510 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20511 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20512 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20513 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20514 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20515 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20516 \t\t Training Loss: 0.0005808740970678627 \t\n",
      "Epoch 20517 \t\t Training Loss: 0.0005808740388602018 \t\n",
      "Epoch 20518 \t\t Training Loss: 0.0005808740388602018 \t\n",
      "Epoch 20519 \t\t Training Loss: 0.0005808740388602018 \t\n",
      "Epoch 20520 \t\t Training Loss: 0.0005808740388602018 \t\n",
      "Epoch 20521 \t\t Training Loss: 0.0005808740388602018 \t\n",
      "Epoch 20522 \t\t Training Loss: 0.0005808739806525409 \t\n",
      "Epoch 20523 \t\t Training Loss: 0.0005808739806525409 \t\n",
      "Epoch 20524 \t\t Training Loss: 0.0005808739806525409 \t\n",
      "Epoch 20525 \t\t Training Loss: 0.0005808739806525409 \t\n",
      "Epoch 20526 \t\t Training Loss: 0.0005808739806525409 \t\n",
      "Epoch 20527 \t\t Training Loss: 0.00058087392244488 \t\n",
      "Epoch 20528 \t\t Training Loss: 0.00058087392244488 \t\n",
      "Epoch 20529 \t\t Training Loss: 0.00058087392244488 \t\n",
      "Epoch 20530 \t\t Training Loss: 0.00058087392244488 \t\n",
      "Epoch 20531 \t\t Training Loss: 0.00058087392244488 \t\n",
      "Epoch 20532 \t\t Training Loss: 0.00058087392244488 \t\n",
      "Epoch 20533 \t\t Training Loss: 0.0005808738642372191 \t\n",
      "Epoch 20534 \t\t Training Loss: 0.00058087392244488 \t\n",
      "Epoch 20535 \t\t Training Loss: 0.00058087392244488 \t\n",
      "Epoch 20536 \t\t Training Loss: 0.00058087392244488 \t\n",
      "Epoch 20537 \t\t Training Loss: 0.00058087392244488 \t\n",
      "Epoch 20538 \t\t Training Loss: 0.00058087392244488 \t\n",
      "Epoch 20539 \t\t Training Loss: 0.0005808738642372191 \t\n",
      "Epoch 20540 \t\t Training Loss: 0.00058087392244488 \t\n",
      "Epoch 20541 \t\t Training Loss: 0.00058087392244488 \t\n",
      "Epoch 20542 \t\t Training Loss: 0.0005808738642372191 \t\n",
      "Epoch 20543 \t\t Training Loss: 0.0005808738642372191 \t\n",
      "Epoch 20544 \t\t Training Loss: 0.0005808738642372191 \t\n",
      "Epoch 20545 \t\t Training Loss: 0.0005808738642372191 \t\n",
      "Epoch 20546 \t\t Training Loss: 0.0005808738642372191 \t\n",
      "Epoch 20547 \t\t Training Loss: 0.0005808738642372191 \t\n",
      "Epoch 20548 \t\t Training Loss: 0.0005808738642372191 \t\n",
      "Epoch 20549 \t\t Training Loss: 0.0005808738642372191 \t\n",
      "Epoch 20550 \t\t Training Loss: 0.0005808738642372191 \t\n",
      "Epoch 20551 \t\t Training Loss: 0.0005808738642372191 \t\n",
      "Epoch 20552 \t\t Training Loss: 0.0005808738642372191 \t\n",
      "Epoch 20553 \t\t Training Loss: 0.0005808738642372191 \t\n",
      "Epoch 20554 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20555 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20556 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20557 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20558 \t\t Training Loss: 0.0005808738642372191 \t\n",
      "Epoch 20559 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20560 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20561 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20562 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20563 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20564 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20565 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20566 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20567 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20568 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20569 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20570 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20571 \t\t Training Loss: 0.0005808738642372191 \t\n",
      "Epoch 20572 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20573 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20574 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20575 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20576 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20577 \t\t Training Loss: 0.0005808738060295582 \t\n",
      "Epoch 20578 \t\t Training Loss: 0.0005808737478218973 \t\n",
      "Epoch 20579 \t\t Training Loss: 0.0005808737478218973 \t\n",
      "Epoch 20580 \t\t Training Loss: 0.0005808737478218973 \t\n",
      "Epoch 20581 \t\t Training Loss: 0.0005808737478218973 \t\n",
      "Epoch 20582 \t\t Training Loss: 0.0005808737478218973 \t\n",
      "Epoch 20583 \t\t Training Loss: 0.0005808737478218973 \t\n",
      "Epoch 20584 \t\t Training Loss: 0.0005808737478218973 \t\n",
      "Epoch 20585 \t\t Training Loss: 0.0005808736896142364 \t\n",
      "Epoch 20586 \t\t Training Loss: 0.0005808736896142364 \t\n",
      "Epoch 20587 \t\t Training Loss: 0.0005808736896142364 \t\n",
      "Epoch 20588 \t\t Training Loss: 0.0005808736896142364 \t\n",
      "Epoch 20589 \t\t Training Loss: 0.0005808736896142364 \t\n",
      "Epoch 20590 \t\t Training Loss: 0.0005808736896142364 \t\n",
      "Epoch 20591 \t\t Training Loss: 0.0005808736896142364 \t\n",
      "Epoch 20592 \t\t Training Loss: 0.0005808736896142364 \t\n",
      "Epoch 20593 \t\t Training Loss: 0.0005808736896142364 \t\n",
      "Epoch 20594 \t\t Training Loss: 0.0005808736314065754 \t\n",
      "Epoch 20595 \t\t Training Loss: 0.0005808736314065754 \t\n",
      "Epoch 20596 \t\t Training Loss: 0.0005808736314065754 \t\n",
      "Epoch 20597 \t\t Training Loss: 0.0005808736314065754 \t\n",
      "Epoch 20598 \t\t Training Loss: 0.0005808736314065754 \t\n",
      "Epoch 20599 \t\t Training Loss: 0.0005808735731989145 \t\n",
      "Epoch 20600 \t\t Training Loss: 0.0005808736314065754 \t\n",
      "Epoch 20601 \t\t Training Loss: 0.0005808735731989145 \t\n",
      "Epoch 20602 \t\t Training Loss: 0.0005808735149912536 \t\n",
      "Epoch 20603 \t\t Training Loss: 0.0005808735149912536 \t\n",
      "Epoch 20604 \t\t Training Loss: 0.0005808735149912536 \t\n",
      "Epoch 20605 \t\t Training Loss: 0.0005808735149912536 \t\n",
      "Epoch 20606 \t\t Training Loss: 0.0005808735149912536 \t\n",
      "Epoch 20607 \t\t Training Loss: 0.0005808735149912536 \t\n",
      "Epoch 20608 \t\t Training Loss: 0.0005808735149912536 \t\n",
      "Epoch 20609 \t\t Training Loss: 0.0005808735149912536 \t\n",
      "Epoch 20610 \t\t Training Loss: 0.0005808735149912536 \t\n",
      "Epoch 20611 \t\t Training Loss: 0.0005808735149912536 \t\n",
      "Epoch 20612 \t\t Training Loss: 0.0005808735149912536 \t\n",
      "Epoch 20613 \t\t Training Loss: 0.0005808734567835927 \t\n",
      "Epoch 20614 \t\t Training Loss: 0.0005808735149912536 \t\n",
      "Epoch 20615 \t\t Training Loss: 0.0005808735149912536 \t\n",
      "Epoch 20616 \t\t Training Loss: 0.0005808734567835927 \t\n",
      "Epoch 20617 \t\t Training Loss: 0.0005808734567835927 \t\n",
      "Epoch 20618 \t\t Training Loss: 0.0005808734567835927 \t\n",
      "Epoch 20619 \t\t Training Loss: 0.0005808734567835927 \t\n",
      "Epoch 20620 \t\t Training Loss: 0.0005808733985759318 \t\n",
      "Epoch 20621 \t\t Training Loss: 0.0005808733985759318 \t\n",
      "Epoch 20622 \t\t Training Loss: 0.0005808733985759318 \t\n",
      "Epoch 20623 \t\t Training Loss: 0.0005808733985759318 \t\n",
      "Epoch 20624 \t\t Training Loss: 0.0005808734567835927 \t\n",
      "Epoch 20625 \t\t Training Loss: 0.0005808734567835927 \t\n",
      "Epoch 20626 \t\t Training Loss: 0.0005808734567835927 \t\n",
      "Epoch 20627 \t\t Training Loss: 0.0005808733985759318 \t\n",
      "Epoch 20628 \t\t Training Loss: 0.0005808733985759318 \t\n",
      "Epoch 20629 \t\t Training Loss: 0.0005808733985759318 \t\n",
      "Epoch 20630 \t\t Training Loss: 0.0005808733985759318 \t\n",
      "Epoch 20631 \t\t Training Loss: 0.0005808733985759318 \t\n",
      "Epoch 20632 \t\t Training Loss: 0.0005808733985759318 \t\n",
      "Epoch 20633 \t\t Training Loss: 0.0005808733985759318 \t\n",
      "Epoch 20634 \t\t Training Loss: 0.0005808733985759318 \t\n",
      "Epoch 20635 \t\t Training Loss: 0.0005808733985759318 \t\n",
      "Epoch 20636 \t\t Training Loss: 0.0005808733403682709 \t\n",
      "Epoch 20637 \t\t Training Loss: 0.0005808733985759318 \t\n",
      "Epoch 20638 \t\t Training Loss: 0.0005808733985759318 \t\n",
      "Epoch 20639 \t\t Training Loss: 0.0005808733403682709 \t\n",
      "Epoch 20640 \t\t Training Loss: 0.0005808733985759318 \t\n",
      "Epoch 20641 \t\t Training Loss: 0.0005808733403682709 \t\n",
      "Epoch 20642 \t\t Training Loss: 0.0005808733403682709 \t\n",
      "Epoch 20643 \t\t Training Loss: 0.0005808733403682709 \t\n",
      "Epoch 20644 \t\t Training Loss: 0.00058087328216061 \t\n",
      "Epoch 20645 \t\t Training Loss: 0.00058087328216061 \t\n",
      "Epoch 20646 \t\t Training Loss: 0.00058087328216061 \t\n",
      "Epoch 20647 \t\t Training Loss: 0.00058087328216061 \t\n",
      "Epoch 20648 \t\t Training Loss: 0.000580873223952949 \t\n",
      "Epoch 20649 \t\t Training Loss: 0.000580873223952949 \t\n",
      "Epoch 20650 \t\t Training Loss: 0.000580873223952949 \t\n",
      "Epoch 20651 \t\t Training Loss: 0.000580873223952949 \t\n",
      "Epoch 20652 \t\t Training Loss: 0.000580873223952949 \t\n",
      "Epoch 20653 \t\t Training Loss: 0.000580873223952949 \t\n",
      "Epoch 20654 \t\t Training Loss: 0.000580873223952949 \t\n",
      "Epoch 20655 \t\t Training Loss: 0.000580873223952949 \t\n",
      "Epoch 20656 \t\t Training Loss: 0.000580873223952949 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20657 \t\t Training Loss: 0.00058087328216061 \t\n",
      "Epoch 20658 \t\t Training Loss: 0.00058087328216061 \t\n",
      "Epoch 20659 \t\t Training Loss: 0.000580873223952949 \t\n",
      "Epoch 20660 \t\t Training Loss: 0.000580873223952949 \t\n",
      "Epoch 20661 \t\t Training Loss: 0.000580873223952949 \t\n",
      "Epoch 20662 \t\t Training Loss: 0.000580873223952949 \t\n",
      "Epoch 20663 \t\t Training Loss: 0.000580873223952949 \t\n",
      "Epoch 20664 \t\t Training Loss: 0.0005808731657452881 \t\n",
      "Epoch 20665 \t\t Training Loss: 0.0005808731657452881 \t\n",
      "Epoch 20666 \t\t Training Loss: 0.0005808731657452881 \t\n",
      "Epoch 20667 \t\t Training Loss: 0.000580873223952949 \t\n",
      "Epoch 20668 \t\t Training Loss: 0.0005808731657452881 \t\n",
      "Epoch 20669 \t\t Training Loss: 0.0005808731657452881 \t\n",
      "Epoch 20670 \t\t Training Loss: 0.0005808731657452881 \t\n",
      "Epoch 20671 \t\t Training Loss: 0.0005808731657452881 \t\n",
      "Epoch 20672 \t\t Training Loss: 0.0005808731657452881 \t\n",
      "Epoch 20673 \t\t Training Loss: 0.0005808731657452881 \t\n",
      "Epoch 20674 \t\t Training Loss: 0.0005808731657452881 \t\n",
      "Epoch 20675 \t\t Training Loss: 0.0005808731075376272 \t\n",
      "Epoch 20676 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20677 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20678 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20679 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20680 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20681 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20682 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20683 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20684 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20685 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20686 \t\t Training Loss: 0.0005808729329146445 \t\n",
      "Epoch 20687 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20688 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20689 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20690 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20691 \t\t Training Loss: 0.0005808729911223054 \t\n",
      "Epoch 20692 \t\t Training Loss: 0.0005808729911223054 \t\n",
      "Epoch 20693 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20694 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20695 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20696 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20697 \t\t Training Loss: 0.0005808730493299663 \t\n",
      "Epoch 20698 \t\t Training Loss: 0.0005808729329146445 \t\n",
      "Epoch 20699 \t\t Training Loss: 0.0005808729329146445 \t\n",
      "Epoch 20700 \t\t Training Loss: 0.0005808729329146445 \t\n",
      "Epoch 20701 \t\t Training Loss: 0.0005808729329146445 \t\n",
      "Epoch 20702 \t\t Training Loss: 0.0005808729329146445 \t\n",
      "Epoch 20703 \t\t Training Loss: 0.0005808729329146445 \t\n",
      "Epoch 20704 \t\t Training Loss: 0.0005808728747069836 \t\n",
      "Epoch 20705 \t\t Training Loss: 0.0005808728747069836 \t\n",
      "Epoch 20706 \t\t Training Loss: 0.0005808728747069836 \t\n",
      "Epoch 20707 \t\t Training Loss: 0.0005808728747069836 \t\n",
      "Epoch 20708 \t\t Training Loss: 0.0005808728747069836 \t\n",
      "Epoch 20709 \t\t Training Loss: 0.0005808728747069836 \t\n",
      "Epoch 20710 \t\t Training Loss: 0.0005808728747069836 \t\n",
      "Epoch 20711 \t\t Training Loss: 0.0005808728747069836 \t\n",
      "Epoch 20712 \t\t Training Loss: 0.0005808728747069836 \t\n",
      "Epoch 20713 \t\t Training Loss: 0.0005808728747069836 \t\n",
      "Epoch 20714 \t\t Training Loss: 0.0005808728747069836 \t\n",
      "Epoch 20715 \t\t Training Loss: 0.0005808728747069836 \t\n",
      "Epoch 20716 \t\t Training Loss: 0.0005808728747069836 \t\n",
      "Epoch 20717 \t\t Training Loss: 0.0005808728164993227 \t\n",
      "Epoch 20718 \t\t Training Loss: 0.0005808728164993227 \t\n",
      "Epoch 20719 \t\t Training Loss: 0.0005808728164993227 \t\n",
      "Epoch 20720 \t\t Training Loss: 0.0005808728164993227 \t\n",
      "Epoch 20721 \t\t Training Loss: 0.0005808728164993227 \t\n",
      "Epoch 20722 \t\t Training Loss: 0.0005808728164993227 \t\n",
      "Epoch 20723 \t\t Training Loss: 0.0005808728164993227 \t\n",
      "Epoch 20724 \t\t Training Loss: 0.0005808728164993227 \t\n",
      "Epoch 20725 \t\t Training Loss: 0.0005808727582916617 \t\n",
      "Epoch 20726 \t\t Training Loss: 0.0005808727582916617 \t\n",
      "Epoch 20727 \t\t Training Loss: 0.0005808727582916617 \t\n",
      "Epoch 20728 \t\t Training Loss: 0.0005808727582916617 \t\n",
      "Epoch 20729 \t\t Training Loss: 0.0005808727582916617 \t\n",
      "Epoch 20730 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20731 \t\t Training Loss: 0.0005808727000840008 \t\n",
      "Epoch 20732 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20733 \t\t Training Loss: 0.0005808727000840008 \t\n",
      "Epoch 20734 \t\t Training Loss: 0.0005808727000840008 \t\n",
      "Epoch 20735 \t\t Training Loss: 0.0005808727000840008 \t\n",
      "Epoch 20736 \t\t Training Loss: 0.0005808727582916617 \t\n",
      "Epoch 20737 \t\t Training Loss: 0.0005808727582916617 \t\n",
      "Epoch 20738 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20739 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20740 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20741 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20742 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20743 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20744 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20745 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20746 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20747 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20748 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20749 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20750 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20751 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20752 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20753 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20754 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20755 \t\t Training Loss: 0.0005808726418763399 \t\n",
      "Epoch 20756 \t\t Training Loss: 0.0005808725254610181 \t\n",
      "Epoch 20757 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20758 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20759 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20760 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20761 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20762 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20763 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20764 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20765 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20766 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20767 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20768 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20769 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20770 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20771 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20772 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20773 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20774 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20775 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20776 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20777 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20778 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20779 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20780 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20781 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20782 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20783 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20784 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20785 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20786 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20787 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20788 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20789 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20790 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20791 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20792 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20793 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20794 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20795 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20796 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20797 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20798 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20799 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20800 \t\t Training Loss: 0.0005808724672533572 \t\n",
      "Epoch 20801 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20802 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20803 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20804 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20805 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20806 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20807 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20808 \t\t Training Loss: 0.0005808724090456963 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20809 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20810 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20811 \t\t Training Loss: 0.0005808724090456963 \t\n",
      "Epoch 20812 \t\t Training Loss: 0.0005808723508380353 \t\n",
      "Epoch 20813 \t\t Training Loss: 0.0005808723508380353 \t\n",
      "Epoch 20814 \t\t Training Loss: 0.0005808722926303744 \t\n",
      "Epoch 20815 \t\t Training Loss: 0.0005808723508380353 \t\n",
      "Epoch 20816 \t\t Training Loss: 0.0005808722926303744 \t\n",
      "Epoch 20817 \t\t Training Loss: 0.0005808722926303744 \t\n",
      "Epoch 20818 \t\t Training Loss: 0.0005808722926303744 \t\n",
      "Epoch 20819 \t\t Training Loss: 0.0005808722926303744 \t\n",
      "Epoch 20820 \t\t Training Loss: 0.0005808722926303744 \t\n",
      "Epoch 20821 \t\t Training Loss: 0.0005808722926303744 \t\n",
      "Epoch 20822 \t\t Training Loss: 0.0005808722926303744 \t\n",
      "Epoch 20823 \t\t Training Loss: 0.0005808722926303744 \t\n",
      "Epoch 20824 \t\t Training Loss: 0.0005808722926303744 \t\n",
      "Epoch 20825 \t\t Training Loss: 0.0005808722926303744 \t\n",
      "Epoch 20826 \t\t Training Loss: 0.0005808722926303744 \t\n",
      "Epoch 20827 \t\t Training Loss: 0.0005808722926303744 \t\n",
      "Epoch 20828 \t\t Training Loss: 0.0005808722926303744 \t\n",
      "Epoch 20829 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20830 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20831 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20832 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20833 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20834 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20835 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20836 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20837 \t\t Training Loss: 0.0005808722926303744 \t\n",
      "Epoch 20838 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20839 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20840 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20841 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20842 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20843 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20844 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20845 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20846 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20847 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20848 \t\t Training Loss: 0.0005808722344227135 \t\n",
      "Epoch 20849 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20850 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20851 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20852 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20853 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20854 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20855 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20856 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20857 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20858 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20859 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20860 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20861 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20862 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20863 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20864 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20865 \t\t Training Loss: 0.0005808721762150526 \t\n",
      "Epoch 20866 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20867 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20868 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20869 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20870 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20871 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20872 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20873 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20874 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20875 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20876 \t\t Training Loss: 0.0005808721180073917 \t\n",
      "Epoch 20877 \t\t Training Loss: 0.0005808721180073917 \t\n",
      "Epoch 20878 \t\t Training Loss: 0.0005808721180073917 \t\n",
      "Epoch 20879 \t\t Training Loss: 0.0005808721180073917 \t\n",
      "Epoch 20880 \t\t Training Loss: 0.0005808721180073917 \t\n",
      "Epoch 20881 \t\t Training Loss: 0.0005808721180073917 \t\n",
      "Epoch 20882 \t\t Training Loss: 0.0005808721180073917 \t\n",
      "Epoch 20883 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20884 \t\t Training Loss: 0.0005808721180073917 \t\n",
      "Epoch 20885 \t\t Training Loss: 0.0005808721180073917 \t\n",
      "Epoch 20886 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20887 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20888 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20889 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20890 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20891 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20892 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20893 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20894 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20895 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20896 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20897 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20898 \t\t Training Loss: 0.0005808720597997308 \t\n",
      "Epoch 20899 \t\t Training Loss: 0.0005808720015920699 \t\n",
      "Epoch 20900 \t\t Training Loss: 0.000580871943384409 \t\n",
      "Epoch 20901 \t\t Training Loss: 0.000580871943384409 \t\n",
      "Epoch 20902 \t\t Training Loss: 0.000580871943384409 \t\n",
      "Epoch 20903 \t\t Training Loss: 0.000580871943384409 \t\n",
      "Epoch 20904 \t\t Training Loss: 0.000580871943384409 \t\n",
      "Epoch 20905 \t\t Training Loss: 0.000580871943384409 \t\n",
      "Epoch 20906 \t\t Training Loss: 0.000580871943384409 \t\n",
      "Epoch 20907 \t\t Training Loss: 0.000580871943384409 \t\n",
      "Epoch 20908 \t\t Training Loss: 0.000580871943384409 \t\n",
      "Epoch 20909 \t\t Training Loss: 0.000580871943384409 \t\n",
      "Epoch 20910 \t\t Training Loss: 0.000580871943384409 \t\n",
      "Epoch 20911 \t\t Training Loss: 0.000580871943384409 \t\n",
      "Epoch 20912 \t\t Training Loss: 0.000580871943384409 \t\n",
      "Epoch 20913 \t\t Training Loss: 0.000580871943384409 \t\n",
      "Epoch 20914 \t\t Training Loss: 0.000580871943384409 \t\n",
      "Epoch 20915 \t\t Training Loss: 0.0005808718269690871 \t\n",
      "Epoch 20916 \t\t Training Loss: 0.0005808718269690871 \t\n",
      "Epoch 20917 \t\t Training Loss: 0.0005808718269690871 \t\n",
      "Epoch 20918 \t\t Training Loss: 0.0005808718269690871 \t\n",
      "Epoch 20919 \t\t Training Loss: 0.0005808718269690871 \t\n",
      "Epoch 20920 \t\t Training Loss: 0.0005808718269690871 \t\n",
      "Epoch 20921 \t\t Training Loss: 0.0005808717687614262 \t\n",
      "Epoch 20922 \t\t Training Loss: 0.0005808717687614262 \t\n",
      "Epoch 20923 \t\t Training Loss: 0.0005808717687614262 \t\n",
      "Epoch 20924 \t\t Training Loss: 0.0005808717687614262 \t\n",
      "Epoch 20925 \t\t Training Loss: 0.0005808717687614262 \t\n",
      "Epoch 20926 \t\t Training Loss: 0.0005808717687614262 \t\n",
      "Epoch 20927 \t\t Training Loss: 0.0005808717687614262 \t\n",
      "Epoch 20928 \t\t Training Loss: 0.0005808717687614262 \t\n",
      "Epoch 20929 \t\t Training Loss: 0.0005808717687614262 \t\n",
      "Epoch 20930 \t\t Training Loss: 0.0005808717687614262 \t\n",
      "Epoch 20931 \t\t Training Loss: 0.0005808717105537653 \t\n",
      "Epoch 20932 \t\t Training Loss: 0.0005808717687614262 \t\n",
      "Epoch 20933 \t\t Training Loss: 0.0005808717105537653 \t\n",
      "Epoch 20934 \t\t Training Loss: 0.0005808717105537653 \t\n",
      "Epoch 20935 \t\t Training Loss: 0.0005808717105537653 \t\n",
      "Epoch 20936 \t\t Training Loss: 0.0005808717105537653 \t\n",
      "Epoch 20937 \t\t Training Loss: 0.0005808717105537653 \t\n",
      "Epoch 20938 \t\t Training Loss: 0.0005808717105537653 \t\n",
      "Epoch 20939 \t\t Training Loss: 0.0005808717105537653 \t\n",
      "Epoch 20940 \t\t Training Loss: 0.0005808717105537653 \t\n",
      "Epoch 20941 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20942 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20943 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20944 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20945 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20946 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20947 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20948 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20949 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20950 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20951 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20952 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20953 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20954 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20955 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20956 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20957 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20958 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20959 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20960 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20961 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20962 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20963 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20964 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20965 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20966 \t\t Training Loss: 0.0005808716523461044 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20967 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20968 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20969 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20970 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20971 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20972 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20973 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20974 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20975 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20976 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20977 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20978 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20979 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20980 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20981 \t\t Training Loss: 0.0005808716523461044 \t\n",
      "Epoch 20982 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20983 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20984 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20985 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20986 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20987 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20988 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20989 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20990 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20991 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20992 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20993 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20994 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20995 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20996 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20997 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20998 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 20999 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21000 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21001 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21002 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21003 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21004 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21005 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21006 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21007 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21008 \t\t Training Loss: 0.0005808714195154607 \t\n",
      "Epoch 21009 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21010 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21011 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21012 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21013 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21014 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21015 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21016 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21017 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21018 \t\t Training Loss: 0.0005808715359307826 \t\n",
      "Epoch 21019 \t\t Training Loss: 0.0005808714195154607 \t\n",
      "Epoch 21020 \t\t Training Loss: 0.0005808714195154607 \t\n",
      "Epoch 21021 \t\t Training Loss: 0.0005808714195154607 \t\n",
      "Epoch 21022 \t\t Training Loss: 0.0005808714195154607 \t\n",
      "Epoch 21023 \t\t Training Loss: 0.0005808714195154607 \t\n",
      "Epoch 21024 \t\t Training Loss: 0.0005808714195154607 \t\n",
      "Epoch 21025 \t\t Training Loss: 0.0005808714195154607 \t\n",
      "Epoch 21026 \t\t Training Loss: 0.0005808714195154607 \t\n",
      "Epoch 21027 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21028 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21029 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21030 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21031 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21032 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21033 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21034 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21035 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21036 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21037 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21038 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21039 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21040 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21041 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21042 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21043 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21044 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21045 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21046 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21047 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21048 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21049 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21050 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21051 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21052 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21053 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21054 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21055 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21056 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21057 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21058 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21059 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21060 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21061 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21062 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21063 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21064 \t\t Training Loss: 0.0005808713613077998 \t\n",
      "Epoch 21065 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21066 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21067 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21068 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21069 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21070 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21071 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21072 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21073 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21074 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21075 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21076 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21077 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21078 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21079 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21080 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21081 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21082 \t\t Training Loss: 0.0005808711284771562 \t\n",
      "Epoch 21083 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21084 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21085 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21086 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21087 \t\t Training Loss: 0.000580871244892478 \t\n",
      "Epoch 21088 \t\t Training Loss: 0.0005808711284771562 \t\n",
      "Epoch 21089 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21090 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21091 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21092 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21093 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21094 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21095 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21096 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21097 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21098 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21099 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21100 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21101 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21102 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21103 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21104 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21105 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21106 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21107 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21108 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21109 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21110 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21111 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21112 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21113 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21114 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21115 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21116 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21117 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21118 \t\t Training Loss: 0.0005808710702694952 \t\n",
      "Epoch 21119 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21120 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21121 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21122 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21123 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21124 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21125 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21126 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21127 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21128 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21129 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21130 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21131 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21132 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21133 \t\t Training Loss: 0.0005808709538541734 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21134 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21135 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21136 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21137 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21138 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21139 \t\t Training Loss: 0.0005808708374388516 \t\n",
      "Epoch 21140 \t\t Training Loss: 0.0005808708374388516 \t\n",
      "Epoch 21141 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21142 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21143 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21144 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21145 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21146 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21147 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21148 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21149 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21150 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21151 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21152 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21153 \t\t Training Loss: 0.0005808709538541734 \t\n",
      "Epoch 21154 \t\t Training Loss: 0.0005808708374388516 \t\n",
      "Epoch 21155 \t\t Training Loss: 0.0005808708374388516 \t\n",
      "Epoch 21156 \t\t Training Loss: 0.0005808708374388516 \t\n",
      "Epoch 21157 \t\t Training Loss: 0.0005808708374388516 \t\n",
      "Epoch 21158 \t\t Training Loss: 0.0005808708374388516 \t\n",
      "Epoch 21159 \t\t Training Loss: 0.0005808708374388516 \t\n",
      "Epoch 21160 \t\t Training Loss: 0.0005808708374388516 \t\n",
      "Epoch 21161 \t\t Training Loss: 0.0005808708374388516 \t\n",
      "Epoch 21162 \t\t Training Loss: 0.0005808708374388516 \t\n",
      "Epoch 21163 \t\t Training Loss: 0.0005808707210235298 \t\n",
      "Epoch 21164 \t\t Training Loss: 0.0005808707210235298 \t\n",
      "Epoch 21165 \t\t Training Loss: 0.0005808707210235298 \t\n",
      "Epoch 21166 \t\t Training Loss: 0.0005808707210235298 \t\n",
      "Epoch 21167 \t\t Training Loss: 0.0005808708374388516 \t\n",
      "Epoch 21168 \t\t Training Loss: 0.0005808707210235298 \t\n",
      "Epoch 21169 \t\t Training Loss: 0.0005808708374388516 \t\n",
      "Epoch 21170 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21171 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21172 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21173 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21174 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21175 \t\t Training Loss: 0.0005808707210235298 \t\n",
      "Epoch 21176 \t\t Training Loss: 0.0005808707210235298 \t\n",
      "Epoch 21177 \t\t Training Loss: 0.0005808707210235298 \t\n",
      "Epoch 21178 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21179 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21180 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21181 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21182 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21183 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21184 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21185 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21186 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21187 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21188 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21189 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21190 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21191 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21192 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21193 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21194 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21195 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21196 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21197 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21198 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21199 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21200 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21201 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21202 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21203 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21204 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21205 \t\t Training Loss: 0.0005808706628158689 \t\n",
      "Epoch 21206 \t\t Training Loss: 0.000580870546400547 \t\n",
      "Epoch 21207 \t\t Training Loss: 0.000580870546400547 \t\n",
      "Epoch 21208 \t\t Training Loss: 0.000580870546400547 \t\n",
      "Epoch 21209 \t\t Training Loss: 0.000580870546400547 \t\n",
      "Epoch 21210 \t\t Training Loss: 0.000580870546400547 \t\n",
      "Epoch 21211 \t\t Training Loss: 0.000580870546400547 \t\n",
      "Epoch 21212 \t\t Training Loss: 0.000580870546400547 \t\n",
      "Epoch 21213 \t\t Training Loss: 0.000580870546400547 \t\n",
      "Epoch 21214 \t\t Training Loss: 0.000580870546400547 \t\n",
      "Epoch 21215 \t\t Training Loss: 0.000580870546400547 \t\n",
      "Epoch 21216 \t\t Training Loss: 0.000580870546400547 \t\n",
      "Epoch 21217 \t\t Training Loss: 0.000580870546400547 \t\n",
      "Epoch 21218 \t\t Training Loss: 0.000580870546400547 \t\n",
      "Epoch 21219 \t\t Training Loss: 0.000580870546400547 \t\n",
      "Epoch 21220 \t\t Training Loss: 0.000580870546400547 \t\n",
      "Epoch 21221 \t\t Training Loss: 0.000580870546400547 \t\n",
      "Epoch 21222 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21223 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21224 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21225 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21226 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21227 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21228 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21229 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21230 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21231 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21232 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21233 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21234 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21235 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21236 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21237 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21238 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21239 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21240 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21241 \t\t Training Loss: 0.0005808704299852252 \t\n",
      "Epoch 21242 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21243 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21244 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21245 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21246 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21247 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21248 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21249 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21250 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21251 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21252 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21253 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21254 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21255 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21256 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21257 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21258 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21259 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21260 \t\t Training Loss: 0.0005808702553622425 \t\n",
      "Epoch 21261 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21262 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21263 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21264 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21265 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21266 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21267 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21268 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21269 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21270 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21271 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21272 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21273 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21274 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21275 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21276 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21277 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21278 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21279 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21280 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21281 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21282 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21283 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21284 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21285 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21286 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21287 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21288 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21289 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21290 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21291 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21292 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21293 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21294 \t\t Training Loss: 0.0005808700807392597 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21295 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21296 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21297 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21298 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21299 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21300 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21301 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21302 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21303 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21304 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21305 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21306 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21307 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21308 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21309 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21310 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21311 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21312 \t\t Training Loss: 0.0005808701389469206 \t\n",
      "Epoch 21313 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21314 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21315 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21316 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21317 \t\t Training Loss: 0.0005808700807392597 \t\n",
      "Epoch 21318 \t\t Training Loss: 0.0005808699643239379 \t\n",
      "Epoch 21319 \t\t Training Loss: 0.0005808699643239379 \t\n",
      "Epoch 21320 \t\t Training Loss: 0.0005808699643239379 \t\n",
      "Epoch 21321 \t\t Training Loss: 0.0005808699643239379 \t\n",
      "Epoch 21322 \t\t Training Loss: 0.0005808699643239379 \t\n",
      "Epoch 21323 \t\t Training Loss: 0.0005808699643239379 \t\n",
      "Epoch 21324 \t\t Training Loss: 0.0005808699643239379 \t\n",
      "Epoch 21325 \t\t Training Loss: 0.0005808699643239379 \t\n",
      "Epoch 21326 \t\t Training Loss: 0.0005808699643239379 \t\n",
      "Epoch 21327 \t\t Training Loss: 0.0005808699643239379 \t\n",
      "Epoch 21328 \t\t Training Loss: 0.0005808699643239379 \t\n",
      "Epoch 21329 \t\t Training Loss: 0.0005808699643239379 \t\n",
      "Epoch 21330 \t\t Training Loss: 0.0005808699643239379 \t\n",
      "Epoch 21331 \t\t Training Loss: 0.0005808699643239379 \t\n",
      "Epoch 21332 \t\t Training Loss: 0.0005808699643239379 \t\n",
      "Epoch 21333 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21334 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21335 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21336 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21337 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21338 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21339 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21340 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21341 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21342 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21343 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21344 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21345 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21346 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21347 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21348 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21349 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21350 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21351 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21352 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21353 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21354 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21355 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21356 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21357 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21358 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21359 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21360 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21361 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21362 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21363 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21364 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21365 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21366 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21367 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21368 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21369 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21370 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21371 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21372 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21373 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21374 \t\t Training Loss: 0.0005808698479086161 \t\n",
      "Epoch 21375 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21376 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21377 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21378 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21379 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21380 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21381 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21382 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21383 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21384 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21385 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21386 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21387 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21388 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21389 \t\t Training Loss: 0.0005808697314932942 \t\n",
      "Epoch 21390 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21391 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21392 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21393 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21394 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21395 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21396 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21397 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21398 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21399 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21400 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21401 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21402 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21403 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21404 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21405 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21406 \t\t Training Loss: 0.0005808696732856333 \t\n",
      "Epoch 21407 \t\t Training Loss: 0.0005808695568703115 \t\n",
      "Epoch 21408 \t\t Training Loss: 0.0005808695568703115 \t\n",
      "Epoch 21409 \t\t Training Loss: 0.0005808695568703115 \t\n",
      "Epoch 21410 \t\t Training Loss: 0.0005808695568703115 \t\n",
      "Epoch 21411 \t\t Training Loss: 0.0005808695568703115 \t\n",
      "Epoch 21412 \t\t Training Loss: 0.0005808695568703115 \t\n",
      "Epoch 21413 \t\t Training Loss: 0.0005808695568703115 \t\n",
      "Epoch 21414 \t\t Training Loss: 0.0005808695568703115 \t\n",
      "Epoch 21415 \t\t Training Loss: 0.0005808695568703115 \t\n",
      "Epoch 21416 \t\t Training Loss: 0.0005808695568703115 \t\n",
      "Epoch 21417 \t\t Training Loss: 0.0005808695568703115 \t\n",
      "Epoch 21418 \t\t Training Loss: 0.0005808695568703115 \t\n",
      "Epoch 21419 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21420 \t\t Training Loss: 0.0005808695568703115 \t\n",
      "Epoch 21421 \t\t Training Loss: 0.0005808695568703115 \t\n",
      "Epoch 21422 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21423 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21424 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21425 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21426 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21427 \t\t Training Loss: 0.0005808695568703115 \t\n",
      "Epoch 21428 \t\t Training Loss: 0.0005808695568703115 \t\n",
      "Epoch 21429 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21430 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21431 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21432 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21433 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21434 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21435 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21436 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21437 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21438 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21439 \t\t Training Loss: 0.0005808694986626506 \t\n",
      "Epoch 21440 \t\t Training Loss: 0.0005808694404549897 \t\n",
      "Epoch 21441 \t\t Training Loss: 0.0005808694404549897 \t\n",
      "Epoch 21442 \t\t Training Loss: 0.0005808694404549897 \t\n",
      "Epoch 21443 \t\t Training Loss: 0.0005808694404549897 \t\n",
      "Epoch 21444 \t\t Training Loss: 0.0005808694404549897 \t\n",
      "Epoch 21445 \t\t Training Loss: 0.0005808694404549897 \t\n",
      "Epoch 21446 \t\t Training Loss: 0.0005808694404549897 \t\n",
      "Epoch 21447 \t\t Training Loss: 0.0005808694404549897 \t\n",
      "Epoch 21448 \t\t Training Loss: 0.0005808694404549897 \t\n",
      "Epoch 21449 \t\t Training Loss: 0.0005808694404549897 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21450 \t\t Training Loss: 0.0005808694404549897 \t\n",
      "Epoch 21451 \t\t Training Loss: 0.0005808693822473288 \t\n",
      "Epoch 21452 \t\t Training Loss: 0.0005808693822473288 \t\n",
      "Epoch 21453 \t\t Training Loss: 0.0005808693822473288 \t\n",
      "Epoch 21454 \t\t Training Loss: 0.0005808693822473288 \t\n",
      "Epoch 21455 \t\t Training Loss: 0.0005808693822473288 \t\n",
      "Epoch 21456 \t\t Training Loss: 0.0005808693822473288 \t\n",
      "Epoch 21457 \t\t Training Loss: 0.0005808693822473288 \t\n",
      "Epoch 21458 \t\t Training Loss: 0.0005808693822473288 \t\n",
      "Epoch 21459 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21460 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21461 \t\t Training Loss: 0.0005808693240396678 \t\n",
      "Epoch 21462 \t\t Training Loss: 0.0005808693240396678 \t\n",
      "Epoch 21463 \t\t Training Loss: 0.0005808693822473288 \t\n",
      "Epoch 21464 \t\t Training Loss: 0.0005808693822473288 \t\n",
      "Epoch 21465 \t\t Training Loss: 0.0005808693822473288 \t\n",
      "Epoch 21466 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21467 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21468 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21469 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21470 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21471 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21472 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21473 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21474 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21475 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21476 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21477 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21478 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21479 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21480 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21481 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21482 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21483 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21484 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21485 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21486 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21487 \t\t Training Loss: 0.0005808692658320069 \t\n",
      "Epoch 21488 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21489 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21490 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21491 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21492 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21493 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21494 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21495 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21496 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21497 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21498 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21499 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21500 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21501 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21502 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21503 \t\t Training Loss: 0.0005808691494166851 \t\n",
      "Epoch 21504 \t\t Training Loss: 0.0005808690912090242 \t\n",
      "Epoch 21505 \t\t Training Loss: 0.0005808690912090242 \t\n",
      "Epoch 21506 \t\t Training Loss: 0.0005808690912090242 \t\n",
      "Epoch 21507 \t\t Training Loss: 0.0005808690330013633 \t\n",
      "Epoch 21508 \t\t Training Loss: 0.0005808690912090242 \t\n",
      "Epoch 21509 \t\t Training Loss: 0.0005808690330013633 \t\n",
      "Epoch 21510 \t\t Training Loss: 0.0005808690330013633 \t\n",
      "Epoch 21511 \t\t Training Loss: 0.0005808690330013633 \t\n",
      "Epoch 21512 \t\t Training Loss: 0.0005808690912090242 \t\n",
      "Epoch 21513 \t\t Training Loss: 0.0005808690330013633 \t\n",
      "Epoch 21514 \t\t Training Loss: 0.0005808690330013633 \t\n",
      "Epoch 21515 \t\t Training Loss: 0.0005808690330013633 \t\n",
      "Epoch 21516 \t\t Training Loss: 0.0005808690330013633 \t\n",
      "Epoch 21517 \t\t Training Loss: 0.0005808690330013633 \t\n",
      "Epoch 21518 \t\t Training Loss: 0.0005808689747937024 \t\n",
      "Epoch 21519 \t\t Training Loss: 0.0005808689747937024 \t\n",
      "Epoch 21520 \t\t Training Loss: 0.0005808689747937024 \t\n",
      "Epoch 21521 \t\t Training Loss: 0.0005808689747937024 \t\n",
      "Epoch 21522 \t\t Training Loss: 0.0005808689747937024 \t\n",
      "Epoch 21523 \t\t Training Loss: 0.0005808689747937024 \t\n",
      "Epoch 21524 \t\t Training Loss: 0.0005808688583783805 \t\n",
      "Epoch 21525 \t\t Training Loss: 0.0005808688583783805 \t\n",
      "Epoch 21526 \t\t Training Loss: 0.0005808688583783805 \t\n",
      "Epoch 21527 \t\t Training Loss: 0.0005808688583783805 \t\n",
      "Epoch 21528 \t\t Training Loss: 0.0005808688583783805 \t\n",
      "Epoch 21529 \t\t Training Loss: 0.0005808688583783805 \t\n",
      "Epoch 21530 \t\t Training Loss: 0.0005808689747937024 \t\n",
      "Epoch 21531 \t\t Training Loss: 0.0005808688583783805 \t\n",
      "Epoch 21532 \t\t Training Loss: 0.0005808688583783805 \t\n",
      "Epoch 21533 \t\t Training Loss: 0.0005808688583783805 \t\n",
      "Epoch 21534 \t\t Training Loss: 0.0005808688583783805 \t\n",
      "Epoch 21535 \t\t Training Loss: 0.0005808688583783805 \t\n",
      "Epoch 21536 \t\t Training Loss: 0.0005808688583783805 \t\n",
      "Epoch 21537 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21538 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21539 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21540 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21541 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21542 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21543 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21544 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21545 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21546 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21547 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21548 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21549 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21550 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21551 \t\t Training Loss: 0.0005808688583783805 \t\n",
      "Epoch 21552 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21553 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21554 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21555 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21556 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21557 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21558 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21559 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21560 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21561 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21562 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21563 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21564 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21565 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21566 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21567 \t\t Training Loss: 0.0005808687419630587 \t\n",
      "Epoch 21568 \t\t Training Loss: 0.0005808687419630587 \t\n",
      "Epoch 21569 \t\t Training Loss: 0.0005808688001707196 \t\n",
      "Epoch 21570 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21571 \t\t Training Loss: 0.0005808687419630587 \t\n",
      "Epoch 21572 \t\t Training Loss: 0.0005808687419630587 \t\n",
      "Epoch 21573 \t\t Training Loss: 0.0005808687419630587 \t\n",
      "Epoch 21574 \t\t Training Loss: 0.0005808687419630587 \t\n",
      "Epoch 21575 \t\t Training Loss: 0.0005808687419630587 \t\n",
      "Epoch 21576 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21577 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21578 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21579 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21580 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21581 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21582 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21583 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21584 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21585 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21586 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21587 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21588 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21589 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21590 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21591 \t\t Training Loss: 0.0005808686837553978 \t\n",
      "Epoch 21592 \t\t Training Loss: 0.000580868567340076 \t\n",
      "Epoch 21593 \t\t Training Loss: 0.000580868567340076 \t\n",
      "Epoch 21594 \t\t Training Loss: 0.000580868567340076 \t\n",
      "Epoch 21595 \t\t Training Loss: 0.0005808685091324151 \t\n",
      "Epoch 21596 \t\t Training Loss: 0.000580868567340076 \t\n",
      "Epoch 21597 \t\t Training Loss: 0.0005808685091324151 \t\n",
      "Epoch 21598 \t\t Training Loss: 0.0005808685091324151 \t\n",
      "Epoch 21599 \t\t Training Loss: 0.0005808685091324151 \t\n",
      "Epoch 21600 \t\t Training Loss: 0.0005808685091324151 \t\n",
      "Epoch 21601 \t\t Training Loss: 0.0005808685091324151 \t\n",
      "Epoch 21602 \t\t Training Loss: 0.0005808685091324151 \t\n",
      "Epoch 21603 \t\t Training Loss: 0.0005808685091324151 \t\n",
      "Epoch 21604 \t\t Training Loss: 0.0005808685091324151 \t\n",
      "Epoch 21605 \t\t Training Loss: 0.0005808685091324151 \t\n",
      "Epoch 21606 \t\t Training Loss: 0.0005808685091324151 \t\n",
      "Epoch 21607 \t\t Training Loss: 0.0005808685091324151 \t\n",
      "Epoch 21608 \t\t Training Loss: 0.0005808685091324151 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21609 \t\t Training Loss: 0.0005808685091324151 \t\n",
      "Epoch 21610 \t\t Training Loss: 0.0005808685091324151 \t\n",
      "Epoch 21611 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21612 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21613 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21614 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21615 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21616 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21617 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21618 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21619 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21620 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21621 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21622 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21623 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21624 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21625 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21626 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21627 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21628 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21629 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21630 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21631 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21632 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21633 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21634 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21635 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21636 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21637 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21638 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21639 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21640 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21641 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21642 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21643 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21644 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21645 \t\t Training Loss: 0.0005808684509247541 \t\n",
      "Epoch 21646 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21647 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21648 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21649 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21650 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21651 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21652 \t\t Training Loss: 0.0005808683927170932 \t\n",
      "Epoch 21653 \t\t Training Loss: 0.0005808683345094323 \t\n",
      "Epoch 21654 \t\t Training Loss: 0.0005808683345094323 \t\n",
      "Epoch 21655 \t\t Training Loss: 0.0005808683345094323 \t\n",
      "Epoch 21656 \t\t Training Loss: 0.0005808683345094323 \t\n",
      "Epoch 21657 \t\t Training Loss: 0.0005808683345094323 \t\n",
      "Epoch 21658 \t\t Training Loss: 0.0005808683345094323 \t\n",
      "Epoch 21659 \t\t Training Loss: 0.0005808683345094323 \t\n",
      "Epoch 21660 \t\t Training Loss: 0.0005808683345094323 \t\n",
      "Epoch 21661 \t\t Training Loss: 0.0005808683345094323 \t\n",
      "Epoch 21662 \t\t Training Loss: 0.0005808683345094323 \t\n",
      "Epoch 21663 \t\t Training Loss: 0.0005808683345094323 \t\n",
      "Epoch 21664 \t\t Training Loss: 0.0005808682763017714 \t\n",
      "Epoch 21665 \t\t Training Loss: 0.0005808682763017714 \t\n",
      "Epoch 21666 \t\t Training Loss: 0.0005808683345094323 \t\n",
      "Epoch 21667 \t\t Training Loss: 0.0005808682763017714 \t\n",
      "Epoch 21668 \t\t Training Loss: 0.0005808682763017714 \t\n",
      "Epoch 21669 \t\t Training Loss: 0.0005808682763017714 \t\n",
      "Epoch 21670 \t\t Training Loss: 0.0005808682763017714 \t\n",
      "Epoch 21671 \t\t Training Loss: 0.0005808682763017714 \t\n",
      "Epoch 21672 \t\t Training Loss: 0.0005808682763017714 \t\n",
      "Epoch 21673 \t\t Training Loss: 0.0005808682763017714 \t\n",
      "Epoch 21674 \t\t Training Loss: 0.0005808682763017714 \t\n",
      "Epoch 21675 \t\t Training Loss: 0.0005808682180941105 \t\n",
      "Epoch 21676 \t\t Training Loss: 0.0005808682180941105 \t\n",
      "Epoch 21677 \t\t Training Loss: 0.0005808682180941105 \t\n",
      "Epoch 21678 \t\t Training Loss: 0.0005808682180941105 \t\n",
      "Epoch 21679 \t\t Training Loss: 0.0005808681598864496 \t\n",
      "Epoch 21680 \t\t Training Loss: 0.0005808681598864496 \t\n",
      "Epoch 21681 \t\t Training Loss: 0.0005808681598864496 \t\n",
      "Epoch 21682 \t\t Training Loss: 0.0005808681016787887 \t\n",
      "Epoch 21683 \t\t Training Loss: 0.0005808681598864496 \t\n",
      "Epoch 21684 \t\t Training Loss: 0.0005808681016787887 \t\n",
      "Epoch 21685 \t\t Training Loss: 0.0005808681016787887 \t\n",
      "Epoch 21686 \t\t Training Loss: 0.0005808681016787887 \t\n",
      "Epoch 21687 \t\t Training Loss: 0.0005808681016787887 \t\n",
      "Epoch 21688 \t\t Training Loss: 0.0005808681016787887 \t\n",
      "Epoch 21689 \t\t Training Loss: 0.0005808681016787887 \t\n",
      "Epoch 21690 \t\t Training Loss: 0.0005808681016787887 \t\n",
      "Epoch 21691 \t\t Training Loss: 0.0005808681016787887 \t\n",
      "Epoch 21692 \t\t Training Loss: 0.0005808681016787887 \t\n",
      "Epoch 21693 \t\t Training Loss: 0.0005808681016787887 \t\n",
      "Epoch 21694 \t\t Training Loss: 0.0005808681016787887 \t\n",
      "Epoch 21695 \t\t Training Loss: 0.0005808681016787887 \t\n",
      "Epoch 21696 \t\t Training Loss: 0.0005808681016787887 \t\n",
      "Epoch 21697 \t\t Training Loss: 0.0005808680434711277 \t\n",
      "Epoch 21698 \t\t Training Loss: 0.0005808680434711277 \t\n",
      "Epoch 21699 \t\t Training Loss: 0.0005808680434711277 \t\n",
      "Epoch 21700 \t\t Training Loss: 0.0005808680434711277 \t\n",
      "Epoch 21701 \t\t Training Loss: 0.0005808680434711277 \t\n",
      "Epoch 21702 \t\t Training Loss: 0.0005808680434711277 \t\n",
      "Epoch 21703 \t\t Training Loss: 0.0005808680434711277 \t\n",
      "Epoch 21704 \t\t Training Loss: 0.0005808680434711277 \t\n",
      "Epoch 21705 \t\t Training Loss: 0.0005808680434711277 \t\n",
      "Epoch 21706 \t\t Training Loss: 0.0005808680434711277 \t\n",
      "Epoch 21707 \t\t Training Loss: 0.0005808680434711277 \t\n",
      "Epoch 21708 \t\t Training Loss: 0.0005808680434711277 \t\n",
      "Epoch 21709 \t\t Training Loss: 0.0005808680434711277 \t\n",
      "Epoch 21710 \t\t Training Loss: 0.0005808679852634668 \t\n",
      "Epoch 21711 \t\t Training Loss: 0.0005808679852634668 \t\n",
      "Epoch 21712 \t\t Training Loss: 0.0005808679270558059 \t\n",
      "Epoch 21713 \t\t Training Loss: 0.0005808679270558059 \t\n",
      "Epoch 21714 \t\t Training Loss: 0.0005808679270558059 \t\n",
      "Epoch 21715 \t\t Training Loss: 0.0005808679270558059 \t\n",
      "Epoch 21716 \t\t Training Loss: 0.0005808679270558059 \t\n",
      "Epoch 21717 \t\t Training Loss: 0.0005808679852634668 \t\n",
      "Epoch 21718 \t\t Training Loss: 0.0005808679852634668 \t\n",
      "Epoch 21719 \t\t Training Loss: 0.000580867868848145 \t\n",
      "Epoch 21720 \t\t Training Loss: 0.000580867868848145 \t\n",
      "Epoch 21721 \t\t Training Loss: 0.000580867868848145 \t\n",
      "Epoch 21722 \t\t Training Loss: 0.000580867868848145 \t\n",
      "Epoch 21723 \t\t Training Loss: 0.000580867868848145 \t\n",
      "Epoch 21724 \t\t Training Loss: 0.000580867868848145 \t\n",
      "Epoch 21725 \t\t Training Loss: 0.000580867868848145 \t\n",
      "Epoch 21726 \t\t Training Loss: 0.000580867868848145 \t\n",
      "Epoch 21727 \t\t Training Loss: 0.000580867868848145 \t\n",
      "Epoch 21728 \t\t Training Loss: 0.000580867868848145 \t\n",
      "Epoch 21729 \t\t Training Loss: 0.000580867868848145 \t\n",
      "Epoch 21730 \t\t Training Loss: 0.000580867868848145 \t\n",
      "Epoch 21731 \t\t Training Loss: 0.000580867868848145 \t\n",
      "Epoch 21732 \t\t Training Loss: 0.000580867868848145 \t\n",
      "Epoch 21733 \t\t Training Loss: 0.0005808678106404841 \t\n",
      "Epoch 21734 \t\t Training Loss: 0.0005808678106404841 \t\n",
      "Epoch 21735 \t\t Training Loss: 0.0005808678106404841 \t\n",
      "Epoch 21736 \t\t Training Loss: 0.0005808678106404841 \t\n",
      "Epoch 21737 \t\t Training Loss: 0.0005808678106404841 \t\n",
      "Epoch 21738 \t\t Training Loss: 0.0005808678106404841 \t\n",
      "Epoch 21739 \t\t Training Loss: 0.0005808678106404841 \t\n",
      "Epoch 21740 \t\t Training Loss: 0.0005808678106404841 \t\n",
      "Epoch 21741 \t\t Training Loss: 0.0005808678106404841 \t\n",
      "Epoch 21742 \t\t Training Loss: 0.0005808678106404841 \t\n",
      "Epoch 21743 \t\t Training Loss: 0.0005808678106404841 \t\n",
      "Epoch 21744 \t\t Training Loss: 0.0005808678106404841 \t\n",
      "Epoch 21745 \t\t Training Loss: 0.0005808677524328232 \t\n",
      "Epoch 21746 \t\t Training Loss: 0.0005808677524328232 \t\n",
      "Epoch 21747 \t\t Training Loss: 0.0005808677524328232 \t\n",
      "Epoch 21748 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21749 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21750 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21751 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21752 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21753 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21754 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21755 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21756 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21757 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21758 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21759 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21760 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21761 \t\t Training Loss: 0.0005808676360175014 \t\n",
      "Epoch 21762 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21763 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21764 \t\t Training Loss: 0.0005808676360175014 \t\n",
      "Epoch 21765 \t\t Training Loss: 0.0005808676360175014 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21766 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21767 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21768 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21769 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21770 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21771 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21772 \t\t Training Loss: 0.0005808676942251623 \t\n",
      "Epoch 21773 \t\t Training Loss: 0.0005808675196021795 \t\n",
      "Epoch 21774 \t\t Training Loss: 0.0005808676360175014 \t\n",
      "Epoch 21775 \t\t Training Loss: 0.0005808675196021795 \t\n",
      "Epoch 21776 \t\t Training Loss: 0.0005808674613945186 \t\n",
      "Epoch 21777 \t\t Training Loss: 0.0005808674613945186 \t\n",
      "Epoch 21778 \t\t Training Loss: 0.0005808674613945186 \t\n",
      "Epoch 21779 \t\t Training Loss: 0.0005808675196021795 \t\n",
      "Epoch 21780 \t\t Training Loss: 0.0005808675196021795 \t\n",
      "Epoch 21781 \t\t Training Loss: 0.0005808675196021795 \t\n",
      "Epoch 21782 \t\t Training Loss: 0.0005808674613945186 \t\n",
      "Epoch 21783 \t\t Training Loss: 0.0005808674613945186 \t\n",
      "Epoch 21784 \t\t Training Loss: 0.0005808674613945186 \t\n",
      "Epoch 21785 \t\t Training Loss: 0.0005808674613945186 \t\n",
      "Epoch 21786 \t\t Training Loss: 0.0005808674613945186 \t\n",
      "Epoch 21787 \t\t Training Loss: 0.0005808674613945186 \t\n",
      "Epoch 21788 \t\t Training Loss: 0.0005808674613945186 \t\n",
      "Epoch 21789 \t\t Training Loss: 0.0005808674613945186 \t\n",
      "Epoch 21790 \t\t Training Loss: 0.0005808674613945186 \t\n",
      "Epoch 21791 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21792 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21793 \t\t Training Loss: 0.0005808674613945186 \t\n",
      "Epoch 21794 \t\t Training Loss: 0.0005808674613945186 \t\n",
      "Epoch 21795 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21796 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21797 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21798 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21799 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21800 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21801 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21802 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21803 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21804 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21805 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21806 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21807 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21808 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21809 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21810 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21811 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21812 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21813 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21814 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21815 \t\t Training Loss: 0.0005808672867715359 \t\n",
      "Epoch 21816 \t\t Training Loss: 0.0005808672867715359 \t\n",
      "Epoch 21817 \t\t Training Loss: 0.0005808674031868577 \t\n",
      "Epoch 21818 \t\t Training Loss: 0.0005808673449791968 \t\n",
      "Epoch 21819 \t\t Training Loss: 0.0005808673449791968 \t\n",
      "Epoch 21820 \t\t Training Loss: 0.0005808673449791968 \t\n",
      "Epoch 21821 \t\t Training Loss: 0.0005808673449791968 \t\n",
      "Epoch 21822 \t\t Training Loss: 0.0005808672867715359 \t\n",
      "Epoch 21823 \t\t Training Loss: 0.0005808672867715359 \t\n",
      "Epoch 21824 \t\t Training Loss: 0.0005808672867715359 \t\n",
      "Epoch 21825 \t\t Training Loss: 0.0005808672867715359 \t\n",
      "Epoch 21826 \t\t Training Loss: 0.0005808672867715359 \t\n",
      "Epoch 21827 \t\t Training Loss: 0.0005808672867715359 \t\n",
      "Epoch 21828 \t\t Training Loss: 0.0005808672867715359 \t\n",
      "Epoch 21829 \t\t Training Loss: 0.0005808672867715359 \t\n",
      "Epoch 21830 \t\t Training Loss: 0.0005808672867715359 \t\n",
      "Epoch 21831 \t\t Training Loss: 0.0005808672867715359 \t\n",
      "Epoch 21832 \t\t Training Loss: 0.0005808672867715359 \t\n",
      "Epoch 21833 \t\t Training Loss: 0.0005808672867715359 \t\n",
      "Epoch 21834 \t\t Training Loss: 0.0005808672867715359 \t\n",
      "Epoch 21835 \t\t Training Loss: 0.000580867228563875 \t\n",
      "Epoch 21836 \t\t Training Loss: 0.000580867228563875 \t\n",
      "Epoch 21837 \t\t Training Loss: 0.000580867228563875 \t\n",
      "Epoch 21838 \t\t Training Loss: 0.000580867228563875 \t\n",
      "Epoch 21839 \t\t Training Loss: 0.000580867228563875 \t\n",
      "Epoch 21840 \t\t Training Loss: 0.000580867228563875 \t\n",
      "Epoch 21841 \t\t Training Loss: 0.000580867170356214 \t\n",
      "Epoch 21842 \t\t Training Loss: 0.000580867170356214 \t\n",
      "Epoch 21843 \t\t Training Loss: 0.000580867170356214 \t\n",
      "Epoch 21844 \t\t Training Loss: 0.000580867170356214 \t\n",
      "Epoch 21845 \t\t Training Loss: 0.000580867170356214 \t\n",
      "Epoch 21846 \t\t Training Loss: 0.000580867170356214 \t\n",
      "Epoch 21847 \t\t Training Loss: 0.000580867170356214 \t\n",
      "Epoch 21848 \t\t Training Loss: 0.000580867170356214 \t\n",
      "Epoch 21849 \t\t Training Loss: 0.000580867170356214 \t\n",
      "Epoch 21850 \t\t Training Loss: 0.0005808671121485531 \t\n",
      "Epoch 21851 \t\t Training Loss: 0.000580867170356214 \t\n",
      "Epoch 21852 \t\t Training Loss: 0.000580867170356214 \t\n",
      "Epoch 21853 \t\t Training Loss: 0.000580867170356214 \t\n",
      "Epoch 21854 \t\t Training Loss: 0.000580867170356214 \t\n",
      "Epoch 21855 \t\t Training Loss: 0.0005808671121485531 \t\n",
      "Epoch 21856 \t\t Training Loss: 0.000580867170356214 \t\n",
      "Epoch 21857 \t\t Training Loss: 0.0005808670539408922 \t\n",
      "Epoch 21858 \t\t Training Loss: 0.0005808671121485531 \t\n",
      "Epoch 21859 \t\t Training Loss: 0.0005808670539408922 \t\n",
      "Epoch 21860 \t\t Training Loss: 0.0005808670539408922 \t\n",
      "Epoch 21861 \t\t Training Loss: 0.0005808669957332313 \t\n",
      "Epoch 21862 \t\t Training Loss: 0.0005808669957332313 \t\n",
      "Epoch 21863 \t\t Training Loss: 0.0005808670539408922 \t\n",
      "Epoch 21864 \t\t Training Loss: 0.0005808669957332313 \t\n",
      "Epoch 21865 \t\t Training Loss: 0.0005808670539408922 \t\n",
      "Epoch 21866 \t\t Training Loss: 0.0005808670539408922 \t\n",
      "Epoch 21867 \t\t Training Loss: 0.0005808669957332313 \t\n",
      "Epoch 21868 \t\t Training Loss: 0.0005808669957332313 \t\n",
      "Epoch 21869 \t\t Training Loss: 0.0005808669375255704 \t\n",
      "Epoch 21870 \t\t Training Loss: 0.0005808669375255704 \t\n",
      "Epoch 21871 \t\t Training Loss: 0.0005808669375255704 \t\n",
      "Epoch 21872 \t\t Training Loss: 0.0005808669375255704 \t\n",
      "Epoch 21873 \t\t Training Loss: 0.0005808669375255704 \t\n",
      "Epoch 21874 \t\t Training Loss: 0.0005808669375255704 \t\n",
      "Epoch 21875 \t\t Training Loss: 0.0005808669375255704 \t\n",
      "Epoch 21876 \t\t Training Loss: 0.0005808669375255704 \t\n",
      "Epoch 21877 \t\t Training Loss: 0.0005808668793179095 \t\n",
      "Epoch 21878 \t\t Training Loss: 0.0005808668793179095 \t\n",
      "Epoch 21879 \t\t Training Loss: 0.0005808668793179095 \t\n",
      "Epoch 21880 \t\t Training Loss: 0.0005808668793179095 \t\n",
      "Epoch 21881 \t\t Training Loss: 0.0005808668793179095 \t\n",
      "Epoch 21882 \t\t Training Loss: 0.0005808668793179095 \t\n",
      "Epoch 21883 \t\t Training Loss: 0.0005808668793179095 \t\n",
      "Epoch 21884 \t\t Training Loss: 0.0005808668793179095 \t\n",
      "Epoch 21885 \t\t Training Loss: 0.0005808668793179095 \t\n",
      "Epoch 21886 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21887 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21888 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21889 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21890 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21891 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21892 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21893 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21894 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21895 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21896 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21897 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21898 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21899 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21900 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21901 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21902 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21903 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21904 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21905 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21906 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21907 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21908 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21909 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21910 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21911 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21912 \t\t Training Loss: 0.0005808668211102486 \t\n",
      "Epoch 21913 \t\t Training Loss: 0.0005808667629025877 \t\n",
      "Epoch 21914 \t\t Training Loss: 0.0005808667629025877 \t\n",
      "Epoch 21915 \t\t Training Loss: 0.0005808667629025877 \t\n",
      "Epoch 21916 \t\t Training Loss: 0.0005808666464872658 \t\n",
      "Epoch 21917 \t\t Training Loss: 0.0005808666464872658 \t\n",
      "Epoch 21918 \t\t Training Loss: 0.0005808666464872658 \t\n",
      "Epoch 21919 \t\t Training Loss: 0.0005808666464872658 \t\n",
      "Epoch 21920 \t\t Training Loss: 0.0005808666464872658 \t\n",
      "Epoch 21921 \t\t Training Loss: 0.0005808666464872658 \t\n",
      "Epoch 21922 \t\t Training Loss: 0.0005808666464872658 \t\n",
      "Epoch 21923 \t\t Training Loss: 0.0005808666464872658 \t\n",
      "Epoch 21924 \t\t Training Loss: 0.0005808666464872658 \t\n",
      "Epoch 21925 \t\t Training Loss: 0.0005808666464872658 \t\n",
      "Epoch 21926 \t\t Training Loss: 0.0005808666464872658 \t\n",
      "Epoch 21927 \t\t Training Loss: 0.0005808666464872658 \t\n",
      "Epoch 21928 \t\t Training Loss: 0.0005808666464872658 \t\n",
      "Epoch 21929 \t\t Training Loss: 0.0005808666464872658 \t\n",
      "Epoch 21930 \t\t Training Loss: 0.0005808666464872658 \t\n",
      "Epoch 21931 \t\t Training Loss: 0.0005808666464872658 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21932 \t\t Training Loss: 0.0005808665882796049 \t\n",
      "Epoch 21933 \t\t Training Loss: 0.0005808665882796049 \t\n",
      "Epoch 21934 \t\t Training Loss: 0.0005808665882796049 \t\n",
      "Epoch 21935 \t\t Training Loss: 0.0005808665882796049 \t\n",
      "Epoch 21936 \t\t Training Loss: 0.0005808665882796049 \t\n",
      "Epoch 21937 \t\t Training Loss: 0.0005808665882796049 \t\n",
      "Epoch 21938 \t\t Training Loss: 0.0005808665882796049 \t\n",
      "Epoch 21939 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21940 \t\t Training Loss: 0.000580866530071944 \t\n",
      "Epoch 21941 \t\t Training Loss: 0.000580866530071944 \t\n",
      "Epoch 21942 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21943 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21944 \t\t Training Loss: 0.0005808665882796049 \t\n",
      "Epoch 21945 \t\t Training Loss: 0.0005808665882796049 \t\n",
      "Epoch 21946 \t\t Training Loss: 0.000580866530071944 \t\n",
      "Epoch 21947 \t\t Training Loss: 0.0005808665882796049 \t\n",
      "Epoch 21948 \t\t Training Loss: 0.0005808665882796049 \t\n",
      "Epoch 21949 \t\t Training Loss: 0.0005808665882796049 \t\n",
      "Epoch 21950 \t\t Training Loss: 0.0005808665882796049 \t\n",
      "Epoch 21951 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21952 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21953 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21954 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21955 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21956 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21957 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21958 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21959 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21960 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21961 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21962 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21963 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21964 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21965 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21966 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21967 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21968 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21969 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 21970 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 21971 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 21972 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 21973 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 21974 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21975 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 21976 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 21977 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21978 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21979 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21980 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21981 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21982 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 21983 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 21984 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 21985 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21986 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21987 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21988 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21989 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21990 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21991 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21992 \t\t Training Loss: 0.0005808664718642831 \t\n",
      "Epoch 21993 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 21994 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 21995 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 21996 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 21997 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 21998 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 21999 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 22000 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 22001 \t\t Training Loss: 0.0005808664136566222 \t\n",
      "Epoch 22002 \t\t Training Loss: 0.0005808662390336394 \t\n",
      "Epoch 22003 \t\t Training Loss: 0.0005808662390336394 \t\n",
      "Epoch 22004 \t\t Training Loss: 0.0005808662390336394 \t\n",
      "Epoch 22005 \t\t Training Loss: 0.0005808662390336394 \t\n",
      "Epoch 22006 \t\t Training Loss: 0.0005808662390336394 \t\n",
      "Epoch 22007 \t\t Training Loss: 0.0005808662390336394 \t\n",
      "Epoch 22008 \t\t Training Loss: 0.0005808661808259785 \t\n",
      "Epoch 22009 \t\t Training Loss: 0.0005808661808259785 \t\n",
      "Epoch 22010 \t\t Training Loss: 0.0005808661808259785 \t\n",
      "Epoch 22011 \t\t Training Loss: 0.0005808661808259785 \t\n",
      "Epoch 22012 \t\t Training Loss: 0.0005808661808259785 \t\n",
      "Epoch 22013 \t\t Training Loss: 0.0005808661808259785 \t\n",
      "Epoch 22014 \t\t Training Loss: 0.0005808661808259785 \t\n",
      "Epoch 22015 \t\t Training Loss: 0.0005808661808259785 \t\n",
      "Epoch 22016 \t\t Training Loss: 0.0005808661808259785 \t\n",
      "Epoch 22017 \t\t Training Loss: 0.0005808661808259785 \t\n",
      "Epoch 22018 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22019 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22020 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22021 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22022 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22023 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22024 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22025 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22026 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22027 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22028 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22029 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22030 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22031 \t\t Training Loss: 0.0005808660644106567 \t\n",
      "Epoch 22032 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22033 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22034 \t\t Training Loss: 0.0005808660644106567 \t\n",
      "Epoch 22035 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22036 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22037 \t\t Training Loss: 0.0005808661226183176 \t\n",
      "Epoch 22038 \t\t Training Loss: 0.0005808660644106567 \t\n",
      "Epoch 22039 \t\t Training Loss: 0.0005808660644106567 \t\n",
      "Epoch 22040 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22041 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22042 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22043 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22044 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22045 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22046 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22047 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22048 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22049 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22050 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22051 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22052 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22053 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22054 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22055 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22056 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22057 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22058 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22059 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22060 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22061 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22062 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22063 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22064 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22065 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22066 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22067 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22068 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22069 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22070 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22071 \t\t Training Loss: 0.0005808660062029958 \t\n",
      "Epoch 22072 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22073 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22074 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22075 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22076 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22077 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22078 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22079 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22080 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22081 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22082 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22083 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22084 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22085 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22086 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22087 \t\t Training Loss: 0.000580865889787674 \t\n",
      "Epoch 22088 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22089 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22090 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22091 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22092 \t\t Training Loss: 0.000580865831580013 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22093 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22094 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22095 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22096 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22097 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22098 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22099 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22100 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22101 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22102 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22103 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22104 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22105 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22106 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22107 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22108 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22109 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22110 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22111 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22112 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22113 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22114 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22115 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22116 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22117 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22118 \t\t Training Loss: 0.000580865831580013 \t\n",
      "Epoch 22119 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22120 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22121 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22122 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22123 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22124 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22125 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22126 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22127 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22128 \t\t Training Loss: 0.0005808657733723521 \t\n",
      "Epoch 22129 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22130 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22131 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22132 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22133 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22134 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22135 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22136 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22137 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22138 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22139 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22140 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22141 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22142 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22143 \t\t Training Loss: 0.0005808657151646912 \t\n",
      "Epoch 22144 \t\t Training Loss: 0.0005808655987493694 \t\n",
      "Epoch 22145 \t\t Training Loss: 0.0005808655987493694 \t\n",
      "Epoch 22146 \t\t Training Loss: 0.0005808655987493694 \t\n",
      "Epoch 22147 \t\t Training Loss: 0.0005808655987493694 \t\n",
      "Epoch 22148 \t\t Training Loss: 0.0005808655987493694 \t\n",
      "Epoch 22149 \t\t Training Loss: 0.0005808655405417085 \t\n",
      "Epoch 22150 \t\t Training Loss: 0.0005808655405417085 \t\n",
      "Epoch 22151 \t\t Training Loss: 0.0005808655405417085 \t\n",
      "Epoch 22152 \t\t Training Loss: 0.0005808655405417085 \t\n",
      "Epoch 22153 \t\t Training Loss: 0.0005808655405417085 \t\n",
      "Epoch 22154 \t\t Training Loss: 0.0005808655405417085 \t\n",
      "Epoch 22155 \t\t Training Loss: 0.0005808655405417085 \t\n",
      "Epoch 22156 \t\t Training Loss: 0.0005808655405417085 \t\n",
      "Epoch 22157 \t\t Training Loss: 0.0005808655405417085 \t\n",
      "Epoch 22158 \t\t Training Loss: 0.0005808655405417085 \t\n",
      "Epoch 22159 \t\t Training Loss: 0.0005808655405417085 \t\n",
      "Epoch 22160 \t\t Training Loss: 0.0005808655405417085 \t\n",
      "Epoch 22161 \t\t Training Loss: 0.0005808655405417085 \t\n",
      "Epoch 22162 \t\t Training Loss: 0.0005808655405417085 \t\n",
      "Epoch 22163 \t\t Training Loss: 0.0005808655405417085 \t\n",
      "Epoch 22164 \t\t Training Loss: 0.0005808655405417085 \t\n",
      "Epoch 22165 \t\t Training Loss: 0.0005808654823340476 \t\n",
      "Epoch 22166 \t\t Training Loss: 0.0005808654823340476 \t\n",
      "Epoch 22167 \t\t Training Loss: 0.0005808654823340476 \t\n",
      "Epoch 22168 \t\t Training Loss: 0.0005808654823340476 \t\n",
      "Epoch 22169 \t\t Training Loss: 0.0005808654823340476 \t\n",
      "Epoch 22170 \t\t Training Loss: 0.0005808654823340476 \t\n",
      "Epoch 22171 \t\t Training Loss: 0.0005808654823340476 \t\n",
      "Epoch 22172 \t\t Training Loss: 0.0005808654823340476 \t\n",
      "Epoch 22173 \t\t Training Loss: 0.0005808654823340476 \t\n",
      "Epoch 22174 \t\t Training Loss: 0.0005808654823340476 \t\n",
      "Epoch 22175 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22176 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22177 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22178 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22179 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22180 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22181 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22182 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22183 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22184 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22185 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22186 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22187 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22188 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22189 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22190 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22191 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22192 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22193 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22194 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22195 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22196 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22197 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22198 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22199 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22200 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22201 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22202 \t\t Training Loss: 0.0005808654241263866 \t\n",
      "Epoch 22203 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22204 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22205 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22206 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22207 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22208 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22209 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22210 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22211 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22212 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22213 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22214 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22215 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22216 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22217 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22218 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22219 \t\t Training Loss: 0.000580865191295743 \t\n",
      "Epoch 22220 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22221 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22222 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22223 \t\t Training Loss: 0.000580865191295743 \t\n",
      "Epoch 22224 \t\t Training Loss: 0.0005808653077110648 \t\n",
      "Epoch 22225 \t\t Training Loss: 0.000580865191295743 \t\n",
      "Epoch 22226 \t\t Training Loss: 0.000580865191295743 \t\n",
      "Epoch 22227 \t\t Training Loss: 0.000580865191295743 \t\n",
      "Epoch 22228 \t\t Training Loss: 0.000580865191295743 \t\n",
      "Epoch 22229 \t\t Training Loss: 0.000580865191295743 \t\n",
      "Epoch 22230 \t\t Training Loss: 0.000580865191295743 \t\n",
      "Epoch 22231 \t\t Training Loss: 0.000580865191295743 \t\n",
      "Epoch 22232 \t\t Training Loss: 0.000580865191295743 \t\n",
      "Epoch 22233 \t\t Training Loss: 0.000580865191295743 \t\n",
      "Epoch 22234 \t\t Training Loss: 0.000580865191295743 \t\n",
      "Epoch 22235 \t\t Training Loss: 0.000580865191295743 \t\n",
      "Epoch 22236 \t\t Training Loss: 0.000580865191295743 \t\n",
      "Epoch 22237 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22238 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22239 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22240 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22241 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22242 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22243 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22244 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22245 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22246 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22247 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22248 \t\t Training Loss: 0.0005808651330880821 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22249 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22250 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22251 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22252 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22253 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22254 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22255 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22256 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22257 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22258 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22259 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22260 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22261 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22262 \t\t Training Loss: 0.0005808650166727602 \t\n",
      "Epoch 22263 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22264 \t\t Training Loss: 0.0005808650166727602 \t\n",
      "Epoch 22265 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22266 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22267 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22268 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22269 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22270 \t\t Training Loss: 0.0005808651330880821 \t\n",
      "Epoch 22271 \t\t Training Loss: 0.0005808650166727602 \t\n",
      "Epoch 22272 \t\t Training Loss: 0.0005808650166727602 \t\n",
      "Epoch 22273 \t\t Training Loss: 0.0005808650166727602 \t\n",
      "Epoch 22274 \t\t Training Loss: 0.0005808650166727602 \t\n",
      "Epoch 22275 \t\t Training Loss: 0.0005808650166727602 \t\n",
      "Epoch 22276 \t\t Training Loss: 0.0005808650166727602 \t\n",
      "Epoch 22277 \t\t Training Loss: 0.0005808650166727602 \t\n",
      "Epoch 22278 \t\t Training Loss: 0.0005808650166727602 \t\n",
      "Epoch 22279 \t\t Training Loss: 0.0005808650166727602 \t\n",
      "Epoch 22280 \t\t Training Loss: 0.0005808650166727602 \t\n",
      "Epoch 22281 \t\t Training Loss: 0.0005808650166727602 \t\n",
      "Epoch 22282 \t\t Training Loss: 0.0005808650166727602 \t\n",
      "Epoch 22283 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22284 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22285 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22286 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22287 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22288 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22289 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22290 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22291 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22292 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22293 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22294 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22295 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22296 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22297 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22298 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22299 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22300 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22301 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22302 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22303 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22304 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22305 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22306 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22307 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22308 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22309 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22310 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22311 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22312 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22313 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22314 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22315 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22316 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22317 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22318 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22319 \t\t Training Loss: 0.0005808649002574384 \t\n",
      "Epoch 22320 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22321 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22322 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22323 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22324 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22325 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22326 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22327 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22328 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22329 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22330 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22331 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22332 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22333 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22334 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22335 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22336 \t\t Training Loss: 0.0005808647256344557 \t\n",
      "Epoch 22337 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22338 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22339 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22340 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22341 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22342 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22343 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22344 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22345 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22346 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22347 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22348 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22349 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22350 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22351 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22352 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22353 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22354 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22355 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22356 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22357 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22358 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22359 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22360 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22361 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22362 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22363 \t\t Training Loss: 0.000580864492803812 \t\n",
      "Epoch 22364 \t\t Training Loss: 0.0005808646092191339 \t\n",
      "Epoch 22365 \t\t Training Loss: 0.000580864492803812 \t\n",
      "Epoch 22366 \t\t Training Loss: 0.000580864492803812 \t\n",
      "Epoch 22367 \t\t Training Loss: 0.000580864492803812 \t\n",
      "Epoch 22368 \t\t Training Loss: 0.000580864492803812 \t\n",
      "Epoch 22369 \t\t Training Loss: 0.000580864492803812 \t\n",
      "Epoch 22370 \t\t Training Loss: 0.0005808644345961511 \t\n",
      "Epoch 22371 \t\t Training Loss: 0.0005808644345961511 \t\n",
      "Epoch 22372 \t\t Training Loss: 0.0005808644345961511 \t\n",
      "Epoch 22373 \t\t Training Loss: 0.0005808644345961511 \t\n",
      "Epoch 22374 \t\t Training Loss: 0.0005808644345961511 \t\n",
      "Epoch 22375 \t\t Training Loss: 0.0005808644345961511 \t\n",
      "Epoch 22376 \t\t Training Loss: 0.0005808644345961511 \t\n",
      "Epoch 22377 \t\t Training Loss: 0.0005808644345961511 \t\n",
      "Epoch 22378 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22379 \t\t Training Loss: 0.0005808644345961511 \t\n",
      "Epoch 22380 \t\t Training Loss: 0.0005808644345961511 \t\n",
      "Epoch 22381 \t\t Training Loss: 0.0005808644345961511 \t\n",
      "Epoch 22382 \t\t Training Loss: 0.0005808644345961511 \t\n",
      "Epoch 22383 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22384 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22385 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22386 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22387 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22388 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22389 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22390 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22391 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22392 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22393 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22394 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22395 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22396 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22397 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22398 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22399 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22400 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22401 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22402 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22403 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22404 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22405 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22406 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22407 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22408 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22409 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22410 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22411 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22412 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22413 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22414 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22415 \t\t Training Loss: 0.0005808643181808293 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22416 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22417 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22418 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22419 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22420 \t\t Training Loss: 0.0005808643181808293 \t\n",
      "Epoch 22421 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22422 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22423 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22424 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22425 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22426 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22427 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22428 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22429 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22430 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22431 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22432 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22433 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22434 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22435 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22436 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22437 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22438 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22439 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22440 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22441 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22442 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22443 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22444 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22445 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22446 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22447 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22448 \t\t Training Loss: 0.0005808642017655075 \t\n",
      "Epoch 22449 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22450 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22451 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22452 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22453 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22454 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22455 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22456 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22457 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22458 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22459 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22460 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22461 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22462 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22463 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22464 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22465 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22466 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22467 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22468 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22469 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22470 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22471 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22472 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22473 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22474 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22475 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22476 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22477 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22478 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22479 \t\t Training Loss: 0.0005808640271425247 \t\n",
      "Epoch 22480 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22481 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22482 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22483 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22484 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22485 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22486 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22487 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22488 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22489 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22490 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22491 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22492 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22493 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22494 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22495 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22496 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22497 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22498 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22499 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22500 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22501 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22502 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22503 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22504 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22505 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22506 \t\t Training Loss: 0.0005808639107272029 \t\n",
      "Epoch 22507 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22508 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22509 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22510 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22511 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22512 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22513 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22514 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22515 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22516 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22517 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22518 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22519 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22520 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22521 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22522 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22523 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22524 \t\t Training Loss: 0.0005808637943118811 \t\n",
      "Epoch 22525 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22526 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22527 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22528 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22529 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22530 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22531 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22532 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22533 \t\t Training Loss: 0.000580863852519542 \t\n",
      "Epoch 22534 \t\t Training Loss: 0.0005808636196888983 \t\n",
      "Epoch 22535 \t\t Training Loss: 0.0005808637361042202 \t\n",
      "Epoch 22536 \t\t Training Loss: 0.0005808637361042202 \t\n",
      "Epoch 22537 \t\t Training Loss: 0.0005808636196888983 \t\n",
      "Epoch 22538 \t\t Training Loss: 0.0005808636196888983 \t\n",
      "Epoch 22539 \t\t Training Loss: 0.0005808636196888983 \t\n",
      "Epoch 22540 \t\t Training Loss: 0.0005808636196888983 \t\n",
      "Epoch 22541 \t\t Training Loss: 0.0005808636196888983 \t\n",
      "Epoch 22542 \t\t Training Loss: 0.0005808636196888983 \t\n",
      "Epoch 22543 \t\t Training Loss: 0.0005808636196888983 \t\n",
      "Epoch 22544 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22545 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22546 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22547 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22548 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22549 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22550 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22551 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22552 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22553 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22554 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22555 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22556 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22557 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22558 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22559 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22560 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22561 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22562 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22563 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22564 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22565 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22566 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22567 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22568 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22569 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22570 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22571 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22572 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22573 \t\t Training Loss: 0.0005808635614812374 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22574 \t\t Training Loss: 0.0005808635614812374 \t\n",
      "Epoch 22575 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22576 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22577 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22578 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22579 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22580 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22581 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22582 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22583 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22584 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22585 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22586 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22587 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22588 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22589 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22590 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22591 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22592 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22593 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22594 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22595 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22596 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22597 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22598 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22599 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22600 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22601 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22602 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22603 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22604 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22605 \t\t Training Loss: 0.0005808634450659156 \t\n",
      "Epoch 22606 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22607 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22608 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22609 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22610 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22611 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22612 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22613 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22614 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22615 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22616 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22617 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22618 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22619 \t\t Training Loss: 0.0005808633286505938 \t\n",
      "Epoch 22620 \t\t Training Loss: 0.0005808632704429328 \t\n",
      "Epoch 22621 \t\t Training Loss: 0.0005808632704429328 \t\n",
      "Epoch 22622 \t\t Training Loss: 0.0005808632704429328 \t\n",
      "Epoch 22623 \t\t Training Loss: 0.0005808632704429328 \t\n",
      "Epoch 22624 \t\t Training Loss: 0.0005808632704429328 \t\n",
      "Epoch 22625 \t\t Training Loss: 0.0005808632704429328 \t\n",
      "Epoch 22626 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22627 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22628 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22629 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22630 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22631 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22632 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22633 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22634 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22635 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22636 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22637 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22638 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22639 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22640 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22641 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22642 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22643 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22644 \t\t Training Loss: 0.000580863154027611 \t\n",
      "Epoch 22645 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22646 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22647 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22648 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22649 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22650 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22651 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22652 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22653 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22654 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22655 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22656 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22657 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22658 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22659 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22660 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22661 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22662 \t\t Training Loss: 0.0005808630376122892 \t\n",
      "Epoch 22663 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22664 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22665 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22666 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22667 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22668 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22669 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22670 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22671 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22672 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22673 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22674 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22675 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22676 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22677 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22678 \t\t Training Loss: 0.0005808629211969674 \t\n",
      "Epoch 22679 \t\t Training Loss: 0.0005808628629893064 \t\n",
      "Epoch 22680 \t\t Training Loss: 0.0005808628629893064 \t\n",
      "Epoch 22681 \t\t Training Loss: 0.0005808628629893064 \t\n",
      "Epoch 22682 \t\t Training Loss: 0.0005808628629893064 \t\n",
      "Epoch 22683 \t\t Training Loss: 0.0005808628047816455 \t\n",
      "Epoch 22684 \t\t Training Loss: 0.0005808628047816455 \t\n",
      "Epoch 22685 \t\t Training Loss: 0.0005808628047816455 \t\n",
      "Epoch 22686 \t\t Training Loss: 0.0005808628047816455 \t\n",
      "Epoch 22687 \t\t Training Loss: 0.0005808628047816455 \t\n",
      "Epoch 22688 \t\t Training Loss: 0.0005808628047816455 \t\n",
      "Epoch 22689 \t\t Training Loss: 0.0005808628047816455 \t\n",
      "Epoch 22690 \t\t Training Loss: 0.0005808628047816455 \t\n",
      "Epoch 22691 \t\t Training Loss: 0.0005808628047816455 \t\n",
      "Epoch 22692 \t\t Training Loss: 0.0005808628047816455 \t\n",
      "Epoch 22693 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22694 \t\t Training Loss: 0.0005808627465739846 \t\n",
      "Epoch 22695 \t\t Training Loss: 0.0005808628047816455 \t\n",
      "Epoch 22696 \t\t Training Loss: 0.0005808627465739846 \t\n",
      "Epoch 22697 \t\t Training Loss: 0.0005808628047816455 \t\n",
      "Epoch 22698 \t\t Training Loss: 0.0005808627465739846 \t\n",
      "Epoch 22699 \t\t Training Loss: 0.0005808627465739846 \t\n",
      "Epoch 22700 \t\t Training Loss: 0.0005808627465739846 \t\n",
      "Epoch 22701 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22702 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22703 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22704 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22705 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22706 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22707 \t\t Training Loss: 0.0005808628047816455 \t\n",
      "Epoch 22708 \t\t Training Loss: 0.0005808628047816455 \t\n",
      "Epoch 22709 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22710 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22711 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22712 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22713 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22714 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22715 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22716 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22717 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22718 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22719 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22720 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22721 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22722 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22723 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22724 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22725 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22726 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22727 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22728 \t\t Training Loss: 0.0005808627465739846 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22729 \t\t Training Loss: 0.0005808627465739846 \t\n",
      "Epoch 22730 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22731 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22732 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22733 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22734 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22735 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22736 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22737 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22738 \t\t Training Loss: 0.0005808626301586628 \t\n",
      "Epoch 22739 \t\t Training Loss: 0.0005808626883663237 \t\n",
      "Epoch 22740 \t\t Training Loss: 0.0005808626301586628 \t\n",
      "Epoch 22741 \t\t Training Loss: 0.0005808626301586628 \t\n",
      "Epoch 22742 \t\t Training Loss: 0.0005808625719510019 \t\n",
      "Epoch 22743 \t\t Training Loss: 0.0005808625719510019 \t\n",
      "Epoch 22744 \t\t Training Loss: 0.0005808626301586628 \t\n",
      "Epoch 22745 \t\t Training Loss: 0.0005808625719510019 \t\n",
      "Epoch 22746 \t\t Training Loss: 0.0005808625719510019 \t\n",
      "Epoch 22747 \t\t Training Loss: 0.0005808625719510019 \t\n",
      "Epoch 22748 \t\t Training Loss: 0.0005808625719510019 \t\n",
      "Epoch 22749 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22750 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22751 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22752 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22753 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22754 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22755 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22756 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22757 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22758 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22759 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22760 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22761 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22762 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22763 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22764 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22765 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22766 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22767 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22768 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22769 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22770 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22771 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22772 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22773 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22774 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22775 \t\t Training Loss: 0.000580862513743341 \t\n",
      "Epoch 22776 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22777 \t\t Training Loss: 0.0005808624555356801 \t\n",
      "Epoch 22778 \t\t Training Loss: 0.0005808624555356801 \t\n",
      "Epoch 22779 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22780 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22781 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22782 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22783 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22784 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22785 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22786 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22787 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22788 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22789 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22790 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22791 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22792 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22793 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22794 \t\t Training Loss: 0.0005808623391203582 \t\n",
      "Epoch 22795 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22796 \t\t Training Loss: 0.0005808623973280191 \t\n",
      "Epoch 22797 \t\t Training Loss: 0.0005808623391203582 \t\n",
      "Epoch 22798 \t\t Training Loss: 0.0005808623391203582 \t\n",
      "Epoch 22799 \t\t Training Loss: 0.0005808623391203582 \t\n",
      "Epoch 22800 \t\t Training Loss: 0.0005808623391203582 \t\n",
      "Epoch 22801 \t\t Training Loss: 0.0005808623391203582 \t\n",
      "Epoch 22802 \t\t Training Loss: 0.0005808622809126973 \t\n",
      "Epoch 22803 \t\t Training Loss: 0.0005808622809126973 \t\n",
      "Epoch 22804 \t\t Training Loss: 0.0005808623391203582 \t\n",
      "Epoch 22805 \t\t Training Loss: 0.0005808623391203582 \t\n",
      "Epoch 22806 \t\t Training Loss: 0.0005808623391203582 \t\n",
      "Epoch 22807 \t\t Training Loss: 0.0005808623391203582 \t\n",
      "Epoch 22808 \t\t Training Loss: 0.0005808623391203582 \t\n",
      "Epoch 22809 \t\t Training Loss: 0.0005808622809126973 \t\n",
      "Epoch 22810 \t\t Training Loss: 0.0005808622809126973 \t\n",
      "Epoch 22811 \t\t Training Loss: 0.0005808623391203582 \t\n",
      "Epoch 22812 \t\t Training Loss: 0.0005808623391203582 \t\n",
      "Epoch 22813 \t\t Training Loss: 0.0005808623391203582 \t\n",
      "Epoch 22814 \t\t Training Loss: 0.0005808622809126973 \t\n",
      "Epoch 22815 \t\t Training Loss: 0.0005808622809126973 \t\n",
      "Epoch 22816 \t\t Training Loss: 0.0005808622809126973 \t\n",
      "Epoch 22817 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22818 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22819 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22820 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22821 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22822 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22823 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22824 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22825 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22826 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22827 \t\t Training Loss: 0.0005808622809126973 \t\n",
      "Epoch 22828 \t\t Training Loss: 0.0005808622809126973 \t\n",
      "Epoch 22829 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22830 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22831 \t\t Training Loss: 0.0005808622809126973 \t\n",
      "Epoch 22832 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22833 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22834 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22835 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22836 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22837 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22838 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22839 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22840 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22841 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22842 \t\t Training Loss: 0.0005808622227050364 \t\n",
      "Epoch 22843 \t\t Training Loss: 0.0005808621062897146 \t\n",
      "Epoch 22844 \t\t Training Loss: 0.0005808621062897146 \t\n",
      "Epoch 22845 \t\t Training Loss: 0.0005808621062897146 \t\n",
      "Epoch 22846 \t\t Training Loss: 0.0005808621062897146 \t\n",
      "Epoch 22847 \t\t Training Loss: 0.0005808621062897146 \t\n",
      "Epoch 22848 \t\t Training Loss: 0.0005808621062897146 \t\n",
      "Epoch 22849 \t\t Training Loss: 0.0005808621062897146 \t\n",
      "Epoch 22850 \t\t Training Loss: 0.0005808621062897146 \t\n",
      "Epoch 22851 \t\t Training Loss: 0.0005808621062897146 \t\n",
      "Epoch 22852 \t\t Training Loss: 0.0005808621062897146 \t\n",
      "Epoch 22853 \t\t Training Loss: 0.0005808621062897146 \t\n",
      "Epoch 22854 \t\t Training Loss: 0.0005808621062897146 \t\n",
      "Epoch 22855 \t\t Training Loss: 0.0005808621062897146 \t\n",
      "Epoch 22856 \t\t Training Loss: 0.0005808621062897146 \t\n",
      "Epoch 22857 \t\t Training Loss: 0.0005808621062897146 \t\n",
      "Epoch 22858 \t\t Training Loss: 0.0005808621062897146 \t\n",
      "Epoch 22859 \t\t Training Loss: 0.0005808619898743927 \t\n",
      "Epoch 22860 \t\t Training Loss: 0.0005808619898743927 \t\n",
      "Epoch 22861 \t\t Training Loss: 0.0005808619898743927 \t\n",
      "Epoch 22862 \t\t Training Loss: 0.0005808619316667318 \t\n",
      "Epoch 22863 \t\t Training Loss: 0.0005808619898743927 \t\n",
      "Epoch 22864 \t\t Training Loss: 0.0005808619898743927 \t\n",
      "Epoch 22865 \t\t Training Loss: 0.0005808619898743927 \t\n",
      "Epoch 22866 \t\t Training Loss: 0.0005808619898743927 \t\n",
      "Epoch 22867 \t\t Training Loss: 0.0005808619316667318 \t\n",
      "Epoch 22868 \t\t Training Loss: 0.0005808619316667318 \t\n",
      "Epoch 22869 \t\t Training Loss: 0.0005808619316667318 \t\n",
      "Epoch 22870 \t\t Training Loss: 0.0005808619316667318 \t\n",
      "Epoch 22871 \t\t Training Loss: 0.0005808619316667318 \t\n",
      "Epoch 22872 \t\t Training Loss: 0.0005808619316667318 \t\n",
      "Epoch 22873 \t\t Training Loss: 0.0005808619316667318 \t\n",
      "Epoch 22874 \t\t Training Loss: 0.0005808619316667318 \t\n",
      "Epoch 22875 \t\t Training Loss: 0.0005808619316667318 \t\n",
      "Epoch 22876 \t\t Training Loss: 0.0005808619316667318 \t\n",
      "Epoch 22877 \t\t Training Loss: 0.0005808618734590709 \t\n",
      "Epoch 22878 \t\t Training Loss: 0.0005808618734590709 \t\n",
      "Epoch 22879 \t\t Training Loss: 0.0005808618734590709 \t\n",
      "Epoch 22880 \t\t Training Loss: 0.0005808618734590709 \t\n",
      "Epoch 22881 \t\t Training Loss: 0.0005808618734590709 \t\n",
      "Epoch 22882 \t\t Training Loss: 0.0005808618734590709 \t\n",
      "Epoch 22883 \t\t Training Loss: 0.0005808618734590709 \t\n",
      "Epoch 22884 \t\t Training Loss: 0.0005808618734590709 \t\n",
      "Epoch 22885 \t\t Training Loss: 0.0005808618734590709 \t\n",
      "Epoch 22886 \t\t Training Loss: 0.0005808617570437491 \t\n",
      "Epoch 22887 \t\t Training Loss: 0.0005808617570437491 \t\n",
      "Epoch 22888 \t\t Training Loss: 0.0005808617570437491 \t\n",
      "Epoch 22889 \t\t Training Loss: 0.0005808617570437491 \t\n",
      "Epoch 22890 \t\t Training Loss: 0.0005808617570437491 \t\n",
      "Epoch 22891 \t\t Training Loss: 0.0005808617570437491 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22892 \t\t Training Loss: 0.0005808617570437491 \t\n",
      "Epoch 22893 \t\t Training Loss: 0.0005808617570437491 \t\n",
      "Epoch 22894 \t\t Training Loss: 0.0005808617570437491 \t\n",
      "Epoch 22895 \t\t Training Loss: 0.0005808617570437491 \t\n",
      "Epoch 22896 \t\t Training Loss: 0.0005808617570437491 \t\n",
      "Epoch 22897 \t\t Training Loss: 0.0005808617570437491 \t\n",
      "Epoch 22898 \t\t Training Loss: 0.0005808616988360882 \t\n",
      "Epoch 22899 \t\t Training Loss: 0.0005808616988360882 \t\n",
      "Epoch 22900 \t\t Training Loss: 0.0005808616988360882 \t\n",
      "Epoch 22901 \t\t Training Loss: 0.0005808616988360882 \t\n",
      "Epoch 22902 \t\t Training Loss: 0.0005808616988360882 \t\n",
      "Epoch 22903 \t\t Training Loss: 0.0005808616988360882 \t\n",
      "Epoch 22904 \t\t Training Loss: 0.0005808616988360882 \t\n",
      "Epoch 22905 \t\t Training Loss: 0.0005808616988360882 \t\n",
      "Epoch 22906 \t\t Training Loss: 0.0005808616406284273 \t\n",
      "Epoch 22907 \t\t Training Loss: 0.0005808616988360882 \t\n",
      "Epoch 22908 \t\t Training Loss: 0.0005808616406284273 \t\n",
      "Epoch 22909 \t\t Training Loss: 0.0005808616988360882 \t\n",
      "Epoch 22910 \t\t Training Loss: 0.0005808616406284273 \t\n",
      "Epoch 22911 \t\t Training Loss: 0.0005808616406284273 \t\n",
      "Epoch 22912 \t\t Training Loss: 0.0005808616406284273 \t\n",
      "Epoch 22913 \t\t Training Loss: 0.0005808616406284273 \t\n",
      "Epoch 22914 \t\t Training Loss: 0.0005808616406284273 \t\n",
      "Epoch 22915 \t\t Training Loss: 0.0005808615824207664 \t\n",
      "Epoch 22916 \t\t Training Loss: 0.0005808616406284273 \t\n",
      "Epoch 22917 \t\t Training Loss: 0.0005808616406284273 \t\n",
      "Epoch 22918 \t\t Training Loss: 0.0005808614660054445 \t\n",
      "Epoch 22919 \t\t Training Loss: 0.0005808615242131054 \t\n",
      "Epoch 22920 \t\t Training Loss: 0.0005808615242131054 \t\n",
      "Epoch 22921 \t\t Training Loss: 0.0005808615824207664 \t\n",
      "Epoch 22922 \t\t Training Loss: 0.0005808615242131054 \t\n",
      "Epoch 22923 \t\t Training Loss: 0.0005808615824207664 \t\n",
      "Epoch 22924 \t\t Training Loss: 0.0005808614660054445 \t\n",
      "Epoch 22925 \t\t Training Loss: 0.0005808614660054445 \t\n",
      "Epoch 22926 \t\t Training Loss: 0.0005808614660054445 \t\n",
      "Epoch 22927 \t\t Training Loss: 0.0005808614660054445 \t\n",
      "Epoch 22928 \t\t Training Loss: 0.0005808615242131054 \t\n",
      "Epoch 22929 \t\t Training Loss: 0.0005808614660054445 \t\n",
      "Epoch 22930 \t\t Training Loss: 0.0005808614660054445 \t\n",
      "Epoch 22931 \t\t Training Loss: 0.0005808614077977836 \t\n",
      "Epoch 22932 \t\t Training Loss: 0.0005808614660054445 \t\n",
      "Epoch 22933 \t\t Training Loss: 0.0005808614077977836 \t\n",
      "Epoch 22934 \t\t Training Loss: 0.0005808614077977836 \t\n",
      "Epoch 22935 \t\t Training Loss: 0.0005808613495901227 \t\n",
      "Epoch 22936 \t\t Training Loss: 0.0005808614077977836 \t\n",
      "Epoch 22937 \t\t Training Loss: 0.0005808613495901227 \t\n",
      "Epoch 22938 \t\t Training Loss: 0.0005808613495901227 \t\n",
      "Epoch 22939 \t\t Training Loss: 0.0005808613495901227 \t\n",
      "Epoch 22940 \t\t Training Loss: 0.0005808613495901227 \t\n",
      "Epoch 22941 \t\t Training Loss: 0.0005808613495901227 \t\n",
      "Epoch 22942 \t\t Training Loss: 0.0005808613495901227 \t\n",
      "Epoch 22943 \t\t Training Loss: 0.0005808613495901227 \t\n",
      "Epoch 22944 \t\t Training Loss: 0.0005808613495901227 \t\n",
      "Epoch 22945 \t\t Training Loss: 0.0005808613495901227 \t\n",
      "Epoch 22946 \t\t Training Loss: 0.0005808613495901227 \t\n",
      "Epoch 22947 \t\t Training Loss: 0.0005808613495901227 \t\n",
      "Epoch 22948 \t\t Training Loss: 0.0005808613495901227 \t\n",
      "Epoch 22949 \t\t Training Loss: 0.0005808613495901227 \t\n",
      "Epoch 22950 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22951 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22952 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22953 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22954 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22955 \t\t Training Loss: 0.0005808612331748009 \t\n",
      "Epoch 22956 \t\t Training Loss: 0.0005808612331748009 \t\n",
      "Epoch 22957 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22958 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22959 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22960 \t\t Training Loss: 0.0005808612331748009 \t\n",
      "Epoch 22961 \t\t Training Loss: 0.0005808612331748009 \t\n",
      "Epoch 22962 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22963 \t\t Training Loss: 0.0005808612331748009 \t\n",
      "Epoch 22964 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22965 \t\t Training Loss: 0.0005808612331748009 \t\n",
      "Epoch 22966 \t\t Training Loss: 0.0005808612331748009 \t\n",
      "Epoch 22967 \t\t Training Loss: 0.0005808612331748009 \t\n",
      "Epoch 22968 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22969 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22970 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22971 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22972 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22973 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22974 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22975 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22976 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22977 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22978 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22979 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22980 \t\t Training Loss: 0.0005808612913824618 \t\n",
      "Epoch 22981 \t\t Training Loss: 0.0005808612331748009 \t\n",
      "Epoch 22982 \t\t Training Loss: 0.0005808612331748009 \t\n",
      "Epoch 22983 \t\t Training Loss: 0.0005808612331748009 \t\n",
      "Epoch 22984 \t\t Training Loss: 0.00058086117496714 \t\n",
      "Epoch 22985 \t\t Training Loss: 0.00058086117496714 \t\n",
      "Epoch 22986 \t\t Training Loss: 0.00058086117496714 \t\n",
      "Epoch 22987 \t\t Training Loss: 0.00058086117496714 \t\n",
      "Epoch 22988 \t\t Training Loss: 0.000580861116759479 \t\n",
      "Epoch 22989 \t\t Training Loss: 0.0005808610585518181 \t\n",
      "Epoch 22990 \t\t Training Loss: 0.0005808610585518181 \t\n",
      "Epoch 22991 \t\t Training Loss: 0.0005808610585518181 \t\n",
      "Epoch 22992 \t\t Training Loss: 0.0005808610585518181 \t\n",
      "Epoch 22993 \t\t Training Loss: 0.0005808610585518181 \t\n",
      "Epoch 22994 \t\t Training Loss: 0.0005808610585518181 \t\n",
      "Epoch 22995 \t\t Training Loss: 0.0005808610585518181 \t\n",
      "Epoch 22996 \t\t Training Loss: 0.0005808610585518181 \t\n",
      "Epoch 22997 \t\t Training Loss: 0.0005808610585518181 \t\n",
      "Epoch 22998 \t\t Training Loss: 0.0005808610585518181 \t\n",
      "Epoch 22999 \t\t Training Loss: 0.0005808610585518181 \t\n",
      "Epoch 23000 \t\t Training Loss: 0.0005808610585518181 \t\n",
      "Epoch 23001 \t\t Training Loss: 0.0005808610585518181 \t\n",
      "Epoch 23002 \t\t Training Loss: 0.0005808610585518181 \t\n",
      "Epoch 23003 \t\t Training Loss: 0.0005808610003441572 \t\n",
      "Epoch 23004 \t\t Training Loss: 0.0005808610003441572 \t\n",
      "Epoch 23005 \t\t Training Loss: 0.0005808610003441572 \t\n",
      "Epoch 23006 \t\t Training Loss: 0.0005808610003441572 \t\n",
      "Epoch 23007 \t\t Training Loss: 0.0005808610003441572 \t\n",
      "Epoch 23008 \t\t Training Loss: 0.0005808610003441572 \t\n",
      "Epoch 23009 \t\t Training Loss: 0.0005808610003441572 \t\n",
      "Epoch 23010 \t\t Training Loss: 0.0005808610003441572 \t\n",
      "Epoch 23011 \t\t Training Loss: 0.0005808610003441572 \t\n",
      "Epoch 23012 \t\t Training Loss: 0.0005808610003441572 \t\n",
      "Epoch 23013 \t\t Training Loss: 0.0005808610003441572 \t\n",
      "Epoch 23014 \t\t Training Loss: 0.0005808610003441572 \t\n",
      "Epoch 23015 \t\t Training Loss: 0.0005808609421364963 \t\n",
      "Epoch 23016 \t\t Training Loss: 0.0005808609421364963 \t\n",
      "Epoch 23017 \t\t Training Loss: 0.0005808609421364963 \t\n",
      "Epoch 23018 \t\t Training Loss: 0.0005808609421364963 \t\n",
      "Epoch 23019 \t\t Training Loss: 0.0005808609421364963 \t\n",
      "Epoch 23020 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23021 \t\t Training Loss: 0.0005808609421364963 \t\n",
      "Epoch 23022 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23023 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23024 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23025 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23026 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23027 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23028 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23029 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23030 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23031 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23032 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23033 \t\t Training Loss: 0.0005808608257211745 \t\n",
      "Epoch 23034 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23035 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23036 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23037 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23038 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23039 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23040 \t\t Training Loss: 0.0005808608257211745 \t\n",
      "Epoch 23041 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23042 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23043 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23044 \t\t Training Loss: 0.0005808608839288354 \t\n",
      "Epoch 23045 \t\t Training Loss: 0.0005808608257211745 \t\n",
      "Epoch 23046 \t\t Training Loss: 0.0005808608257211745 \t\n",
      "Epoch 23047 \t\t Training Loss: 0.0005808608257211745 \t\n",
      "Epoch 23048 \t\t Training Loss: 0.0005808607675135136 \t\n",
      "Epoch 23049 \t\t Training Loss: 0.0005808607675135136 \t\n",
      "Epoch 23050 \t\t Training Loss: 0.0005808607675135136 \t\n",
      "Epoch 23051 \t\t Training Loss: 0.0005808607675135136 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23052 \t\t Training Loss: 0.0005808607675135136 \t\n",
      "Epoch 23053 \t\t Training Loss: 0.0005808607093058527 \t\n",
      "Epoch 23054 \t\t Training Loss: 0.0005808607675135136 \t\n",
      "Epoch 23055 \t\t Training Loss: 0.0005808607675135136 \t\n",
      "Epoch 23056 \t\t Training Loss: 0.0005808607675135136 \t\n",
      "Epoch 23057 \t\t Training Loss: 0.0005808607093058527 \t\n",
      "Epoch 23058 \t\t Training Loss: 0.0005808607093058527 \t\n",
      "Epoch 23059 \t\t Training Loss: 0.0005808607093058527 \t\n",
      "Epoch 23060 \t\t Training Loss: 0.0005808607093058527 \t\n",
      "Epoch 23061 \t\t Training Loss: 0.0005808607093058527 \t\n",
      "Epoch 23062 \t\t Training Loss: 0.0005808607093058527 \t\n",
      "Epoch 23063 \t\t Training Loss: 0.0005808607093058527 \t\n",
      "Epoch 23064 \t\t Training Loss: 0.0005808607093058527 \t\n",
      "Epoch 23065 \t\t Training Loss: 0.0005808607093058527 \t\n",
      "Epoch 23066 \t\t Training Loss: 0.0005808607093058527 \t\n",
      "Epoch 23067 \t\t Training Loss: 0.0005808607093058527 \t\n",
      "Epoch 23068 \t\t Training Loss: 0.0005808606510981917 \t\n",
      "Epoch 23069 \t\t Training Loss: 0.0005808606510981917 \t\n",
      "Epoch 23070 \t\t Training Loss: 0.0005808606510981917 \t\n",
      "Epoch 23071 \t\t Training Loss: 0.0005808606510981917 \t\n",
      "Epoch 23072 \t\t Training Loss: 0.0005808606510981917 \t\n",
      "Epoch 23073 \t\t Training Loss: 0.0005808606510981917 \t\n",
      "Epoch 23074 \t\t Training Loss: 0.0005808605928905308 \t\n",
      "Epoch 23075 \t\t Training Loss: 0.0005808605928905308 \t\n",
      "Epoch 23076 \t\t Training Loss: 0.0005808606510981917 \t\n",
      "Epoch 23077 \t\t Training Loss: 0.0005808606510981917 \t\n",
      "Epoch 23078 \t\t Training Loss: 0.0005808605928905308 \t\n",
      "Epoch 23079 \t\t Training Loss: 0.0005808605928905308 \t\n",
      "Epoch 23080 \t\t Training Loss: 0.0005808605928905308 \t\n",
      "Epoch 23081 \t\t Training Loss: 0.0005808605928905308 \t\n",
      "Epoch 23082 \t\t Training Loss: 0.0005808605928905308 \t\n",
      "Epoch 23083 \t\t Training Loss: 0.0005808605928905308 \t\n",
      "Epoch 23084 \t\t Training Loss: 0.0005808605928905308 \t\n",
      "Epoch 23085 \t\t Training Loss: 0.0005808605928905308 \t\n",
      "Epoch 23086 \t\t Training Loss: 0.0005808605928905308 \t\n",
      "Epoch 23087 \t\t Training Loss: 0.0005808605928905308 \t\n",
      "Epoch 23088 \t\t Training Loss: 0.0005808605928905308 \t\n",
      "Epoch 23089 \t\t Training Loss: 0.0005808605928905308 \t\n",
      "Epoch 23090 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23091 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23092 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23093 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23094 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23095 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23096 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23097 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23098 \t\t Training Loss: 0.0005808605928905308 \t\n",
      "Epoch 23099 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23100 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23101 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23102 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23103 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23104 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23105 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23106 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23107 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23108 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23109 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23110 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23111 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23112 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23113 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23114 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23115 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23116 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23117 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23118 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23119 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23120 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23121 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23122 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23123 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23124 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23125 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23126 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23127 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23128 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23129 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23130 \t\t Training Loss: 0.000580860476475209 \t\n",
      "Epoch 23131 \t\t Training Loss: 0.0005808603600598872 \t\n",
      "Epoch 23132 \t\t Training Loss: 0.0005808603600598872 \t\n",
      "Epoch 23133 \t\t Training Loss: 0.0005808603600598872 \t\n",
      "Epoch 23134 \t\t Training Loss: 0.0005808603600598872 \t\n",
      "Epoch 23135 \t\t Training Loss: 0.0005808603600598872 \t\n",
      "Epoch 23136 \t\t Training Loss: 0.0005808603600598872 \t\n",
      "Epoch 23137 \t\t Training Loss: 0.0005808603600598872 \t\n",
      "Epoch 23138 \t\t Training Loss: 0.0005808603018522263 \t\n",
      "Epoch 23139 \t\t Training Loss: 0.0005808603018522263 \t\n",
      "Epoch 23140 \t\t Training Loss: 0.0005808603018522263 \t\n",
      "Epoch 23141 \t\t Training Loss: 0.0005808603018522263 \t\n",
      "Epoch 23142 \t\t Training Loss: 0.0005808603018522263 \t\n",
      "Epoch 23143 \t\t Training Loss: 0.0005808603018522263 \t\n",
      "Epoch 23144 \t\t Training Loss: 0.0005808603018522263 \t\n",
      "Epoch 23145 \t\t Training Loss: 0.0005808603018522263 \t\n",
      "Epoch 23146 \t\t Training Loss: 0.0005808603018522263 \t\n",
      "Epoch 23147 \t\t Training Loss: 0.0005808603018522263 \t\n",
      "Epoch 23148 \t\t Training Loss: 0.0005808603018522263 \t\n",
      "Epoch 23149 \t\t Training Loss: 0.0005808603018522263 \t\n",
      "Epoch 23150 \t\t Training Loss: 0.0005808602436445653 \t\n",
      "Epoch 23151 \t\t Training Loss: 0.0005808602436445653 \t\n",
      "Epoch 23152 \t\t Training Loss: 0.0005808602436445653 \t\n",
      "Epoch 23153 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23154 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23155 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23156 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23157 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23158 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23159 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23160 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23161 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23162 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23163 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23164 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23165 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23166 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23167 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23168 \t\t Training Loss: 0.0005808602436445653 \t\n",
      "Epoch 23169 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23170 \t\t Training Loss: 0.0005808602436445653 \t\n",
      "Epoch 23171 \t\t Training Loss: 0.0005808602436445653 \t\n",
      "Epoch 23172 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23173 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23174 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23175 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23176 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23177 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23178 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23179 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23180 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23181 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23182 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23183 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23184 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23185 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23186 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23187 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23188 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23189 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23190 \t\t Training Loss: 0.0005808602436445653 \t\n",
      "Epoch 23191 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23192 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23193 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23194 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23195 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23196 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23197 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23198 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23199 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23200 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23201 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23202 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23203 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23204 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23205 \t\t Training Loss: 0.0005808601854369044 \t\n",
      "Epoch 23206 \t\t Training Loss: 0.0005808600690215826 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23207 \t\t Training Loss: 0.0005808600690215826 \t\n",
      "Epoch 23208 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23209 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23210 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23211 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23212 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23213 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23214 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23215 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23216 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23217 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23218 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23219 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23220 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23221 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23222 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23223 \t\t Training Loss: 0.0005808599526062608 \t\n",
      "Epoch 23224 \t\t Training Loss: 0.0005808599526062608 \t\n",
      "Epoch 23225 \t\t Training Loss: 0.0005808599526062608 \t\n",
      "Epoch 23226 \t\t Training Loss: 0.0005808599526062608 \t\n",
      "Epoch 23227 \t\t Training Loss: 0.0005808599526062608 \t\n",
      "Epoch 23228 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23229 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23230 \t\t Training Loss: 0.0005808598943985999 \t\n",
      "Epoch 23231 \t\t Training Loss: 0.0005808598943985999 \t\n",
      "Epoch 23232 \t\t Training Loss: 0.0005808598943985999 \t\n",
      "Epoch 23233 \t\t Training Loss: 0.0005808598943985999 \t\n",
      "Epoch 23234 \t\t Training Loss: 0.0005808598943985999 \t\n",
      "Epoch 23235 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23236 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23237 \t\t Training Loss: 0.0005808600108139217 \t\n",
      "Epoch 23238 \t\t Training Loss: 0.0005808598943985999 \t\n",
      "Epoch 23239 \t\t Training Loss: 0.0005808598943985999 \t\n",
      "Epoch 23240 \t\t Training Loss: 0.0005808598943985999 \t\n",
      "Epoch 23241 \t\t Training Loss: 0.0005808598943985999 \t\n",
      "Epoch 23242 \t\t Training Loss: 0.0005808598943985999 \t\n",
      "Epoch 23243 \t\t Training Loss: 0.0005808598943985999 \t\n",
      "Epoch 23244 \t\t Training Loss: 0.0005808598943985999 \t\n",
      "Epoch 23245 \t\t Training Loss: 0.000580859836190939 \t\n",
      "Epoch 23246 \t\t Training Loss: 0.0005808598943985999 \t\n",
      "Epoch 23247 \t\t Training Loss: 0.000580859836190939 \t\n",
      "Epoch 23248 \t\t Training Loss: 0.000580859836190939 \t\n",
      "Epoch 23249 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23250 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23251 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23252 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23253 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23254 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23255 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23256 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23257 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23258 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23259 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23260 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23261 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23262 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23263 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23264 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23265 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23266 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23267 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23268 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23269 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23270 \t\t Training Loss: 0.000580859777983278 \t\n",
      "Epoch 23271 \t\t Training Loss: 0.0005808596615679562 \t\n",
      "Epoch 23272 \t\t Training Loss: 0.0005808596615679562 \t\n",
      "Epoch 23273 \t\t Training Loss: 0.0005808596615679562 \t\n",
      "Epoch 23274 \t\t Training Loss: 0.0005808596615679562 \t\n",
      "Epoch 23275 \t\t Training Loss: 0.0005808596033602953 \t\n",
      "Epoch 23276 \t\t Training Loss: 0.0005808596033602953 \t\n",
      "Epoch 23277 \t\t Training Loss: 0.0005808596033602953 \t\n",
      "Epoch 23278 \t\t Training Loss: 0.0005808596615679562 \t\n",
      "Epoch 23279 \t\t Training Loss: 0.0005808596615679562 \t\n",
      "Epoch 23280 \t\t Training Loss: 0.0005808596615679562 \t\n",
      "Epoch 23281 \t\t Training Loss: 0.0005808596615679562 \t\n",
      "Epoch 23282 \t\t Training Loss: 0.0005808596615679562 \t\n",
      "Epoch 23283 \t\t Training Loss: 0.0005808596033602953 \t\n",
      "Epoch 23284 \t\t Training Loss: 0.0005808596033602953 \t\n",
      "Epoch 23285 \t\t Training Loss: 0.0005808596033602953 \t\n",
      "Epoch 23286 \t\t Training Loss: 0.0005808596033602953 \t\n",
      "Epoch 23287 \t\t Training Loss: 0.0005808596033602953 \t\n",
      "Epoch 23288 \t\t Training Loss: 0.0005808596033602953 \t\n",
      "Epoch 23289 \t\t Training Loss: 0.0005808596033602953 \t\n",
      "Epoch 23290 \t\t Training Loss: 0.0005808595451526344 \t\n",
      "Epoch 23291 \t\t Training Loss: 0.0005808595451526344 \t\n",
      "Epoch 23292 \t\t Training Loss: 0.0005808596033602953 \t\n",
      "Epoch 23293 \t\t Training Loss: 0.0005808595451526344 \t\n",
      "Epoch 23294 \t\t Training Loss: 0.0005808596033602953 \t\n",
      "Epoch 23295 \t\t Training Loss: 0.0005808595451526344 \t\n",
      "Epoch 23296 \t\t Training Loss: 0.0005808595451526344 \t\n",
      "Epoch 23297 \t\t Training Loss: 0.0005808595451526344 \t\n",
      "Epoch 23298 \t\t Training Loss: 0.0005808595451526344 \t\n",
      "Epoch 23299 \t\t Training Loss: 0.0005808595451526344 \t\n",
      "Epoch 23300 \t\t Training Loss: 0.0005808595451526344 \t\n",
      "Epoch 23301 \t\t Training Loss: 0.0005808595451526344 \t\n",
      "Epoch 23302 \t\t Training Loss: 0.0005808595451526344 \t\n",
      "Epoch 23303 \t\t Training Loss: 0.0005808595451526344 \t\n",
      "Epoch 23304 \t\t Training Loss: 0.0005808594869449735 \t\n",
      "Epoch 23305 \t\t Training Loss: 0.0005808594869449735 \t\n",
      "Epoch 23306 \t\t Training Loss: 0.0005808594869449735 \t\n",
      "Epoch 23307 \t\t Training Loss: 0.0005808594869449735 \t\n",
      "Epoch 23308 \t\t Training Loss: 0.0005808594869449735 \t\n",
      "Epoch 23309 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23310 \t\t Training Loss: 0.0005808594869449735 \t\n",
      "Epoch 23311 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23312 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23313 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23314 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23315 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23316 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23317 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23318 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23319 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23320 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23321 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23322 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23323 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23324 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23325 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23326 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23327 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23328 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23329 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23330 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23331 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23332 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23333 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23334 \t\t Training Loss: 0.0005808593705296516 \t\n",
      "Epoch 23335 \t\t Training Loss: 0.0005808592541143298 \t\n",
      "Epoch 23336 \t\t Training Loss: 0.0005808592541143298 \t\n",
      "Epoch 23337 \t\t Training Loss: 0.0005808592541143298 \t\n",
      "Epoch 23338 \t\t Training Loss: 0.0005808592541143298 \t\n",
      "Epoch 23339 \t\t Training Loss: 0.0005808592541143298 \t\n",
      "Epoch 23340 \t\t Training Loss: 0.0005808592541143298 \t\n",
      "Epoch 23341 \t\t Training Loss: 0.0005808592541143298 \t\n",
      "Epoch 23342 \t\t Training Loss: 0.0005808592541143298 \t\n",
      "Epoch 23343 \t\t Training Loss: 0.0005808592541143298 \t\n",
      "Epoch 23344 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23345 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23346 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23347 \t\t Training Loss: 0.0005808592541143298 \t\n",
      "Epoch 23348 \t\t Training Loss: 0.0005808592541143298 \t\n",
      "Epoch 23349 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23350 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23351 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23352 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23353 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23354 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23355 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23356 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23357 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23358 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23359 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23360 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23361 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23362 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23363 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23364 \t\t Training Loss: 0.0005808591959066689 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23365 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23366 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23367 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23368 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23369 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23370 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23371 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23372 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23373 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23374 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23375 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23376 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23377 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23378 \t\t Training Loss: 0.0005808591959066689 \t\n",
      "Epoch 23379 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23380 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23381 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23382 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23383 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23384 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23385 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23386 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23387 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23388 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23389 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23390 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23391 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23392 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23393 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23394 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23395 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23396 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23397 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23398 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23399 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23400 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23401 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23402 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23403 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23404 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23405 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23406 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23407 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23408 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23409 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23410 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23411 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23412 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23413 \t\t Training Loss: 0.0005808590794913471 \t\n",
      "Epoch 23414 \t\t Training Loss: 0.0005808589630760252 \t\n",
      "Epoch 23415 \t\t Training Loss: 0.0005808589630760252 \t\n",
      "Epoch 23416 \t\t Training Loss: 0.0005808589630760252 \t\n",
      "Epoch 23417 \t\t Training Loss: 0.0005808589630760252 \t\n",
      "Epoch 23418 \t\t Training Loss: 0.0005808589630760252 \t\n",
      "Epoch 23419 \t\t Training Loss: 0.0005808589630760252 \t\n",
      "Epoch 23420 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23421 \t\t Training Loss: 0.0005808589630760252 \t\n",
      "Epoch 23422 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23423 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23424 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23425 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23426 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23427 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23428 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23429 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23430 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23431 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23432 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23433 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23434 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23435 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23436 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23437 \t\t Training Loss: 0.0005808587884530425 \t\n",
      "Epoch 23438 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23439 \t\t Training Loss: 0.0005808587884530425 \t\n",
      "Epoch 23440 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23441 \t\t Training Loss: 0.0005808587884530425 \t\n",
      "Epoch 23442 \t\t Training Loss: 0.0005808589048683643 \t\n",
      "Epoch 23443 \t\t Training Loss: 0.0005808587884530425 \t\n",
      "Epoch 23444 \t\t Training Loss: 0.0005808587884530425 \t\n",
      "Epoch 23445 \t\t Training Loss: 0.0005808587884530425 \t\n",
      "Epoch 23446 \t\t Training Loss: 0.0005808587884530425 \t\n",
      "Epoch 23447 \t\t Training Loss: 0.0005808587884530425 \t\n",
      "Epoch 23448 \t\t Training Loss: 0.0005808587884530425 \t\n",
      "Epoch 23449 \t\t Training Loss: 0.0005808587884530425 \t\n",
      "Epoch 23450 \t\t Training Loss: 0.0005808587884530425 \t\n",
      "Epoch 23451 \t\t Training Loss: 0.0005808587884530425 \t\n",
      "Epoch 23452 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23453 \t\t Training Loss: 0.0005808587884530425 \t\n",
      "Epoch 23454 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23455 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23456 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23457 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23458 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23459 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23460 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23461 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23462 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23463 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23464 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23465 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23466 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23467 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23468 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23469 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23470 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23471 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23472 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23473 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23474 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23475 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23476 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23477 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23478 \t\t Training Loss: 0.0005808586720377207 \t\n",
      "Epoch 23479 \t\t Training Loss: 0.0005808586138300598 \t\n",
      "Epoch 23480 \t\t Training Loss: 0.0005808586138300598 \t\n",
      "Epoch 23481 \t\t Training Loss: 0.0005808586138300598 \t\n",
      "Epoch 23482 \t\t Training Loss: 0.0005808586138300598 \t\n",
      "Epoch 23483 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23484 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23485 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23486 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23487 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23488 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23489 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23490 \t\t Training Loss: 0.0005808586138300598 \t\n",
      "Epoch 23491 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23492 \t\t Training Loss: 0.0005808586138300598 \t\n",
      "Epoch 23493 \t\t Training Loss: 0.0005808586138300598 \t\n",
      "Epoch 23494 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23495 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23496 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23497 \t\t Training Loss: 0.0005808586138300598 \t\n",
      "Epoch 23498 \t\t Training Loss: 0.0005808586138300598 \t\n",
      "Epoch 23499 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23500 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23501 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23502 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23503 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23504 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23505 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23506 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23507 \t\t Training Loss: 0.0005808584974147379 \t\n",
      "Epoch 23508 \t\t Training Loss: 0.0005808583809994161 \t\n",
      "Epoch 23509 \t\t Training Loss: 0.0005808583809994161 \t\n",
      "Epoch 23510 \t\t Training Loss: 0.0005808583809994161 \t\n",
      "Epoch 23511 \t\t Training Loss: 0.0005808583809994161 \t\n",
      "Epoch 23512 \t\t Training Loss: 0.0005808583809994161 \t\n",
      "Epoch 23513 \t\t Training Loss: 0.0005808583809994161 \t\n",
      "Epoch 23514 \t\t Training Loss: 0.0005808583809994161 \t\n",
      "Epoch 23515 \t\t Training Loss: 0.0005808583809994161 \t\n",
      "Epoch 23516 \t\t Training Loss: 0.0005808583809994161 \t\n",
      "Epoch 23517 \t\t Training Loss: 0.0005808583809994161 \t\n",
      "Epoch 23518 \t\t Training Loss: 0.0005808583809994161 \t\n",
      "Epoch 23519 \t\t Training Loss: 0.0005808582645840943 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23520 \t\t Training Loss: 0.0005808583809994161 \t\n",
      "Epoch 23521 \t\t Training Loss: 0.0005808583809994161 \t\n",
      "Epoch 23522 \t\t Training Loss: 0.0005808583809994161 \t\n",
      "Epoch 23523 \t\t Training Loss: 0.0005808583809994161 \t\n",
      "Epoch 23524 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23525 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23526 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23527 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23528 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23529 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23530 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23531 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23532 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23533 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23534 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23535 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23536 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23537 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23538 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23539 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23540 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23541 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23542 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23543 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23544 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23545 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23546 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23547 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23548 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23549 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23550 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23551 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23552 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23553 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23554 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23555 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23556 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23557 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23558 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23559 \t\t Training Loss: 0.0005808582645840943 \t\n",
      "Epoch 23560 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23561 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23562 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23563 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23564 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23565 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23566 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23567 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23568 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23569 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23570 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23571 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23572 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23573 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23574 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23575 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23576 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23577 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23578 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23579 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23580 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23581 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23582 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23583 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23584 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23585 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23586 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23587 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23588 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23589 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23590 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23591 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23592 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23593 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23594 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23595 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23596 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23597 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23598 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23599 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23600 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23601 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23602 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23603 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23604 \t\t Training Loss: 0.0005808580899611115 \t\n",
      "Epoch 23605 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23606 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23607 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23608 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23609 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23610 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23611 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23612 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23613 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23614 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23615 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23616 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23617 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23618 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23619 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23620 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23621 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23622 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23623 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23624 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23625 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23626 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23627 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23628 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23629 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23630 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23631 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23632 \t\t Training Loss: 0.0005808579735457897 \t\n",
      "Epoch 23633 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23634 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23635 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23636 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23637 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23638 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23639 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23640 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23641 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23642 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23643 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23644 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23645 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23646 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23647 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23648 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23649 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23650 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23651 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23652 \t\t Training Loss: 0.000580857798922807 \t\n",
      "Epoch 23653 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23654 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23655 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23656 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23657 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23658 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23659 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23660 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23661 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23662 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23663 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23664 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23665 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23666 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23667 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23668 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23669 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23670 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23671 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23672 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23673 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23674 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23675 \t\t Training Loss: 0.0005808576825074852 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23676 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23677 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23678 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23679 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23680 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23681 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23682 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23683 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23684 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23685 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23686 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23687 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23688 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23689 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23690 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23691 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23692 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23693 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23694 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23695 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23696 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23697 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23698 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23699 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23700 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23701 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23702 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23703 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23704 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23705 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23706 \t\t Training Loss: 0.0005808576825074852 \t\n",
      "Epoch 23707 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23708 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23709 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23710 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23711 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23712 \t\t Training Loss: 0.0005808576242998242 \t\n",
      "Epoch 23713 \t\t Training Loss: 0.0005808575660921633 \t\n",
      "Epoch 23714 \t\t Training Loss: 0.0005808575660921633 \t\n",
      "Epoch 23715 \t\t Training Loss: 0.0005808575660921633 \t\n",
      "Epoch 23716 \t\t Training Loss: 0.0005808575660921633 \t\n",
      "Epoch 23717 \t\t Training Loss: 0.0005808575078845024 \t\n",
      "Epoch 23718 \t\t Training Loss: 0.0005808575078845024 \t\n",
      "Epoch 23719 \t\t Training Loss: 0.0005808575078845024 \t\n",
      "Epoch 23720 \t\t Training Loss: 0.0005808575078845024 \t\n",
      "Epoch 23721 \t\t Training Loss: 0.0005808575078845024 \t\n",
      "Epoch 23722 \t\t Training Loss: 0.0005808575078845024 \t\n",
      "Epoch 23723 \t\t Training Loss: 0.0005808575078845024 \t\n",
      "Epoch 23724 \t\t Training Loss: 0.0005808575078845024 \t\n",
      "Epoch 23725 \t\t Training Loss: 0.0005808575078845024 \t\n",
      "Epoch 23726 \t\t Training Loss: 0.0005808575078845024 \t\n",
      "Epoch 23727 \t\t Training Loss: 0.0005808575078845024 \t\n",
      "Epoch 23728 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23729 \t\t Training Loss: 0.0005808575078845024 \t\n",
      "Epoch 23730 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23731 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23732 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23733 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23734 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23735 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23736 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23737 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23738 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23739 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23740 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23741 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23742 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23743 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23744 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23745 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23746 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23747 \t\t Training Loss: 0.0005808573914691806 \t\n",
      "Epoch 23748 \t\t Training Loss: 0.0005808572750538588 \t\n",
      "Epoch 23749 \t\t Training Loss: 0.0005808572750538588 \t\n",
      "Epoch 23750 \t\t Training Loss: 0.0005808572750538588 \t\n",
      "Epoch 23751 \t\t Training Loss: 0.0005808572750538588 \t\n",
      "Epoch 23752 \t\t Training Loss: 0.0005808572168461978 \t\n",
      "Epoch 23753 \t\t Training Loss: 0.0005808572750538588 \t\n",
      "Epoch 23754 \t\t Training Loss: 0.0005808572750538588 \t\n",
      "Epoch 23755 \t\t Training Loss: 0.0005808572168461978 \t\n",
      "Epoch 23756 \t\t Training Loss: 0.0005808572168461978 \t\n",
      "Epoch 23757 \t\t Training Loss: 0.0005808572168461978 \t\n",
      "Epoch 23758 \t\t Training Loss: 0.0005808572168461978 \t\n",
      "Epoch 23759 \t\t Training Loss: 0.0005808572168461978 \t\n",
      "Epoch 23760 \t\t Training Loss: 0.0005808571586385369 \t\n",
      "Epoch 23761 \t\t Training Loss: 0.0005808571586385369 \t\n",
      "Epoch 23762 \t\t Training Loss: 0.0005808571586385369 \t\n",
      "Epoch 23763 \t\t Training Loss: 0.0005808571586385369 \t\n",
      "Epoch 23764 \t\t Training Loss: 0.0005808571586385369 \t\n",
      "Epoch 23765 \t\t Training Loss: 0.0005808571586385369 \t\n",
      "Epoch 23766 \t\t Training Loss: 0.000580857100430876 \t\n",
      "Epoch 23767 \t\t Training Loss: 0.000580857100430876 \t\n",
      "Epoch 23768 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23769 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23770 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23771 \t\t Training Loss: 0.000580857100430876 \t\n",
      "Epoch 23772 \t\t Training Loss: 0.000580857100430876 \t\n",
      "Epoch 23773 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23774 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23775 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23776 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23777 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23778 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23779 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23780 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23781 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23782 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23783 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23784 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23785 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23786 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23787 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23788 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23789 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23790 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23791 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23792 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23793 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23794 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23795 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23796 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23797 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23798 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23799 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23800 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23801 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23802 \t\t Training Loss: 0.0005808570422232151 \t\n",
      "Epoch 23803 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23804 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23805 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23806 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23807 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23808 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23809 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23810 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23811 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23812 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23813 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23814 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23815 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23816 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23817 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23818 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23819 \t\t Training Loss: 0.0005808569840155542 \t\n",
      "Epoch 23820 \t\t Training Loss: 0.0005808569258078933 \t\n",
      "Epoch 23821 \t\t Training Loss: 0.0005808569258078933 \t\n",
      "Epoch 23822 \t\t Training Loss: 0.0005808569258078933 \t\n",
      "Epoch 23823 \t\t Training Loss: 0.0005808569258078933 \t\n",
      "Epoch 23824 \t\t Training Loss: 0.0005808569258078933 \t\n",
      "Epoch 23825 \t\t Training Loss: 0.0005808569258078933 \t\n",
      "Epoch 23826 \t\t Training Loss: 0.0005808569258078933 \t\n",
      "Epoch 23827 \t\t Training Loss: 0.0005808568093925714 \t\n",
      "Epoch 23828 \t\t Training Loss: 0.0005808568093925714 \t\n",
      "Epoch 23829 \t\t Training Loss: 0.0005808568093925714 \t\n",
      "Epoch 23830 \t\t Training Loss: 0.0005808568093925714 \t\n",
      "Epoch 23831 \t\t Training Loss: 0.0005808568093925714 \t\n",
      "Epoch 23832 \t\t Training Loss: 0.0005808568093925714 \t\n",
      "Epoch 23833 \t\t Training Loss: 0.0005808568093925714 \t\n",
      "Epoch 23834 \t\t Training Loss: 0.0005808568093925714 \t\n",
      "Epoch 23835 \t\t Training Loss: 0.0005808568093925714 \t\n",
      "Epoch 23836 \t\t Training Loss: 0.0005808568093925714 \t\n",
      "Epoch 23837 \t\t Training Loss: 0.0005808568093925714 \t\n",
      "Epoch 23838 \t\t Training Loss: 0.0005808568093925714 \t\n",
      "Epoch 23839 \t\t Training Loss: 0.0005808568093925714 \t\n",
      "Epoch 23840 \t\t Training Loss: 0.0005808568093925714 \t\n",
      "Epoch 23841 \t\t Training Loss: 0.0005808568093925714 \t\n",
      "Epoch 23842 \t\t Training Loss: 0.0005808566929772496 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23843 \t\t Training Loss: 0.0005808566929772496 \t\n",
      "Epoch 23844 \t\t Training Loss: 0.0005808566929772496 \t\n",
      "Epoch 23845 \t\t Training Loss: 0.0005808566929772496 \t\n",
      "Epoch 23846 \t\t Training Loss: 0.0005808566929772496 \t\n",
      "Epoch 23847 \t\t Training Loss: 0.0005808566929772496 \t\n",
      "Epoch 23848 \t\t Training Loss: 0.0005808566929772496 \t\n",
      "Epoch 23849 \t\t Training Loss: 0.0005808566929772496 \t\n",
      "Epoch 23850 \t\t Training Loss: 0.0005808566347695887 \t\n",
      "Epoch 23851 \t\t Training Loss: 0.0005808566347695887 \t\n",
      "Epoch 23852 \t\t Training Loss: 0.0005808566347695887 \t\n",
      "Epoch 23853 \t\t Training Loss: 0.0005808566347695887 \t\n",
      "Epoch 23854 \t\t Training Loss: 0.0005808566347695887 \t\n",
      "Epoch 23855 \t\t Training Loss: 0.0005808566347695887 \t\n",
      "Epoch 23856 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23857 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23858 \t\t Training Loss: 0.0005808565183542669 \t\n",
      "Epoch 23859 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23860 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23861 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23862 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23863 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23864 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23865 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23866 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23867 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23868 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23869 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23870 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23871 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23872 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23873 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23874 \t\t Training Loss: 0.0005808565183542669 \t\n",
      "Epoch 23875 \t\t Training Loss: 0.0005808565765619278 \t\n",
      "Epoch 23876 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23877 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23878 \t\t Training Loss: 0.0005808565183542669 \t\n",
      "Epoch 23879 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23880 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23881 \t\t Training Loss: 0.0005808565183542669 \t\n",
      "Epoch 23882 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23883 \t\t Training Loss: 0.0005808565183542669 \t\n",
      "Epoch 23884 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23885 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23886 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23887 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23888 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23889 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23890 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23891 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23892 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23893 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23894 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23895 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23896 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23897 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23898 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23899 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23900 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23901 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23902 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23903 \t\t Training Loss: 0.0005808564019389451 \t\n",
      "Epoch 23904 \t\t Training Loss: 0.0005808564019389451 \t\n",
      "Epoch 23905 \t\t Training Loss: 0.0005808563437312841 \t\n",
      "Epoch 23906 \t\t Training Loss: 0.0005808564019389451 \t\n",
      "Epoch 23907 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23908 \t\t Training Loss: 0.0005808564019389451 \t\n",
      "Epoch 23909 \t\t Training Loss: 0.0005808564019389451 \t\n",
      "Epoch 23910 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23911 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23912 \t\t Training Loss: 0.000580856460146606 \t\n",
      "Epoch 23913 \t\t Training Loss: 0.0005808564019389451 \t\n",
      "Epoch 23914 \t\t Training Loss: 0.0005808563437312841 \t\n",
      "Epoch 23915 \t\t Training Loss: 0.0005808563437312841 \t\n",
      "Epoch 23916 \t\t Training Loss: 0.0005808563437312841 \t\n",
      "Epoch 23917 \t\t Training Loss: 0.0005808563437312841 \t\n",
      "Epoch 23918 \t\t Training Loss: 0.0005808564019389451 \t\n",
      "Epoch 23919 \t\t Training Loss: 0.0005808563437312841 \t\n",
      "Epoch 23920 \t\t Training Loss: 0.0005808563437312841 \t\n",
      "Epoch 23921 \t\t Training Loss: 0.0005808563437312841 \t\n",
      "Epoch 23922 \t\t Training Loss: 0.0005808564019389451 \t\n",
      "Epoch 23923 \t\t Training Loss: 0.0005808563437312841 \t\n",
      "Epoch 23924 \t\t Training Loss: 0.0005808563437312841 \t\n",
      "Epoch 23925 \t\t Training Loss: 0.0005808563437312841 \t\n",
      "Epoch 23926 \t\t Training Loss: 0.0005808563437312841 \t\n",
      "Epoch 23927 \t\t Training Loss: 0.0005808563437312841 \t\n",
      "Epoch 23928 \t\t Training Loss: 0.0005808563437312841 \t\n",
      "Epoch 23929 \t\t Training Loss: 0.0005808563437312841 \t\n",
      "Epoch 23930 \t\t Training Loss: 0.0005808562273159623 \t\n",
      "Epoch 23931 \t\t Training Loss: 0.0005808562273159623 \t\n",
      "Epoch 23932 \t\t Training Loss: 0.0005808562273159623 \t\n",
      "Epoch 23933 \t\t Training Loss: 0.0005808562273159623 \t\n",
      "Epoch 23934 \t\t Training Loss: 0.0005808562273159623 \t\n",
      "Epoch 23935 \t\t Training Loss: 0.0005808562273159623 \t\n",
      "Epoch 23936 \t\t Training Loss: 0.0005808562273159623 \t\n",
      "Epoch 23937 \t\t Training Loss: 0.0005808561691083014 \t\n",
      "Epoch 23938 \t\t Training Loss: 0.0005808562273159623 \t\n",
      "Epoch 23939 \t\t Training Loss: 0.0005808561691083014 \t\n",
      "Epoch 23940 \t\t Training Loss: 0.0005808561691083014 \t\n",
      "Epoch 23941 \t\t Training Loss: 0.0005808561691083014 \t\n",
      "Epoch 23942 \t\t Training Loss: 0.0005808561691083014 \t\n",
      "Epoch 23943 \t\t Training Loss: 0.0005808562273159623 \t\n",
      "Epoch 23944 \t\t Training Loss: 0.0005808561691083014 \t\n",
      "Epoch 23945 \t\t Training Loss: 0.0005808561691083014 \t\n",
      "Epoch 23946 \t\t Training Loss: 0.0005808561691083014 \t\n",
      "Epoch 23947 \t\t Training Loss: 0.0005808561691083014 \t\n",
      "Epoch 23948 \t\t Training Loss: 0.0005808561691083014 \t\n",
      "Epoch 23949 \t\t Training Loss: 0.0005808561691083014 \t\n",
      "Epoch 23950 \t\t Training Loss: 0.0005808561691083014 \t\n",
      "Epoch 23951 \t\t Training Loss: 0.0005808561691083014 \t\n",
      "Epoch 23952 \t\t Training Loss: 0.0005808561691083014 \t\n",
      "Epoch 23953 \t\t Training Loss: 0.0005808561691083014 \t\n",
      "Epoch 23954 \t\t Training Loss: 0.0005808561109006405 \t\n",
      "Epoch 23955 \t\t Training Loss: 0.0005808561109006405 \t\n",
      "Epoch 23956 \t\t Training Loss: 0.0005808561109006405 \t\n",
      "Epoch 23957 \t\t Training Loss: 0.0005808561691083014 \t\n",
      "Epoch 23958 \t\t Training Loss: 0.0005808561109006405 \t\n",
      "Epoch 23959 \t\t Training Loss: 0.0005808561109006405 \t\n",
      "Epoch 23960 \t\t Training Loss: 0.0005808561109006405 \t\n",
      "Epoch 23961 \t\t Training Loss: 0.0005808561109006405 \t\n",
      "Epoch 23962 \t\t Training Loss: 0.0005808561109006405 \t\n",
      "Epoch 23963 \t\t Training Loss: 0.0005808561109006405 \t\n",
      "Epoch 23964 \t\t Training Loss: 0.0005808560526929796 \t\n",
      "Epoch 23965 \t\t Training Loss: 0.0005808561109006405 \t\n",
      "Epoch 23966 \t\t Training Loss: 0.0005808560526929796 \t\n",
      "Epoch 23967 \t\t Training Loss: 0.0005808559944853187 \t\n",
      "Epoch 23968 \t\t Training Loss: 0.0005808559944853187 \t\n",
      "Epoch 23969 \t\t Training Loss: 0.0005808559944853187 \t\n",
      "Epoch 23970 \t\t Training Loss: 0.0005808559944853187 \t\n",
      "Epoch 23971 \t\t Training Loss: 0.0005808559944853187 \t\n",
      "Epoch 23972 \t\t Training Loss: 0.0005808559944853187 \t\n",
      "Epoch 23973 \t\t Training Loss: 0.0005808559944853187 \t\n",
      "Epoch 23974 \t\t Training Loss: 0.0005808559944853187 \t\n",
      "Epoch 23975 \t\t Training Loss: 0.0005808559944853187 \t\n",
      "Epoch 23976 \t\t Training Loss: 0.0005808559362776577 \t\n",
      "Epoch 23977 \t\t Training Loss: 0.0005808559362776577 \t\n",
      "Epoch 23978 \t\t Training Loss: 0.0005808559362776577 \t\n",
      "Epoch 23979 \t\t Training Loss: 0.0005808559362776577 \t\n",
      "Epoch 23980 \t\t Training Loss: 0.0005808559362776577 \t\n",
      "Epoch 23981 \t\t Training Loss: 0.0005808558780699968 \t\n",
      "Epoch 23982 \t\t Training Loss: 0.0005808558780699968 \t\n",
      "Epoch 23983 \t\t Training Loss: 0.0005808558780699968 \t\n",
      "Epoch 23984 \t\t Training Loss: 0.0005808558780699968 \t\n",
      "Epoch 23985 \t\t Training Loss: 0.0005808558198623359 \t\n",
      "Epoch 23986 \t\t Training Loss: 0.0005808558198623359 \t\n",
      "Epoch 23987 \t\t Training Loss: 0.0005808558198623359 \t\n",
      "Epoch 23988 \t\t Training Loss: 0.0005808558198623359 \t\n",
      "Epoch 23989 \t\t Training Loss: 0.0005808558198623359 \t\n",
      "Epoch 23990 \t\t Training Loss: 0.0005808558198623359 \t\n",
      "Epoch 23991 \t\t Training Loss: 0.0005808558198623359 \t\n",
      "Epoch 23992 \t\t Training Loss: 0.0005808558198623359 \t\n",
      "Epoch 23993 \t\t Training Loss: 0.0005808558198623359 \t\n",
      "Epoch 23994 \t\t Training Loss: 0.0005808558198623359 \t\n",
      "Epoch 23995 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 23996 \t\t Training Loss: 0.0005808558198623359 \t\n",
      "Epoch 23997 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 23998 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 23999 \t\t Training Loss: 0.0005808558198623359 \t\n",
      "Epoch 24000 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24001 \t\t Training Loss: 0.000580855761654675 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24002 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24003 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24004 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24005 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24006 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24007 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24008 \t\t Training Loss: 0.0005808556452393532 \t\n",
      "Epoch 24009 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24010 \t\t Training Loss: 0.0005808556452393532 \t\n",
      "Epoch 24011 \t\t Training Loss: 0.0005808556452393532 \t\n",
      "Epoch 24012 \t\t Training Loss: 0.0005808556452393532 \t\n",
      "Epoch 24013 \t\t Training Loss: 0.0005808556452393532 \t\n",
      "Epoch 24014 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24015 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24016 \t\t Training Loss: 0.0005808557034470141 \t\n",
      "Epoch 24017 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24018 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24019 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24020 \t\t Training Loss: 0.0005808556452393532 \t\n",
      "Epoch 24021 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24022 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24023 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24024 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24025 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24026 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24027 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24028 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24029 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24030 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24031 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24032 \t\t Training Loss: 0.0005808556452393532 \t\n",
      "Epoch 24033 \t\t Training Loss: 0.0005808555870316923 \t\n",
      "Epoch 24034 \t\t Training Loss: 0.0005808555870316923 \t\n",
      "Epoch 24035 \t\t Training Loss: 0.0005808556452393532 \t\n",
      "Epoch 24036 \t\t Training Loss: 0.0005808556452393532 \t\n",
      "Epoch 24037 \t\t Training Loss: 0.000580855761654675 \t\n",
      "Epoch 24038 \t\t Training Loss: 0.0005808556452393532 \t\n",
      "Epoch 24039 \t\t Training Loss: 0.0005808556452393532 \t\n",
      "Epoch 24040 \t\t Training Loss: 0.0005808556452393532 \t\n",
      "Epoch 24041 \t\t Training Loss: 0.0005808556452393532 \t\n",
      "Epoch 24042 \t\t Training Loss: 0.0005808556452393532 \t\n",
      "Epoch 24043 \t\t Training Loss: 0.0005808556452393532 \t\n",
      "Epoch 24044 \t\t Training Loss: 0.0005808556452393532 \t\n",
      "Epoch 24045 \t\t Training Loss: 0.0005808555870316923 \t\n",
      "Epoch 24046 \t\t Training Loss: 0.0005808555870316923 \t\n",
      "Epoch 24047 \t\t Training Loss: 0.0005808555870316923 \t\n",
      "Epoch 24048 \t\t Training Loss: 0.0005808555870316923 \t\n",
      "Epoch 24049 \t\t Training Loss: 0.0005808555870316923 \t\n",
      "Epoch 24050 \t\t Training Loss: 0.0005808555870316923 \t\n",
      "Epoch 24051 \t\t Training Loss: 0.0005808555870316923 \t\n",
      "Epoch 24052 \t\t Training Loss: 0.0005808555870316923 \t\n",
      "Epoch 24053 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24054 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24055 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24056 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24057 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24058 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24059 \t\t Training Loss: 0.0005808555870316923 \t\n",
      "Epoch 24060 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24061 \t\t Training Loss: 0.0005808555870316923 \t\n",
      "Epoch 24062 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24063 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24064 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24065 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24066 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24067 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24068 \t\t Training Loss: 0.0005808554706163704 \t\n",
      "Epoch 24069 \t\t Training Loss: 0.0005808554706163704 \t\n",
      "Epoch 24070 \t\t Training Loss: 0.0005808554706163704 \t\n",
      "Epoch 24071 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24072 \t\t Training Loss: 0.0005808554706163704 \t\n",
      "Epoch 24073 \t\t Training Loss: 0.0005808554706163704 \t\n",
      "Epoch 24074 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24075 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24076 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24077 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24078 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24079 \t\t Training Loss: 0.0005808555288240314 \t\n",
      "Epoch 24080 \t\t Training Loss: 0.0005808554706163704 \t\n",
      "Epoch 24081 \t\t Training Loss: 0.0005808554706163704 \t\n",
      "Epoch 24082 \t\t Training Loss: 0.0005808554706163704 \t\n",
      "Epoch 24083 \t\t Training Loss: 0.0005808554706163704 \t\n",
      "Epoch 24084 \t\t Training Loss: 0.0005808554706163704 \t\n",
      "Epoch 24085 \t\t Training Loss: 0.0005808554124087095 \t\n",
      "Epoch 24086 \t\t Training Loss: 0.0005808554124087095 \t\n",
      "Epoch 24087 \t\t Training Loss: 0.0005808554706163704 \t\n",
      "Epoch 24088 \t\t Training Loss: 0.0005808554124087095 \t\n",
      "Epoch 24089 \t\t Training Loss: 0.0005808554124087095 \t\n",
      "Epoch 24090 \t\t Training Loss: 0.0005808554124087095 \t\n",
      "Epoch 24091 \t\t Training Loss: 0.0005808554124087095 \t\n",
      "Epoch 24092 \t\t Training Loss: 0.0005808554124087095 \t\n",
      "Epoch 24093 \t\t Training Loss: 0.0005808554124087095 \t\n",
      "Epoch 24094 \t\t Training Loss: 0.0005808554124087095 \t\n",
      "Epoch 24095 \t\t Training Loss: 0.0005808554124087095 \t\n",
      "Epoch 24096 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24097 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24098 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24099 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24100 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24101 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24102 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24103 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24104 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24105 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24106 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24107 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24108 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24109 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24110 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24111 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24112 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24113 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24114 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24115 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24116 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24117 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24118 \t\t Training Loss: 0.0005808552959933877 \t\n",
      "Epoch 24119 \t\t Training Loss: 0.0005808552959933877 \t\n",
      "Epoch 24120 \t\t Training Loss: 0.0005808552959933877 \t\n",
      "Epoch 24121 \t\t Training Loss: 0.0005808552959933877 \t\n",
      "Epoch 24122 \t\t Training Loss: 0.0005808552959933877 \t\n",
      "Epoch 24123 \t\t Training Loss: 0.0005808552959933877 \t\n",
      "Epoch 24124 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24125 \t\t Training Loss: 0.0005808553542010486 \t\n",
      "Epoch 24126 \t\t Training Loss: 0.0005808552959933877 \t\n",
      "Epoch 24127 \t\t Training Loss: 0.0005808552959933877 \t\n",
      "Epoch 24128 \t\t Training Loss: 0.0005808552959933877 \t\n",
      "Epoch 24129 \t\t Training Loss: 0.0005808552959933877 \t\n",
      "Epoch 24130 \t\t Training Loss: 0.0005808552959933877 \t\n",
      "Epoch 24131 \t\t Training Loss: 0.0005808552377857268 \t\n",
      "Epoch 24132 \t\t Training Loss: 0.0005808551795780659 \t\n",
      "Epoch 24133 \t\t Training Loss: 0.0005808552377857268 \t\n",
      "Epoch 24134 \t\t Training Loss: 0.0005808551795780659 \t\n",
      "Epoch 24135 \t\t Training Loss: 0.0005808552377857268 \t\n",
      "Epoch 24136 \t\t Training Loss: 0.0005808552377857268 \t\n",
      "Epoch 24137 \t\t Training Loss: 0.0005808552377857268 \t\n",
      "Epoch 24138 \t\t Training Loss: 0.0005808552377857268 \t\n",
      "Epoch 24139 \t\t Training Loss: 0.0005808552377857268 \t\n",
      "Epoch 24140 \t\t Training Loss: 0.0005808552959933877 \t\n",
      "Epoch 24141 \t\t Training Loss: 0.0005808552959933877 \t\n",
      "Epoch 24142 \t\t Training Loss: 0.0005808552959933877 \t\n",
      "Epoch 24143 \t\t Training Loss: 0.0005808551795780659 \t\n",
      "Epoch 24144 \t\t Training Loss: 0.0005808551795780659 \t\n",
      "Epoch 24145 \t\t Training Loss: 0.0005808551795780659 \t\n",
      "Epoch 24146 \t\t Training Loss: 0.0005808551795780659 \t\n",
      "Epoch 24147 \t\t Training Loss: 0.0005808551795780659 \t\n",
      "Epoch 24148 \t\t Training Loss: 0.0005808551795780659 \t\n",
      "Epoch 24149 \t\t Training Loss: 0.0005808551795780659 \t\n",
      "Epoch 24150 \t\t Training Loss: 0.0005808551795780659 \t\n",
      "Epoch 24151 \t\t Training Loss: 0.0005808551795780659 \t\n",
      "Epoch 24152 \t\t Training Loss: 0.0005808551795780659 \t\n",
      "Epoch 24153 \t\t Training Loss: 0.0005808551795780659 \t\n",
      "Epoch 24154 \t\t Training Loss: 0.000580855063162744 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24155 \t\t Training Loss: 0.000580855063162744 \t\n",
      "Epoch 24156 \t\t Training Loss: 0.0005808551795780659 \t\n",
      "Epoch 24157 \t\t Training Loss: 0.000580855063162744 \t\n",
      "Epoch 24158 \t\t Training Loss: 0.000580855063162744 \t\n",
      "Epoch 24159 \t\t Training Loss: 0.000580855063162744 \t\n",
      "Epoch 24160 \t\t Training Loss: 0.000580855063162744 \t\n",
      "Epoch 24161 \t\t Training Loss: 0.000580855063162744 \t\n",
      "Epoch 24162 \t\t Training Loss: 0.000580855063162744 \t\n",
      "Epoch 24163 \t\t Training Loss: 0.000580855063162744 \t\n",
      "Epoch 24164 \t\t Training Loss: 0.000580855063162744 \t\n",
      "Epoch 24165 \t\t Training Loss: 0.000580855063162744 \t\n",
      "Epoch 24166 \t\t Training Loss: 0.000580855063162744 \t\n",
      "Epoch 24167 \t\t Training Loss: 0.0005808550049550831 \t\n",
      "Epoch 24168 \t\t Training Loss: 0.000580855063162744 \t\n",
      "Epoch 24169 \t\t Training Loss: 0.000580855063162744 \t\n",
      "Epoch 24170 \t\t Training Loss: 0.0005808550049550831 \t\n",
      "Epoch 24171 \t\t Training Loss: 0.0005808550049550831 \t\n",
      "Epoch 24172 \t\t Training Loss: 0.0005808549467474222 \t\n",
      "Epoch 24173 \t\t Training Loss: 0.0005808549467474222 \t\n",
      "Epoch 24174 \t\t Training Loss: 0.0005808549467474222 \t\n",
      "Epoch 24175 \t\t Training Loss: 0.0005808549467474222 \t\n",
      "Epoch 24176 \t\t Training Loss: 0.0005808549467474222 \t\n",
      "Epoch 24177 \t\t Training Loss: 0.0005808549467474222 \t\n",
      "Epoch 24178 \t\t Training Loss: 0.0005808549467474222 \t\n",
      "Epoch 24179 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24180 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24181 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24182 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24183 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24184 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24185 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24186 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24187 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24188 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24189 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24190 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24191 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24192 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24193 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24194 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24195 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24196 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24197 \t\t Training Loss: 0.0005808548885397613 \t\n",
      "Epoch 24198 \t\t Training Loss: 0.0005808548303321004 \t\n",
      "Epoch 24199 \t\t Training Loss: 0.0005808548303321004 \t\n",
      "Epoch 24200 \t\t Training Loss: 0.0005808548303321004 \t\n",
      "Epoch 24201 \t\t Training Loss: 0.0005808548303321004 \t\n",
      "Epoch 24202 \t\t Training Loss: 0.0005808548303321004 \t\n",
      "Epoch 24203 \t\t Training Loss: 0.0005808548303321004 \t\n",
      "Epoch 24204 \t\t Training Loss: 0.0005808548303321004 \t\n",
      "Epoch 24205 \t\t Training Loss: 0.0005808547721244395 \t\n",
      "Epoch 24206 \t\t Training Loss: 0.0005808547721244395 \t\n",
      "Epoch 24207 \t\t Training Loss: 0.0005808547721244395 \t\n",
      "Epoch 24208 \t\t Training Loss: 0.0005808547721244395 \t\n",
      "Epoch 24209 \t\t Training Loss: 0.0005808547139167786 \t\n",
      "Epoch 24210 \t\t Training Loss: 0.0005808547139167786 \t\n",
      "Epoch 24211 \t\t Training Loss: 0.0005808547139167786 \t\n",
      "Epoch 24212 \t\t Training Loss: 0.0005808547139167786 \t\n",
      "Epoch 24213 \t\t Training Loss: 0.0005808547139167786 \t\n",
      "Epoch 24214 \t\t Training Loss: 0.0005808547139167786 \t\n",
      "Epoch 24215 \t\t Training Loss: 0.0005808547139167786 \t\n",
      "Epoch 24216 \t\t Training Loss: 0.0005808547139167786 \t\n",
      "Epoch 24217 \t\t Training Loss: 0.0005808547139167786 \t\n",
      "Epoch 24218 \t\t Training Loss: 0.0005808547139167786 \t\n",
      "Epoch 24219 \t\t Training Loss: 0.0005808545975014567 \t\n",
      "Epoch 24220 \t\t Training Loss: 0.0005808545975014567 \t\n",
      "Epoch 24221 \t\t Training Loss: 0.0005808545975014567 \t\n",
      "Epoch 24222 \t\t Training Loss: 0.0005808545975014567 \t\n",
      "Epoch 24223 \t\t Training Loss: 0.0005808545392937958 \t\n",
      "Epoch 24224 \t\t Training Loss: 0.0005808545392937958 \t\n",
      "Epoch 24225 \t\t Training Loss: 0.0005808545392937958 \t\n",
      "Epoch 24226 \t\t Training Loss: 0.0005808545392937958 \t\n",
      "Epoch 24227 \t\t Training Loss: 0.0005808545392937958 \t\n",
      "Epoch 24228 \t\t Training Loss: 0.0005808545392937958 \t\n",
      "Epoch 24229 \t\t Training Loss: 0.0005808545392937958 \t\n",
      "Epoch 24230 \t\t Training Loss: 0.0005808545392937958 \t\n",
      "Epoch 24231 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24232 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24233 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24234 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24235 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24236 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24237 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24238 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24239 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24240 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24241 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24242 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24243 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24244 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24245 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24246 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24247 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24248 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24249 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24250 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24251 \t\t Training Loss: 0.0005808544810861349 \t\n",
      "Epoch 24252 \t\t Training Loss: 0.000580854422878474 \t\n",
      "Epoch 24253 \t\t Training Loss: 0.000580854422878474 \t\n",
      "Epoch 24254 \t\t Training Loss: 0.000580854422878474 \t\n",
      "Epoch 24255 \t\t Training Loss: 0.000580854422878474 \t\n",
      "Epoch 24256 \t\t Training Loss: 0.0005808543646708131 \t\n",
      "Epoch 24257 \t\t Training Loss: 0.0005808543646708131 \t\n",
      "Epoch 24258 \t\t Training Loss: 0.0005808543646708131 \t\n",
      "Epoch 24259 \t\t Training Loss: 0.0005808543064631522 \t\n",
      "Epoch 24260 \t\t Training Loss: 0.0005808543064631522 \t\n",
      "Epoch 24261 \t\t Training Loss: 0.0005808542482554913 \t\n",
      "Epoch 24262 \t\t Training Loss: 0.0005808542482554913 \t\n",
      "Epoch 24263 \t\t Training Loss: 0.0005808542482554913 \t\n",
      "Epoch 24264 \t\t Training Loss: 0.0005808542482554913 \t\n",
      "Epoch 24265 \t\t Training Loss: 0.0005808542482554913 \t\n",
      "Epoch 24266 \t\t Training Loss: 0.0005808542482554913 \t\n",
      "Epoch 24267 \t\t Training Loss: 0.0005808542482554913 \t\n",
      "Epoch 24268 \t\t Training Loss: 0.0005808542482554913 \t\n",
      "Epoch 24269 \t\t Training Loss: 0.0005808542482554913 \t\n",
      "Epoch 24270 \t\t Training Loss: 0.0005808542482554913 \t\n",
      "Epoch 24271 \t\t Training Loss: 0.0005808542482554913 \t\n",
      "Epoch 24272 \t\t Training Loss: 0.0005808542482554913 \t\n",
      "Epoch 24273 \t\t Training Loss: 0.0005808542482554913 \t\n",
      "Epoch 24274 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24275 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24276 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24277 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24278 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24279 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24280 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24281 \t\t Training Loss: 0.0005808540736325085 \t\n",
      "Epoch 24282 \t\t Training Loss: 0.0005808540736325085 \t\n",
      "Epoch 24283 \t\t Training Loss: 0.0005808540736325085 \t\n",
      "Epoch 24284 \t\t Training Loss: 0.0005808540736325085 \t\n",
      "Epoch 24285 \t\t Training Loss: 0.0005808540736325085 \t\n",
      "Epoch 24286 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24287 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24288 \t\t Training Loss: 0.0005808540736325085 \t\n",
      "Epoch 24289 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24290 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24291 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24292 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24293 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24294 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24295 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24296 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24297 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24298 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24299 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24300 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24301 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24302 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24303 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24304 \t\t Training Loss: 0.0005808541318401694 \t\n",
      "Epoch 24305 \t\t Training Loss: 0.0005808540736325085 \t\n",
      "Epoch 24306 \t\t Training Loss: 0.0005808540736325085 \t\n",
      "Epoch 24307 \t\t Training Loss: 0.0005808540736325085 \t\n",
      "Epoch 24308 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24309 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24310 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24311 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24312 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24313 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24314 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24315 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24316 \t\t Training Loss: 0.0005808540736325085 \t\n",
      "Epoch 24317 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24318 \t\t Training Loss: 0.0005808540154248476 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24319 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24320 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24321 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24322 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24323 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24324 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24325 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24326 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24327 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24328 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24329 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24330 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24331 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24332 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24333 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24334 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24335 \t\t Training Loss: 0.0005808540154248476 \t\n",
      "Epoch 24336 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24337 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24338 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24339 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24340 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24341 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24342 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24343 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24344 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24345 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24346 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24347 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24348 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24349 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24350 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24351 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24352 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24353 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24354 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24355 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24356 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24357 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24358 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24359 \t\t Training Loss: 0.0005808539572171867 \t\n",
      "Epoch 24360 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24361 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24362 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24363 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24364 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24365 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24366 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24367 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24368 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24369 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24370 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24371 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24372 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24373 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24374 \t\t Training Loss: 0.0005808538408018649 \t\n",
      "Epoch 24375 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24376 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24377 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24378 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24379 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24380 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24381 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24382 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24383 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24384 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24385 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24386 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24387 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24388 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24389 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24390 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24391 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24392 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24393 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24394 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24395 \t\t Training Loss: 0.000580853724386543 \t\n",
      "Epoch 24396 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24397 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24398 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24399 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24400 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24401 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24402 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24403 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24404 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24405 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24406 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24407 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24408 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24409 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24410 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24411 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24412 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24413 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24414 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24415 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24416 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24417 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24418 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24419 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24420 \t\t Training Loss: 0.0005808536661788821 \t\n",
      "Epoch 24421 \t\t Training Loss: 0.0005808535497635603 \t\n",
      "Epoch 24422 \t\t Training Loss: 0.0005808535497635603 \t\n",
      "Epoch 24423 \t\t Training Loss: 0.0005808535497635603 \t\n",
      "Epoch 24424 \t\t Training Loss: 0.0005808535497635603 \t\n",
      "Epoch 24425 \t\t Training Loss: 0.0005808535497635603 \t\n",
      "Epoch 24426 \t\t Training Loss: 0.0005808535497635603 \t\n",
      "Epoch 24427 \t\t Training Loss: 0.0005808535497635603 \t\n",
      "Epoch 24428 \t\t Training Loss: 0.0005808535497635603 \t\n",
      "Epoch 24429 \t\t Training Loss: 0.0005808535497635603 \t\n",
      "Epoch 24430 \t\t Training Loss: 0.0005808535497635603 \t\n",
      "Epoch 24431 \t\t Training Loss: 0.0005808535497635603 \t\n",
      "Epoch 24432 \t\t Training Loss: 0.0005808535497635603 \t\n",
      "Epoch 24433 \t\t Training Loss: 0.0005808535497635603 \t\n",
      "Epoch 24434 \t\t Training Loss: 0.0005808535497635603 \t\n",
      "Epoch 24435 \t\t Training Loss: 0.0005808535497635603 \t\n",
      "Epoch 24436 \t\t Training Loss: 0.0005808535497635603 \t\n",
      "Epoch 24437 \t\t Training Loss: 0.0005808534333482385 \t\n",
      "Epoch 24438 \t\t Training Loss: 0.0005808534333482385 \t\n",
      "Epoch 24439 \t\t Training Loss: 0.0005808533751405776 \t\n",
      "Epoch 24440 \t\t Training Loss: 0.0005808533751405776 \t\n",
      "Epoch 24441 \t\t Training Loss: 0.0005808533751405776 \t\n",
      "Epoch 24442 \t\t Training Loss: 0.0005808533751405776 \t\n",
      "Epoch 24443 \t\t Training Loss: 0.0005808533751405776 \t\n",
      "Epoch 24444 \t\t Training Loss: 0.0005808533751405776 \t\n",
      "Epoch 24445 \t\t Training Loss: 0.0005808533751405776 \t\n",
      "Epoch 24446 \t\t Training Loss: 0.0005808533751405776 \t\n",
      "Epoch 24447 \t\t Training Loss: 0.0005808533751405776 \t\n",
      "Epoch 24448 \t\t Training Loss: 0.0005808534333482385 \t\n",
      "Epoch 24449 \t\t Training Loss: 0.0005808533751405776 \t\n",
      "Epoch 24450 \t\t Training Loss: 0.0005808533751405776 \t\n",
      "Epoch 24451 \t\t Training Loss: 0.0005808533751405776 \t\n",
      "Epoch 24452 \t\t Training Loss: 0.0005808533751405776 \t\n",
      "Epoch 24453 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24454 \t\t Training Loss: 0.0005808533751405776 \t\n",
      "Epoch 24455 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24456 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24457 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24458 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24459 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24460 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24461 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24462 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24463 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24464 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24465 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24466 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24467 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24468 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24469 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24470 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24471 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24472 \t\t Training Loss: 0.0005808533169329166 \t\n",
      "Epoch 24473 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24474 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24475 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24476 \t\t Training Loss: 0.0005808532587252557 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24477 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24478 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24479 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24480 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24481 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24482 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24483 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24484 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24485 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24486 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24487 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24488 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24489 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24490 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24491 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24492 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24493 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24494 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24495 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24496 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24497 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24498 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24499 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24500 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24501 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24502 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24503 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24504 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24505 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24506 \t\t Training Loss: 0.0005808532587252557 \t\n",
      "Epoch 24507 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24508 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24509 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24510 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24511 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24512 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24513 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24514 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24515 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24516 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24517 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24518 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24519 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24520 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24521 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24522 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24523 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24524 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24525 \t\t Training Loss: 0.0005808531423099339 \t\n",
      "Epoch 24526 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24527 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24528 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24529 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24530 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24531 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24532 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24533 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24534 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24535 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24536 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24537 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24538 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24539 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24540 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24541 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24542 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24543 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24544 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24545 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24546 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24547 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24548 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24549 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24550 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24551 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24552 \t\t Training Loss: 0.0005808529676869512 \t\n",
      "Epoch 24553 \t\t Training Loss: 0.0005808528512716293 \t\n",
      "Epoch 24554 \t\t Training Loss: 0.0005808528512716293 \t\n",
      "Epoch 24555 \t\t Training Loss: 0.0005808528512716293 \t\n",
      "Epoch 24556 \t\t Training Loss: 0.0005808528512716293 \t\n",
      "Epoch 24557 \t\t Training Loss: 0.0005808528512716293 \t\n",
      "Epoch 24558 \t\t Training Loss: 0.0005808528512716293 \t\n",
      "Epoch 24559 \t\t Training Loss: 0.0005808528512716293 \t\n",
      "Epoch 24560 \t\t Training Loss: 0.0005808528512716293 \t\n",
      "Epoch 24561 \t\t Training Loss: 0.0005808528512716293 \t\n",
      "Epoch 24562 \t\t Training Loss: 0.0005808528512716293 \t\n",
      "Epoch 24563 \t\t Training Loss: 0.0005808528512716293 \t\n",
      "Epoch 24564 \t\t Training Loss: 0.0005808528512716293 \t\n",
      "Epoch 24565 \t\t Training Loss: 0.0005808528512716293 \t\n",
      "Epoch 24566 \t\t Training Loss: 0.0005808528512716293 \t\n",
      "Epoch 24567 \t\t Training Loss: 0.0005808527348563075 \t\n",
      "Epoch 24568 \t\t Training Loss: 0.0005808527348563075 \t\n",
      "Epoch 24569 \t\t Training Loss: 0.0005808527348563075 \t\n",
      "Epoch 24570 \t\t Training Loss: 0.0005808527348563075 \t\n",
      "Epoch 24571 \t\t Training Loss: 0.0005808527348563075 \t\n",
      "Epoch 24572 \t\t Training Loss: 0.0005808527348563075 \t\n",
      "Epoch 24573 \t\t Training Loss: 0.0005808527348563075 \t\n",
      "Epoch 24574 \t\t Training Loss: 0.0005808527348563075 \t\n",
      "Epoch 24575 \t\t Training Loss: 0.0005808527348563075 \t\n",
      "Epoch 24576 \t\t Training Loss: 0.0005808527348563075 \t\n",
      "Epoch 24577 \t\t Training Loss: 0.0005808527348563075 \t\n",
      "Epoch 24578 \t\t Training Loss: 0.0005808527348563075 \t\n",
      "Epoch 24579 \t\t Training Loss: 0.0005808527348563075 \t\n",
      "Epoch 24580 \t\t Training Loss: 0.0005808527348563075 \t\n",
      "Epoch 24581 \t\t Training Loss: 0.0005808527348563075 \t\n",
      "Epoch 24582 \t\t Training Loss: 0.0005808527348563075 \t\n",
      "Epoch 24583 \t\t Training Loss: 0.0005808526766486466 \t\n",
      "Epoch 24584 \t\t Training Loss: 0.0005808526766486466 \t\n",
      "Epoch 24585 \t\t Training Loss: 0.0005808526766486466 \t\n",
      "Epoch 24586 \t\t Training Loss: 0.0005808526766486466 \t\n",
      "Epoch 24587 \t\t Training Loss: 0.0005808526766486466 \t\n",
      "Epoch 24588 \t\t Training Loss: 0.0005808526766486466 \t\n",
      "Epoch 24589 \t\t Training Loss: 0.0005808526766486466 \t\n",
      "Epoch 24590 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24591 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24592 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24593 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24594 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24595 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24596 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24597 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24598 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24599 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24600 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24601 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24602 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24603 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24604 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24605 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24606 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24607 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24608 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24609 \t\t Training Loss: 0.0005808525602333248 \t\n",
      "Epoch 24610 \t\t Training Loss: 0.0005808524438180029 \t\n",
      "Epoch 24611 \t\t Training Loss: 0.0005808524438180029 \t\n",
      "Epoch 24612 \t\t Training Loss: 0.0005808524438180029 \t\n",
      "Epoch 24613 \t\t Training Loss: 0.000580852385610342 \t\n",
      "Epoch 24614 \t\t Training Loss: 0.000580852385610342 \t\n",
      "Epoch 24615 \t\t Training Loss: 0.0005808524438180029 \t\n",
      "Epoch 24616 \t\t Training Loss: 0.000580852385610342 \t\n",
      "Epoch 24617 \t\t Training Loss: 0.000580852385610342 \t\n",
      "Epoch 24618 \t\t Training Loss: 0.0005808524438180029 \t\n",
      "Epoch 24619 \t\t Training Loss: 0.000580852385610342 \t\n",
      "Epoch 24620 \t\t Training Loss: 0.000580852385610342 \t\n",
      "Epoch 24621 \t\t Training Loss: 0.000580852385610342 \t\n",
      "Epoch 24622 \t\t Training Loss: 0.000580852385610342 \t\n",
      "Epoch 24623 \t\t Training Loss: 0.0005808522691950202 \t\n",
      "Epoch 24624 \t\t Training Loss: 0.0005808522691950202 \t\n",
      "Epoch 24625 \t\t Training Loss: 0.000580852385610342 \t\n",
      "Epoch 24626 \t\t Training Loss: 0.0005808522691950202 \t\n",
      "Epoch 24627 \t\t Training Loss: 0.0005808522691950202 \t\n",
      "Epoch 24628 \t\t Training Loss: 0.000580852385610342 \t\n",
      "Epoch 24629 \t\t Training Loss: 0.000580852385610342 \t\n",
      "Epoch 24630 \t\t Training Loss: 0.0005808522691950202 \t\n",
      "Epoch 24631 \t\t Training Loss: 0.0005808522691950202 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24632 \t\t Training Loss: 0.0005808522691950202 \t\n",
      "Epoch 24633 \t\t Training Loss: 0.0005808522691950202 \t\n",
      "Epoch 24634 \t\t Training Loss: 0.0005808522691950202 \t\n",
      "Epoch 24635 \t\t Training Loss: 0.0005808522691950202 \t\n",
      "Epoch 24636 \t\t Training Loss: 0.0005808522691950202 \t\n",
      "Epoch 24637 \t\t Training Loss: 0.0005808522691950202 \t\n",
      "Epoch 24638 \t\t Training Loss: 0.0005808522691950202 \t\n",
      "Epoch 24639 \t\t Training Loss: 0.0005808522691950202 \t\n",
      "Epoch 24640 \t\t Training Loss: 0.0005808522691950202 \t\n",
      "Epoch 24641 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24642 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24643 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24644 \t\t Training Loss: 0.0005808522691950202 \t\n",
      "Epoch 24645 \t\t Training Loss: 0.0005808522691950202 \t\n",
      "Epoch 24646 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24647 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24648 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24649 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24650 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24651 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24652 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24653 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24654 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24655 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24656 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24657 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24658 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24659 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24660 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24661 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24662 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24663 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24664 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24665 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24666 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24667 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24668 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24669 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24670 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24671 \t\t Training Loss: 0.0005808521527796984 \t\n",
      "Epoch 24672 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24673 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24674 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24675 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24676 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24677 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24678 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24679 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24680 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24681 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24682 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24683 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24684 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24685 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24686 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24687 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24688 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24689 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24690 \t\t Training Loss: 0.0005808519781567156 \t\n",
      "Epoch 24691 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24692 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24693 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24694 \t\t Training Loss: 0.0005808519781567156 \t\n",
      "Epoch 24695 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24696 \t\t Training Loss: 0.0005808520363643765 \t\n",
      "Epoch 24697 \t\t Training Loss: 0.0005808519781567156 \t\n",
      "Epoch 24698 \t\t Training Loss: 0.0005808519781567156 \t\n",
      "Epoch 24699 \t\t Training Loss: 0.0005808519781567156 \t\n",
      "Epoch 24700 \t\t Training Loss: 0.0005808519781567156 \t\n",
      "Epoch 24701 \t\t Training Loss: 0.0005808519781567156 \t\n",
      "Epoch 24702 \t\t Training Loss: 0.0005808519781567156 \t\n",
      "Epoch 24703 \t\t Training Loss: 0.0005808519781567156 \t\n",
      "Epoch 24704 \t\t Training Loss: 0.0005808519781567156 \t\n",
      "Epoch 24705 \t\t Training Loss: 0.0005808519781567156 \t\n",
      "Epoch 24706 \t\t Training Loss: 0.0005808519781567156 \t\n",
      "Epoch 24707 \t\t Training Loss: 0.0005808519781567156 \t\n",
      "Epoch 24708 \t\t Training Loss: 0.0005808518617413938 \t\n",
      "Epoch 24709 \t\t Training Loss: 0.0005808518617413938 \t\n",
      "Epoch 24710 \t\t Training Loss: 0.0005808519781567156 \t\n",
      "Epoch 24711 \t\t Training Loss: 0.0005808519781567156 \t\n",
      "Epoch 24712 \t\t Training Loss: 0.0005808518617413938 \t\n",
      "Epoch 24713 \t\t Training Loss: 0.0005808518617413938 \t\n",
      "Epoch 24714 \t\t Training Loss: 0.0005808518617413938 \t\n",
      "Epoch 24715 \t\t Training Loss: 0.0005808518617413938 \t\n",
      "Epoch 24716 \t\t Training Loss: 0.0005808518617413938 \t\n",
      "Epoch 24717 \t\t Training Loss: 0.0005808518617413938 \t\n",
      "Epoch 24718 \t\t Training Loss: 0.0005808518617413938 \t\n",
      "Epoch 24719 \t\t Training Loss: 0.0005808518617413938 \t\n",
      "Epoch 24720 \t\t Training Loss: 0.0005808518617413938 \t\n",
      "Epoch 24721 \t\t Training Loss: 0.0005808518617413938 \t\n",
      "Epoch 24722 \t\t Training Loss: 0.0005808518617413938 \t\n",
      "Epoch 24723 \t\t Training Loss: 0.0005808518617413938 \t\n",
      "Epoch 24724 \t\t Training Loss: 0.000580851745326072 \t\n",
      "Epoch 24725 \t\t Training Loss: 0.000580851745326072 \t\n",
      "Epoch 24726 \t\t Training Loss: 0.000580851745326072 \t\n",
      "Epoch 24727 \t\t Training Loss: 0.000580851745326072 \t\n",
      "Epoch 24728 \t\t Training Loss: 0.000580851745326072 \t\n",
      "Epoch 24729 \t\t Training Loss: 0.000580851745326072 \t\n",
      "Epoch 24730 \t\t Training Loss: 0.000580851745326072 \t\n",
      "Epoch 24731 \t\t Training Loss: 0.000580851745326072 \t\n",
      "Epoch 24732 \t\t Training Loss: 0.000580851745326072 \t\n",
      "Epoch 24733 \t\t Training Loss: 0.000580851745326072 \t\n",
      "Epoch 24734 \t\t Training Loss: 0.000580851745326072 \t\n",
      "Epoch 24735 \t\t Training Loss: 0.000580851745326072 \t\n",
      "Epoch 24736 \t\t Training Loss: 0.0005808516871184111 \t\n",
      "Epoch 24737 \t\t Training Loss: 0.0005808516871184111 \t\n",
      "Epoch 24738 \t\t Training Loss: 0.0005808516871184111 \t\n",
      "Epoch 24739 \t\t Training Loss: 0.0005808516871184111 \t\n",
      "Epoch 24740 \t\t Training Loss: 0.0005808516871184111 \t\n",
      "Epoch 24741 \t\t Training Loss: 0.0005808516871184111 \t\n",
      "Epoch 24742 \t\t Training Loss: 0.000580851745326072 \t\n",
      "Epoch 24743 \t\t Training Loss: 0.0005808516871184111 \t\n",
      "Epoch 24744 \t\t Training Loss: 0.0005808516871184111 \t\n",
      "Epoch 24745 \t\t Training Loss: 0.0005808516871184111 \t\n",
      "Epoch 24746 \t\t Training Loss: 0.0005808516871184111 \t\n",
      "Epoch 24747 \t\t Training Loss: 0.0005808516289107502 \t\n",
      "Epoch 24748 \t\t Training Loss: 0.0005808516289107502 \t\n",
      "Epoch 24749 \t\t Training Loss: 0.0005808516289107502 \t\n",
      "Epoch 24750 \t\t Training Loss: 0.0005808516289107502 \t\n",
      "Epoch 24751 \t\t Training Loss: 0.0005808516289107502 \t\n",
      "Epoch 24752 \t\t Training Loss: 0.0005808516289107502 \t\n",
      "Epoch 24753 \t\t Training Loss: 0.0005808516289107502 \t\n",
      "Epoch 24754 \t\t Training Loss: 0.0005808516289107502 \t\n",
      "Epoch 24755 \t\t Training Loss: 0.0005808516289107502 \t\n",
      "Epoch 24756 \t\t Training Loss: 0.0005808516289107502 \t\n",
      "Epoch 24757 \t\t Training Loss: 0.0005808515707030892 \t\n",
      "Epoch 24758 \t\t Training Loss: 0.0005808516289107502 \t\n",
      "Epoch 24759 \t\t Training Loss: 0.0005808515707030892 \t\n",
      "Epoch 24760 \t\t Training Loss: 0.0005808515707030892 \t\n",
      "Epoch 24761 \t\t Training Loss: 0.0005808515707030892 \t\n",
      "Epoch 24762 \t\t Training Loss: 0.0005808515707030892 \t\n",
      "Epoch 24763 \t\t Training Loss: 0.0005808515707030892 \t\n",
      "Epoch 24764 \t\t Training Loss: 0.0005808515707030892 \t\n",
      "Epoch 24765 \t\t Training Loss: 0.0005808515707030892 \t\n",
      "Epoch 24766 \t\t Training Loss: 0.0005808515707030892 \t\n",
      "Epoch 24767 \t\t Training Loss: 0.0005808515707030892 \t\n",
      "Epoch 24768 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24769 \t\t Training Loss: 0.0005808515707030892 \t\n",
      "Epoch 24770 \t\t Training Loss: 0.0005808515707030892 \t\n",
      "Epoch 24771 \t\t Training Loss: 0.0005808515707030892 \t\n",
      "Epoch 24772 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24773 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24774 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24775 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24776 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24777 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24778 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24779 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24780 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24781 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24782 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24783 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24784 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24785 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24786 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24787 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24788 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24789 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24790 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24791 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24792 \t\t Training Loss: 0.0005808514542877674 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24793 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24794 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24795 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24796 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24797 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24798 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24799 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24800 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24801 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24802 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24803 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24804 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24805 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24806 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24807 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24808 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24809 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24810 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24811 \t\t Training Loss: 0.0005808514542877674 \t\n",
      "Epoch 24812 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24813 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24814 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24815 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24816 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24817 \t\t Training Loss: 0.0005808513960801065 \t\n",
      "Epoch 24818 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24819 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24820 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24821 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24822 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24823 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24824 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24825 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24826 \t\t Training Loss: 0.0005808512796647847 \t\n",
      "Epoch 24827 \t\t Training Loss: 0.0005808512796647847 \t\n",
      "Epoch 24828 \t\t Training Loss: 0.0005808512796647847 \t\n",
      "Epoch 24829 \t\t Training Loss: 0.0005808512796647847 \t\n",
      "Epoch 24830 \t\t Training Loss: 0.0005808512796647847 \t\n",
      "Epoch 24831 \t\t Training Loss: 0.0005808512796647847 \t\n",
      "Epoch 24832 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24833 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24834 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24835 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24836 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24837 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24838 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24839 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24840 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24841 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24842 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24843 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24844 \t\t Training Loss: 0.0005808512796647847 \t\n",
      "Epoch 24845 \t\t Training Loss: 0.0005808512796647847 \t\n",
      "Epoch 24846 \t\t Training Loss: 0.0005808512796647847 \t\n",
      "Epoch 24847 \t\t Training Loss: 0.0005808512796647847 \t\n",
      "Epoch 24848 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24849 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24850 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24851 \t\t Training Loss: 0.0005808512796647847 \t\n",
      "Epoch 24852 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24853 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24854 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24855 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24856 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24857 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24858 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24859 \t\t Training Loss: 0.0005808512796647847 \t\n",
      "Epoch 24860 \t\t Training Loss: 0.0005808512796647847 \t\n",
      "Epoch 24861 \t\t Training Loss: 0.0005808512796647847 \t\n",
      "Epoch 24862 \t\t Training Loss: 0.0005808513378724456 \t\n",
      "Epoch 24863 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24864 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24865 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24866 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24867 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24868 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24869 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24870 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24871 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24872 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24873 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24874 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24875 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24876 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24877 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24878 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24879 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24880 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24881 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24882 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24883 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24884 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24885 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24886 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24887 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24888 \t\t Training Loss: 0.0005808511632494628 \t\n",
      "Epoch 24889 \t\t Training Loss: 0.0005808511050418019 \t\n",
      "Epoch 24890 \t\t Training Loss: 0.0005808511050418019 \t\n",
      "Epoch 24891 \t\t Training Loss: 0.0005808511050418019 \t\n",
      "Epoch 24892 \t\t Training Loss: 0.0005808511050418019 \t\n",
      "Epoch 24893 \t\t Training Loss: 0.0005808511050418019 \t\n",
      "Epoch 24894 \t\t Training Loss: 0.0005808511050418019 \t\n",
      "Epoch 24895 \t\t Training Loss: 0.0005808511050418019 \t\n",
      "Epoch 24896 \t\t Training Loss: 0.0005808511050418019 \t\n",
      "Epoch 24897 \t\t Training Loss: 0.0005808511050418019 \t\n",
      "Epoch 24898 \t\t Training Loss: 0.0005808511050418019 \t\n",
      "Epoch 24899 \t\t Training Loss: 0.000580851046834141 \t\n",
      "Epoch 24900 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24901 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24902 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24903 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24904 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24905 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24906 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24907 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24908 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24909 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24910 \t\t Training Loss: 0.000580851046834141 \t\n",
      "Epoch 24911 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24912 \t\t Training Loss: 0.000580851046834141 \t\n",
      "Epoch 24913 \t\t Training Loss: 0.000580851046834141 \t\n",
      "Epoch 24914 \t\t Training Loss: 0.000580851046834141 \t\n",
      "Epoch 24915 \t\t Training Loss: 0.000580851046834141 \t\n",
      "Epoch 24916 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24917 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24918 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24919 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24920 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24921 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24922 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24923 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24924 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24925 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24926 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24927 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24928 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24929 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24930 \t\t Training Loss: 0.0005808509304188192 \t\n",
      "Epoch 24931 \t\t Training Loss: 0.0005808509304188192 \t\n",
      "Epoch 24932 \t\t Training Loss: 0.0005808509304188192 \t\n",
      "Epoch 24933 \t\t Training Loss: 0.0005808509304188192 \t\n",
      "Epoch 24934 \t\t Training Loss: 0.0005808509304188192 \t\n",
      "Epoch 24935 \t\t Training Loss: 0.0005808509304188192 \t\n",
      "Epoch 24936 \t\t Training Loss: 0.0005808509304188192 \t\n",
      "Epoch 24937 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24938 \t\t Training Loss: 0.0005808509304188192 \t\n",
      "Epoch 24939 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24940 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24941 \t\t Training Loss: 0.0005808509304188192 \t\n",
      "Epoch 24942 \t\t Training Loss: 0.0005808509886264801 \t\n",
      "Epoch 24943 \t\t Training Loss: 0.0005808509304188192 \t\n",
      "Epoch 24944 \t\t Training Loss: 0.0005808509304188192 \t\n",
      "Epoch 24945 \t\t Training Loss: 0.0005808509304188192 \t\n",
      "Epoch 24946 \t\t Training Loss: 0.0005808509304188192 \t\n",
      "Epoch 24947 \t\t Training Loss: 0.0005808508722111583 \t\n",
      "Epoch 24948 \t\t Training Loss: 0.0005808508140034974 \t\n",
      "Epoch 24949 \t\t Training Loss: 0.0005808508140034974 \t\n",
      "Epoch 24950 \t\t Training Loss: 0.0005808508140034974 \t\n",
      "Epoch 24951 \t\t Training Loss: 0.0005808508140034974 \t\n",
      "Epoch 24952 \t\t Training Loss: 0.0005808508140034974 \t\n",
      "Epoch 24953 \t\t Training Loss: 0.0005808508140034974 \t\n",
      "Epoch 24954 \t\t Training Loss: 0.0005808509304188192 \t\n",
      "Epoch 24955 \t\t Training Loss: 0.0005808509304188192 \t\n",
      "Epoch 24956 \t\t Training Loss: 0.0005808509304188192 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24957 \t\t Training Loss: 0.0005808509304188192 \t\n",
      "Epoch 24958 \t\t Training Loss: 0.0005808508140034974 \t\n",
      "Epoch 24959 \t\t Training Loss: 0.0005808508140034974 \t\n",
      "Epoch 24960 \t\t Training Loss: 0.0005808508140034974 \t\n",
      "Epoch 24961 \t\t Training Loss: 0.0005808507557958364 \t\n",
      "Epoch 24962 \t\t Training Loss: 0.0005808508140034974 \t\n",
      "Epoch 24963 \t\t Training Loss: 0.0005808508140034974 \t\n",
      "Epoch 24964 \t\t Training Loss: 0.0005808508140034974 \t\n",
      "Epoch 24965 \t\t Training Loss: 0.0005808508140034974 \t\n",
      "Epoch 24966 \t\t Training Loss: 0.0005808507557958364 \t\n",
      "Epoch 24967 \t\t Training Loss: 0.0005808507557958364 \t\n",
      "Epoch 24968 \t\t Training Loss: 0.0005808507557958364 \t\n",
      "Epoch 24969 \t\t Training Loss: 0.0005808507557958364 \t\n",
      "Epoch 24970 \t\t Training Loss: 0.0005808507557958364 \t\n",
      "Epoch 24971 \t\t Training Loss: 0.0005808507557958364 \t\n",
      "Epoch 24972 \t\t Training Loss: 0.0005808507557958364 \t\n",
      "Epoch 24973 \t\t Training Loss: 0.0005808507557958364 \t\n",
      "Epoch 24974 \t\t Training Loss: 0.0005808507557958364 \t\n",
      "Epoch 24975 \t\t Training Loss: 0.0005808507557958364 \t\n",
      "Epoch 24976 \t\t Training Loss: 0.0005808506975881755 \t\n",
      "Epoch 24977 \t\t Training Loss: 0.0005808506975881755 \t\n",
      "Epoch 24978 \t\t Training Loss: 0.0005808506975881755 \t\n",
      "Epoch 24979 \t\t Training Loss: 0.0005808506393805146 \t\n",
      "Epoch 24980 \t\t Training Loss: 0.0005808506393805146 \t\n",
      "Epoch 24981 \t\t Training Loss: 0.0005808506393805146 \t\n",
      "Epoch 24982 \t\t Training Loss: 0.0005808506393805146 \t\n",
      "Epoch 24983 \t\t Training Loss: 0.0005808506393805146 \t\n",
      "Epoch 24984 \t\t Training Loss: 0.0005808506975881755 \t\n",
      "Epoch 24985 \t\t Training Loss: 0.0005808506975881755 \t\n",
      "Epoch 24986 \t\t Training Loss: 0.0005808506975881755 \t\n",
      "Epoch 24987 \t\t Training Loss: 0.0005808506393805146 \t\n",
      "Epoch 24988 \t\t Training Loss: 0.0005808506393805146 \t\n",
      "Epoch 24989 \t\t Training Loss: 0.0005808506393805146 \t\n",
      "Epoch 24990 \t\t Training Loss: 0.0005808505811728537 \t\n",
      "Epoch 24991 \t\t Training Loss: 0.0005808506393805146 \t\n",
      "Epoch 24992 \t\t Training Loss: 0.0005808506393805146 \t\n",
      "Epoch 24993 \t\t Training Loss: 0.0005808506393805146 \t\n",
      "Epoch 24994 \t\t Training Loss: 0.0005808506393805146 \t\n",
      "Epoch 24995 \t\t Training Loss: 0.0005808506393805146 \t\n",
      "Epoch 24996 \t\t Training Loss: 0.0005808505811728537 \t\n",
      "Epoch 24997 \t\t Training Loss: 0.0005808505811728537 \t\n",
      "Epoch 24998 \t\t Training Loss: 0.0005808505811728537 \t\n",
      "Epoch 24999 \t\t Training Loss: 0.0005808505811728537 \t\n",
      "Epoch 25000 \t\t Training Loss: 0.0005808505811728537 \t\n",
      "Epoch 25001 \t\t Training Loss: 0.0005808505811728537 \t\n",
      "Epoch 25002 \t\t Training Loss: 0.0005808505811728537 \t\n",
      "Epoch 25003 \t\t Training Loss: 0.0005808505811728537 \t\n",
      "Epoch 25004 \t\t Training Loss: 0.0005808505811728537 \t\n",
      "Epoch 25005 \t\t Training Loss: 0.0005808505811728537 \t\n",
      "Epoch 25006 \t\t Training Loss: 0.0005808505811728537 \t\n",
      "Epoch 25007 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25008 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25009 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25010 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25011 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25012 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25013 \t\t Training Loss: 0.0005808505811728537 \t\n",
      "Epoch 25014 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25015 \t\t Training Loss: 0.0005808505811728537 \t\n",
      "Epoch 25016 \t\t Training Loss: 0.0005808505811728537 \t\n",
      "Epoch 25017 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25018 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25019 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25020 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25021 \t\t Training Loss: 0.0005808505811728537 \t\n",
      "Epoch 25022 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25023 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25024 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25025 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25026 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25027 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25028 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25029 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25030 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25031 \t\t Training Loss: 0.0005808505229651928 \t\n",
      "Epoch 25032 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25033 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25034 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25035 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25036 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25037 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25038 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25039 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25040 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25041 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25042 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25043 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25044 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25045 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25046 \t\t Training Loss: 0.0005808503483422101 \t\n",
      "Epoch 25047 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25048 \t\t Training Loss: 0.000580850406549871 \t\n",
      "Epoch 25049 \t\t Training Loss: 0.0005808503483422101 \t\n",
      "Epoch 25050 \t\t Training Loss: 0.0005808503483422101 \t\n",
      "Epoch 25051 \t\t Training Loss: 0.0005808503483422101 \t\n",
      "Epoch 25052 \t\t Training Loss: 0.0005808502319268882 \t\n",
      "Epoch 25053 \t\t Training Loss: 0.0005808503483422101 \t\n",
      "Epoch 25054 \t\t Training Loss: 0.0005808503483422101 \t\n",
      "Epoch 25055 \t\t Training Loss: 0.0005808503483422101 \t\n",
      "Epoch 25056 \t\t Training Loss: 0.0005808503483422101 \t\n",
      "Epoch 25057 \t\t Training Loss: 0.0005808503483422101 \t\n",
      "Epoch 25058 \t\t Training Loss: 0.0005808503483422101 \t\n",
      "Epoch 25059 \t\t Training Loss: 0.0005808502319268882 \t\n",
      "Epoch 25060 \t\t Training Loss: 0.0005808502319268882 \t\n",
      "Epoch 25061 \t\t Training Loss: 0.0005808502319268882 \t\n",
      "Epoch 25062 \t\t Training Loss: 0.0005808502319268882 \t\n",
      "Epoch 25063 \t\t Training Loss: 0.0005808502319268882 \t\n",
      "Epoch 25064 \t\t Training Loss: 0.0005808502319268882 \t\n",
      "Epoch 25065 \t\t Training Loss: 0.0005808502319268882 \t\n",
      "Epoch 25066 \t\t Training Loss: 0.0005808502319268882 \t\n",
      "Epoch 25067 \t\t Training Loss: 0.0005808501737192273 \t\n",
      "Epoch 25068 \t\t Training Loss: 0.0005808501737192273 \t\n",
      "Epoch 25069 \t\t Training Loss: 0.0005808501737192273 \t\n",
      "Epoch 25070 \t\t Training Loss: 0.0005808501737192273 \t\n",
      "Epoch 25071 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25072 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25073 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25074 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25075 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25076 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25077 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25078 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25079 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25080 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25081 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25082 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25083 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25084 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25085 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25086 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25087 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25088 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25089 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25090 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25091 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25092 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25093 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25094 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25095 \t\t Training Loss: 0.0005808501155115664 \t\n",
      "Epoch 25096 \t\t Training Loss: 0.0005808500573039055 \t\n",
      "Epoch 25097 \t\t Training Loss: 0.0005808500573039055 \t\n",
      "Epoch 25098 \t\t Training Loss: 0.0005808500573039055 \t\n",
      "Epoch 25099 \t\t Training Loss: 0.0005808499990962446 \t\n",
      "Epoch 25100 \t\t Training Loss: 0.0005808499990962446 \t\n",
      "Epoch 25101 \t\t Training Loss: 0.0005808500573039055 \t\n",
      "Epoch 25102 \t\t Training Loss: 0.0005808499990962446 \t\n",
      "Epoch 25103 \t\t Training Loss: 0.0005808499990962446 \t\n",
      "Epoch 25104 \t\t Training Loss: 0.0005808499990962446 \t\n",
      "Epoch 25105 \t\t Training Loss: 0.0005808499990962446 \t\n",
      "Epoch 25106 \t\t Training Loss: 0.0005808499990962446 \t\n",
      "Epoch 25107 \t\t Training Loss: 0.0005808499990962446 \t\n",
      "Epoch 25108 \t\t Training Loss: 0.0005808499408885837 \t\n",
      "Epoch 25109 \t\t Training Loss: 0.0005808499408885837 \t\n",
      "Epoch 25110 \t\t Training Loss: 0.0005808499408885837 \t\n",
      "Epoch 25111 \t\t Training Loss: 0.0005808499408885837 \t\n",
      "Epoch 25112 \t\t Training Loss: 0.0005808499408885837 \t\n",
      "Epoch 25113 \t\t Training Loss: 0.0005808499408885837 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25114 \t\t Training Loss: 0.0005808498826809227 \t\n",
      "Epoch 25115 \t\t Training Loss: 0.0005808498826809227 \t\n",
      "Epoch 25116 \t\t Training Loss: 0.0005808498826809227 \t\n",
      "Epoch 25117 \t\t Training Loss: 0.0005808498826809227 \t\n",
      "Epoch 25118 \t\t Training Loss: 0.0005808498826809227 \t\n",
      "Epoch 25119 \t\t Training Loss: 0.0005808498826809227 \t\n",
      "Epoch 25120 \t\t Training Loss: 0.0005808498826809227 \t\n",
      "Epoch 25121 \t\t Training Loss: 0.0005808498826809227 \t\n",
      "Epoch 25122 \t\t Training Loss: 0.0005808498826809227 \t\n",
      "Epoch 25123 \t\t Training Loss: 0.0005808498826809227 \t\n",
      "Epoch 25124 \t\t Training Loss: 0.0005808498244732618 \t\n",
      "Epoch 25125 \t\t Training Loss: 0.0005808497662656009 \t\n",
      "Epoch 25126 \t\t Training Loss: 0.0005808497662656009 \t\n",
      "Epoch 25127 \t\t Training Loss: 0.0005808497662656009 \t\n",
      "Epoch 25128 \t\t Training Loss: 0.0005808497662656009 \t\n",
      "Epoch 25129 \t\t Training Loss: 0.0005808497662656009 \t\n",
      "Epoch 25130 \t\t Training Loss: 0.0005808497662656009 \t\n",
      "Epoch 25131 \t\t Training Loss: 0.0005808497662656009 \t\n",
      "Epoch 25132 \t\t Training Loss: 0.0005808497662656009 \t\n",
      "Epoch 25133 \t\t Training Loss: 0.0005808497662656009 \t\n",
      "Epoch 25134 \t\t Training Loss: 0.0005808497662656009 \t\n",
      "Epoch 25135 \t\t Training Loss: 0.0005808497662656009 \t\n",
      "Epoch 25136 \t\t Training Loss: 0.0005808497662656009 \t\n",
      "Epoch 25137 \t\t Training Loss: 0.00058084970805794 \t\n",
      "Epoch 25138 \t\t Training Loss: 0.00058084970805794 \t\n",
      "Epoch 25139 \t\t Training Loss: 0.00058084970805794 \t\n",
      "Epoch 25140 \t\t Training Loss: 0.0005808497662656009 \t\n",
      "Epoch 25141 \t\t Training Loss: 0.0005808497662656009 \t\n",
      "Epoch 25142 \t\t Training Loss: 0.0005808497662656009 \t\n",
      "Epoch 25143 \t\t Training Loss: 0.0005808497662656009 \t\n",
      "Epoch 25144 \t\t Training Loss: 0.0005808496498502791 \t\n",
      "Epoch 25145 \t\t Training Loss: 0.0005808496498502791 \t\n",
      "Epoch 25146 \t\t Training Loss: 0.0005808496498502791 \t\n",
      "Epoch 25147 \t\t Training Loss: 0.0005808496498502791 \t\n",
      "Epoch 25148 \t\t Training Loss: 0.0005808496498502791 \t\n",
      "Epoch 25149 \t\t Training Loss: 0.0005808496498502791 \t\n",
      "Epoch 25150 \t\t Training Loss: 0.0005808496498502791 \t\n",
      "Epoch 25151 \t\t Training Loss: 0.0005808496498502791 \t\n",
      "Epoch 25152 \t\t Training Loss: 0.0005808496498502791 \t\n",
      "Epoch 25153 \t\t Training Loss: 0.0005808496498502791 \t\n",
      "Epoch 25154 \t\t Training Loss: 0.0005808496498502791 \t\n",
      "Epoch 25155 \t\t Training Loss: 0.0005808496498502791 \t\n",
      "Epoch 25156 \t\t Training Loss: 0.0005808496498502791 \t\n",
      "Epoch 25157 \t\t Training Loss: 0.0005808495916426182 \t\n",
      "Epoch 25158 \t\t Training Loss: 0.0005808496498502791 \t\n",
      "Epoch 25159 \t\t Training Loss: 0.0005808495916426182 \t\n",
      "Epoch 25160 \t\t Training Loss: 0.0005808495916426182 \t\n",
      "Epoch 25161 \t\t Training Loss: 0.0005808495916426182 \t\n",
      "Epoch 25162 \t\t Training Loss: 0.0005808495916426182 \t\n",
      "Epoch 25163 \t\t Training Loss: 0.0005808495916426182 \t\n",
      "Epoch 25164 \t\t Training Loss: 0.0005808495334349573 \t\n",
      "Epoch 25165 \t\t Training Loss: 0.0005808495334349573 \t\n",
      "Epoch 25166 \t\t Training Loss: 0.0005808495334349573 \t\n",
      "Epoch 25167 \t\t Training Loss: 0.0005808495334349573 \t\n",
      "Epoch 25168 \t\t Training Loss: 0.0005808495334349573 \t\n",
      "Epoch 25169 \t\t Training Loss: 0.0005808494752272964 \t\n",
      "Epoch 25170 \t\t Training Loss: 0.0005808494752272964 \t\n",
      "Epoch 25171 \t\t Training Loss: 0.0005808494752272964 \t\n",
      "Epoch 25172 \t\t Training Loss: 0.0005808494752272964 \t\n",
      "Epoch 25173 \t\t Training Loss: 0.0005808494752272964 \t\n",
      "Epoch 25174 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25175 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25176 \t\t Training Loss: 0.0005808494752272964 \t\n",
      "Epoch 25177 \t\t Training Loss: 0.0005808494752272964 \t\n",
      "Epoch 25178 \t\t Training Loss: 0.0005808494752272964 \t\n",
      "Epoch 25179 \t\t Training Loss: 0.0005808494752272964 \t\n",
      "Epoch 25180 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25181 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25182 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25183 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25184 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25185 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25186 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25187 \t\t Training Loss: 0.0005808494752272964 \t\n",
      "Epoch 25188 \t\t Training Loss: 0.0005808494752272964 \t\n",
      "Epoch 25189 \t\t Training Loss: 0.0005808494752272964 \t\n",
      "Epoch 25190 \t\t Training Loss: 0.0005808494752272964 \t\n",
      "Epoch 25191 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25192 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25193 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25194 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25195 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25196 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25197 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25198 \t\t Training Loss: 0.0005808494752272964 \t\n",
      "Epoch 25199 \t\t Training Loss: 0.0005808494752272964 \t\n",
      "Epoch 25200 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25201 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25202 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25203 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25204 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25205 \t\t Training Loss: 0.0005808493588119745 \t\n",
      "Epoch 25206 \t\t Training Loss: 0.0005808493588119745 \t\n",
      "Epoch 25207 \t\t Training Loss: 0.0005808493588119745 \t\n",
      "Epoch 25208 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25209 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25210 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25211 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25212 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25213 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25214 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25215 \t\t Training Loss: 0.0005808494170196354 \t\n",
      "Epoch 25216 \t\t Training Loss: 0.0005808493588119745 \t\n",
      "Epoch 25217 \t\t Training Loss: 0.0005808493588119745 \t\n",
      "Epoch 25218 \t\t Training Loss: 0.0005808493006043136 \t\n",
      "Epoch 25219 \t\t Training Loss: 0.0005808493006043136 \t\n",
      "Epoch 25220 \t\t Training Loss: 0.0005808493006043136 \t\n",
      "Epoch 25221 \t\t Training Loss: 0.0005808493006043136 \t\n",
      "Epoch 25222 \t\t Training Loss: 0.0005808493006043136 \t\n",
      "Epoch 25223 \t\t Training Loss: 0.0005808493006043136 \t\n",
      "Epoch 25224 \t\t Training Loss: 0.0005808491841889918 \t\n",
      "Epoch 25225 \t\t Training Loss: 0.0005808493006043136 \t\n",
      "Epoch 25226 \t\t Training Loss: 0.0005808491841889918 \t\n",
      "Epoch 25227 \t\t Training Loss: 0.0005808491841889918 \t\n",
      "Epoch 25228 \t\t Training Loss: 0.0005808491841889918 \t\n",
      "Epoch 25229 \t\t Training Loss: 0.0005808491841889918 \t\n",
      "Epoch 25230 \t\t Training Loss: 0.0005808491841889918 \t\n",
      "Epoch 25231 \t\t Training Loss: 0.0005808491841889918 \t\n",
      "Epoch 25232 \t\t Training Loss: 0.0005808491841889918 \t\n",
      "Epoch 25233 \t\t Training Loss: 0.0005808491841889918 \t\n",
      "Epoch 25234 \t\t Training Loss: 0.0005808491841889918 \t\n",
      "Epoch 25235 \t\t Training Loss: 0.0005808491259813309 \t\n",
      "Epoch 25236 \t\t Training Loss: 0.0005808491259813309 \t\n",
      "Epoch 25237 \t\t Training Loss: 0.00058084906777367 \t\n",
      "Epoch 25238 \t\t Training Loss: 0.00058084906777367 \t\n",
      "Epoch 25239 \t\t Training Loss: 0.00058084906777367 \t\n",
      "Epoch 25240 \t\t Training Loss: 0.00058084906777367 \t\n",
      "Epoch 25241 \t\t Training Loss: 0.0005808491259813309 \t\n",
      "Epoch 25242 \t\t Training Loss: 0.00058084906777367 \t\n",
      "Epoch 25243 \t\t Training Loss: 0.00058084906777367 \t\n",
      "Epoch 25244 \t\t Training Loss: 0.000580849009566009 \t\n",
      "Epoch 25245 \t\t Training Loss: 0.000580849009566009 \t\n",
      "Epoch 25246 \t\t Training Loss: 0.000580849009566009 \t\n",
      "Epoch 25247 \t\t Training Loss: 0.000580849009566009 \t\n",
      "Epoch 25248 \t\t Training Loss: 0.000580849009566009 \t\n",
      "Epoch 25249 \t\t Training Loss: 0.000580849009566009 \t\n",
      "Epoch 25250 \t\t Training Loss: 0.000580849009566009 \t\n",
      "Epoch 25251 \t\t Training Loss: 0.000580849009566009 \t\n",
      "Epoch 25252 \t\t Training Loss: 0.000580849009566009 \t\n",
      "Epoch 25253 \t\t Training Loss: 0.000580849009566009 \t\n",
      "Epoch 25254 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25255 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25256 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25257 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25258 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25259 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25260 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25261 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25262 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25263 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25264 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25265 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25266 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25267 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25268 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25269 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25270 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25271 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25272 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25273 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25274 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25275 \t\t Training Loss: 0.0005808489513583481 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25276 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25277 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25278 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25279 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25280 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25281 \t\t Training Loss: 0.0005808488931506872 \t\n",
      "Epoch 25282 \t\t Training Loss: 0.0005808489513583481 \t\n",
      "Epoch 25283 \t\t Training Loss: 0.0005808488349430263 \t\n",
      "Epoch 25284 \t\t Training Loss: 0.0005808488349430263 \t\n",
      "Epoch 25285 \t\t Training Loss: 0.0005808488349430263 \t\n",
      "Epoch 25286 \t\t Training Loss: 0.0005808488349430263 \t\n",
      "Epoch 25287 \t\t Training Loss: 0.0005808488349430263 \t\n",
      "Epoch 25288 \t\t Training Loss: 0.0005808488349430263 \t\n",
      "Epoch 25289 \t\t Training Loss: 0.0005808488349430263 \t\n",
      "Epoch 25290 \t\t Training Loss: 0.0005808488349430263 \t\n",
      "Epoch 25291 \t\t Training Loss: 0.0005808488349430263 \t\n",
      "Epoch 25292 \t\t Training Loss: 0.0005808487185277045 \t\n",
      "Epoch 25293 \t\t Training Loss: 0.0005808487767353654 \t\n",
      "Epoch 25294 \t\t Training Loss: 0.0005808487185277045 \t\n",
      "Epoch 25295 \t\t Training Loss: 0.0005808487185277045 \t\n",
      "Epoch 25296 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25297 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25298 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25299 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25300 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25301 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25302 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25303 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25304 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25305 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25306 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25307 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25308 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25309 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25310 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25311 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25312 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25313 \t\t Training Loss: 0.0005808486603200436 \t\n",
      "Epoch 25314 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25315 \t\t Training Loss: 0.0005808487185277045 \t\n",
      "Epoch 25316 \t\t Training Loss: 0.0005808487185277045 \t\n",
      "Epoch 25317 \t\t Training Loss: 0.0005808487185277045 \t\n",
      "Epoch 25318 \t\t Training Loss: 0.0005808487185277045 \t\n",
      "Epoch 25319 \t\t Training Loss: 0.0005808487185277045 \t\n",
      "Epoch 25320 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25321 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25322 \t\t Training Loss: 0.0005808487185277045 \t\n",
      "Epoch 25323 \t\t Training Loss: 0.0005808487185277045 \t\n",
      "Epoch 25324 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25325 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25326 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25327 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25328 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25329 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25330 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25331 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25332 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25333 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25334 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25335 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25336 \t\t Training Loss: 0.0005808486021123827 \t\n",
      "Epoch 25337 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25338 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25339 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25340 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25341 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25342 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25343 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25344 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25345 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25346 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25347 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25348 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25349 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25350 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25351 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25352 \t\t Training Loss: 0.0005808484856970608 \t\n",
      "Epoch 25353 \t\t Training Loss: 0.0005808484856970608 \t\n",
      "Epoch 25354 \t\t Training Loss: 0.0005808484856970608 \t\n",
      "Epoch 25355 \t\t Training Loss: 0.0005808484856970608 \t\n",
      "Epoch 25356 \t\t Training Loss: 0.0005808484856970608 \t\n",
      "Epoch 25357 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25358 \t\t Training Loss: 0.0005808484856970608 \t\n",
      "Epoch 25359 \t\t Training Loss: 0.0005808484856970608 \t\n",
      "Epoch 25360 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25361 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25362 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25363 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25364 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25365 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25366 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25367 \t\t Training Loss: 0.0005808484856970608 \t\n",
      "Epoch 25368 \t\t Training Loss: 0.0005808485439047217 \t\n",
      "Epoch 25369 \t\t Training Loss: 0.0005808484856970608 \t\n",
      "Epoch 25370 \t\t Training Loss: 0.0005808484856970608 \t\n",
      "Epoch 25371 \t\t Training Loss: 0.0005808484856970608 \t\n",
      "Epoch 25372 \t\t Training Loss: 0.0005808484856970608 \t\n",
      "Epoch 25373 \t\t Training Loss: 0.0005808484856970608 \t\n",
      "Epoch 25374 \t\t Training Loss: 0.0005808484274893999 \t\n",
      "Epoch 25375 \t\t Training Loss: 0.0005808484274893999 \t\n",
      "Epoch 25376 \t\t Training Loss: 0.0005808484274893999 \t\n",
      "Epoch 25377 \t\t Training Loss: 0.0005808484274893999 \t\n",
      "Epoch 25378 \t\t Training Loss: 0.0005808484274893999 \t\n",
      "Epoch 25379 \t\t Training Loss: 0.000580848369281739 \t\n",
      "Epoch 25380 \t\t Training Loss: 0.000580848369281739 \t\n",
      "Epoch 25381 \t\t Training Loss: 0.000580848369281739 \t\n",
      "Epoch 25382 \t\t Training Loss: 0.000580848369281739 \t\n",
      "Epoch 25383 \t\t Training Loss: 0.000580848369281739 \t\n",
      "Epoch 25384 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25385 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25386 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25387 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25388 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25389 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25390 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25391 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25392 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25393 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25394 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25395 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25396 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25397 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25398 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25399 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25400 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25401 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25402 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25403 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25404 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25405 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25406 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25407 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25408 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25409 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25410 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25411 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25412 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25413 \t\t Training Loss: 0.0005808483110740781 \t\n",
      "Epoch 25414 \t\t Training Loss: 0.0005808481364510953 \t\n",
      "Epoch 25415 \t\t Training Loss: 0.0005808481364510953 \t\n",
      "Epoch 25416 \t\t Training Loss: 0.0005808481946587563 \t\n",
      "Epoch 25417 \t\t Training Loss: 0.0005808481946587563 \t\n",
      "Epoch 25418 \t\t Training Loss: 0.0005808481364510953 \t\n",
      "Epoch 25419 \t\t Training Loss: 0.0005808481946587563 \t\n",
      "Epoch 25420 \t\t Training Loss: 0.0005808481946587563 \t\n",
      "Epoch 25421 \t\t Training Loss: 0.0005808481946587563 \t\n",
      "Epoch 25422 \t\t Training Loss: 0.0005808481364510953 \t\n",
      "Epoch 25423 \t\t Training Loss: 0.0005808481364510953 \t\n",
      "Epoch 25424 \t\t Training Loss: 0.0005808481364510953 \t\n",
      "Epoch 25425 \t\t Training Loss: 0.0005808481364510953 \t\n",
      "Epoch 25426 \t\t Training Loss: 0.0005808481364510953 \t\n",
      "Epoch 25427 \t\t Training Loss: 0.0005808481364510953 \t\n",
      "Epoch 25428 \t\t Training Loss: 0.0005808481364510953 \t\n",
      "Epoch 25429 \t\t Training Loss: 0.0005808481364510953 \t\n",
      "Epoch 25430 \t\t Training Loss: 0.0005808480782434344 \t\n",
      "Epoch 25431 \t\t Training Loss: 0.0005808480782434344 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25432 \t\t Training Loss: 0.0005808480782434344 \t\n",
      "Epoch 25433 \t\t Training Loss: 0.0005808480782434344 \t\n",
      "Epoch 25434 \t\t Training Loss: 0.0005808480782434344 \t\n",
      "Epoch 25435 \t\t Training Loss: 0.0005808480782434344 \t\n",
      "Epoch 25436 \t\t Training Loss: 0.0005808480782434344 \t\n",
      "Epoch 25437 \t\t Training Loss: 0.0005808480782434344 \t\n",
      "Epoch 25438 \t\t Training Loss: 0.0005808480782434344 \t\n",
      "Epoch 25439 \t\t Training Loss: 0.0005808480782434344 \t\n",
      "Epoch 25440 \t\t Training Loss: 0.0005808480782434344 \t\n",
      "Epoch 25441 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25442 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25443 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25444 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25445 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25446 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25447 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25448 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25449 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25450 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25451 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25452 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25453 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25454 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25455 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25456 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25457 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25458 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25459 \t\t Training Loss: 0.0005808479036204517 \t\n",
      "Epoch 25460 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25461 \t\t Training Loss: 0.0005808479036204517 \t\n",
      "Epoch 25462 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25463 \t\t Training Loss: 0.0005808480200357735 \t\n",
      "Epoch 25464 \t\t Training Loss: 0.0005808479036204517 \t\n",
      "Epoch 25465 \t\t Training Loss: 0.0005808479036204517 \t\n",
      "Epoch 25466 \t\t Training Loss: 0.0005808479036204517 \t\n",
      "Epoch 25467 \t\t Training Loss: 0.0005808479036204517 \t\n",
      "Epoch 25468 \t\t Training Loss: 0.0005808479036204517 \t\n",
      "Epoch 25469 \t\t Training Loss: 0.0005808479036204517 \t\n",
      "Epoch 25470 \t\t Training Loss: 0.0005808479036204517 \t\n",
      "Epoch 25471 \t\t Training Loss: 0.0005808479036204517 \t\n",
      "Epoch 25472 \t\t Training Loss: 0.0005808479036204517 \t\n",
      "Epoch 25473 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25474 \t\t Training Loss: 0.0005808479036204517 \t\n",
      "Epoch 25475 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25476 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25477 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25478 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25479 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25480 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25481 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25482 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25483 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25484 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25485 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25486 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25487 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25488 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25489 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25490 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25491 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25492 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25493 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25494 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25495 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25496 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25497 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25498 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25499 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25500 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25501 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25502 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25503 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25504 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25505 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25506 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25507 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25508 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25509 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25510 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25511 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25512 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25513 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25514 \t\t Training Loss: 0.0005808477872051299 \t\n",
      "Epoch 25515 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25516 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25517 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25518 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25519 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25520 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25521 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25522 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25523 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25524 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25525 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25526 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25527 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25528 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25529 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25530 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25531 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25532 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25533 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25534 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25535 \t\t Training Loss: 0.000580847728997469 \t\n",
      "Epoch 25536 \t\t Training Loss: 0.0005808476125821471 \t\n",
      "Epoch 25537 \t\t Training Loss: 0.0005808476125821471 \t\n",
      "Epoch 25538 \t\t Training Loss: 0.0005808474961668253 \t\n",
      "Epoch 25539 \t\t Training Loss: 0.0005808474961668253 \t\n",
      "Epoch 25540 \t\t Training Loss: 0.0005808476125821471 \t\n",
      "Epoch 25541 \t\t Training Loss: 0.0005808474961668253 \t\n",
      "Epoch 25542 \t\t Training Loss: 0.0005808474961668253 \t\n",
      "Epoch 25543 \t\t Training Loss: 0.0005808474961668253 \t\n",
      "Epoch 25544 \t\t Training Loss: 0.0005808474961668253 \t\n",
      "Epoch 25545 \t\t Training Loss: 0.0005808474961668253 \t\n",
      "Epoch 25546 \t\t Training Loss: 0.0005808474961668253 \t\n",
      "Epoch 25547 \t\t Training Loss: 0.0005808474961668253 \t\n",
      "Epoch 25548 \t\t Training Loss: 0.0005808474961668253 \t\n",
      "Epoch 25549 \t\t Training Loss: 0.0005808474961668253 \t\n",
      "Epoch 25550 \t\t Training Loss: 0.0005808474961668253 \t\n",
      "Epoch 25551 \t\t Training Loss: 0.0005808474961668253 \t\n",
      "Epoch 25552 \t\t Training Loss: 0.0005808474961668253 \t\n",
      "Epoch 25553 \t\t Training Loss: 0.0005808474961668253 \t\n",
      "Epoch 25554 \t\t Training Loss: 0.0005808474961668253 \t\n",
      "Epoch 25555 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25556 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25557 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25558 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25559 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25560 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25561 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25562 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25563 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25564 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25565 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25566 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25567 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25568 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25569 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25570 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25571 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25572 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25573 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25574 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25575 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25576 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25577 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25578 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25579 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25580 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25581 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25582 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25583 \t\t Training Loss: 0.0005808473797515035 \t\n",
      "Epoch 25584 \t\t Training Loss: 0.0005808473797515035 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25585 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25586 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25587 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25588 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25589 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25590 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25591 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25592 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25593 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25594 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25595 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25596 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25597 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25598 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25599 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25600 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25601 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25602 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25603 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25604 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25605 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25606 \t\t Training Loss: 0.0005808473215438426 \t\n",
      "Epoch 25607 \t\t Training Loss: 0.0005808472051285207 \t\n",
      "Epoch 25608 \t\t Training Loss: 0.0005808472051285207 \t\n",
      "Epoch 25609 \t\t Training Loss: 0.0005808472051285207 \t\n",
      "Epoch 25610 \t\t Training Loss: 0.0005808472051285207 \t\n",
      "Epoch 25611 \t\t Training Loss: 0.0005808472051285207 \t\n",
      "Epoch 25612 \t\t Training Loss: 0.0005808472051285207 \t\n",
      "Epoch 25613 \t\t Training Loss: 0.0005808472051285207 \t\n",
      "Epoch 25614 \t\t Training Loss: 0.0005808472051285207 \t\n",
      "Epoch 25615 \t\t Training Loss: 0.0005808470887131989 \t\n",
      "Epoch 25616 \t\t Training Loss: 0.0005808470887131989 \t\n",
      "Epoch 25617 \t\t Training Loss: 0.0005808470887131989 \t\n",
      "Epoch 25618 \t\t Training Loss: 0.0005808470887131989 \t\n",
      "Epoch 25619 \t\t Training Loss: 0.0005808470887131989 \t\n",
      "Epoch 25620 \t\t Training Loss: 0.0005808470887131989 \t\n",
      "Epoch 25621 \t\t Training Loss: 0.0005808470887131989 \t\n",
      "Epoch 25622 \t\t Training Loss: 0.0005808470887131989 \t\n",
      "Epoch 25623 \t\t Training Loss: 0.0005808470887131989 \t\n",
      "Epoch 25624 \t\t Training Loss: 0.0005808470887131989 \t\n",
      "Epoch 25625 \t\t Training Loss: 0.000580847030505538 \t\n",
      "Epoch 25626 \t\t Training Loss: 0.000580847030505538 \t\n",
      "Epoch 25627 \t\t Training Loss: 0.0005808470887131989 \t\n",
      "Epoch 25628 \t\t Training Loss: 0.0005808470887131989 \t\n",
      "Epoch 25629 \t\t Training Loss: 0.0005808470887131989 \t\n",
      "Epoch 25630 \t\t Training Loss: 0.000580847030505538 \t\n",
      "Epoch 25631 \t\t Training Loss: 0.000580847030505538 \t\n",
      "Epoch 25632 \t\t Training Loss: 0.000580847030505538 \t\n",
      "Epoch 25633 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25634 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25635 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25636 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25637 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25638 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25639 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25640 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25641 \t\t Training Loss: 0.000580847030505538 \t\n",
      "Epoch 25642 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25643 \t\t Training Loss: 0.000580847030505538 \t\n",
      "Epoch 25644 \t\t Training Loss: 0.000580847030505538 \t\n",
      "Epoch 25645 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25646 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25647 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25648 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25649 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25650 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25651 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25652 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25653 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25654 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25655 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25656 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25657 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25658 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25659 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25660 \t\t Training Loss: 0.0005808469140902162 \t\n",
      "Epoch 25661 \t\t Training Loss: 0.0005808467976748943 \t\n",
      "Epoch 25662 \t\t Training Loss: 0.0005808467976748943 \t\n",
      "Epoch 25663 \t\t Training Loss: 0.0005808467976748943 \t\n",
      "Epoch 25664 \t\t Training Loss: 0.0005808467976748943 \t\n",
      "Epoch 25665 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25666 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25667 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25668 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25669 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25670 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25671 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25672 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25673 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25674 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25675 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25676 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25677 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25678 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25679 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25680 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25681 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25682 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25683 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25684 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25685 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25686 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25687 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25688 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25689 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25690 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25691 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25692 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25693 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25694 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25695 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25696 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25697 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25698 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25699 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25700 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25701 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25702 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25703 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25704 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25705 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25706 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25707 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25708 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25709 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25710 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25711 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25712 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25713 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25714 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25715 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25716 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25717 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25718 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25719 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25720 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25721 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25722 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25723 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25724 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25725 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25726 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25727 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25728 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25729 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25730 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25731 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25732 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25733 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25734 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25735 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25736 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25737 \t\t Training Loss: 0.0005808467394672334 \t\n",
      "Epoch 25738 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25739 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25740 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25741 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25742 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25743 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25744 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25745 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25746 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25747 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25748 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25749 \t\t Training Loss: 0.0005808466230519116 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25750 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25751 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25752 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25753 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25754 \t\t Training Loss: 0.0005808465066365898 \t\n",
      "Epoch 25755 \t\t Training Loss: 0.0005808465066365898 \t\n",
      "Epoch 25756 \t\t Training Loss: 0.0005808465066365898 \t\n",
      "Epoch 25757 \t\t Training Loss: 0.0005808465066365898 \t\n",
      "Epoch 25758 \t\t Training Loss: 0.0005808466230519116 \t\n",
      "Epoch 25759 \t\t Training Loss: 0.0005808465066365898 \t\n",
      "Epoch 25760 \t\t Training Loss: 0.0005808465066365898 \t\n",
      "Epoch 25761 \t\t Training Loss: 0.0005808465066365898 \t\n",
      "Epoch 25762 \t\t Training Loss: 0.0005808465066365898 \t\n",
      "Epoch 25763 \t\t Training Loss: 0.0005808465066365898 \t\n",
      "Epoch 25764 \t\t Training Loss: 0.0005808465066365898 \t\n",
      "Epoch 25765 \t\t Training Loss: 0.0005808465066365898 \t\n",
      "Epoch 25766 \t\t Training Loss: 0.0005808464484289289 \t\n",
      "Epoch 25767 \t\t Training Loss: 0.0005808464484289289 \t\n",
      "Epoch 25768 \t\t Training Loss: 0.0005808464484289289 \t\n",
      "Epoch 25769 \t\t Training Loss: 0.0005808464484289289 \t\n",
      "Epoch 25770 \t\t Training Loss: 0.0005808464484289289 \t\n",
      "Epoch 25771 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25772 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25773 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25774 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25775 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25776 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25777 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25778 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25779 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25780 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25781 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25782 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25783 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25784 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25785 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25786 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25787 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25788 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25789 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25790 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25791 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25792 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25793 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25794 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25795 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25796 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25797 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25798 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25799 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25800 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25801 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25802 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25803 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25804 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25805 \t\t Training Loss: 0.000580846332013607 \t\n",
      "Epoch 25806 \t\t Training Loss: 0.0005808462155982852 \t\n",
      "Epoch 25807 \t\t Training Loss: 0.0005808462155982852 \t\n",
      "Epoch 25808 \t\t Training Loss: 0.0005808462155982852 \t\n",
      "Epoch 25809 \t\t Training Loss: 0.0005808462155982852 \t\n",
      "Epoch 25810 \t\t Training Loss: 0.0005808462155982852 \t\n",
      "Epoch 25811 \t\t Training Loss: 0.0005808462155982852 \t\n",
      "Epoch 25812 \t\t Training Loss: 0.0005808462155982852 \t\n",
      "Epoch 25813 \t\t Training Loss: 0.0005808462155982852 \t\n",
      "Epoch 25814 \t\t Training Loss: 0.0005808462155982852 \t\n",
      "Epoch 25815 \t\t Training Loss: 0.0005808462155982852 \t\n",
      "Epoch 25816 \t\t Training Loss: 0.0005808462155982852 \t\n",
      "Epoch 25817 \t\t Training Loss: 0.0005808462155982852 \t\n",
      "Epoch 25818 \t\t Training Loss: 0.0005808462155982852 \t\n",
      "Epoch 25819 \t\t Training Loss: 0.0005808461573906243 \t\n",
      "Epoch 25820 \t\t Training Loss: 0.0005808461573906243 \t\n",
      "Epoch 25821 \t\t Training Loss: 0.0005808461573906243 \t\n",
      "Epoch 25822 \t\t Training Loss: 0.0005808461573906243 \t\n",
      "Epoch 25823 \t\t Training Loss: 0.0005808461573906243 \t\n",
      "Epoch 25824 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25825 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25826 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25827 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25828 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25829 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25830 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25831 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25832 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25833 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25834 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25835 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25836 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25837 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25838 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25839 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25840 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25841 \t\t Training Loss: 0.0005808460991829634 \t\n",
      "Epoch 25842 \t\t Training Loss: 0.0005808459245599806 \t\n",
      "Epoch 25843 \t\t Training Loss: 0.0005808459245599806 \t\n",
      "Epoch 25844 \t\t Training Loss: 0.0005808459245599806 \t\n",
      "Epoch 25845 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25846 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25847 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25848 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25849 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25850 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25851 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25852 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25853 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25854 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25855 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25856 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25857 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25858 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25859 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25860 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25861 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25862 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25863 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25864 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25865 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25866 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25867 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25868 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25869 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25870 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25871 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25872 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25873 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25874 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25875 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25876 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25877 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25878 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25879 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25880 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25881 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25882 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25883 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25884 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25885 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25886 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25887 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25888 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25889 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25890 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25891 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25892 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25893 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25894 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25895 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25896 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25897 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25898 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25899 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25900 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25901 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25902 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25903 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25904 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25905 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25906 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25907 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25908 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25909 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25910 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25911 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25912 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25913 \t\t Training Loss: 0.0005808458081446588 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25914 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25915 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25916 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25917 \t\t Training Loss: 0.0005808458081446588 \t\n",
      "Epoch 25918 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25919 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25920 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25921 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25922 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25923 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25924 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25925 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25926 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25927 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25928 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25929 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25930 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25931 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25932 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25933 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25934 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25935 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25936 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25937 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25938 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25939 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25940 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25941 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25942 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25943 \t\t Training Loss: 0.0005808457499369979 \t\n",
      "Epoch 25944 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25945 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25946 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25947 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25948 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25949 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25950 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25951 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25952 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25953 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25954 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25955 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25956 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25957 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25958 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25959 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25960 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25961 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25962 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25963 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25964 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25965 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25966 \t\t Training Loss: 0.0005808456335216761 \t\n",
      "Epoch 25967 \t\t Training Loss: 0.0005808455171063542 \t\n",
      "Epoch 25968 \t\t Training Loss: 0.0005808455171063542 \t\n",
      "Epoch 25969 \t\t Training Loss: 0.0005808455171063542 \t\n",
      "Epoch 25970 \t\t Training Loss: 0.0005808455171063542 \t\n",
      "Epoch 25971 \t\t Training Loss: 0.0005808455171063542 \t\n",
      "Epoch 25972 \t\t Training Loss: 0.0005808454006910324 \t\n",
      "Epoch 25973 \t\t Training Loss: 0.0005808454006910324 \t\n",
      "Epoch 25974 \t\t Training Loss: 0.0005808454006910324 \t\n",
      "Epoch 25975 \t\t Training Loss: 0.0005808454006910324 \t\n",
      "Epoch 25976 \t\t Training Loss: 0.0005808454006910324 \t\n",
      "Epoch 25977 \t\t Training Loss: 0.0005808454006910324 \t\n",
      "Epoch 25978 \t\t Training Loss: 0.0005808453424833715 \t\n",
      "Epoch 25979 \t\t Training Loss: 0.0005808453424833715 \t\n",
      "Epoch 25980 \t\t Training Loss: 0.0005808453424833715 \t\n",
      "Epoch 25981 \t\t Training Loss: 0.0005808453424833715 \t\n",
      "Epoch 25982 \t\t Training Loss: 0.0005808453424833715 \t\n",
      "Epoch 25983 \t\t Training Loss: 0.0005808453424833715 \t\n",
      "Epoch 25984 \t\t Training Loss: 0.0005808453424833715 \t\n",
      "Epoch 25985 \t\t Training Loss: 0.0005808453424833715 \t\n",
      "Epoch 25986 \t\t Training Loss: 0.0005808453424833715 \t\n",
      "Epoch 25987 \t\t Training Loss: 0.0005808453424833715 \t\n",
      "Epoch 25988 \t\t Training Loss: 0.0005808453424833715 \t\n",
      "Epoch 25989 \t\t Training Loss: 0.0005808453424833715 \t\n",
      "Epoch 25990 \t\t Training Loss: 0.0005808453424833715 \t\n",
      "Epoch 25991 \t\t Training Loss: 0.0005808452260680497 \t\n",
      "Epoch 25992 \t\t Training Loss: 0.0005808452260680497 \t\n",
      "Epoch 25993 \t\t Training Loss: 0.0005808452260680497 \t\n",
      "Epoch 25994 \t\t Training Loss: 0.0005808452260680497 \t\n",
      "Epoch 25995 \t\t Training Loss: 0.0005808452260680497 \t\n",
      "Epoch 25996 \t\t Training Loss: 0.0005808452260680497 \t\n",
      "Epoch 25997 \t\t Training Loss: 0.0005808452260680497 \t\n",
      "Epoch 25998 \t\t Training Loss: 0.0005808452260680497 \t\n",
      "Epoch 25999 \t\t Training Loss: 0.0005808452260680497 \t\n",
      "Epoch 26000 \t\t Training Loss: 0.0005808452260680497 \t\n",
      "Epoch 26001 \t\t Training Loss: 0.0005808451678603888 \t\n",
      "Epoch 26002 \t\t Training Loss: 0.0005808452260680497 \t\n",
      "Epoch 26003 \t\t Training Loss: 0.0005808451678603888 \t\n",
      "Epoch 26004 \t\t Training Loss: 0.0005808451678603888 \t\n",
      "Epoch 26005 \t\t Training Loss: 0.0005808451096527278 \t\n",
      "Epoch 26006 \t\t Training Loss: 0.0005808451096527278 \t\n",
      "Epoch 26007 \t\t Training Loss: 0.0005808451096527278 \t\n",
      "Epoch 26008 \t\t Training Loss: 0.0005808451096527278 \t\n",
      "Epoch 26009 \t\t Training Loss: 0.0005808451096527278 \t\n",
      "Epoch 26010 \t\t Training Loss: 0.0005808451096527278 \t\n",
      "Epoch 26011 \t\t Training Loss: 0.0005808451096527278 \t\n",
      "Epoch 26012 \t\t Training Loss: 0.0005808450514450669 \t\n",
      "Epoch 26013 \t\t Training Loss: 0.0005808451096527278 \t\n",
      "Epoch 26014 \t\t Training Loss: 0.0005808451096527278 \t\n",
      "Epoch 26015 \t\t Training Loss: 0.0005808451096527278 \t\n",
      "Epoch 26016 \t\t Training Loss: 0.0005808450514450669 \t\n",
      "Epoch 26017 \t\t Training Loss: 0.0005808450514450669 \t\n",
      "Epoch 26018 \t\t Training Loss: 0.0005808450514450669 \t\n",
      "Epoch 26019 \t\t Training Loss: 0.0005808450514450669 \t\n",
      "Epoch 26020 \t\t Training Loss: 0.0005808450514450669 \t\n",
      "Epoch 26021 \t\t Training Loss: 0.0005808450514450669 \t\n",
      "Epoch 26022 \t\t Training Loss: 0.0005808450514450669 \t\n",
      "Epoch 26023 \t\t Training Loss: 0.0005808450514450669 \t\n",
      "Epoch 26024 \t\t Training Loss: 0.0005808450514450669 \t\n",
      "Epoch 26025 \t\t Training Loss: 0.0005808450514450669 \t\n",
      "Epoch 26026 \t\t Training Loss: 0.0005808450514450669 \t\n",
      "Epoch 26027 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26028 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26029 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26030 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26031 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26032 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26033 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26034 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26035 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26036 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26037 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26038 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26039 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26040 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26041 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26042 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26043 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26044 \t\t Training Loss: 0.0005808449350297451 \t\n",
      "Epoch 26045 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26046 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26047 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26048 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26049 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26050 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26051 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26052 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26053 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26054 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26055 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26056 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26057 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26058 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26059 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26060 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26061 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26062 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26063 \t\t Training Loss: 0.0005808448186144233 \t\n",
      "Epoch 26064 \t\t Training Loss: 0.0005808447604067624 \t\n",
      "Epoch 26065 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26066 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26067 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26068 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26069 \t\t Training Loss: 0.0005808447021991014 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26070 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26071 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26072 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26073 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26074 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26075 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26076 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26077 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26078 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26079 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26080 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26081 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26082 \t\t Training Loss: 0.0005808447021991014 \t\n",
      "Epoch 26083 \t\t Training Loss: 0.0005808446439914405 \t\n",
      "Epoch 26084 \t\t Training Loss: 0.0005808446439914405 \t\n",
      "Epoch 26085 \t\t Training Loss: 0.0005808446439914405 \t\n",
      "Epoch 26086 \t\t Training Loss: 0.0005808446439914405 \t\n",
      "Epoch 26087 \t\t Training Loss: 0.0005808446439914405 \t\n",
      "Epoch 26088 \t\t Training Loss: 0.0005808446439914405 \t\n",
      "Epoch 26089 \t\t Training Loss: 0.0005808445857837796 \t\n",
      "Epoch 26090 \t\t Training Loss: 0.0005808445857837796 \t\n",
      "Epoch 26091 \t\t Training Loss: 0.0005808445857837796 \t\n",
      "Epoch 26092 \t\t Training Loss: 0.0005808445857837796 \t\n",
      "Epoch 26093 \t\t Training Loss: 0.0005808445857837796 \t\n",
      "Epoch 26094 \t\t Training Loss: 0.0005808445857837796 \t\n",
      "Epoch 26095 \t\t Training Loss: 0.0005808445857837796 \t\n",
      "Epoch 26096 \t\t Training Loss: 0.0005808445857837796 \t\n",
      "Epoch 26097 \t\t Training Loss: 0.0005808445857837796 \t\n",
      "Epoch 26098 \t\t Training Loss: 0.0005808445857837796 \t\n",
      "Epoch 26099 \t\t Training Loss: 0.0005808445857837796 \t\n",
      "Epoch 26100 \t\t Training Loss: 0.0005808445857837796 \t\n",
      "Epoch 26101 \t\t Training Loss: 0.0005808445857837796 \t\n",
      "Epoch 26102 \t\t Training Loss: 0.0005808445275761187 \t\n",
      "Epoch 26103 \t\t Training Loss: 0.0005808445275761187 \t\n",
      "Epoch 26104 \t\t Training Loss: 0.0005808445857837796 \t\n",
      "Epoch 26105 \t\t Training Loss: 0.0005808445275761187 \t\n",
      "Epoch 26106 \t\t Training Loss: 0.0005808444693684578 \t\n",
      "Epoch 26107 \t\t Training Loss: 0.0005808444693684578 \t\n",
      "Epoch 26108 \t\t Training Loss: 0.0005808444693684578 \t\n",
      "Epoch 26109 \t\t Training Loss: 0.0005808444693684578 \t\n",
      "Epoch 26110 \t\t Training Loss: 0.0005808444693684578 \t\n",
      "Epoch 26111 \t\t Training Loss: 0.0005808445275761187 \t\n",
      "Epoch 26112 \t\t Training Loss: 0.0005808444693684578 \t\n",
      "Epoch 26113 \t\t Training Loss: 0.0005808444693684578 \t\n",
      "Epoch 26114 \t\t Training Loss: 0.0005808444693684578 \t\n",
      "Epoch 26115 \t\t Training Loss: 0.0005808444693684578 \t\n",
      "Epoch 26116 \t\t Training Loss: 0.000580844352953136 \t\n",
      "Epoch 26117 \t\t Training Loss: 0.0005808444693684578 \t\n",
      "Epoch 26118 \t\t Training Loss: 0.0005808444111607969 \t\n",
      "Epoch 26119 \t\t Training Loss: 0.0005808444693684578 \t\n",
      "Epoch 26120 \t\t Training Loss: 0.000580844352953136 \t\n",
      "Epoch 26121 \t\t Training Loss: 0.000580844352953136 \t\n",
      "Epoch 26122 \t\t Training Loss: 0.000580844352953136 \t\n",
      "Epoch 26123 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26124 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26125 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26126 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26127 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26128 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26129 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26130 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26131 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26132 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26133 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26134 \t\t Training Loss: 0.000580844352953136 \t\n",
      "Epoch 26135 \t\t Training Loss: 0.000580844352953136 \t\n",
      "Epoch 26136 \t\t Training Loss: 0.000580844352953136 \t\n",
      "Epoch 26137 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26138 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26139 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26140 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26141 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26142 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26143 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26144 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26145 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26146 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26147 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26148 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26149 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26150 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26151 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26152 \t\t Training Loss: 0.0005808442365378141 \t\n",
      "Epoch 26153 \t\t Training Loss: 0.0005808442365378141 \t\n",
      "Epoch 26154 \t\t Training Loss: 0.0005808442365378141 \t\n",
      "Epoch 26155 \t\t Training Loss: 0.0005808442365378141 \t\n",
      "Epoch 26156 \t\t Training Loss: 0.0005808442365378141 \t\n",
      "Epoch 26157 \t\t Training Loss: 0.0005808442365378141 \t\n",
      "Epoch 26158 \t\t Training Loss: 0.0005808442365378141 \t\n",
      "Epoch 26159 \t\t Training Loss: 0.0005808442365378141 \t\n",
      "Epoch 26160 \t\t Training Loss: 0.0005808442365378141 \t\n",
      "Epoch 26161 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26162 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26163 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26164 \t\t Training Loss: 0.000580844294745475 \t\n",
      "Epoch 26165 \t\t Training Loss: 0.0005808442365378141 \t\n",
      "Epoch 26166 \t\t Training Loss: 0.0005808442365378141 \t\n",
      "Epoch 26167 \t\t Training Loss: 0.0005808442365378141 \t\n",
      "Epoch 26168 \t\t Training Loss: 0.0005808442365378141 \t\n",
      "Epoch 26169 \t\t Training Loss: 0.0005808442365378141 \t\n",
      "Epoch 26170 \t\t Training Loss: 0.0005808441783301532 \t\n",
      "Epoch 26171 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26172 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26173 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26174 \t\t Training Loss: 0.0005808441783301532 \t\n",
      "Epoch 26175 \t\t Training Loss: 0.0005808441783301532 \t\n",
      "Epoch 26176 \t\t Training Loss: 0.0005808441783301532 \t\n",
      "Epoch 26177 \t\t Training Loss: 0.0005808441783301532 \t\n",
      "Epoch 26178 \t\t Training Loss: 0.0005808441783301532 \t\n",
      "Epoch 26179 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26180 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26181 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26182 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26183 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26184 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26185 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26186 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26187 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26188 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26189 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26190 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26191 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26192 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26193 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26194 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26195 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26196 \t\t Training Loss: 0.0005808441783301532 \t\n",
      "Epoch 26197 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26198 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26199 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26200 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26201 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26202 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26203 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26204 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26205 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26206 \t\t Training Loss: 0.0005808441201224923 \t\n",
      "Epoch 26207 \t\t Training Loss: 0.0005808440619148314 \t\n",
      "Epoch 26208 \t\t Training Loss: 0.0005808440619148314 \t\n",
      "Epoch 26209 \t\t Training Loss: 0.0005808440619148314 \t\n",
      "Epoch 26210 \t\t Training Loss: 0.0005808440619148314 \t\n",
      "Epoch 26211 \t\t Training Loss: 0.0005808440619148314 \t\n",
      "Epoch 26212 \t\t Training Loss: 0.0005808440619148314 \t\n",
      "Epoch 26213 \t\t Training Loss: 0.0005808440619148314 \t\n",
      "Epoch 26214 \t\t Training Loss: 0.0005808440619148314 \t\n",
      "Epoch 26215 \t\t Training Loss: 0.0005808440619148314 \t\n",
      "Epoch 26216 \t\t Training Loss: 0.0005808440619148314 \t\n",
      "Epoch 26217 \t\t Training Loss: 0.0005808440619148314 \t\n",
      "Epoch 26218 \t\t Training Loss: 0.0005808440037071705 \t\n",
      "Epoch 26219 \t\t Training Loss: 0.0005808440037071705 \t\n",
      "Epoch 26220 \t\t Training Loss: 0.0005808439454995096 \t\n",
      "Epoch 26221 \t\t Training Loss: 0.0005808440037071705 \t\n",
      "Epoch 26222 \t\t Training Loss: 0.0005808440037071705 \t\n",
      "Epoch 26223 \t\t Training Loss: 0.0005808439454995096 \t\n",
      "Epoch 26224 \t\t Training Loss: 0.0005808439454995096 \t\n",
      "Epoch 26225 \t\t Training Loss: 0.0005808439454995096 \t\n",
      "Epoch 26226 \t\t Training Loss: 0.0005808439454995096 \t\n",
      "Epoch 26227 \t\t Training Loss: 0.0005808438872918487 \t\n",
      "Epoch 26228 \t\t Training Loss: 0.0005808438872918487 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26229 \t\t Training Loss: 0.0005808438872918487 \t\n",
      "Epoch 26230 \t\t Training Loss: 0.0005808438872918487 \t\n",
      "Epoch 26231 \t\t Training Loss: 0.0005808438872918487 \t\n",
      "Epoch 26232 \t\t Training Loss: 0.0005808438872918487 \t\n",
      "Epoch 26233 \t\t Training Loss: 0.0005808438872918487 \t\n",
      "Epoch 26234 \t\t Training Loss: 0.0005808439454995096 \t\n",
      "Epoch 26235 \t\t Training Loss: 0.0005808439454995096 \t\n",
      "Epoch 26236 \t\t Training Loss: 0.0005808438872918487 \t\n",
      "Epoch 26237 \t\t Training Loss: 0.0005808438872918487 \t\n",
      "Epoch 26238 \t\t Training Loss: 0.0005808438872918487 \t\n",
      "Epoch 26239 \t\t Training Loss: 0.0005808438872918487 \t\n",
      "Epoch 26240 \t\t Training Loss: 0.0005808438290841877 \t\n",
      "Epoch 26241 \t\t Training Loss: 0.0005808438290841877 \t\n",
      "Epoch 26242 \t\t Training Loss: 0.0005808438290841877 \t\n",
      "Epoch 26243 \t\t Training Loss: 0.0005808437708765268 \t\n",
      "Epoch 26244 \t\t Training Loss: 0.0005808437708765268 \t\n",
      "Epoch 26245 \t\t Training Loss: 0.0005808437708765268 \t\n",
      "Epoch 26246 \t\t Training Loss: 0.0005808437708765268 \t\n",
      "Epoch 26247 \t\t Training Loss: 0.0005808437708765268 \t\n",
      "Epoch 26248 \t\t Training Loss: 0.0005808437708765268 \t\n",
      "Epoch 26249 \t\t Training Loss: 0.0005808437708765268 \t\n",
      "Epoch 26250 \t\t Training Loss: 0.0005808437708765268 \t\n",
      "Epoch 26251 \t\t Training Loss: 0.0005808437708765268 \t\n",
      "Epoch 26252 \t\t Training Loss: 0.0005808437708765268 \t\n",
      "Epoch 26253 \t\t Training Loss: 0.0005808437708765268 \t\n",
      "Epoch 26254 \t\t Training Loss: 0.0005808437708765268 \t\n",
      "Epoch 26255 \t\t Training Loss: 0.0005808437708765268 \t\n",
      "Epoch 26256 \t\t Training Loss: 0.0005808437708765268 \t\n",
      "Epoch 26257 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26258 \t\t Training Loss: 0.0005808437126688659 \t\n",
      "Epoch 26259 \t\t Training Loss: 0.0005808437708765268 \t\n",
      "Epoch 26260 \t\t Training Loss: 0.0005808437126688659 \t\n",
      "Epoch 26261 \t\t Training Loss: 0.0005808437126688659 \t\n",
      "Epoch 26262 \t\t Training Loss: 0.0005808437126688659 \t\n",
      "Epoch 26263 \t\t Training Loss: 0.0005808437126688659 \t\n",
      "Epoch 26264 \t\t Training Loss: 0.0005808437126688659 \t\n",
      "Epoch 26265 \t\t Training Loss: 0.0005808437126688659 \t\n",
      "Epoch 26266 \t\t Training Loss: 0.0005808437126688659 \t\n",
      "Epoch 26267 \t\t Training Loss: 0.0005808437126688659 \t\n",
      "Epoch 26268 \t\t Training Loss: 0.0005808437126688659 \t\n",
      "Epoch 26269 \t\t Training Loss: 0.0005808437126688659 \t\n",
      "Epoch 26270 \t\t Training Loss: 0.0005808437126688659 \t\n",
      "Epoch 26271 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26272 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26273 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26274 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26275 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26276 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26277 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26278 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26279 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26280 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26281 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26282 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26283 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26284 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26285 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26286 \t\t Training Loss: 0.0005808435380458832 \t\n",
      "Epoch 26287 \t\t Training Loss: 0.000580843654461205 \t\n",
      "Epoch 26288 \t\t Training Loss: 0.0005808435380458832 \t\n",
      "Epoch 26289 \t\t Training Loss: 0.0005808435380458832 \t\n",
      "Epoch 26290 \t\t Training Loss: 0.0005808435380458832 \t\n",
      "Epoch 26291 \t\t Training Loss: 0.0005808435380458832 \t\n",
      "Epoch 26292 \t\t Training Loss: 0.0005808435380458832 \t\n",
      "Epoch 26293 \t\t Training Loss: 0.0005808435380458832 \t\n",
      "Epoch 26294 \t\t Training Loss: 0.0005808435380458832 \t\n",
      "Epoch 26295 \t\t Training Loss: 0.0005808435380458832 \t\n",
      "Epoch 26296 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26297 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26298 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26299 \t\t Training Loss: 0.0005808435380458832 \t\n",
      "Epoch 26300 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26301 \t\t Training Loss: 0.0005808435380458832 \t\n",
      "Epoch 26302 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26303 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26304 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26305 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26306 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26307 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26308 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26309 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26310 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26311 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26312 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26313 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26314 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26315 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26316 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26317 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26318 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26319 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26320 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26321 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26322 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26323 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26324 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26325 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26326 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26327 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26328 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26329 \t\t Training Loss: 0.0005808434798382223 \t\n",
      "Epoch 26330 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26331 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26332 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26333 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26334 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26335 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26336 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26337 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26338 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26339 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26340 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26341 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26342 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26343 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26344 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26345 \t\t Training Loss: 0.0005808433052152395 \t\n",
      "Epoch 26346 \t\t Training Loss: 0.0005808433052152395 \t\n",
      "Epoch 26347 \t\t Training Loss: 0.0005808433052152395 \t\n",
      "Epoch 26348 \t\t Training Loss: 0.0005808433052152395 \t\n",
      "Epoch 26349 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26350 \t\t Training Loss: 0.0005808433052152395 \t\n",
      "Epoch 26351 \t\t Training Loss: 0.0005808434216305614 \t\n",
      "Epoch 26352 \t\t Training Loss: 0.0005808433052152395 \t\n",
      "Epoch 26353 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26354 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26355 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26356 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26357 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26358 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26359 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26360 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26361 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26362 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26363 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26364 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26365 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26366 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26367 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26368 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26369 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26370 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26371 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26372 \t\t Training Loss: 0.0005808432470075786 \t\n",
      "Epoch 26373 \t\t Training Loss: 0.0005808431887999177 \t\n",
      "Epoch 26374 \t\t Training Loss: 0.0005808431305922568 \t\n",
      "Epoch 26375 \t\t Training Loss: 0.0005808431305922568 \t\n",
      "Epoch 26376 \t\t Training Loss: 0.0005808431305922568 \t\n",
      "Epoch 26377 \t\t Training Loss: 0.0005808431305922568 \t\n",
      "Epoch 26378 \t\t Training Loss: 0.0005808430723845959 \t\n",
      "Epoch 26379 \t\t Training Loss: 0.0005808430723845959 \t\n",
      "Epoch 26380 \t\t Training Loss: 0.0005808431305922568 \t\n",
      "Epoch 26381 \t\t Training Loss: 0.0005808430723845959 \t\n",
      "Epoch 26382 \t\t Training Loss: 0.0005808430723845959 \t\n",
      "Epoch 26383 \t\t Training Loss: 0.0005808431305922568 \t\n",
      "Epoch 26384 \t\t Training Loss: 0.0005808431305922568 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26385 \t\t Training Loss: 0.0005808430723845959 \t\n",
      "Epoch 26386 \t\t Training Loss: 0.0005808430723845959 \t\n",
      "Epoch 26387 \t\t Training Loss: 0.0005808430723845959 \t\n",
      "Epoch 26388 \t\t Training Loss: 0.0005808430723845959 \t\n",
      "Epoch 26389 \t\t Training Loss: 0.0005808430723845959 \t\n",
      "Epoch 26390 \t\t Training Loss: 0.000580843014176935 \t\n",
      "Epoch 26391 \t\t Training Loss: 0.0005808430723845959 \t\n",
      "Epoch 26392 \t\t Training Loss: 0.0005808430723845959 \t\n",
      "Epoch 26393 \t\t Training Loss: 0.000580843014176935 \t\n",
      "Epoch 26394 \t\t Training Loss: 0.000580843014176935 \t\n",
      "Epoch 26395 \t\t Training Loss: 0.0005808430723845959 \t\n",
      "Epoch 26396 \t\t Training Loss: 0.000580843014176935 \t\n",
      "Epoch 26397 \t\t Training Loss: 0.000580843014176935 \t\n",
      "Epoch 26398 \t\t Training Loss: 0.000580843014176935 \t\n",
      "Epoch 26399 \t\t Training Loss: 0.000580843014176935 \t\n",
      "Epoch 26400 \t\t Training Loss: 0.000580842955969274 \t\n",
      "Epoch 26401 \t\t Training Loss: 0.000580842955969274 \t\n",
      "Epoch 26402 \t\t Training Loss: 0.000580842955969274 \t\n",
      "Epoch 26403 \t\t Training Loss: 0.000580843014176935 \t\n",
      "Epoch 26404 \t\t Training Loss: 0.000580843014176935 \t\n",
      "Epoch 26405 \t\t Training Loss: 0.000580842955969274 \t\n",
      "Epoch 26406 \t\t Training Loss: 0.000580842955969274 \t\n",
      "Epoch 26407 \t\t Training Loss: 0.000580842955969274 \t\n",
      "Epoch 26408 \t\t Training Loss: 0.000580842955969274 \t\n",
      "Epoch 26409 \t\t Training Loss: 0.000580842955969274 \t\n",
      "Epoch 26410 \t\t Training Loss: 0.0005808428977616131 \t\n",
      "Epoch 26411 \t\t Training Loss: 0.0005808428395539522 \t\n",
      "Epoch 26412 \t\t Training Loss: 0.0005808428395539522 \t\n",
      "Epoch 26413 \t\t Training Loss: 0.0005808428395539522 \t\n",
      "Epoch 26414 \t\t Training Loss: 0.0005808428977616131 \t\n",
      "Epoch 26415 \t\t Training Loss: 0.0005808428395539522 \t\n",
      "Epoch 26416 \t\t Training Loss: 0.0005808428395539522 \t\n",
      "Epoch 26417 \t\t Training Loss: 0.0005808428395539522 \t\n",
      "Epoch 26418 \t\t Training Loss: 0.0005808428395539522 \t\n",
      "Epoch 26419 \t\t Training Loss: 0.0005808428395539522 \t\n",
      "Epoch 26420 \t\t Training Loss: 0.0005808428395539522 \t\n",
      "Epoch 26421 \t\t Training Loss: 0.0005808428395539522 \t\n",
      "Epoch 26422 \t\t Training Loss: 0.0005808428395539522 \t\n",
      "Epoch 26423 \t\t Training Loss: 0.0005808428395539522 \t\n",
      "Epoch 26424 \t\t Training Loss: 0.0005808428395539522 \t\n",
      "Epoch 26425 \t\t Training Loss: 0.0005808427231386304 \t\n",
      "Epoch 26426 \t\t Training Loss: 0.0005808427231386304 \t\n",
      "Epoch 26427 \t\t Training Loss: 0.0005808427231386304 \t\n",
      "Epoch 26428 \t\t Training Loss: 0.0005808427231386304 \t\n",
      "Epoch 26429 \t\t Training Loss: 0.0005808426649309695 \t\n",
      "Epoch 26430 \t\t Training Loss: 0.0005808426649309695 \t\n",
      "Epoch 26431 \t\t Training Loss: 0.0005808426649309695 \t\n",
      "Epoch 26432 \t\t Training Loss: 0.0005808427231386304 \t\n",
      "Epoch 26433 \t\t Training Loss: 0.0005808427231386304 \t\n",
      "Epoch 26434 \t\t Training Loss: 0.0005808426649309695 \t\n",
      "Epoch 26435 \t\t Training Loss: 0.0005808426649309695 \t\n",
      "Epoch 26436 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26437 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26438 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26439 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26440 \t\t Training Loss: 0.0005808426649309695 \t\n",
      "Epoch 26441 \t\t Training Loss: 0.0005808426649309695 \t\n",
      "Epoch 26442 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26443 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26444 \t\t Training Loss: 0.0005808426649309695 \t\n",
      "Epoch 26445 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26446 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26447 \t\t Training Loss: 0.0005808426649309695 \t\n",
      "Epoch 26448 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26449 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26450 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26451 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26452 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26453 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26454 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26455 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26456 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26457 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26458 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26459 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26460 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26461 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26462 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26463 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26464 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26465 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26466 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26467 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26468 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26469 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26470 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26471 \t\t Training Loss: 0.0005808426067233086 \t\n",
      "Epoch 26472 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26473 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26474 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26475 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26476 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26477 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26478 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26479 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26480 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26481 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26482 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26483 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26484 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26485 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26486 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26487 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26488 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26489 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26490 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26491 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26492 \t\t Training Loss: 0.0005808425485156476 \t\n",
      "Epoch 26493 \t\t Training Loss: 0.0005808424903079867 \t\n",
      "Epoch 26494 \t\t Training Loss: 0.0005808424903079867 \t\n",
      "Epoch 26495 \t\t Training Loss: 0.0005808424321003258 \t\n",
      "Epoch 26496 \t\t Training Loss: 0.0005808424903079867 \t\n",
      "Epoch 26497 \t\t Training Loss: 0.0005808424903079867 \t\n",
      "Epoch 26498 \t\t Training Loss: 0.0005808424903079867 \t\n",
      "Epoch 26499 \t\t Training Loss: 0.0005808424903079867 \t\n",
      "Epoch 26500 \t\t Training Loss: 0.0005808424321003258 \t\n",
      "Epoch 26501 \t\t Training Loss: 0.0005808424321003258 \t\n",
      "Epoch 26502 \t\t Training Loss: 0.0005808424321003258 \t\n",
      "Epoch 26503 \t\t Training Loss: 0.0005808424321003258 \t\n",
      "Epoch 26504 \t\t Training Loss: 0.0005808424321003258 \t\n",
      "Epoch 26505 \t\t Training Loss: 0.0005808423738926649 \t\n",
      "Epoch 26506 \t\t Training Loss: 0.0005808423738926649 \t\n",
      "Epoch 26507 \t\t Training Loss: 0.0005808424321003258 \t\n",
      "Epoch 26508 \t\t Training Loss: 0.0005808423738926649 \t\n",
      "Epoch 26509 \t\t Training Loss: 0.0005808423738926649 \t\n",
      "Epoch 26510 \t\t Training Loss: 0.0005808424321003258 \t\n",
      "Epoch 26511 \t\t Training Loss: 0.0005808424321003258 \t\n",
      "Epoch 26512 \t\t Training Loss: 0.0005808424321003258 \t\n",
      "Epoch 26513 \t\t Training Loss: 0.0005808423738926649 \t\n",
      "Epoch 26514 \t\t Training Loss: 0.0005808423738926649 \t\n",
      "Epoch 26515 \t\t Training Loss: 0.0005808423738926649 \t\n",
      "Epoch 26516 \t\t Training Loss: 0.0005808423738926649 \t\n",
      "Epoch 26517 \t\t Training Loss: 0.0005808423738926649 \t\n",
      "Epoch 26518 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26519 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26520 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26521 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26522 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26523 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26524 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26525 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26526 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26527 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26528 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26529 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26530 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26531 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26532 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26533 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26534 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26535 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26536 \t\t Training Loss: 0.0005808423738926649 \t\n",
      "Epoch 26537 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26538 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26539 \t\t Training Loss: 0.0005808422574773431 \t\n",
      "Epoch 26540 \t\t Training Loss: 0.0005808421992696822 \t\n",
      "Epoch 26541 \t\t Training Loss: 0.0005808421992696822 \t\n",
      "Epoch 26542 \t\t Training Loss: 0.0005808421992696822 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26543 \t\t Training Loss: 0.0005808421410620213 \t\n",
      "Epoch 26544 \t\t Training Loss: 0.0005808421410620213 \t\n",
      "Epoch 26545 \t\t Training Loss: 0.0005808421410620213 \t\n",
      "Epoch 26546 \t\t Training Loss: 0.0005808421410620213 \t\n",
      "Epoch 26547 \t\t Training Loss: 0.0005808421992696822 \t\n",
      "Epoch 26548 \t\t Training Loss: 0.0005808421992696822 \t\n",
      "Epoch 26549 \t\t Training Loss: 0.0005808421992696822 \t\n",
      "Epoch 26550 \t\t Training Loss: 0.0005808421410620213 \t\n",
      "Epoch 26551 \t\t Training Loss: 0.0005808421410620213 \t\n",
      "Epoch 26552 \t\t Training Loss: 0.0005808421410620213 \t\n",
      "Epoch 26553 \t\t Training Loss: 0.0005808420828543603 \t\n",
      "Epoch 26554 \t\t Training Loss: 0.0005808421410620213 \t\n",
      "Epoch 26555 \t\t Training Loss: 0.0005808420828543603 \t\n",
      "Epoch 26556 \t\t Training Loss: 0.0005808421410620213 \t\n",
      "Epoch 26557 \t\t Training Loss: 0.0005808420828543603 \t\n",
      "Epoch 26558 \t\t Training Loss: 0.0005808420828543603 \t\n",
      "Epoch 26559 \t\t Training Loss: 0.0005808420828543603 \t\n",
      "Epoch 26560 \t\t Training Loss: 0.0005808420828543603 \t\n",
      "Epoch 26561 \t\t Training Loss: 0.0005808420828543603 \t\n",
      "Epoch 26562 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26563 \t\t Training Loss: 0.0005808420828543603 \t\n",
      "Epoch 26564 \t\t Training Loss: 0.0005808420828543603 \t\n",
      "Epoch 26565 \t\t Training Loss: 0.0005808420828543603 \t\n",
      "Epoch 26566 \t\t Training Loss: 0.0005808420828543603 \t\n",
      "Epoch 26567 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26568 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26569 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26570 \t\t Training Loss: 0.0005808420828543603 \t\n",
      "Epoch 26571 \t\t Training Loss: 0.0005808420828543603 \t\n",
      "Epoch 26572 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26573 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26574 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26575 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26576 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26577 \t\t Training Loss: 0.0005808420828543603 \t\n",
      "Epoch 26578 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26579 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26580 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26581 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26582 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26583 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26584 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26585 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26586 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26587 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26588 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26589 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26590 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26591 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26592 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26593 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26594 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26595 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26596 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26597 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26598 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26599 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26600 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26601 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26602 \t\t Training Loss: 0.0005808419082313776 \t\n",
      "Epoch 26603 \t\t Training Loss: 0.0005808419664390385 \t\n",
      "Epoch 26604 \t\t Training Loss: 0.0005808419082313776 \t\n",
      "Epoch 26605 \t\t Training Loss: 0.0005808418500237167 \t\n",
      "Epoch 26606 \t\t Training Loss: 0.0005808418500237167 \t\n",
      "Epoch 26607 \t\t Training Loss: 0.0005808418500237167 \t\n",
      "Epoch 26608 \t\t Training Loss: 0.0005808418500237167 \t\n",
      "Epoch 26609 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26610 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26611 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26612 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26613 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26614 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26615 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26616 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26617 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26618 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26619 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26620 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26621 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26622 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26623 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26624 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26625 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26626 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26627 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26628 \t\t Training Loss: 0.0005808417918160558 \t\n",
      "Epoch 26629 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26630 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26631 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26632 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26633 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26634 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26635 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26636 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26637 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26638 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26639 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26640 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26641 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26642 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26643 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26644 \t\t Training Loss: 0.0005808415589854121 \t\n",
      "Epoch 26645 \t\t Training Loss: 0.0005808415589854121 \t\n",
      "Epoch 26646 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26647 \t\t Training Loss: 0.0005808415589854121 \t\n",
      "Epoch 26648 \t\t Training Loss: 0.0005808415589854121 \t\n",
      "Epoch 26649 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26650 \t\t Training Loss: 0.0005808415589854121 \t\n",
      "Epoch 26651 \t\t Training Loss: 0.0005808415589854121 \t\n",
      "Epoch 26652 \t\t Training Loss: 0.000580841675400734 \t\n",
      "Epoch 26653 \t\t Training Loss: 0.0005808415589854121 \t\n",
      "Epoch 26654 \t\t Training Loss: 0.0005808415589854121 \t\n",
      "Epoch 26655 \t\t Training Loss: 0.0005808415589854121 \t\n",
      "Epoch 26656 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26657 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26658 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26659 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26660 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26661 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26662 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26663 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26664 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26665 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26666 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26667 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26668 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26669 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26670 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26671 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26672 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26673 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26674 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26675 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26676 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26677 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26678 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26679 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26680 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26681 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26682 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26683 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26684 \t\t Training Loss: 0.0005808415007777512 \t\n",
      "Epoch 26685 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26686 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26687 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26688 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26689 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26690 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26691 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26692 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26693 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26694 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26695 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26696 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26697 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26698 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26699 \t\t Training Loss: 0.0005808413843624294 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26700 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26701 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26702 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26703 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26704 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26705 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26706 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26707 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26708 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26709 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26710 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26711 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26712 \t\t Training Loss: 0.0005808413843624294 \t\n",
      "Epoch 26713 \t\t Training Loss: 0.0005808412679471076 \t\n",
      "Epoch 26714 \t\t Training Loss: 0.0005808412097394466 \t\n",
      "Epoch 26715 \t\t Training Loss: 0.0005808412097394466 \t\n",
      "Epoch 26716 \t\t Training Loss: 0.0005808412097394466 \t\n",
      "Epoch 26717 \t\t Training Loss: 0.0005808412097394466 \t\n",
      "Epoch 26718 \t\t Training Loss: 0.0005808412097394466 \t\n",
      "Epoch 26719 \t\t Training Loss: 0.0005808412097394466 \t\n",
      "Epoch 26720 \t\t Training Loss: 0.0005808412097394466 \t\n",
      "Epoch 26721 \t\t Training Loss: 0.0005808412679471076 \t\n",
      "Epoch 26722 \t\t Training Loss: 0.0005808412097394466 \t\n",
      "Epoch 26723 \t\t Training Loss: 0.0005808412097394466 \t\n",
      "Epoch 26724 \t\t Training Loss: 0.0005808412097394466 \t\n",
      "Epoch 26725 \t\t Training Loss: 0.0005808411515317857 \t\n",
      "Epoch 26726 \t\t Training Loss: 0.0005808411515317857 \t\n",
      "Epoch 26727 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26728 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26729 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26730 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26731 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26732 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26733 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26734 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26735 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26736 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26737 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26738 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26739 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26740 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26741 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26742 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26743 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26744 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26745 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26746 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26747 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26748 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26749 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26750 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26751 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26752 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26753 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26754 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26755 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26756 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26757 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26758 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26759 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26760 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26761 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26762 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26763 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26764 \t\t Training Loss: 0.0005808410933241248 \t\n",
      "Epoch 26765 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26766 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26767 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26768 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26769 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26770 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26771 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26772 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26773 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26774 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26775 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26776 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26777 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26778 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26779 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26780 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26781 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26782 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26783 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26784 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26785 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26786 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26787 \t\t Training Loss: 0.0005808408604934812 \t\n",
      "Epoch 26788 \t\t Training Loss: 0.000580840976908803 \t\n",
      "Epoch 26789 \t\t Training Loss: 0.0005808408604934812 \t\n",
      "Epoch 26790 \t\t Training Loss: 0.0005808408604934812 \t\n",
      "Epoch 26791 \t\t Training Loss: 0.0005808408604934812 \t\n",
      "Epoch 26792 \t\t Training Loss: 0.0005808408604934812 \t\n",
      "Epoch 26793 \t\t Training Loss: 0.0005808408604934812 \t\n",
      "Epoch 26794 \t\t Training Loss: 0.0005808408604934812 \t\n",
      "Epoch 26795 \t\t Training Loss: 0.0005808408604934812 \t\n",
      "Epoch 26796 \t\t Training Loss: 0.0005808408604934812 \t\n",
      "Epoch 26797 \t\t Training Loss: 0.0005808408604934812 \t\n",
      "Epoch 26798 \t\t Training Loss: 0.0005808408604934812 \t\n",
      "Epoch 26799 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26800 \t\t Training Loss: 0.0005808408604934812 \t\n",
      "Epoch 26801 \t\t Training Loss: 0.0005808408604934812 \t\n",
      "Epoch 26802 \t\t Training Loss: 0.0005808408604934812 \t\n",
      "Epoch 26803 \t\t Training Loss: 0.0005808408604934812 \t\n",
      "Epoch 26804 \t\t Training Loss: 0.0005808408604934812 \t\n",
      "Epoch 26805 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26806 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26807 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26808 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26809 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26810 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26811 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26812 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26813 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26814 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26815 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26816 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26817 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26818 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26819 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26820 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26821 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26822 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26823 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26824 \t\t Training Loss: 0.0005808408022858202 \t\n",
      "Epoch 26825 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26826 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26827 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26828 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26829 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26830 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26831 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26832 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26833 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26834 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26835 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26836 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26837 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26838 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26839 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26840 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26841 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26842 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26843 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26844 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26845 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26846 \t\t Training Loss: 0.0005808405694551766 \t\n",
      "Epoch 26847 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26848 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26849 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26850 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26851 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26852 \t\t Training Loss: 0.0005808405694551766 \t\n",
      "Epoch 26853 \t\t Training Loss: 0.0005808405694551766 \t\n",
      "Epoch 26854 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26855 \t\t Training Loss: 0.0005808405694551766 \t\n",
      "Epoch 26856 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26857 \t\t Training Loss: 0.0005808405694551766 \t\n",
      "Epoch 26858 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26859 \t\t Training Loss: 0.0005808405694551766 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26860 \t\t Training Loss: 0.0005808405694551766 \t\n",
      "Epoch 26861 \t\t Training Loss: 0.0005808406858704984 \t\n",
      "Epoch 26862 \t\t Training Loss: 0.0005808405694551766 \t\n",
      "Epoch 26863 \t\t Training Loss: 0.0005808405694551766 \t\n",
      "Epoch 26864 \t\t Training Loss: 0.0005808405694551766 \t\n",
      "Epoch 26865 \t\t Training Loss: 0.0005808405694551766 \t\n",
      "Epoch 26866 \t\t Training Loss: 0.0005808405694551766 \t\n",
      "Epoch 26867 \t\t Training Loss: 0.0005808405112475157 \t\n",
      "Epoch 26868 \t\t Training Loss: 0.0005808405112475157 \t\n",
      "Epoch 26869 \t\t Training Loss: 0.0005808405112475157 \t\n",
      "Epoch 26870 \t\t Training Loss: 0.0005808405112475157 \t\n",
      "Epoch 26871 \t\t Training Loss: 0.0005808405112475157 \t\n",
      "Epoch 26872 \t\t Training Loss: 0.0005808405112475157 \t\n",
      "Epoch 26873 \t\t Training Loss: 0.0005808405112475157 \t\n",
      "Epoch 26874 \t\t Training Loss: 0.0005808403948321939 \t\n",
      "Epoch 26875 \t\t Training Loss: 0.0005808403948321939 \t\n",
      "Epoch 26876 \t\t Training Loss: 0.0005808405112475157 \t\n",
      "Epoch 26877 \t\t Training Loss: 0.0005808403948321939 \t\n",
      "Epoch 26878 \t\t Training Loss: 0.0005808405112475157 \t\n",
      "Epoch 26879 \t\t Training Loss: 0.0005808403948321939 \t\n",
      "Epoch 26880 \t\t Training Loss: 0.0005808403948321939 \t\n",
      "Epoch 26881 \t\t Training Loss: 0.0005808403948321939 \t\n",
      "Epoch 26882 \t\t Training Loss: 0.0005808403948321939 \t\n",
      "Epoch 26883 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26884 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26885 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26886 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26887 \t\t Training Loss: 0.0005808403948321939 \t\n",
      "Epoch 26888 \t\t Training Loss: 0.0005808403948321939 \t\n",
      "Epoch 26889 \t\t Training Loss: 0.0005808403948321939 \t\n",
      "Epoch 26890 \t\t Training Loss: 0.0005808403948321939 \t\n",
      "Epoch 26891 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26892 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26893 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26894 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26895 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26896 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26897 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26898 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26899 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26900 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26901 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26902 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26903 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26904 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26905 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26906 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26907 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26908 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26909 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26910 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26911 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26912 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26913 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26914 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26915 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26916 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26917 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26918 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26919 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26920 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26921 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26922 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26923 \t\t Training Loss: 0.000580840278416872 \t\n",
      "Epoch 26924 \t\t Training Loss: 0.0005808402202092111 \t\n",
      "Epoch 26925 \t\t Training Loss: 0.0005808402202092111 \t\n",
      "Epoch 26926 \t\t Training Loss: 0.0005808402202092111 \t\n",
      "Epoch 26927 \t\t Training Loss: 0.0005808402202092111 \t\n",
      "Epoch 26928 \t\t Training Loss: 0.0005808401037938893 \t\n",
      "Epoch 26929 \t\t Training Loss: 0.0005808401037938893 \t\n",
      "Epoch 26930 \t\t Training Loss: 0.0005808401037938893 \t\n",
      "Epoch 26931 \t\t Training Loss: 0.0005808401037938893 \t\n",
      "Epoch 26932 \t\t Training Loss: 0.0005808401037938893 \t\n",
      "Epoch 26933 \t\t Training Loss: 0.0005808401037938893 \t\n",
      "Epoch 26934 \t\t Training Loss: 0.0005808401037938893 \t\n",
      "Epoch 26935 \t\t Training Loss: 0.0005808401037938893 \t\n",
      "Epoch 26936 \t\t Training Loss: 0.0005808401037938893 \t\n",
      "Epoch 26937 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26938 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26939 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26940 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26941 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26942 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26943 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26944 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26945 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26946 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26947 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26948 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26949 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26950 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26951 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26952 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26953 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26954 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26955 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26956 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26957 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26958 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26959 \t\t Training Loss: 0.0005808399873785675 \t\n",
      "Epoch 26960 \t\t Training Loss: 0.0005808399291709065 \t\n",
      "Epoch 26961 \t\t Training Loss: 0.0005808399291709065 \t\n",
      "Epoch 26962 \t\t Training Loss: 0.0005808399291709065 \t\n",
      "Epoch 26963 \t\t Training Loss: 0.0005808398709632456 \t\n",
      "Epoch 26964 \t\t Training Loss: 0.0005808398709632456 \t\n",
      "Epoch 26965 \t\t Training Loss: 0.0005808398709632456 \t\n",
      "Epoch 26966 \t\t Training Loss: 0.0005808398709632456 \t\n",
      "Epoch 26967 \t\t Training Loss: 0.0005808398709632456 \t\n",
      "Epoch 26968 \t\t Training Loss: 0.0005808398709632456 \t\n",
      "Epoch 26969 \t\t Training Loss: 0.0005808398127555847 \t\n",
      "Epoch 26970 \t\t Training Loss: 0.0005808398127555847 \t\n",
      "Epoch 26971 \t\t Training Loss: 0.0005808398127555847 \t\n",
      "Epoch 26972 \t\t Training Loss: 0.0005808398127555847 \t\n",
      "Epoch 26973 \t\t Training Loss: 0.0005808398127555847 \t\n",
      "Epoch 26974 \t\t Training Loss: 0.0005808398127555847 \t\n",
      "Epoch 26975 \t\t Training Loss: 0.0005808398127555847 \t\n",
      "Epoch 26976 \t\t Training Loss: 0.0005808398127555847 \t\n",
      "Epoch 26977 \t\t Training Loss: 0.0005808398127555847 \t\n",
      "Epoch 26978 \t\t Training Loss: 0.0005808398127555847 \t\n",
      "Epoch 26979 \t\t Training Loss: 0.0005808398127555847 \t\n",
      "Epoch 26980 \t\t Training Loss: 0.0005808398127555847 \t\n",
      "Epoch 26981 \t\t Training Loss: 0.0005808398127555847 \t\n",
      "Epoch 26982 \t\t Training Loss: 0.0005808398127555847 \t\n",
      "Epoch 26983 \t\t Training Loss: 0.0005808398127555847 \t\n",
      "Epoch 26984 \t\t Training Loss: 0.0005808396963402629 \t\n",
      "Epoch 26985 \t\t Training Loss: 0.0005808396963402629 \t\n",
      "Epoch 26986 \t\t Training Loss: 0.0005808396963402629 \t\n",
      "Epoch 26987 \t\t Training Loss: 0.0005808395799249411 \t\n",
      "Epoch 26988 \t\t Training Loss: 0.0005808395799249411 \t\n",
      "Epoch 26989 \t\t Training Loss: 0.0005808395799249411 \t\n",
      "Epoch 26990 \t\t Training Loss: 0.0005808395799249411 \t\n",
      "Epoch 26991 \t\t Training Loss: 0.0005808395799249411 \t\n",
      "Epoch 26992 \t\t Training Loss: 0.0005808395799249411 \t\n",
      "Epoch 26993 \t\t Training Loss: 0.0005808395799249411 \t\n",
      "Epoch 26994 \t\t Training Loss: 0.0005808395799249411 \t\n",
      "Epoch 26995 \t\t Training Loss: 0.0005808395799249411 \t\n",
      "Epoch 26996 \t\t Training Loss: 0.0005808395799249411 \t\n",
      "Epoch 26997 \t\t Training Loss: 0.0005808395799249411 \t\n",
      "Epoch 26998 \t\t Training Loss: 0.0005808395799249411 \t\n",
      "Epoch 26999 \t\t Training Loss: 0.0005808395799249411 \t\n",
      "Epoch 27000 \t\t Training Loss: 0.0005808395799249411 \t\n",
      "Epoch 27001 \t\t Training Loss: 0.0005808395217172801 \t\n",
      "Epoch 27002 \t\t Training Loss: 0.0005808395217172801 \t\n",
      "Epoch 27003 \t\t Training Loss: 0.0005808395217172801 \t\n",
      "Epoch 27004 \t\t Training Loss: 0.0005808395217172801 \t\n",
      "Epoch 27005 \t\t Training Loss: 0.0005808395217172801 \t\n",
      "Epoch 27006 \t\t Training Loss: 0.0005808395217172801 \t\n",
      "Epoch 27007 \t\t Training Loss: 0.0005808395217172801 \t\n",
      "Epoch 27008 \t\t Training Loss: 0.0005808395217172801 \t\n",
      "Epoch 27009 \t\t Training Loss: 0.0005808395217172801 \t\n",
      "Epoch 27010 \t\t Training Loss: 0.0005808394635096192 \t\n",
      "Epoch 27011 \t\t Training Loss: 0.0005808395217172801 \t\n",
      "Epoch 27012 \t\t Training Loss: 0.0005808394635096192 \t\n",
      "Epoch 27013 \t\t Training Loss: 0.0005808394635096192 \t\n",
      "Epoch 27014 \t\t Training Loss: 0.0005808395217172801 \t\n",
      "Epoch 27015 \t\t Training Loss: 0.0005808394635096192 \t\n",
      "Epoch 27016 \t\t Training Loss: 0.0005808394635096192 \t\n",
      "Epoch 27017 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27018 \t\t Training Loss: 0.0005808394635096192 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27019 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27020 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27021 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27022 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27023 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27024 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27025 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27026 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27027 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27028 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27029 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27030 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27031 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27032 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27033 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27034 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27035 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27036 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27037 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27038 \t\t Training Loss: 0.0005808394053019583 \t\n",
      "Epoch 27039 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27040 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27041 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27042 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27043 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27044 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27045 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27046 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27047 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27048 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27049 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27050 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27051 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27052 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27053 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27054 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27055 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27056 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27057 \t\t Training Loss: 0.0005808392888866365 \t\n",
      "Epoch 27058 \t\t Training Loss: 0.0005808392306789756 \t\n",
      "Epoch 27059 \t\t Training Loss: 0.0005808392306789756 \t\n",
      "Epoch 27060 \t\t Training Loss: 0.0005808392306789756 \t\n",
      "Epoch 27061 \t\t Training Loss: 0.0005808392306789756 \t\n",
      "Epoch 27062 \t\t Training Loss: 0.0005808392306789756 \t\n",
      "Epoch 27063 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27064 \t\t Training Loss: 0.0005808392306789756 \t\n",
      "Epoch 27065 \t\t Training Loss: 0.0005808392306789756 \t\n",
      "Epoch 27066 \t\t Training Loss: 0.0005808392306789756 \t\n",
      "Epoch 27067 \t\t Training Loss: 0.0005808392306789756 \t\n",
      "Epoch 27068 \t\t Training Loss: 0.0005808392306789756 \t\n",
      "Epoch 27069 \t\t Training Loss: 0.0005808392306789756 \t\n",
      "Epoch 27070 \t\t Training Loss: 0.0005808392306789756 \t\n",
      "Epoch 27071 \t\t Training Loss: 0.0005808392306789756 \t\n",
      "Epoch 27072 \t\t Training Loss: 0.0005808392306789756 \t\n",
      "Epoch 27073 \t\t Training Loss: 0.0005808392306789756 \t\n",
      "Epoch 27074 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27075 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27076 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27077 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27078 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27079 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27080 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27081 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27082 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27083 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27084 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27085 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27086 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27087 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27088 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27089 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27090 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27091 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27092 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27093 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27094 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27095 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27096 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27097 \t\t Training Loss: 0.0005808391142636538 \t\n",
      "Epoch 27098 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27099 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27100 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27101 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27102 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27103 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27104 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27105 \t\t Training Loss: 0.0005808391724713147 \t\n",
      "Epoch 27106 \t\t Training Loss: 0.0005808391142636538 \t\n",
      "Epoch 27107 \t\t Training Loss: 0.0005808389978483319 \t\n",
      "Epoch 27108 \t\t Training Loss: 0.0005808389978483319 \t\n",
      "Epoch 27109 \t\t Training Loss: 0.0005808389978483319 \t\n",
      "Epoch 27110 \t\t Training Loss: 0.0005808389978483319 \t\n",
      "Epoch 27111 \t\t Training Loss: 0.0005808389978483319 \t\n",
      "Epoch 27112 \t\t Training Loss: 0.0005808389978483319 \t\n",
      "Epoch 27113 \t\t Training Loss: 0.0005808389978483319 \t\n",
      "Epoch 27114 \t\t Training Loss: 0.0005808389978483319 \t\n",
      "Epoch 27115 \t\t Training Loss: 0.0005808389978483319 \t\n",
      "Epoch 27116 \t\t Training Loss: 0.0005808389978483319 \t\n",
      "Epoch 27117 \t\t Training Loss: 0.0005808389978483319 \t\n",
      "Epoch 27118 \t\t Training Loss: 0.0005808389978483319 \t\n",
      "Epoch 27119 \t\t Training Loss: 0.000580838939640671 \t\n",
      "Epoch 27120 \t\t Training Loss: 0.000580838939640671 \t\n",
      "Epoch 27121 \t\t Training Loss: 0.000580838939640671 \t\n",
      "Epoch 27122 \t\t Training Loss: 0.000580838939640671 \t\n",
      "Epoch 27123 \t\t Training Loss: 0.000580838939640671 \t\n",
      "Epoch 27124 \t\t Training Loss: 0.000580838939640671 \t\n",
      "Epoch 27125 \t\t Training Loss: 0.000580838939640671 \t\n",
      "Epoch 27126 \t\t Training Loss: 0.000580838939640671 \t\n",
      "Epoch 27127 \t\t Training Loss: 0.000580838939640671 \t\n",
      "Epoch 27128 \t\t Training Loss: 0.000580838939640671 \t\n",
      "Epoch 27129 \t\t Training Loss: 0.000580838939640671 \t\n",
      "Epoch 27130 \t\t Training Loss: 0.000580838939640671 \t\n",
      "Epoch 27131 \t\t Training Loss: 0.000580838939640671 \t\n",
      "Epoch 27132 \t\t Training Loss: 0.000580838939640671 \t\n",
      "Epoch 27133 \t\t Training Loss: 0.000580838939640671 \t\n",
      "Epoch 27134 \t\t Training Loss: 0.0005808388814330101 \t\n",
      "Epoch 27135 \t\t Training Loss: 0.0005808388814330101 \t\n",
      "Epoch 27136 \t\t Training Loss: 0.0005808388814330101 \t\n",
      "Epoch 27137 \t\t Training Loss: 0.0005808388814330101 \t\n",
      "Epoch 27138 \t\t Training Loss: 0.0005808388814330101 \t\n",
      "Epoch 27139 \t\t Training Loss: 0.0005808388814330101 \t\n",
      "Epoch 27140 \t\t Training Loss: 0.0005808388232253492 \t\n",
      "Epoch 27141 \t\t Training Loss: 0.0005808387650176883 \t\n",
      "Epoch 27142 \t\t Training Loss: 0.0005808387068100274 \t\n",
      "Epoch 27143 \t\t Training Loss: 0.0005808387068100274 \t\n",
      "Epoch 27144 \t\t Training Loss: 0.0005808387068100274 \t\n",
      "Epoch 27145 \t\t Training Loss: 0.0005808387068100274 \t\n",
      "Epoch 27146 \t\t Training Loss: 0.0005808387068100274 \t\n",
      "Epoch 27147 \t\t Training Loss: 0.0005808387068100274 \t\n",
      "Epoch 27148 \t\t Training Loss: 0.0005808387068100274 \t\n",
      "Epoch 27149 \t\t Training Loss: 0.0005808386486023664 \t\n",
      "Epoch 27150 \t\t Training Loss: 0.0005808386486023664 \t\n",
      "Epoch 27151 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27152 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27153 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27154 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27155 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27156 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27157 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27158 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27159 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27160 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27161 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27162 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27163 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27164 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27165 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27166 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27167 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27168 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27169 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27170 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27171 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27172 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27173 \t\t Training Loss: 0.0005808385903947055 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27174 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27175 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27176 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27177 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27178 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27179 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27180 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27181 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27182 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27183 \t\t Training Loss: 0.0005808385321870446 \t\n",
      "Epoch 27184 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27185 \t\t Training Loss: 0.0005808385321870446 \t\n",
      "Epoch 27186 \t\t Training Loss: 0.0005808385321870446 \t\n",
      "Epoch 27187 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27188 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27189 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27190 \t\t Training Loss: 0.0005808385903947055 \t\n",
      "Epoch 27191 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27192 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27193 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27194 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27195 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27196 \t\t Training Loss: 0.0005808384157717228 \t\n",
      "Epoch 27197 \t\t Training Loss: 0.0005808384157717228 \t\n",
      "Epoch 27198 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27199 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27200 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27201 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27202 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27203 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27204 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27205 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27206 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27207 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27208 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27209 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27210 \t\t Training Loss: 0.0005808384157717228 \t\n",
      "Epoch 27211 \t\t Training Loss: 0.0005808384739793837 \t\n",
      "Epoch 27212 \t\t Training Loss: 0.0005808384157717228 \t\n",
      "Epoch 27213 \t\t Training Loss: 0.0005808384157717228 \t\n",
      "Epoch 27214 \t\t Training Loss: 0.0005808384157717228 \t\n",
      "Epoch 27215 \t\t Training Loss: 0.0005808384157717228 \t\n",
      "Epoch 27216 \t\t Training Loss: 0.0005808384157717228 \t\n",
      "Epoch 27217 \t\t Training Loss: 0.0005808383575640619 \t\n",
      "Epoch 27218 \t\t Training Loss: 0.0005808383575640619 \t\n",
      "Epoch 27219 \t\t Training Loss: 0.0005808383575640619 \t\n",
      "Epoch 27220 \t\t Training Loss: 0.0005808383575640619 \t\n",
      "Epoch 27221 \t\t Training Loss: 0.0005808383575640619 \t\n",
      "Epoch 27222 \t\t Training Loss: 0.000580838299356401 \t\n",
      "Epoch 27223 \t\t Training Loss: 0.000580838299356401 \t\n",
      "Epoch 27224 \t\t Training Loss: 0.0005808383575640619 \t\n",
      "Epoch 27225 \t\t Training Loss: 0.0005808383575640619 \t\n",
      "Epoch 27226 \t\t Training Loss: 0.0005808383575640619 \t\n",
      "Epoch 27227 \t\t Training Loss: 0.0005808383575640619 \t\n",
      "Epoch 27228 \t\t Training Loss: 0.0005808383575640619 \t\n",
      "Epoch 27229 \t\t Training Loss: 0.000580838299356401 \t\n",
      "Epoch 27230 \t\t Training Loss: 0.0005808383575640619 \t\n",
      "Epoch 27231 \t\t Training Loss: 0.000580838299356401 \t\n",
      "Epoch 27232 \t\t Training Loss: 0.000580838299356401 \t\n",
      "Epoch 27233 \t\t Training Loss: 0.000580838299356401 \t\n",
      "Epoch 27234 \t\t Training Loss: 0.000580838299356401 \t\n",
      "Epoch 27235 \t\t Training Loss: 0.000580838299356401 \t\n",
      "Epoch 27236 \t\t Training Loss: 0.000580838299356401 \t\n",
      "Epoch 27237 \t\t Training Loss: 0.00058083824114874 \t\n",
      "Epoch 27238 \t\t Training Loss: 0.00058083824114874 \t\n",
      "Epoch 27239 \t\t Training Loss: 0.00058083824114874 \t\n",
      "Epoch 27240 \t\t Training Loss: 0.00058083824114874 \t\n",
      "Epoch 27241 \t\t Training Loss: 0.00058083824114874 \t\n",
      "Epoch 27242 \t\t Training Loss: 0.00058083824114874 \t\n",
      "Epoch 27243 \t\t Training Loss: 0.00058083824114874 \t\n",
      "Epoch 27244 \t\t Training Loss: 0.00058083824114874 \t\n",
      "Epoch 27245 \t\t Training Loss: 0.00058083824114874 \t\n",
      "Epoch 27246 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27247 \t\t Training Loss: 0.0005808381829410791 \t\n",
      "Epoch 27248 \t\t Training Loss: 0.0005808381829410791 \t\n",
      "Epoch 27249 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27250 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27251 \t\t Training Loss: 0.0005808381829410791 \t\n",
      "Epoch 27252 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27253 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27254 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27255 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27256 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27257 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27258 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27259 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27260 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27261 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27262 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27263 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27264 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27265 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27266 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27267 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27268 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27269 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27270 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27271 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27272 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27273 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27274 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27275 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27276 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27277 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27278 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27279 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27280 \t\t Training Loss: 0.0005808381247334182 \t\n",
      "Epoch 27281 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27282 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27283 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27284 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27285 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27286 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27287 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27288 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27289 \t\t Training Loss: 0.0005808380083180964 \t\n",
      "Epoch 27290 \t\t Training Loss: 0.0005808378919027746 \t\n",
      "Epoch 27291 \t\t Training Loss: 0.0005808378919027746 \t\n",
      "Epoch 27292 \t\t Training Loss: 0.0005808378919027746 \t\n",
      "Epoch 27293 \t\t Training Loss: 0.0005808378919027746 \t\n",
      "Epoch 27294 \t\t Training Loss: 0.0005808378919027746 \t\n",
      "Epoch 27295 \t\t Training Loss: 0.0005808378919027746 \t\n",
      "Epoch 27296 \t\t Training Loss: 0.0005808378919027746 \t\n",
      "Epoch 27297 \t\t Training Loss: 0.0005808378919027746 \t\n",
      "Epoch 27298 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27299 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27300 \t\t Training Loss: 0.0005808378919027746 \t\n",
      "Epoch 27301 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27302 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27303 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27304 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27305 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27306 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27307 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27308 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27309 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27310 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27311 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27312 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27313 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27314 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27315 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27316 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27317 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27318 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27319 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27320 \t\t Training Loss: 0.0005808378336951137 \t\n",
      "Epoch 27321 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27322 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27323 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27324 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27325 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27326 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27327 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27328 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27329 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27330 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27331 \t\t Training Loss: 0.0005808377172797918 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27332 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27333 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27334 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27335 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27336 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27337 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27338 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27339 \t\t Training Loss: 0.0005808376590721309 \t\n",
      "Epoch 27340 \t\t Training Loss: 0.0005808376590721309 \t\n",
      "Epoch 27341 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27342 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27343 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27344 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27345 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27346 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27347 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27348 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27349 \t\t Training Loss: 0.0005808377172797918 \t\n",
      "Epoch 27350 \t\t Training Loss: 0.0005808376590721309 \t\n",
      "Epoch 27351 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27352 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27353 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27354 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27355 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27356 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27357 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27358 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27359 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27360 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27361 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27362 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27363 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27364 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27365 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27366 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27367 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27368 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27369 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27370 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27371 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27372 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27373 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27374 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27375 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27376 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27377 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27378 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27379 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27380 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27381 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27382 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27383 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27384 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27385 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27386 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27387 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27388 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27389 \t\t Training Loss: 0.00058083760086447 \t\n",
      "Epoch 27390 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27391 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27392 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27393 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27394 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27395 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27396 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27397 \t\t Training Loss: 0.0005808375426568091 \t\n",
      "Epoch 27398 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27399 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27400 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27401 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27402 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27403 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27404 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27405 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27406 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27407 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27408 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27409 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27410 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27411 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27412 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27413 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27414 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27415 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27416 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27417 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27418 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27419 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27420 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27421 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27422 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27423 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27424 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27425 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27426 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27427 \t\t Training Loss: 0.0005808373098261654 \t\n",
      "Epoch 27428 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27429 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27430 \t\t Training Loss: 0.0005808374262414873 \t\n",
      "Epoch 27431 \t\t Training Loss: 0.0005808373098261654 \t\n",
      "Epoch 27432 \t\t Training Loss: 0.0005808373098261654 \t\n",
      "Epoch 27433 \t\t Training Loss: 0.0005808373098261654 \t\n",
      "Epoch 27434 \t\t Training Loss: 0.0005808373098261654 \t\n",
      "Epoch 27435 \t\t Training Loss: 0.0005808373098261654 \t\n",
      "Epoch 27436 \t\t Training Loss: 0.0005808373098261654 \t\n",
      "Epoch 27437 \t\t Training Loss: 0.0005808373098261654 \t\n",
      "Epoch 27438 \t\t Training Loss: 0.0005808373098261654 \t\n",
      "Epoch 27439 \t\t Training Loss: 0.0005808373098261654 \t\n",
      "Epoch 27440 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27441 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27442 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27443 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27444 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27445 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27446 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27447 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27448 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27449 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27450 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27451 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27452 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27453 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27454 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27455 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27456 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27457 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27458 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27459 \t\t Training Loss: 0.0005808371934108436 \t\n",
      "Epoch 27460 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27461 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27462 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27463 \t\t Training Loss: 0.0005808372516185045 \t\n",
      "Epoch 27464 \t\t Training Loss: 0.0005808371934108436 \t\n",
      "Epoch 27465 \t\t Training Loss: 0.0005808371934108436 \t\n",
      "Epoch 27466 \t\t Training Loss: 0.0005808371934108436 \t\n",
      "Epoch 27467 \t\t Training Loss: 0.0005808371934108436 \t\n",
      "Epoch 27468 \t\t Training Loss: 0.0005808371934108436 \t\n",
      "Epoch 27469 \t\t Training Loss: 0.0005808371934108436 \t\n",
      "Epoch 27470 \t\t Training Loss: 0.0005808371934108436 \t\n",
      "Epoch 27471 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27472 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27473 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27474 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27475 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27476 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27477 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27478 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27479 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27480 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27481 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27482 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27483 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27484 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27485 \t\t Training Loss: 0.0005808370769955218 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27486 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27487 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27488 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27489 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27490 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27491 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27492 \t\t Training Loss: 0.0005808370187878609 \t\n",
      "Epoch 27493 \t\t Training Loss: 0.0005808370187878609 \t\n",
      "Epoch 27494 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27495 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27496 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27497 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27498 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27499 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27500 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27501 \t\t Training Loss: 0.0005808370187878609 \t\n",
      "Epoch 27502 \t\t Training Loss: 0.0005808370769955218 \t\n",
      "Epoch 27503 \t\t Training Loss: 0.0005808370187878609 \t\n",
      "Epoch 27504 \t\t Training Loss: 0.0005808370187878609 \t\n",
      "Epoch 27505 \t\t Training Loss: 0.0005808370187878609 \t\n",
      "Epoch 27506 \t\t Training Loss: 0.0005808370187878609 \t\n",
      "Epoch 27507 \t\t Training Loss: 0.0005808370187878609 \t\n",
      "Epoch 27508 \t\t Training Loss: 0.0005808370187878609 \t\n",
      "Epoch 27509 \t\t Training Loss: 0.0005808370187878609 \t\n",
      "Epoch 27510 \t\t Training Loss: 0.0005808369605802 \t\n",
      "Epoch 27511 \t\t Training Loss: 0.0005808369605802 \t\n",
      "Epoch 27512 \t\t Training Loss: 0.0005808369605802 \t\n",
      "Epoch 27513 \t\t Training Loss: 0.0005808369605802 \t\n",
      "Epoch 27514 \t\t Training Loss: 0.0005808369605802 \t\n",
      "Epoch 27515 \t\t Training Loss: 0.0005808369605802 \t\n",
      "Epoch 27516 \t\t Training Loss: 0.000580836902372539 \t\n",
      "Epoch 27517 \t\t Training Loss: 0.0005808369605802 \t\n",
      "Epoch 27518 \t\t Training Loss: 0.0005808369605802 \t\n",
      "Epoch 27519 \t\t Training Loss: 0.0005808369605802 \t\n",
      "Epoch 27520 \t\t Training Loss: 0.0005808368441648781 \t\n",
      "Epoch 27521 \t\t Training Loss: 0.0005808368441648781 \t\n",
      "Epoch 27522 \t\t Training Loss: 0.0005808368441648781 \t\n",
      "Epoch 27523 \t\t Training Loss: 0.0005808368441648781 \t\n",
      "Epoch 27524 \t\t Training Loss: 0.0005808368441648781 \t\n",
      "Epoch 27525 \t\t Training Loss: 0.0005808368441648781 \t\n",
      "Epoch 27526 \t\t Training Loss: 0.0005808368441648781 \t\n",
      "Epoch 27527 \t\t Training Loss: 0.0005808367859572172 \t\n",
      "Epoch 27528 \t\t Training Loss: 0.0005808367859572172 \t\n",
      "Epoch 27529 \t\t Training Loss: 0.0005808367859572172 \t\n",
      "Epoch 27530 \t\t Training Loss: 0.0005808367859572172 \t\n",
      "Epoch 27531 \t\t Training Loss: 0.0005808367859572172 \t\n",
      "Epoch 27532 \t\t Training Loss: 0.0005808367859572172 \t\n",
      "Epoch 27533 \t\t Training Loss: 0.0005808367859572172 \t\n",
      "Epoch 27534 \t\t Training Loss: 0.0005808367859572172 \t\n",
      "Epoch 27535 \t\t Training Loss: 0.0005808367277495563 \t\n",
      "Epoch 27536 \t\t Training Loss: 0.0005808366695418954 \t\n",
      "Epoch 27537 \t\t Training Loss: 0.0005808366695418954 \t\n",
      "Epoch 27538 \t\t Training Loss: 0.0005808366695418954 \t\n",
      "Epoch 27539 \t\t Training Loss: 0.0005808366695418954 \t\n",
      "Epoch 27540 \t\t Training Loss: 0.0005808366695418954 \t\n",
      "Epoch 27541 \t\t Training Loss: 0.0005808366695418954 \t\n",
      "Epoch 27542 \t\t Training Loss: 0.0005808366695418954 \t\n",
      "Epoch 27543 \t\t Training Loss: 0.0005808366695418954 \t\n",
      "Epoch 27544 \t\t Training Loss: 0.0005808366695418954 \t\n",
      "Epoch 27545 \t\t Training Loss: 0.0005808366695418954 \t\n",
      "Epoch 27546 \t\t Training Loss: 0.0005808366113342345 \t\n",
      "Epoch 27547 \t\t Training Loss: 0.0005808366113342345 \t\n",
      "Epoch 27548 \t\t Training Loss: 0.0005808366113342345 \t\n",
      "Epoch 27549 \t\t Training Loss: 0.0005808366695418954 \t\n",
      "Epoch 27550 \t\t Training Loss: 0.0005808366113342345 \t\n",
      "Epoch 27551 \t\t Training Loss: 0.0005808366113342345 \t\n",
      "Epoch 27552 \t\t Training Loss: 0.0005808366113342345 \t\n",
      "Epoch 27553 \t\t Training Loss: 0.0005808366113342345 \t\n",
      "Epoch 27554 \t\t Training Loss: 0.0005808366113342345 \t\n",
      "Epoch 27555 \t\t Training Loss: 0.0005808366113342345 \t\n",
      "Epoch 27556 \t\t Training Loss: 0.0005808366113342345 \t\n",
      "Epoch 27557 \t\t Training Loss: 0.0005808366113342345 \t\n",
      "Epoch 27558 \t\t Training Loss: 0.0005808366113342345 \t\n",
      "Epoch 27559 \t\t Training Loss: 0.0005808366113342345 \t\n",
      "Epoch 27560 \t\t Training Loss: 0.0005808366113342345 \t\n",
      "Epoch 27561 \t\t Training Loss: 0.0005808366113342345 \t\n",
      "Epoch 27562 \t\t Training Loss: 0.0005808366113342345 \t\n",
      "Epoch 27563 \t\t Training Loss: 0.0005808365531265736 \t\n",
      "Epoch 27564 \t\t Training Loss: 0.0005808365531265736 \t\n",
      "Epoch 27565 \t\t Training Loss: 0.0005808365531265736 \t\n",
      "Epoch 27566 \t\t Training Loss: 0.0005808365531265736 \t\n",
      "Epoch 27567 \t\t Training Loss: 0.0005808365531265736 \t\n",
      "Epoch 27568 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27569 \t\t Training Loss: 0.0005808365531265736 \t\n",
      "Epoch 27570 \t\t Training Loss: 0.0005808365531265736 \t\n",
      "Epoch 27571 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27572 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27573 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27574 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27575 \t\t Training Loss: 0.0005808365531265736 \t\n",
      "Epoch 27576 \t\t Training Loss: 0.0005808365531265736 \t\n",
      "Epoch 27577 \t\t Training Loss: 0.0005808365531265736 \t\n",
      "Epoch 27578 \t\t Training Loss: 0.0005808365531265736 \t\n",
      "Epoch 27579 \t\t Training Loss: 0.0005808365531265736 \t\n",
      "Epoch 27580 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27581 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27582 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27583 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27584 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27585 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27586 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27587 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27588 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27589 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27590 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27591 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27592 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27593 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27594 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27595 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27596 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27597 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27598 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27599 \t\t Training Loss: 0.0005808364949189126 \t\n",
      "Epoch 27600 \t\t Training Loss: 0.0005808363785035908 \t\n",
      "Epoch 27601 \t\t Training Loss: 0.0005808363785035908 \t\n",
      "Epoch 27602 \t\t Training Loss: 0.0005808363785035908 \t\n",
      "Epoch 27603 \t\t Training Loss: 0.0005808363785035908 \t\n",
      "Epoch 27604 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27605 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27606 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27607 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27608 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27609 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27610 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27611 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27612 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27613 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27614 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27615 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27616 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27617 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27618 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27619 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27620 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27621 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27622 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27623 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27624 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27625 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27626 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27627 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27628 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27629 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27630 \t\t Training Loss: 0.000580836262088269 \t\n",
      "Epoch 27631 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27632 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27633 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27634 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27635 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27636 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27637 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27638 \t\t Training Loss: 0.0005808362038806081 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27639 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27640 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27641 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27642 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27643 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27644 \t\t Training Loss: 0.0005808362038806081 \t\n",
      "Epoch 27645 \t\t Training Loss: 0.0005808361456729472 \t\n",
      "Epoch 27646 \t\t Training Loss: 0.0005808361456729472 \t\n",
      "Epoch 27647 \t\t Training Loss: 0.0005808361456729472 \t\n",
      "Epoch 27648 \t\t Training Loss: 0.0005808361456729472 \t\n",
      "Epoch 27649 \t\t Training Loss: 0.0005808361456729472 \t\n",
      "Epoch 27650 \t\t Training Loss: 0.0005808361456729472 \t\n",
      "Epoch 27651 \t\t Training Loss: 0.0005808361456729472 \t\n",
      "Epoch 27652 \t\t Training Loss: 0.0005808360292576253 \t\n",
      "Epoch 27653 \t\t Training Loss: 0.0005808360292576253 \t\n",
      "Epoch 27654 \t\t Training Loss: 0.0005808360292576253 \t\n",
      "Epoch 27655 \t\t Training Loss: 0.0005808360292576253 \t\n",
      "Epoch 27656 \t\t Training Loss: 0.0005808360292576253 \t\n",
      "Epoch 27657 \t\t Training Loss: 0.0005808360292576253 \t\n",
      "Epoch 27658 \t\t Training Loss: 0.0005808360292576253 \t\n",
      "Epoch 27659 \t\t Training Loss: 0.0005808360292576253 \t\n",
      "Epoch 27660 \t\t Training Loss: 0.0005808360292576253 \t\n",
      "Epoch 27661 \t\t Training Loss: 0.0005808360292576253 \t\n",
      "Epoch 27662 \t\t Training Loss: 0.0005808360292576253 \t\n",
      "Epoch 27663 \t\t Training Loss: 0.0005808360292576253 \t\n",
      "Epoch 27664 \t\t Training Loss: 0.0005808360292576253 \t\n",
      "Epoch 27665 \t\t Training Loss: 0.0005808360292576253 \t\n",
      "Epoch 27666 \t\t Training Loss: 0.0005808359710499644 \t\n",
      "Epoch 27667 \t\t Training Loss: 0.0005808359710499644 \t\n",
      "Epoch 27668 \t\t Training Loss: 0.0005808359710499644 \t\n",
      "Epoch 27669 \t\t Training Loss: 0.0005808359710499644 \t\n",
      "Epoch 27670 \t\t Training Loss: 0.0005808359128423035 \t\n",
      "Epoch 27671 \t\t Training Loss: 0.0005808359128423035 \t\n",
      "Epoch 27672 \t\t Training Loss: 0.0005808359128423035 \t\n",
      "Epoch 27673 \t\t Training Loss: 0.0005808359128423035 \t\n",
      "Epoch 27674 \t\t Training Loss: 0.0005808359128423035 \t\n",
      "Epoch 27675 \t\t Training Loss: 0.0005808359128423035 \t\n",
      "Epoch 27676 \t\t Training Loss: 0.0005808359128423035 \t\n",
      "Epoch 27677 \t\t Training Loss: 0.0005808359128423035 \t\n",
      "Epoch 27678 \t\t Training Loss: 0.0005808359128423035 \t\n",
      "Epoch 27679 \t\t Training Loss: 0.0005808359128423035 \t\n",
      "Epoch 27680 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27681 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27682 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27683 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27684 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27685 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27686 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27687 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27688 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27689 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27690 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27691 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27692 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27693 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27694 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27695 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27696 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27697 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27698 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27699 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27700 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27701 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27702 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27703 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27704 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27705 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27706 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27707 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27708 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27709 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27710 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27711 \t\t Training Loss: 0.0005808358546346426 \t\n",
      "Epoch 27712 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27713 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27714 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27715 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27716 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27717 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27718 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27719 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27720 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27721 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27722 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27723 \t\t Training Loss: 0.0005808357382193208 \t\n",
      "Epoch 27724 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27725 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27726 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27727 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27728 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27729 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27730 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27731 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27732 \t\t Training Loss: 0.000580835621803999 \t\n",
      "Epoch 27733 \t\t Training Loss: 0.000580835621803999 \t\n",
      "Epoch 27734 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27735 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27736 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27737 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27738 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27739 \t\t Training Loss: 0.0005808356800116599 \t\n",
      "Epoch 27740 \t\t Training Loss: 0.000580835621803999 \t\n",
      "Epoch 27741 \t\t Training Loss: 0.000580835621803999 \t\n",
      "Epoch 27742 \t\t Training Loss: 0.000580835621803999 \t\n",
      "Epoch 27743 \t\t Training Loss: 0.000580835621803999 \t\n",
      "Epoch 27744 \t\t Training Loss: 0.000580835563596338 \t\n",
      "Epoch 27745 \t\t Training Loss: 0.000580835563596338 \t\n",
      "Epoch 27746 \t\t Training Loss: 0.000580835563596338 \t\n",
      "Epoch 27747 \t\t Training Loss: 0.000580835563596338 \t\n",
      "Epoch 27748 \t\t Training Loss: 0.000580835563596338 \t\n",
      "Epoch 27749 \t\t Training Loss: 0.000580835563596338 \t\n",
      "Epoch 27750 \t\t Training Loss: 0.000580835563596338 \t\n",
      "Epoch 27751 \t\t Training Loss: 0.000580835563596338 \t\n",
      "Epoch 27752 \t\t Training Loss: 0.000580835563596338 \t\n",
      "Epoch 27753 \t\t Training Loss: 0.000580835563596338 \t\n",
      "Epoch 27754 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27755 \t\t Training Loss: 0.000580835563596338 \t\n",
      "Epoch 27756 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27757 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27758 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27759 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27760 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27761 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27762 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27763 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27764 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27765 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27766 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27767 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27768 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27769 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27770 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27771 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27772 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27773 \t\t Training Loss: 0.0005808354471810162 \t\n",
      "Epoch 27774 \t\t Training Loss: 0.0005808353307656944 \t\n",
      "Epoch 27775 \t\t Training Loss: 0.0005808353307656944 \t\n",
      "Epoch 27776 \t\t Training Loss: 0.0005808353307656944 \t\n",
      "Epoch 27777 \t\t Training Loss: 0.0005808353307656944 \t\n",
      "Epoch 27778 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27779 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27780 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27781 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27782 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27783 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27784 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27785 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27786 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27787 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27788 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27789 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27790 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27791 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27792 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27793 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27794 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27795 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27796 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27797 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27798 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27799 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27800 \t\t Training Loss: 0.0005808351561427116 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27801 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27802 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27803 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27804 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27805 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27806 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27807 \t\t Training Loss: 0.0005808352725580335 \t\n",
      "Epoch 27808 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27809 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27810 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27811 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27812 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27813 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27814 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27815 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27816 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27817 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27818 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27819 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27820 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27821 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27822 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27823 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27824 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27825 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27826 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27827 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27828 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27829 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27830 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27831 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27832 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27833 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27834 \t\t Training Loss: 0.0005808351561427116 \t\n",
      "Epoch 27835 \t\t Training Loss: 0.0005808350397273898 \t\n",
      "Epoch 27836 \t\t Training Loss: 0.0005808350397273898 \t\n",
      "Epoch 27837 \t\t Training Loss: 0.0005808350397273898 \t\n",
      "Epoch 27838 \t\t Training Loss: 0.0005808350397273898 \t\n",
      "Epoch 27839 \t\t Training Loss: 0.0005808350397273898 \t\n",
      "Epoch 27840 \t\t Training Loss: 0.0005808350397273898 \t\n",
      "Epoch 27841 \t\t Training Loss: 0.0005808350397273898 \t\n",
      "Epoch 27842 \t\t Training Loss: 0.0005808350397273898 \t\n",
      "Epoch 27843 \t\t Training Loss: 0.0005808350397273898 \t\n",
      "Epoch 27844 \t\t Training Loss: 0.0005808350397273898 \t\n",
      "Epoch 27845 \t\t Training Loss: 0.0005808349815197289 \t\n",
      "Epoch 27846 \t\t Training Loss: 0.0005808349815197289 \t\n",
      "Epoch 27847 \t\t Training Loss: 0.0005808349815197289 \t\n",
      "Epoch 27848 \t\t Training Loss: 0.0005808349815197289 \t\n",
      "Epoch 27849 \t\t Training Loss: 0.0005808349815197289 \t\n",
      "Epoch 27850 \t\t Training Loss: 0.0005808349815197289 \t\n",
      "Epoch 27851 \t\t Training Loss: 0.0005808349815197289 \t\n",
      "Epoch 27852 \t\t Training Loss: 0.0005808349815197289 \t\n",
      "Epoch 27853 \t\t Training Loss: 0.0005808349815197289 \t\n",
      "Epoch 27854 \t\t Training Loss: 0.0005808349815197289 \t\n",
      "Epoch 27855 \t\t Training Loss: 0.000580834923312068 \t\n",
      "Epoch 27856 \t\t Training Loss: 0.0005808349815197289 \t\n",
      "Epoch 27857 \t\t Training Loss: 0.0005808349815197289 \t\n",
      "Epoch 27858 \t\t Training Loss: 0.0005808349815197289 \t\n",
      "Epoch 27859 \t\t Training Loss: 0.0005808349815197289 \t\n",
      "Epoch 27860 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27861 \t\t Training Loss: 0.000580834923312068 \t\n",
      "Epoch 27862 \t\t Training Loss: 0.000580834923312068 \t\n",
      "Epoch 27863 \t\t Training Loss: 0.000580834923312068 \t\n",
      "Epoch 27864 \t\t Training Loss: 0.000580834923312068 \t\n",
      "Epoch 27865 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27866 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27867 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27868 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27869 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27870 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27871 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27872 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27873 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27874 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27875 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27876 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27877 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27878 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27879 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27880 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27881 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27882 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27883 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27884 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27885 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27886 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27887 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27888 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27889 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27890 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27891 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27892 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27893 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27894 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27895 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27896 \t\t Training Loss: 0.0005808348651044071 \t\n",
      "Epoch 27897 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27898 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27899 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27900 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27901 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27902 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27903 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27904 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27905 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27906 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27907 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27908 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27909 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27910 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27911 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27912 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27913 \t\t Training Loss: 0.0005808346322737634 \t\n",
      "Epoch 27914 \t\t Training Loss: 0.0005808346322737634 \t\n",
      "Epoch 27915 \t\t Training Loss: 0.0005808346322737634 \t\n",
      "Epoch 27916 \t\t Training Loss: 0.0005808346322737634 \t\n",
      "Epoch 27917 \t\t Training Loss: 0.0005808346322737634 \t\n",
      "Epoch 27918 \t\t Training Loss: 0.0005808347486890852 \t\n",
      "Epoch 27919 \t\t Training Loss: 0.0005808346322737634 \t\n",
      "Epoch 27920 \t\t Training Loss: 0.0005808346322737634 \t\n",
      "Epoch 27921 \t\t Training Loss: 0.0005808346322737634 \t\n",
      "Epoch 27922 \t\t Training Loss: 0.0005808346322737634 \t\n",
      "Epoch 27923 \t\t Training Loss: 0.0005808346322737634 \t\n",
      "Epoch 27924 \t\t Training Loss: 0.0005808346322737634 \t\n",
      "Epoch 27925 \t\t Training Loss: 0.0005808346322737634 \t\n",
      "Epoch 27926 \t\t Training Loss: 0.0005808345740661025 \t\n",
      "Epoch 27927 \t\t Training Loss: 0.0005808345740661025 \t\n",
      "Epoch 27928 \t\t Training Loss: 0.0005808346322737634 \t\n",
      "Epoch 27929 \t\t Training Loss: 0.0005808345740661025 \t\n",
      "Epoch 27930 \t\t Training Loss: 0.0005808345740661025 \t\n",
      "Epoch 27931 \t\t Training Loss: 0.0005808345740661025 \t\n",
      "Epoch 27932 \t\t Training Loss: 0.0005808346322737634 \t\n",
      "Epoch 27933 \t\t Training Loss: 0.0005808345740661025 \t\n",
      "Epoch 27934 \t\t Training Loss: 0.0005808345740661025 \t\n",
      "Epoch 27935 \t\t Training Loss: 0.0005808345740661025 \t\n",
      "Epoch 27936 \t\t Training Loss: 0.0005808345740661025 \t\n",
      "Epoch 27937 \t\t Training Loss: 0.0005808345740661025 \t\n",
      "Epoch 27938 \t\t Training Loss: 0.0005808345740661025 \t\n",
      "Epoch 27939 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27940 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27941 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27942 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27943 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27944 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27945 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27946 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27947 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27948 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27949 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27950 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27951 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27952 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27953 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27954 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27955 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27956 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27957 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27958 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27959 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27960 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27961 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27962 \t\t Training Loss: 0.0005808344576507807 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27963 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27964 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27965 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27966 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27967 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27968 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27969 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27970 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27971 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27972 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27973 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27974 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27975 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27976 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27977 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27978 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27979 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27980 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27981 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27982 \t\t Training Loss: 0.0005808344576507807 \t\n",
      "Epoch 27983 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 27984 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 27985 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 27986 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 27987 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 27988 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 27989 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 27990 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 27991 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 27992 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 27993 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 27994 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 27995 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 27996 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 27997 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 27998 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 27999 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 28000 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 28001 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 28002 \t\t Training Loss: 0.0005808343412354589 \t\n",
      "Epoch 28003 \t\t Training Loss: 0.0005808342830277979 \t\n",
      "Epoch 28004 \t\t Training Loss: 0.0005808342830277979 \t\n",
      "Epoch 28005 \t\t Training Loss: 0.0005808342830277979 \t\n",
      "Epoch 28006 \t\t Training Loss: 0.0005808342830277979 \t\n",
      "Epoch 28007 \t\t Training Loss: 0.0005808341666124761 \t\n",
      "Epoch 28008 \t\t Training Loss: 0.0005808341666124761 \t\n",
      "Epoch 28009 \t\t Training Loss: 0.0005808341666124761 \t\n",
      "Epoch 28010 \t\t Training Loss: 0.0005808342830277979 \t\n",
      "Epoch 28011 \t\t Training Loss: 0.0005808341666124761 \t\n",
      "Epoch 28012 \t\t Training Loss: 0.0005808341666124761 \t\n",
      "Epoch 28013 \t\t Training Loss: 0.0005808341666124761 \t\n",
      "Epoch 28014 \t\t Training Loss: 0.0005808341666124761 \t\n",
      "Epoch 28015 \t\t Training Loss: 0.0005808341666124761 \t\n",
      "Epoch 28016 \t\t Training Loss: 0.0005808341666124761 \t\n",
      "Epoch 28017 \t\t Training Loss: 0.0005808341666124761 \t\n",
      "Epoch 28018 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28019 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28020 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28021 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28022 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28023 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28024 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28025 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28026 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28027 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28028 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28029 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28030 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28031 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28032 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28033 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28034 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28035 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28036 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28037 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28038 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28039 \t\t Training Loss: 0.0005808339919894934 \t\n",
      "Epoch 28040 \t\t Training Loss: 0.0005808339919894934 \t\n",
      "Epoch 28041 \t\t Training Loss: 0.0005808339919894934 \t\n",
      "Epoch 28042 \t\t Training Loss: 0.0005808340501971543 \t\n",
      "Epoch 28043 \t\t Training Loss: 0.0005808339919894934 \t\n",
      "Epoch 28044 \t\t Training Loss: 0.0005808339919894934 \t\n",
      "Epoch 28045 \t\t Training Loss: 0.0005808339919894934 \t\n",
      "Epoch 28046 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28047 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28048 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28049 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28050 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28051 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28052 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28053 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28054 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28055 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28056 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28057 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28058 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28059 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28060 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28061 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28062 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28063 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28064 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28065 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28066 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28067 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28068 \t\t Training Loss: 0.0005808338755741715 \t\n",
      "Epoch 28069 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28070 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28071 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28072 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28073 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28074 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28075 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28076 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28077 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28078 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28079 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28080 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28081 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28082 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28083 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28084 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28085 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28086 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28087 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28088 \t\t Training Loss: 0.0005808337591588497 \t\n",
      "Epoch 28089 \t\t Training Loss: 0.0005808336427435279 \t\n",
      "Epoch 28090 \t\t Training Loss: 0.0005808336427435279 \t\n",
      "Epoch 28091 \t\t Training Loss: 0.0005808336427435279 \t\n",
      "Epoch 28092 \t\t Training Loss: 0.0005808336427435279 \t\n",
      "Epoch 28093 \t\t Training Loss: 0.0005808336427435279 \t\n",
      "Epoch 28094 \t\t Training Loss: 0.0005808336427435279 \t\n",
      "Epoch 28095 \t\t Training Loss: 0.0005808336427435279 \t\n",
      "Epoch 28096 \t\t Training Loss: 0.0005808336427435279 \t\n",
      "Epoch 28097 \t\t Training Loss: 0.0005808336427435279 \t\n",
      "Epoch 28098 \t\t Training Loss: 0.0005808336427435279 \t\n",
      "Epoch 28099 \t\t Training Loss: 0.000580833584535867 \t\n",
      "Epoch 28100 \t\t Training Loss: 0.000580833584535867 \t\n",
      "Epoch 28101 \t\t Training Loss: 0.000580833584535867 \t\n",
      "Epoch 28102 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28103 \t\t Training Loss: 0.000580833584535867 \t\n",
      "Epoch 28104 \t\t Training Loss: 0.000580833584535867 \t\n",
      "Epoch 28105 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28106 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28107 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28108 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28109 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28110 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28111 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28112 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28113 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28114 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28115 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28116 \t\t Training Loss: 0.0005808334681205451 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28117 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28118 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28119 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28120 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28121 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28122 \t\t Training Loss: 0.0005808334681205451 \t\n",
      "Epoch 28123 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28124 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28125 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28126 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28127 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28128 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28129 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28130 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28131 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28132 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28133 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28134 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28135 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28136 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28137 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28138 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28139 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28140 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28141 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28142 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28143 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28144 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28145 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28146 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28147 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28148 \t\t Training Loss: 0.0005808333517052233 \t\n",
      "Epoch 28149 \t\t Training Loss: 0.0005808332934975624 \t\n",
      "Epoch 28150 \t\t Training Loss: 0.0005808332934975624 \t\n",
      "Epoch 28151 \t\t Training Loss: 0.0005808332934975624 \t\n",
      "Epoch 28152 \t\t Training Loss: 0.0005808332352899015 \t\n",
      "Epoch 28153 \t\t Training Loss: 0.0005808332934975624 \t\n",
      "Epoch 28154 \t\t Training Loss: 0.0005808332934975624 \t\n",
      "Epoch 28155 \t\t Training Loss: 0.0005808332934975624 \t\n",
      "Epoch 28156 \t\t Training Loss: 0.0005808332352899015 \t\n",
      "Epoch 28157 \t\t Training Loss: 0.0005808332352899015 \t\n",
      "Epoch 28158 \t\t Training Loss: 0.0005808332352899015 \t\n",
      "Epoch 28159 \t\t Training Loss: 0.0005808332352899015 \t\n",
      "Epoch 28160 \t\t Training Loss: 0.0005808332352899015 \t\n",
      "Epoch 28161 \t\t Training Loss: 0.0005808332352899015 \t\n",
      "Epoch 28162 \t\t Training Loss: 0.0005808332352899015 \t\n",
      "Epoch 28163 \t\t Training Loss: 0.0005808332352899015 \t\n",
      "Epoch 28164 \t\t Training Loss: 0.0005808332352899015 \t\n",
      "Epoch 28165 \t\t Training Loss: 0.0005808332352899015 \t\n",
      "Epoch 28166 \t\t Training Loss: 0.0005808332352899015 \t\n",
      "Epoch 28167 \t\t Training Loss: 0.0005808332352899015 \t\n",
      "Epoch 28168 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28169 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28170 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28171 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28172 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28173 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28174 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28175 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28176 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28177 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28178 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28179 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28180 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28181 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28182 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28183 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28184 \t\t Training Loss: 0.0005808330606669188 \t\n",
      "Epoch 28185 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28186 \t\t Training Loss: 0.0005808330606669188 \t\n",
      "Epoch 28187 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28188 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28189 \t\t Training Loss: 0.0005808331770822406 \t\n",
      "Epoch 28190 \t\t Training Loss: 0.0005808330606669188 \t\n",
      "Epoch 28191 \t\t Training Loss: 0.0005808330606669188 \t\n",
      "Epoch 28192 \t\t Training Loss: 0.0005808330606669188 \t\n",
      "Epoch 28193 \t\t Training Loss: 0.0005808330606669188 \t\n",
      "Epoch 28194 \t\t Training Loss: 0.0005808330606669188 \t\n",
      "Epoch 28195 \t\t Training Loss: 0.0005808330606669188 \t\n",
      "Epoch 28196 \t\t Training Loss: 0.0005808330606669188 \t\n",
      "Epoch 28197 \t\t Training Loss: 0.0005808330606669188 \t\n",
      "Epoch 28198 \t\t Training Loss: 0.0005808330606669188 \t\n",
      "Epoch 28199 \t\t Training Loss: 0.0005808330024592578 \t\n",
      "Epoch 28200 \t\t Training Loss: 0.0005808330606669188 \t\n",
      "Epoch 28201 \t\t Training Loss: 0.0005808330024592578 \t\n",
      "Epoch 28202 \t\t Training Loss: 0.0005808330024592578 \t\n",
      "Epoch 28203 \t\t Training Loss: 0.0005808330024592578 \t\n",
      "Epoch 28204 \t\t Training Loss: 0.0005808330024592578 \t\n",
      "Epoch 28205 \t\t Training Loss: 0.0005808330024592578 \t\n",
      "Epoch 28206 \t\t Training Loss: 0.0005808330024592578 \t\n",
      "Epoch 28207 \t\t Training Loss: 0.0005808330024592578 \t\n",
      "Epoch 28208 \t\t Training Loss: 0.0005808330024592578 \t\n",
      "Epoch 28209 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28210 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28211 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28212 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28213 \t\t Training Loss: 0.0005808330024592578 \t\n",
      "Epoch 28214 \t\t Training Loss: 0.0005808330024592578 \t\n",
      "Epoch 28215 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28216 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28217 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28218 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28219 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28220 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28221 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28222 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28223 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28224 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28225 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28226 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28227 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28228 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28229 \t\t Training Loss: 0.0005808329442515969 \t\n",
      "Epoch 28230 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28231 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28232 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28233 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28234 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28235 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28236 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28237 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28238 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28239 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28240 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28241 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28242 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28243 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28244 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28245 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28246 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28247 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28248 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28249 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28250 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28251 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28252 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28253 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28254 \t\t Training Loss: 0.000580832886043936 \t\n",
      "Epoch 28255 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28256 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28257 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28258 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28259 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28260 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28261 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28262 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28263 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28264 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28265 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28266 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28267 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28268 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28269 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28270 \t\t Training Loss: 0.0005808327696286142 \t\n",
      "Epoch 28271 \t\t Training Loss: 0.0005808327114209533 \t\n",
      "Epoch 28272 \t\t Training Loss: 0.0005808327114209533 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28273 \t\t Training Loss: 0.0005808327114209533 \t\n",
      "Epoch 28274 \t\t Training Loss: 0.0005808327114209533 \t\n",
      "Epoch 28275 \t\t Training Loss: 0.0005808327114209533 \t\n",
      "Epoch 28276 \t\t Training Loss: 0.0005808327114209533 \t\n",
      "Epoch 28277 \t\t Training Loss: 0.0005808327114209533 \t\n",
      "Epoch 28278 \t\t Training Loss: 0.0005808327114209533 \t\n",
      "Epoch 28279 \t\t Training Loss: 0.0005808327114209533 \t\n",
      "Epoch 28280 \t\t Training Loss: 0.0005808327114209533 \t\n",
      "Epoch 28281 \t\t Training Loss: 0.0005808326532132924 \t\n",
      "Epoch 28282 \t\t Training Loss: 0.0005808326532132924 \t\n",
      "Epoch 28283 \t\t Training Loss: 0.0005808326532132924 \t\n",
      "Epoch 28284 \t\t Training Loss: 0.0005808326532132924 \t\n",
      "Epoch 28285 \t\t Training Loss: 0.0005808326532132924 \t\n",
      "Epoch 28286 \t\t Training Loss: 0.0005808326532132924 \t\n",
      "Epoch 28287 \t\t Training Loss: 0.0005808326532132924 \t\n",
      "Epoch 28288 \t\t Training Loss: 0.0005808326532132924 \t\n",
      "Epoch 28289 \t\t Training Loss: 0.0005808326532132924 \t\n",
      "Epoch 28290 \t\t Training Loss: 0.0005808326532132924 \t\n",
      "Epoch 28291 \t\t Training Loss: 0.0005808325950056314 \t\n",
      "Epoch 28292 \t\t Training Loss: 0.0005808325950056314 \t\n",
      "Epoch 28293 \t\t Training Loss: 0.0005808325950056314 \t\n",
      "Epoch 28294 \t\t Training Loss: 0.0005808325950056314 \t\n",
      "Epoch 28295 \t\t Training Loss: 0.0005808325950056314 \t\n",
      "Epoch 28296 \t\t Training Loss: 0.0005808325950056314 \t\n",
      "Epoch 28297 \t\t Training Loss: 0.0005808325950056314 \t\n",
      "Epoch 28298 \t\t Training Loss: 0.0005808325367979705 \t\n",
      "Epoch 28299 \t\t Training Loss: 0.0005808324785903096 \t\n",
      "Epoch 28300 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28301 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28302 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28303 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28304 \t\t Training Loss: 0.0005808324785903096 \t\n",
      "Epoch 28305 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28306 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28307 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28308 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28309 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28310 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28311 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28312 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28313 \t\t Training Loss: 0.0005808324785903096 \t\n",
      "Epoch 28314 \t\t Training Loss: 0.0005808324785903096 \t\n",
      "Epoch 28315 \t\t Training Loss: 0.0005808324785903096 \t\n",
      "Epoch 28316 \t\t Training Loss: 0.0005808324785903096 \t\n",
      "Epoch 28317 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28318 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28319 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28320 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28321 \t\t Training Loss: 0.0005808324785903096 \t\n",
      "Epoch 28322 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28323 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28324 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28325 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28326 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28327 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28328 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28329 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28330 \t\t Training Loss: 0.0005808324203826487 \t\n",
      "Epoch 28331 \t\t Training Loss: 0.0005808323621749878 \t\n",
      "Epoch 28332 \t\t Training Loss: 0.0005808323621749878 \t\n",
      "Epoch 28333 \t\t Training Loss: 0.0005808323621749878 \t\n",
      "Epoch 28334 \t\t Training Loss: 0.0005808323039673269 \t\n",
      "Epoch 28335 \t\t Training Loss: 0.0005808323039673269 \t\n",
      "Epoch 28336 \t\t Training Loss: 0.0005808323039673269 \t\n",
      "Epoch 28337 \t\t Training Loss: 0.0005808323039673269 \t\n",
      "Epoch 28338 \t\t Training Loss: 0.0005808323039673269 \t\n",
      "Epoch 28339 \t\t Training Loss: 0.0005808323039673269 \t\n",
      "Epoch 28340 \t\t Training Loss: 0.0005808323039673269 \t\n",
      "Epoch 28341 \t\t Training Loss: 0.0005808323039673269 \t\n",
      "Epoch 28342 \t\t Training Loss: 0.0005808323039673269 \t\n",
      "Epoch 28343 \t\t Training Loss: 0.0005808323039673269 \t\n",
      "Epoch 28344 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28345 \t\t Training Loss: 0.0005808323039673269 \t\n",
      "Epoch 28346 \t\t Training Loss: 0.0005808323039673269 \t\n",
      "Epoch 28347 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28348 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28349 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28350 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28351 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28352 \t\t Training Loss: 0.0005808323039673269 \t\n",
      "Epoch 28353 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28354 \t\t Training Loss: 0.0005808323039673269 \t\n",
      "Epoch 28355 \t\t Training Loss: 0.0005808323039673269 \t\n",
      "Epoch 28356 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28357 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28358 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28359 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28360 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28361 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28362 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28363 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28364 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28365 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28366 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28367 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28368 \t\t Training Loss: 0.000580832245759666 \t\n",
      "Epoch 28369 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28370 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28371 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28372 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28373 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28374 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28375 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28376 \t\t Training Loss: 0.0005808321293443441 \t\n",
      "Epoch 28377 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28378 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28379 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28380 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28381 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28382 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28383 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28384 \t\t Training Loss: 0.000580832187552005 \t\n",
      "Epoch 28385 \t\t Training Loss: 0.0005808321293443441 \t\n",
      "Epoch 28386 \t\t Training Loss: 0.0005808321293443441 \t\n",
      "Epoch 28387 \t\t Training Loss: 0.0005808321293443441 \t\n",
      "Epoch 28388 \t\t Training Loss: 0.0005808321293443441 \t\n",
      "Epoch 28389 \t\t Training Loss: 0.0005808321293443441 \t\n",
      "Epoch 28390 \t\t Training Loss: 0.0005808320711366832 \t\n",
      "Epoch 28391 \t\t Training Loss: 0.0005808320711366832 \t\n",
      "Epoch 28392 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28393 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28394 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28395 \t\t Training Loss: 0.0005808320711366832 \t\n",
      "Epoch 28396 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28397 \t\t Training Loss: 0.0005808320711366832 \t\n",
      "Epoch 28398 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28399 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28400 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28401 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28402 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28403 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28404 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28405 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28406 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28407 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28408 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28409 \t\t Training Loss: 0.0005808320711366832 \t\n",
      "Epoch 28410 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28411 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28412 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28413 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28414 \t\t Training Loss: 0.0005808319547213614 \t\n",
      "Epoch 28415 \t\t Training Loss: 0.0005808319547213614 \t\n",
      "Epoch 28416 \t\t Training Loss: 0.0005808318965137005 \t\n",
      "Epoch 28417 \t\t Training Loss: 0.0005808318965137005 \t\n",
      "Epoch 28418 \t\t Training Loss: 0.0005808319547213614 \t\n",
      "Epoch 28419 \t\t Training Loss: 0.0005808319547213614 \t\n",
      "Epoch 28420 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28421 \t\t Training Loss: 0.0005808320129290223 \t\n",
      "Epoch 28422 \t\t Training Loss: 0.0005808318965137005 \t\n",
      "Epoch 28423 \t\t Training Loss: 0.0005808318965137005 \t\n",
      "Epoch 28424 \t\t Training Loss: 0.0005808318965137005 \t\n",
      "Epoch 28425 \t\t Training Loss: 0.0005808318965137005 \t\n",
      "Epoch 28426 \t\t Training Loss: 0.0005808318965137005 \t\n",
      "Epoch 28427 \t\t Training Loss: 0.0005808318965137005 \t\n",
      "Epoch 28428 \t\t Training Loss: 0.0005808318965137005 \t\n",
      "Epoch 28429 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28430 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28431 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28432 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28433 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28434 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28435 \t\t Training Loss: 0.0005808318383060396 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28436 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28437 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28438 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28439 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28440 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28441 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28442 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28443 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28444 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28445 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28446 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28447 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28448 \t\t Training Loss: 0.0005808318383060396 \t\n",
      "Epoch 28449 \t\t Training Loss: 0.0005808317800983787 \t\n",
      "Epoch 28450 \t\t Training Loss: 0.0005808317800983787 \t\n",
      "Epoch 28451 \t\t Training Loss: 0.0005808317800983787 \t\n",
      "Epoch 28452 \t\t Training Loss: 0.0005808317800983787 \t\n",
      "Epoch 28453 \t\t Training Loss: 0.0005808317800983787 \t\n",
      "Epoch 28454 \t\t Training Loss: 0.0005808317800983787 \t\n",
      "Epoch 28455 \t\t Training Loss: 0.0005808317800983787 \t\n",
      "Epoch 28456 \t\t Training Loss: 0.0005808317218907177 \t\n",
      "Epoch 28457 \t\t Training Loss: 0.0005808316636830568 \t\n",
      "Epoch 28458 \t\t Training Loss: 0.0005808317218907177 \t\n",
      "Epoch 28459 \t\t Training Loss: 0.0005808317218907177 \t\n",
      "Epoch 28460 \t\t Training Loss: 0.0005808316636830568 \t\n",
      "Epoch 28461 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28462 \t\t Training Loss: 0.0005808316636830568 \t\n",
      "Epoch 28463 \t\t Training Loss: 0.0005808317218907177 \t\n",
      "Epoch 28464 \t\t Training Loss: 0.0005808316636830568 \t\n",
      "Epoch 28465 \t\t Training Loss: 0.0005808316636830568 \t\n",
      "Epoch 28466 \t\t Training Loss: 0.0005808316636830568 \t\n",
      "Epoch 28467 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28468 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28469 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28470 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28471 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28472 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28473 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28474 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28475 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28476 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28477 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28478 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28479 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28480 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28481 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28482 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28483 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28484 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28485 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28486 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28487 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28488 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28489 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28490 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28491 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28492 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28493 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28494 \t\t Training Loss: 0.0005808316054753959 \t\n",
      "Epoch 28495 \t\t Training Loss: 0.000580831547267735 \t\n",
      "Epoch 28496 \t\t Training Loss: 0.000580831547267735 \t\n",
      "Epoch 28497 \t\t Training Loss: 0.000580831547267735 \t\n",
      "Epoch 28498 \t\t Training Loss: 0.000580831547267735 \t\n",
      "Epoch 28499 \t\t Training Loss: 0.000580831547267735 \t\n",
      "Epoch 28500 \t\t Training Loss: 0.0005808314890600741 \t\n",
      "Epoch 28501 \t\t Training Loss: 0.0005808314308524132 \t\n",
      "Epoch 28502 \t\t Training Loss: 0.000580831547267735 \t\n",
      "Epoch 28503 \t\t Training Loss: 0.000580831547267735 \t\n",
      "Epoch 28504 \t\t Training Loss: 0.000580831547267735 \t\n",
      "Epoch 28505 \t\t Training Loss: 0.0005808314308524132 \t\n",
      "Epoch 28506 \t\t Training Loss: 0.0005808314308524132 \t\n",
      "Epoch 28507 \t\t Training Loss: 0.0005808314308524132 \t\n",
      "Epoch 28508 \t\t Training Loss: 0.0005808314308524132 \t\n",
      "Epoch 28509 \t\t Training Loss: 0.0005808314308524132 \t\n",
      "Epoch 28510 \t\t Training Loss: 0.0005808314308524132 \t\n",
      "Epoch 28511 \t\t Training Loss: 0.0005808314308524132 \t\n",
      "Epoch 28512 \t\t Training Loss: 0.0005808314308524132 \t\n",
      "Epoch 28513 \t\t Training Loss: 0.0005808314308524132 \t\n",
      "Epoch 28514 \t\t Training Loss: 0.0005808314308524132 \t\n",
      "Epoch 28515 \t\t Training Loss: 0.0005808314308524132 \t\n",
      "Epoch 28516 \t\t Training Loss: 0.0005808314308524132 \t\n",
      "Epoch 28517 \t\t Training Loss: 0.0005808314308524132 \t\n",
      "Epoch 28518 \t\t Training Loss: 0.0005808314308524132 \t\n",
      "Epoch 28519 \t\t Training Loss: 0.0005808313726447523 \t\n",
      "Epoch 28520 \t\t Training Loss: 0.0005808313726447523 \t\n",
      "Epoch 28521 \t\t Training Loss: 0.0005808313726447523 \t\n",
      "Epoch 28522 \t\t Training Loss: 0.0005808313726447523 \t\n",
      "Epoch 28523 \t\t Training Loss: 0.0005808313726447523 \t\n",
      "Epoch 28524 \t\t Training Loss: 0.0005808313726447523 \t\n",
      "Epoch 28525 \t\t Training Loss: 0.0005808313726447523 \t\n",
      "Epoch 28526 \t\t Training Loss: 0.0005808313726447523 \t\n",
      "Epoch 28527 \t\t Training Loss: 0.0005808313144370914 \t\n",
      "Epoch 28528 \t\t Training Loss: 0.0005808313144370914 \t\n",
      "Epoch 28529 \t\t Training Loss: 0.0005808313144370914 \t\n",
      "Epoch 28530 \t\t Training Loss: 0.0005808313144370914 \t\n",
      "Epoch 28531 \t\t Training Loss: 0.0005808313144370914 \t\n",
      "Epoch 28532 \t\t Training Loss: 0.0005808313144370914 \t\n",
      "Epoch 28533 \t\t Training Loss: 0.0005808313144370914 \t\n",
      "Epoch 28534 \t\t Training Loss: 0.0005808313144370914 \t\n",
      "Epoch 28535 \t\t Training Loss: 0.0005808313144370914 \t\n",
      "Epoch 28536 \t\t Training Loss: 0.0005808312562294304 \t\n",
      "Epoch 28537 \t\t Training Loss: 0.0005808312562294304 \t\n",
      "Epoch 28538 \t\t Training Loss: 0.0005808312562294304 \t\n",
      "Epoch 28539 \t\t Training Loss: 0.0005808313144370914 \t\n",
      "Epoch 28540 \t\t Training Loss: 0.0005808313144370914 \t\n",
      "Epoch 28541 \t\t Training Loss: 0.0005808312562294304 \t\n",
      "Epoch 28542 \t\t Training Loss: 0.0005808313144370914 \t\n",
      "Epoch 28543 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28544 \t\t Training Loss: 0.0005808312562294304 \t\n",
      "Epoch 28545 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28546 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28547 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28548 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28549 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28550 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28551 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28552 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28553 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28554 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28555 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28556 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28557 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28558 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28559 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28560 \t\t Training Loss: 0.0005808311980217695 \t\n",
      "Epoch 28561 \t\t Training Loss: 0.0005808311398141086 \t\n",
      "Epoch 28562 \t\t Training Loss: 0.0005808311398141086 \t\n",
      "Epoch 28563 \t\t Training Loss: 0.0005808311398141086 \t\n",
      "Epoch 28564 \t\t Training Loss: 0.0005808311398141086 \t\n",
      "Epoch 28565 \t\t Training Loss: 0.0005808311398141086 \t\n",
      "Epoch 28566 \t\t Training Loss: 0.0005808311398141086 \t\n",
      "Epoch 28567 \t\t Training Loss: 0.0005808311398141086 \t\n",
      "Epoch 28568 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28569 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28570 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28571 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28572 \t\t Training Loss: 0.0005808311398141086 \t\n",
      "Epoch 28573 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28574 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28575 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28576 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28577 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28578 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28579 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28580 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28581 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28582 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28583 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28584 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28585 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28586 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28587 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28588 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28589 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28590 \t\t Training Loss: 0.0005808310233987868 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28591 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28592 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28593 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28594 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28595 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28596 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28597 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28598 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28599 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28600 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28601 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28602 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28603 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28604 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28605 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28606 \t\t Training Loss: 0.0005808309651911259 \t\n",
      "Epoch 28607 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28608 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28609 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28610 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28611 \t\t Training Loss: 0.0005808310233987868 \t\n",
      "Epoch 28612 \t\t Training Loss: 0.0005808309651911259 \t\n",
      "Epoch 28613 \t\t Training Loss: 0.0005808309651911259 \t\n",
      "Epoch 28614 \t\t Training Loss: 0.0005808309651911259 \t\n",
      "Epoch 28615 \t\t Training Loss: 0.0005808309651911259 \t\n",
      "Epoch 28616 \t\t Training Loss: 0.0005808309651911259 \t\n",
      "Epoch 28617 \t\t Training Loss: 0.0005808309651911259 \t\n",
      "Epoch 28618 \t\t Training Loss: 0.0005808309651911259 \t\n",
      "Epoch 28619 \t\t Training Loss: 0.0005808309651911259 \t\n",
      "Epoch 28620 \t\t Training Loss: 0.000580830906983465 \t\n",
      "Epoch 28621 \t\t Training Loss: 0.000580830906983465 \t\n",
      "Epoch 28622 \t\t Training Loss: 0.000580830906983465 \t\n",
      "Epoch 28623 \t\t Training Loss: 0.000580830906983465 \t\n",
      "Epoch 28624 \t\t Training Loss: 0.000580830848775804 \t\n",
      "Epoch 28625 \t\t Training Loss: 0.000580830848775804 \t\n",
      "Epoch 28626 \t\t Training Loss: 0.000580830848775804 \t\n",
      "Epoch 28627 \t\t Training Loss: 0.000580830848775804 \t\n",
      "Epoch 28628 \t\t Training Loss: 0.000580830848775804 \t\n",
      "Epoch 28629 \t\t Training Loss: 0.000580830848775804 \t\n",
      "Epoch 28630 \t\t Training Loss: 0.000580830848775804 \t\n",
      "Epoch 28631 \t\t Training Loss: 0.000580830848775804 \t\n",
      "Epoch 28632 \t\t Training Loss: 0.000580830848775804 \t\n",
      "Epoch 28633 \t\t Training Loss: 0.000580830848775804 \t\n",
      "Epoch 28634 \t\t Training Loss: 0.000580830848775804 \t\n",
      "Epoch 28635 \t\t Training Loss: 0.000580830848775804 \t\n",
      "Epoch 28636 \t\t Training Loss: 0.000580830848775804 \t\n",
      "Epoch 28637 \t\t Training Loss: 0.000580830848775804 \t\n",
      "Epoch 28638 \t\t Training Loss: 0.0005808307905681431 \t\n",
      "Epoch 28639 \t\t Training Loss: 0.0005808307905681431 \t\n",
      "Epoch 28640 \t\t Training Loss: 0.000580830848775804 \t\n",
      "Epoch 28641 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28642 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28643 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28644 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28645 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28646 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28647 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28648 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28649 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28650 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28651 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28652 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28653 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28654 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28655 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28656 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28657 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28658 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28659 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28660 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28661 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28662 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28663 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28664 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28665 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28666 \t\t Training Loss: 0.0005808306741528213 \t\n",
      "Epoch 28667 \t\t Training Loss: 0.0005808307323604822 \t\n",
      "Epoch 28668 \t\t Training Loss: 0.0005808306159451604 \t\n",
      "Epoch 28669 \t\t Training Loss: 0.0005808306159451604 \t\n",
      "Epoch 28670 \t\t Training Loss: 0.0005808306159451604 \t\n",
      "Epoch 28671 \t\t Training Loss: 0.0005808306159451604 \t\n",
      "Epoch 28672 \t\t Training Loss: 0.0005808306159451604 \t\n",
      "Epoch 28673 \t\t Training Loss: 0.0005808306159451604 \t\n",
      "Epoch 28674 \t\t Training Loss: 0.0005808306159451604 \t\n",
      "Epoch 28675 \t\t Training Loss: 0.0005808306159451604 \t\n",
      "Epoch 28676 \t\t Training Loss: 0.0005808306159451604 \t\n",
      "Epoch 28677 \t\t Training Loss: 0.0005808306159451604 \t\n",
      "Epoch 28678 \t\t Training Loss: 0.0005808306159451604 \t\n",
      "Epoch 28679 \t\t Training Loss: 0.0005808306159451604 \t\n",
      "Epoch 28680 \t\t Training Loss: 0.0005808305577374995 \t\n",
      "Epoch 28681 \t\t Training Loss: 0.0005808305577374995 \t\n",
      "Epoch 28682 \t\t Training Loss: 0.0005808305577374995 \t\n",
      "Epoch 28683 \t\t Training Loss: 0.0005808305577374995 \t\n",
      "Epoch 28684 \t\t Training Loss: 0.0005808304995298386 \t\n",
      "Epoch 28685 \t\t Training Loss: 0.0005808304995298386 \t\n",
      "Epoch 28686 \t\t Training Loss: 0.0005808304995298386 \t\n",
      "Epoch 28687 \t\t Training Loss: 0.0005808304995298386 \t\n",
      "Epoch 28688 \t\t Training Loss: 0.0005808304995298386 \t\n",
      "Epoch 28689 \t\t Training Loss: 0.0005808304995298386 \t\n",
      "Epoch 28690 \t\t Training Loss: 0.0005808304995298386 \t\n",
      "Epoch 28691 \t\t Training Loss: 0.0005808304413221776 \t\n",
      "Epoch 28692 \t\t Training Loss: 0.0005808304413221776 \t\n",
      "Epoch 28693 \t\t Training Loss: 0.0005808304413221776 \t\n",
      "Epoch 28694 \t\t Training Loss: 0.0005808304413221776 \t\n",
      "Epoch 28695 \t\t Training Loss: 0.0005808304413221776 \t\n",
      "Epoch 28696 \t\t Training Loss: 0.0005808303831145167 \t\n",
      "Epoch 28697 \t\t Training Loss: 0.0005808304413221776 \t\n",
      "Epoch 28698 \t\t Training Loss: 0.0005808303831145167 \t\n",
      "Epoch 28699 \t\t Training Loss: 0.0005808304413221776 \t\n",
      "Epoch 28700 \t\t Training Loss: 0.0005808303831145167 \t\n",
      "Epoch 28701 \t\t Training Loss: 0.0005808303831145167 \t\n",
      "Epoch 28702 \t\t Training Loss: 0.0005808303831145167 \t\n",
      "Epoch 28703 \t\t Training Loss: 0.0005808303831145167 \t\n",
      "Epoch 28704 \t\t Training Loss: 0.0005808303831145167 \t\n",
      "Epoch 28705 \t\t Training Loss: 0.0005808303831145167 \t\n",
      "Epoch 28706 \t\t Training Loss: 0.0005808303831145167 \t\n",
      "Epoch 28707 \t\t Training Loss: 0.0005808303831145167 \t\n",
      "Epoch 28708 \t\t Training Loss: 0.0005808303831145167 \t\n",
      "Epoch 28709 \t\t Training Loss: 0.0005808303831145167 \t\n",
      "Epoch 28710 \t\t Training Loss: 0.0005808303831145167 \t\n",
      "Epoch 28711 \t\t Training Loss: 0.0005808302666991949 \t\n",
      "Epoch 28712 \t\t Training Loss: 0.0005808302666991949 \t\n",
      "Epoch 28713 \t\t Training Loss: 0.0005808302666991949 \t\n",
      "Epoch 28714 \t\t Training Loss: 0.0005808302666991949 \t\n",
      "Epoch 28715 \t\t Training Loss: 0.0005808302666991949 \t\n",
      "Epoch 28716 \t\t Training Loss: 0.0005808302666991949 \t\n",
      "Epoch 28717 \t\t Training Loss: 0.0005808303831145167 \t\n",
      "Epoch 28718 \t\t Training Loss: 0.0005808303831145167 \t\n",
      "Epoch 28719 \t\t Training Loss: 0.0005808302666991949 \t\n",
      "Epoch 28720 \t\t Training Loss: 0.0005808302666991949 \t\n",
      "Epoch 28721 \t\t Training Loss: 0.0005808302666991949 \t\n",
      "Epoch 28722 \t\t Training Loss: 0.0005808302666991949 \t\n",
      "Epoch 28723 \t\t Training Loss: 0.0005808302666991949 \t\n",
      "Epoch 28724 \t\t Training Loss: 0.0005808302666991949 \t\n",
      "Epoch 28725 \t\t Training Loss: 0.0005808302666991949 \t\n",
      "Epoch 28726 \t\t Training Loss: 0.0005808302666991949 \t\n",
      "Epoch 28727 \t\t Training Loss: 0.000580830208491534 \t\n",
      "Epoch 28728 \t\t Training Loss: 0.0005808301502838731 \t\n",
      "Epoch 28729 \t\t Training Loss: 0.0005808301502838731 \t\n",
      "Epoch 28730 \t\t Training Loss: 0.0005808301502838731 \t\n",
      "Epoch 28731 \t\t Training Loss: 0.0005808301502838731 \t\n",
      "Epoch 28732 \t\t Training Loss: 0.0005808301502838731 \t\n",
      "Epoch 28733 \t\t Training Loss: 0.0005808301502838731 \t\n",
      "Epoch 28734 \t\t Training Loss: 0.0005808301502838731 \t\n",
      "Epoch 28735 \t\t Training Loss: 0.0005808301502838731 \t\n",
      "Epoch 28736 \t\t Training Loss: 0.0005808301502838731 \t\n",
      "Epoch 28737 \t\t Training Loss: 0.0005808301502838731 \t\n",
      "Epoch 28738 \t\t Training Loss: 0.0005808301502838731 \t\n",
      "Epoch 28739 \t\t Training Loss: 0.0005808301502838731 \t\n",
      "Epoch 28740 \t\t Training Loss: 0.0005808301502838731 \t\n",
      "Epoch 28741 \t\t Training Loss: 0.0005808301502838731 \t\n",
      "Epoch 28742 \t\t Training Loss: 0.0005808301502838731 \t\n",
      "Epoch 28743 \t\t Training Loss: 0.0005808301502838731 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28744 \t\t Training Loss: 0.0005808301502838731 \t\n",
      "Epoch 28745 \t\t Training Loss: 0.0005808300920762122 \t\n",
      "Epoch 28746 \t\t Training Loss: 0.0005808300920762122 \t\n",
      "Epoch 28747 \t\t Training Loss: 0.0005808300920762122 \t\n",
      "Epoch 28748 \t\t Training Loss: 0.0005808300338685513 \t\n",
      "Epoch 28749 \t\t Training Loss: 0.0005808300338685513 \t\n",
      "Epoch 28750 \t\t Training Loss: 0.0005808300338685513 \t\n",
      "Epoch 28751 \t\t Training Loss: 0.0005808300338685513 \t\n",
      "Epoch 28752 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28753 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28754 \t\t Training Loss: 0.0005808300338685513 \t\n",
      "Epoch 28755 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28756 \t\t Training Loss: 0.0005808300338685513 \t\n",
      "Epoch 28757 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28758 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28759 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28760 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28761 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28762 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28763 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28764 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28765 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28766 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28767 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28768 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28769 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28770 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28771 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28772 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28773 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28774 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28775 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28776 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28777 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28778 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28779 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28780 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28781 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28782 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28783 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28784 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28785 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28786 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28787 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28788 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28789 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28790 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28791 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28792 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28793 \t\t Training Loss: 0.0005808299756608903 \t\n",
      "Epoch 28794 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28795 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28796 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28797 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28798 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28799 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28800 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28801 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28802 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28803 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28804 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28805 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28806 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28807 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28808 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28809 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28810 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28811 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28812 \t\t Training Loss: 0.0005808299174532294 \t\n",
      "Epoch 28813 \t\t Training Loss: 0.0005808298010379076 \t\n",
      "Epoch 28814 \t\t Training Loss: 0.0005808298010379076 \t\n",
      "Epoch 28815 \t\t Training Loss: 0.0005808298010379076 \t\n",
      "Epoch 28816 \t\t Training Loss: 0.0005808298010379076 \t\n",
      "Epoch 28817 \t\t Training Loss: 0.0005808298010379076 \t\n",
      "Epoch 28818 \t\t Training Loss: 0.0005808298010379076 \t\n",
      "Epoch 28819 \t\t Training Loss: 0.0005808296846225858 \t\n",
      "Epoch 28820 \t\t Training Loss: 0.0005808296846225858 \t\n",
      "Epoch 28821 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28822 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28823 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28824 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28825 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28826 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28827 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28828 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28829 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28830 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28831 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28832 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28833 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28834 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28835 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28836 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28837 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28838 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28839 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28840 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28841 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28842 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28843 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28844 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28845 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28846 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28847 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28848 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28849 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28850 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28851 \t\t Training Loss: 0.000580829509999603 \t\n",
      "Epoch 28852 \t\t Training Loss: 0.000580829509999603 \t\n",
      "Epoch 28853 \t\t Training Loss: 0.000580829509999603 \t\n",
      "Epoch 28854 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28855 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28856 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28857 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28858 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28859 \t\t Training Loss: 0.0005808296264149249 \t\n",
      "Epoch 28860 \t\t Training Loss: 0.000580829509999603 \t\n",
      "Epoch 28861 \t\t Training Loss: 0.000580829509999603 \t\n",
      "Epoch 28862 \t\t Training Loss: 0.000580829509999603 \t\n",
      "Epoch 28863 \t\t Training Loss: 0.000580829509999603 \t\n",
      "Epoch 28864 \t\t Training Loss: 0.000580829509999603 \t\n",
      "Epoch 28865 \t\t Training Loss: 0.000580829509999603 \t\n",
      "Epoch 28866 \t\t Training Loss: 0.0005808294517919421 \t\n",
      "Epoch 28867 \t\t Training Loss: 0.0005808294517919421 \t\n",
      "Epoch 28868 \t\t Training Loss: 0.0005808294517919421 \t\n",
      "Epoch 28869 \t\t Training Loss: 0.0005808293935842812 \t\n",
      "Epoch 28870 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28871 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28872 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28873 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28874 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28875 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28876 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28877 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28878 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28879 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28880 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28881 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28882 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28883 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28884 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28885 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28886 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28887 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28888 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28889 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28890 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28891 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28892 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28893 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28894 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28895 \t\t Training Loss: 0.0005808293353766203 \t\n",
      "Epoch 28896 \t\t Training Loss: 0.0005808292189612985 \t\n",
      "Epoch 28897 \t\t Training Loss: 0.0005808292189612985 \t\n",
      "Epoch 28898 \t\t Training Loss: 0.0005808292189612985 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28899 \t\t Training Loss: 0.0005808292189612985 \t\n",
      "Epoch 28900 \t\t Training Loss: 0.0005808292189612985 \t\n",
      "Epoch 28901 \t\t Training Loss: 0.0005808292189612985 \t\n",
      "Epoch 28902 \t\t Training Loss: 0.0005808292189612985 \t\n",
      "Epoch 28903 \t\t Training Loss: 0.0005808292189612985 \t\n",
      "Epoch 28904 \t\t Training Loss: 0.0005808292189612985 \t\n",
      "Epoch 28905 \t\t Training Loss: 0.0005808292189612985 \t\n",
      "Epoch 28906 \t\t Training Loss: 0.0005808292189612985 \t\n",
      "Epoch 28907 \t\t Training Loss: 0.0005808292189612985 \t\n",
      "Epoch 28908 \t\t Training Loss: 0.0005808292189612985 \t\n",
      "Epoch 28909 \t\t Training Loss: 0.0005808292189612985 \t\n",
      "Epoch 28910 \t\t Training Loss: 0.0005808292189612985 \t\n",
      "Epoch 28911 \t\t Training Loss: 0.0005808291025459766 \t\n",
      "Epoch 28912 \t\t Training Loss: 0.0005808291025459766 \t\n",
      "Epoch 28913 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28914 \t\t Training Loss: 0.0005808291025459766 \t\n",
      "Epoch 28915 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28916 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28917 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28918 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28919 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28920 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28921 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28922 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28923 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28924 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28925 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28926 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28927 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28928 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28929 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28930 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28931 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28932 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28933 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28934 \t\t Training Loss: 0.0005808289279229939 \t\n",
      "Epoch 28935 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28936 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28937 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28938 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28939 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28940 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28941 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28942 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28943 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28944 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28945 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28946 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28947 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28948 \t\t Training Loss: 0.0005808289279229939 \t\n",
      "Epoch 28949 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28950 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28951 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28952 \t\t Training Loss: 0.0005808289279229939 \t\n",
      "Epoch 28953 \t\t Training Loss: 0.0005808290443383157 \t\n",
      "Epoch 28954 \t\t Training Loss: 0.0005808289279229939 \t\n",
      "Epoch 28955 \t\t Training Loss: 0.0005808289279229939 \t\n",
      "Epoch 28956 \t\t Training Loss: 0.0005808289279229939 \t\n",
      "Epoch 28957 \t\t Training Loss: 0.0005808289279229939 \t\n",
      "Epoch 28958 \t\t Training Loss: 0.0005808289279229939 \t\n",
      "Epoch 28959 \t\t Training Loss: 0.0005808289279229939 \t\n",
      "Epoch 28960 \t\t Training Loss: 0.0005808289279229939 \t\n",
      "Epoch 28961 \t\t Training Loss: 0.0005808289279229939 \t\n",
      "Epoch 28962 \t\t Training Loss: 0.0005808289279229939 \t\n",
      "Epoch 28963 \t\t Training Loss: 0.0005808289279229939 \t\n",
      "Epoch 28964 \t\t Training Loss: 0.0005808289279229939 \t\n",
      "Epoch 28965 \t\t Training Loss: 0.0005808289279229939 \t\n",
      "Epoch 28966 \t\t Training Loss: 0.0005808288115076721 \t\n",
      "Epoch 28967 \t\t Training Loss: 0.0005808288115076721 \t\n",
      "Epoch 28968 \t\t Training Loss: 0.0005808288115076721 \t\n",
      "Epoch 28969 \t\t Training Loss: 0.0005808288115076721 \t\n",
      "Epoch 28970 \t\t Training Loss: 0.0005808288115076721 \t\n",
      "Epoch 28971 \t\t Training Loss: 0.0005808288115076721 \t\n",
      "Epoch 28972 \t\t Training Loss: 0.0005808288115076721 \t\n",
      "Epoch 28973 \t\t Training Loss: 0.0005808288115076721 \t\n",
      "Epoch 28974 \t\t Training Loss: 0.0005808288115076721 \t\n",
      "Epoch 28975 \t\t Training Loss: 0.0005808288115076721 \t\n",
      "Epoch 28976 \t\t Training Loss: 0.0005808288115076721 \t\n",
      "Epoch 28977 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28978 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28979 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28980 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28981 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28982 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28983 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28984 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28985 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28986 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28987 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28988 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28989 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28990 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28991 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28992 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28993 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28994 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28995 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28996 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28997 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28998 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 28999 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 29000 \t\t Training Loss: 0.0005808286950923502 \t\n",
      "Epoch 29001 \t\t Training Loss: 0.0005808286368846893 \t\n",
      "Epoch 29002 \t\t Training Loss: 0.0005808286368846893 \t\n",
      "Epoch 29003 \t\t Training Loss: 0.0005808286368846893 \t\n",
      "Epoch 29004 \t\t Training Loss: 0.0005808286368846893 \t\n",
      "Epoch 29005 \t\t Training Loss: 0.0005808286368846893 \t\n",
      "Epoch 29006 \t\t Training Loss: 0.0005808286368846893 \t\n",
      "Epoch 29007 \t\t Training Loss: 0.0005808286368846893 \t\n",
      "Epoch 29008 \t\t Training Loss: 0.0005808286368846893 \t\n",
      "Epoch 29009 \t\t Training Loss: 0.0005808286368846893 \t\n",
      "Epoch 29010 \t\t Training Loss: 0.0005808286368846893 \t\n",
      "Epoch 29011 \t\t Training Loss: 0.0005808286368846893 \t\n",
      "Epoch 29012 \t\t Training Loss: 0.0005808286368846893 \t\n",
      "Epoch 29013 \t\t Training Loss: 0.0005808286368846893 \t\n",
      "Epoch 29014 \t\t Training Loss: 0.0005808285204693675 \t\n",
      "Epoch 29015 \t\t Training Loss: 0.0005808285204693675 \t\n",
      "Epoch 29016 \t\t Training Loss: 0.0005808285204693675 \t\n",
      "Epoch 29017 \t\t Training Loss: 0.0005808285204693675 \t\n",
      "Epoch 29018 \t\t Training Loss: 0.0005808285204693675 \t\n",
      "Epoch 29019 \t\t Training Loss: 0.0005808285204693675 \t\n",
      "Epoch 29020 \t\t Training Loss: 0.0005808285204693675 \t\n",
      "Epoch 29021 \t\t Training Loss: 0.0005808285204693675 \t\n",
      "Epoch 29022 \t\t Training Loss: 0.0005808285204693675 \t\n",
      "Epoch 29023 \t\t Training Loss: 0.0005808285204693675 \t\n",
      "Epoch 29024 \t\t Training Loss: 0.0005808285204693675 \t\n",
      "Epoch 29025 \t\t Training Loss: 0.0005808285204693675 \t\n",
      "Epoch 29026 \t\t Training Loss: 0.0005808285204693675 \t\n",
      "Epoch 29027 \t\t Training Loss: 0.0005808285204693675 \t\n",
      "Epoch 29028 \t\t Training Loss: 0.0005808285204693675 \t\n",
      "Epoch 29029 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29030 \t\t Training Loss: 0.0005808285204693675 \t\n",
      "Epoch 29031 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29032 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29033 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29034 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29035 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29036 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29037 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29038 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29039 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29040 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29041 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29042 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29043 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29044 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29045 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29046 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29047 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29048 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29049 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29050 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29051 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29052 \t\t Training Loss: 0.0005808284040540457 \t\n",
      "Epoch 29053 \t\t Training Loss: 0.0005808283458463848 \t\n",
      "Epoch 29054 \t\t Training Loss: 0.0005808283458463848 \t\n",
      "Epoch 29055 \t\t Training Loss: 0.0005808283458463848 \t\n",
      "Epoch 29056 \t\t Training Loss: 0.0005808283458463848 \t\n",
      "Epoch 29057 \t\t Training Loss: 0.0005808283458463848 \t\n",
      "Epoch 29058 \t\t Training Loss: 0.0005808283458463848 \t\n",
      "Epoch 29059 \t\t Training Loss: 0.0005808283458463848 \t\n",
      "Epoch 29060 \t\t Training Loss: 0.0005808283458463848 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29061 \t\t Training Loss: 0.0005808283458463848 \t\n",
      "Epoch 29062 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29063 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29064 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29065 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29066 \t\t Training Loss: 0.0005808283458463848 \t\n",
      "Epoch 29067 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29068 \t\t Training Loss: 0.0005808283458463848 \t\n",
      "Epoch 29069 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29070 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29071 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29072 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29073 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29074 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29075 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29076 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29077 \t\t Training Loss: 0.0005808283458463848 \t\n",
      "Epoch 29078 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29079 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29080 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29081 \t\t Training Loss: 0.0005808283458463848 \t\n",
      "Epoch 29082 \t\t Training Loss: 0.0005808283458463848 \t\n",
      "Epoch 29083 \t\t Training Loss: 0.0005808283458463848 \t\n",
      "Epoch 29084 \t\t Training Loss: 0.0005808283458463848 \t\n",
      "Epoch 29085 \t\t Training Loss: 0.0005808283458463848 \t\n",
      "Epoch 29086 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29087 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29088 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29089 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29090 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29091 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29092 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29093 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29094 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29095 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29096 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29097 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29098 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29099 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29100 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29101 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29102 \t\t Training Loss: 0.0005808281130157411 \t\n",
      "Epoch 29103 \t\t Training Loss: 0.0005808281130157411 \t\n",
      "Epoch 29104 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29105 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29106 \t\t Training Loss: 0.0005808281130157411 \t\n",
      "Epoch 29107 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29108 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29109 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29110 \t\t Training Loss: 0.0005808282294310629 \t\n",
      "Epoch 29111 \t\t Training Loss: 0.0005808281130157411 \t\n",
      "Epoch 29112 \t\t Training Loss: 0.0005808281130157411 \t\n",
      "Epoch 29113 \t\t Training Loss: 0.0005808281130157411 \t\n",
      "Epoch 29114 \t\t Training Loss: 0.0005808281130157411 \t\n",
      "Epoch 29115 \t\t Training Loss: 0.0005808281130157411 \t\n",
      "Epoch 29116 \t\t Training Loss: 0.0005808281130157411 \t\n",
      "Epoch 29117 \t\t Training Loss: 0.0005808281130157411 \t\n",
      "Epoch 29118 \t\t Training Loss: 0.0005808281130157411 \t\n",
      "Epoch 29119 \t\t Training Loss: 0.0005808281130157411 \t\n",
      "Epoch 29120 \t\t Training Loss: 0.0005808281130157411 \t\n",
      "Epoch 29121 \t\t Training Loss: 0.0005808281130157411 \t\n",
      "Epoch 29122 \t\t Training Loss: 0.0005808281130157411 \t\n",
      "Epoch 29123 \t\t Training Loss: 0.0005808281130157411 \t\n",
      "Epoch 29124 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29125 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29126 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29127 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29128 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29129 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29130 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29131 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29132 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29133 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29134 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29135 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29136 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29137 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29138 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29139 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29140 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29141 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29142 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29143 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29144 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29145 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29146 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29147 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29148 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29149 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29150 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29151 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29152 \t\t Training Loss: 0.0005808279383927584 \t\n",
      "Epoch 29153 \t\t Training Loss: 0.0005808278219774365 \t\n",
      "Epoch 29154 \t\t Training Loss: 0.0005808278219774365 \t\n",
      "Epoch 29155 \t\t Training Loss: 0.0005808278219774365 \t\n",
      "Epoch 29156 \t\t Training Loss: 0.0005808278219774365 \t\n",
      "Epoch 29157 \t\t Training Loss: 0.0005808278219774365 \t\n",
      "Epoch 29158 \t\t Training Loss: 0.0005808278219774365 \t\n",
      "Epoch 29159 \t\t Training Loss: 0.0005808278219774365 \t\n",
      "Epoch 29160 \t\t Training Loss: 0.0005808278219774365 \t\n",
      "Epoch 29161 \t\t Training Loss: 0.0005808278219774365 \t\n",
      "Epoch 29162 \t\t Training Loss: 0.0005808278219774365 \t\n",
      "Epoch 29163 \t\t Training Loss: 0.0005808278219774365 \t\n",
      "Epoch 29164 \t\t Training Loss: 0.0005808278219774365 \t\n",
      "Epoch 29165 \t\t Training Loss: 0.0005808278219774365 \t\n",
      "Epoch 29166 \t\t Training Loss: 0.0005808278219774365 \t\n",
      "Epoch 29167 \t\t Training Loss: 0.0005808278219774365 \t\n",
      "Epoch 29168 \t\t Training Loss: 0.0005808277637697756 \t\n",
      "Epoch 29169 \t\t Training Loss: 0.0005808277637697756 \t\n",
      "Epoch 29170 \t\t Training Loss: 0.0005808277637697756 \t\n",
      "Epoch 29171 \t\t Training Loss: 0.0005808277637697756 \t\n",
      "Epoch 29172 \t\t Training Loss: 0.0005808277637697756 \t\n",
      "Epoch 29173 \t\t Training Loss: 0.0005808277637697756 \t\n",
      "Epoch 29174 \t\t Training Loss: 0.0005808277637697756 \t\n",
      "Epoch 29175 \t\t Training Loss: 0.0005808277637697756 \t\n",
      "Epoch 29176 \t\t Training Loss: 0.0005808277637697756 \t\n",
      "Epoch 29177 \t\t Training Loss: 0.0005808277637697756 \t\n",
      "Epoch 29178 \t\t Training Loss: 0.0005808277055621147 \t\n",
      "Epoch 29179 \t\t Training Loss: 0.0005808277055621147 \t\n",
      "Epoch 29180 \t\t Training Loss: 0.0005808277637697756 \t\n",
      "Epoch 29181 \t\t Training Loss: 0.0005808277055621147 \t\n",
      "Epoch 29182 \t\t Training Loss: 0.0005808277055621147 \t\n",
      "Epoch 29183 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29184 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29185 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29186 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29187 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29188 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29189 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29190 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29191 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29192 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29193 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29194 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29195 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29196 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29197 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29198 \t\t Training Loss: 0.000580827530939132 \t\n",
      "Epoch 29199 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29200 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29201 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29202 \t\t Training Loss: 0.000580827530939132 \t\n",
      "Epoch 29203 \t\t Training Loss: 0.0005808276473544538 \t\n",
      "Epoch 29204 \t\t Training Loss: 0.000580827530939132 \t\n",
      "Epoch 29205 \t\t Training Loss: 0.000580827530939132 \t\n",
      "Epoch 29206 \t\t Training Loss: 0.000580827530939132 \t\n",
      "Epoch 29207 \t\t Training Loss: 0.000580827530939132 \t\n",
      "Epoch 29208 \t\t Training Loss: 0.000580827530939132 \t\n",
      "Epoch 29209 \t\t Training Loss: 0.000580827530939132 \t\n",
      "Epoch 29210 \t\t Training Loss: 0.000580827530939132 \t\n",
      "Epoch 29211 \t\t Training Loss: 0.000580827530939132 \t\n",
      "Epoch 29212 \t\t Training Loss: 0.000580827530939132 \t\n",
      "Epoch 29213 \t\t Training Loss: 0.000580827530939132 \t\n",
      "Epoch 29214 \t\t Training Loss: 0.000580827530939132 \t\n",
      "Epoch 29215 \t\t Training Loss: 0.000580827530939132 \t\n",
      "Epoch 29216 \t\t Training Loss: 0.000580827530939132 \t\n",
      "Epoch 29217 \t\t Training Loss: 0.000580827530939132 \t\n",
      "Epoch 29218 \t\t Training Loss: 0.0005808274145238101 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29219 \t\t Training Loss: 0.0005808274145238101 \t\n",
      "Epoch 29220 \t\t Training Loss: 0.0005808274145238101 \t\n",
      "Epoch 29221 \t\t Training Loss: 0.0005808274145238101 \t\n",
      "Epoch 29222 \t\t Training Loss: 0.0005808274145238101 \t\n",
      "Epoch 29223 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29224 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29225 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29226 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29227 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29228 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29229 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29230 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29231 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29232 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29233 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29234 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29235 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29236 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29237 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29238 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29239 \t\t Training Loss: 0.0005808273563161492 \t\n",
      "Epoch 29240 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29241 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29242 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29243 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29244 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29245 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29246 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29247 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29248 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29249 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29250 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29251 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29252 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29253 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29254 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29255 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29256 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29257 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29258 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29259 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29260 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29261 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29262 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29263 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29264 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29265 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29266 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29267 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29268 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29269 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29270 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29271 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29272 \t\t Training Loss: 0.0005808272399008274 \t\n",
      "Epoch 29273 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29274 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29275 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29276 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29277 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29278 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29279 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29280 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29281 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29282 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29283 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29284 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29285 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29286 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29287 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29288 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29289 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29290 \t\t Training Loss: 0.0005808271234855056 \t\n",
      "Epoch 29291 \t\t Training Loss: 0.0005808270652778447 \t\n",
      "Epoch 29292 \t\t Training Loss: 0.0005808270652778447 \t\n",
      "Epoch 29293 \t\t Training Loss: 0.0005808270652778447 \t\n",
      "Epoch 29294 \t\t Training Loss: 0.0005808270652778447 \t\n",
      "Epoch 29295 \t\t Training Loss: 0.0005808270652778447 \t\n",
      "Epoch 29296 \t\t Training Loss: 0.0005808270652778447 \t\n",
      "Epoch 29297 \t\t Training Loss: 0.0005808270070701838 \t\n",
      "Epoch 29298 \t\t Training Loss: 0.0005808270070701838 \t\n",
      "Epoch 29299 \t\t Training Loss: 0.0005808270070701838 \t\n",
      "Epoch 29300 \t\t Training Loss: 0.0005808270070701838 \t\n",
      "Epoch 29301 \t\t Training Loss: 0.0005808270070701838 \t\n",
      "Epoch 29302 \t\t Training Loss: 0.0005808269488625228 \t\n",
      "Epoch 29303 \t\t Training Loss: 0.0005808269488625228 \t\n",
      "Epoch 29304 \t\t Training Loss: 0.0005808269488625228 \t\n",
      "Epoch 29305 \t\t Training Loss: 0.0005808269488625228 \t\n",
      "Epoch 29306 \t\t Training Loss: 0.0005808269488625228 \t\n",
      "Epoch 29307 \t\t Training Loss: 0.0005808269488625228 \t\n",
      "Epoch 29308 \t\t Training Loss: 0.0005808269488625228 \t\n",
      "Epoch 29309 \t\t Training Loss: 0.000580826832447201 \t\n",
      "Epoch 29310 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29311 \t\t Training Loss: 0.0005808269488625228 \t\n",
      "Epoch 29312 \t\t Training Loss: 0.000580826832447201 \t\n",
      "Epoch 29313 \t\t Training Loss: 0.000580826832447201 \t\n",
      "Epoch 29314 \t\t Training Loss: 0.000580826832447201 \t\n",
      "Epoch 29315 \t\t Training Loss: 0.0005808269488625228 \t\n",
      "Epoch 29316 \t\t Training Loss: 0.000580826832447201 \t\n",
      "Epoch 29317 \t\t Training Loss: 0.000580826832447201 \t\n",
      "Epoch 29318 \t\t Training Loss: 0.000580826832447201 \t\n",
      "Epoch 29319 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29320 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29321 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29322 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29323 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29324 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29325 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29326 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29327 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29328 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29329 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29330 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29331 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29332 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29333 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29334 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29335 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29336 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29337 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29338 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29339 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29340 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29341 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29342 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29343 \t\t Training Loss: 0.0005808267742395401 \t\n",
      "Epoch 29344 \t\t Training Loss: 0.0005808267160318792 \t\n",
      "Epoch 29345 \t\t Training Loss: 0.0005808267160318792 \t\n",
      "Epoch 29346 \t\t Training Loss: 0.0005808267160318792 \t\n",
      "Epoch 29347 \t\t Training Loss: 0.0005808267160318792 \t\n",
      "Epoch 29348 \t\t Training Loss: 0.0005808267160318792 \t\n",
      "Epoch 29349 \t\t Training Loss: 0.0005808266578242183 \t\n",
      "Epoch 29350 \t\t Training Loss: 0.0005808267160318792 \t\n",
      "Epoch 29351 \t\t Training Loss: 0.0005808266578242183 \t\n",
      "Epoch 29352 \t\t Training Loss: 0.0005808266578242183 \t\n",
      "Epoch 29353 \t\t Training Loss: 0.0005808266578242183 \t\n",
      "Epoch 29354 \t\t Training Loss: 0.0005808266578242183 \t\n",
      "Epoch 29355 \t\t Training Loss: 0.0005808265996165574 \t\n",
      "Epoch 29356 \t\t Training Loss: 0.0005808265996165574 \t\n",
      "Epoch 29357 \t\t Training Loss: 0.0005808265996165574 \t\n",
      "Epoch 29358 \t\t Training Loss: 0.0005808265996165574 \t\n",
      "Epoch 29359 \t\t Training Loss: 0.0005808265996165574 \t\n",
      "Epoch 29360 \t\t Training Loss: 0.0005808265996165574 \t\n",
      "Epoch 29361 \t\t Training Loss: 0.0005808265996165574 \t\n",
      "Epoch 29362 \t\t Training Loss: 0.0005808265996165574 \t\n",
      "Epoch 29363 \t\t Training Loss: 0.0005808265414088964 \t\n",
      "Epoch 29364 \t\t Training Loss: 0.0005808265414088964 \t\n",
      "Epoch 29365 \t\t Training Loss: 0.0005808265996165574 \t\n",
      "Epoch 29366 \t\t Training Loss: 0.0005808265996165574 \t\n",
      "Epoch 29367 \t\t Training Loss: 0.0005808265414088964 \t\n",
      "Epoch 29368 \t\t Training Loss: 0.0005808265996165574 \t\n",
      "Epoch 29369 \t\t Training Loss: 0.0005808265414088964 \t\n",
      "Epoch 29370 \t\t Training Loss: 0.0005808265996165574 \t\n",
      "Epoch 29371 \t\t Training Loss: 0.0005808265414088964 \t\n",
      "Epoch 29372 \t\t Training Loss: 0.0005808265414088964 \t\n",
      "Epoch 29373 \t\t Training Loss: 0.0005808265414088964 \t\n",
      "Epoch 29374 \t\t Training Loss: 0.0005808265414088964 \t\n",
      "Epoch 29375 \t\t Training Loss: 0.0005808265414088964 \t\n",
      "Epoch 29376 \t\t Training Loss: 0.0005808265414088964 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29377 \t\t Training Loss: 0.0005808265414088964 \t\n",
      "Epoch 29378 \t\t Training Loss: 0.0005808265414088964 \t\n",
      "Epoch 29379 \t\t Training Loss: 0.0005808265414088964 \t\n",
      "Epoch 29380 \t\t Training Loss: 0.0005808265414088964 \t\n",
      "Epoch 29381 \t\t Training Loss: 0.0005808265414088964 \t\n",
      "Epoch 29382 \t\t Training Loss: 0.0005808265414088964 \t\n",
      "Epoch 29383 \t\t Training Loss: 0.0005808264832012355 \t\n",
      "Epoch 29384 \t\t Training Loss: 0.0005808264832012355 \t\n",
      "Epoch 29385 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29386 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29387 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29388 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29389 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29390 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29391 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29392 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29393 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29394 \t\t Training Loss: 0.0005808263085782528 \t\n",
      "Epoch 29395 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29396 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29397 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29398 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29399 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29400 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29401 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29402 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29403 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29404 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29405 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29406 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29407 \t\t Training Loss: 0.0005808263667859137 \t\n",
      "Epoch 29408 \t\t Training Loss: 0.0005808263085782528 \t\n",
      "Epoch 29409 \t\t Training Loss: 0.0005808263085782528 \t\n",
      "Epoch 29410 \t\t Training Loss: 0.0005808263085782528 \t\n",
      "Epoch 29411 \t\t Training Loss: 0.0005808263085782528 \t\n",
      "Epoch 29412 \t\t Training Loss: 0.0005808262503705919 \t\n",
      "Epoch 29413 \t\t Training Loss: 0.0005808263085782528 \t\n",
      "Epoch 29414 \t\t Training Loss: 0.0005808263085782528 \t\n",
      "Epoch 29415 \t\t Training Loss: 0.0005808262503705919 \t\n",
      "Epoch 29416 \t\t Training Loss: 0.0005808262503705919 \t\n",
      "Epoch 29417 \t\t Training Loss: 0.0005808262503705919 \t\n",
      "Epoch 29418 \t\t Training Loss: 0.0005808262503705919 \t\n",
      "Epoch 29419 \t\t Training Loss: 0.0005808262503705919 \t\n",
      "Epoch 29420 \t\t Training Loss: 0.0005808262503705919 \t\n",
      "Epoch 29421 \t\t Training Loss: 0.0005808262503705919 \t\n",
      "Epoch 29422 \t\t Training Loss: 0.000580826192162931 \t\n",
      "Epoch 29423 \t\t Training Loss: 0.000580826192162931 \t\n",
      "Epoch 29424 \t\t Training Loss: 0.000580826192162931 \t\n",
      "Epoch 29425 \t\t Training Loss: 0.000580826192162931 \t\n",
      "Epoch 29426 \t\t Training Loss: 0.000580826192162931 \t\n",
      "Epoch 29427 \t\t Training Loss: 0.000580826192162931 \t\n",
      "Epoch 29428 \t\t Training Loss: 0.00058082613395527 \t\n",
      "Epoch 29429 \t\t Training Loss: 0.00058082613395527 \t\n",
      "Epoch 29430 \t\t Training Loss: 0.00058082613395527 \t\n",
      "Epoch 29431 \t\t Training Loss: 0.0005808260757476091 \t\n",
      "Epoch 29432 \t\t Training Loss: 0.0005808260757476091 \t\n",
      "Epoch 29433 \t\t Training Loss: 0.0005808260757476091 \t\n",
      "Epoch 29434 \t\t Training Loss: 0.0005808260757476091 \t\n",
      "Epoch 29435 \t\t Training Loss: 0.0005808260757476091 \t\n",
      "Epoch 29436 \t\t Training Loss: 0.0005808260175399482 \t\n",
      "Epoch 29437 \t\t Training Loss: 0.0005808260757476091 \t\n",
      "Epoch 29438 \t\t Training Loss: 0.0005808260757476091 \t\n",
      "Epoch 29439 \t\t Training Loss: 0.0005808260757476091 \t\n",
      "Epoch 29440 \t\t Training Loss: 0.0005808260757476091 \t\n",
      "Epoch 29441 \t\t Training Loss: 0.0005808260757476091 \t\n",
      "Epoch 29442 \t\t Training Loss: 0.0005808260757476091 \t\n",
      "Epoch 29443 \t\t Training Loss: 0.0005808260757476091 \t\n",
      "Epoch 29444 \t\t Training Loss: 0.0005808260757476091 \t\n",
      "Epoch 29445 \t\t Training Loss: 0.0005808260757476091 \t\n",
      "Epoch 29446 \t\t Training Loss: 0.0005808260175399482 \t\n",
      "Epoch 29447 \t\t Training Loss: 0.0005808260757476091 \t\n",
      "Epoch 29448 \t\t Training Loss: 0.0005808260175399482 \t\n",
      "Epoch 29449 \t\t Training Loss: 0.0005808260175399482 \t\n",
      "Epoch 29450 \t\t Training Loss: 0.0005808260175399482 \t\n",
      "Epoch 29451 \t\t Training Loss: 0.0005808260175399482 \t\n",
      "Epoch 29452 \t\t Training Loss: 0.0005808260175399482 \t\n",
      "Epoch 29453 \t\t Training Loss: 0.0005808260175399482 \t\n",
      "Epoch 29454 \t\t Training Loss: 0.0005808260175399482 \t\n",
      "Epoch 29455 \t\t Training Loss: 0.0005808260175399482 \t\n",
      "Epoch 29456 \t\t Training Loss: 0.0005808259593322873 \t\n",
      "Epoch 29457 \t\t Training Loss: 0.0005808260175399482 \t\n",
      "Epoch 29458 \t\t Training Loss: 0.0005808259593322873 \t\n",
      "Epoch 29459 \t\t Training Loss: 0.0005808259593322873 \t\n",
      "Epoch 29460 \t\t Training Loss: 0.0005808260175399482 \t\n",
      "Epoch 29461 \t\t Training Loss: 0.0005808259593322873 \t\n",
      "Epoch 29462 \t\t Training Loss: 0.0005808259593322873 \t\n",
      "Epoch 29463 \t\t Training Loss: 0.0005808259593322873 \t\n",
      "Epoch 29464 \t\t Training Loss: 0.0005808259593322873 \t\n",
      "Epoch 29465 \t\t Training Loss: 0.0005808259593322873 \t\n",
      "Epoch 29466 \t\t Training Loss: 0.0005808259593322873 \t\n",
      "Epoch 29467 \t\t Training Loss: 0.0005808259593322873 \t\n",
      "Epoch 29468 \t\t Training Loss: 0.0005808259593322873 \t\n",
      "Epoch 29469 \t\t Training Loss: 0.0005808259593322873 \t\n",
      "Epoch 29470 \t\t Training Loss: 0.0005808259593322873 \t\n",
      "Epoch 29471 \t\t Training Loss: 0.0005808259593322873 \t\n",
      "Epoch 29472 \t\t Training Loss: 0.0005808259011246264 \t\n",
      "Epoch 29473 \t\t Training Loss: 0.0005808259593322873 \t\n",
      "Epoch 29474 \t\t Training Loss: 0.0005808259593322873 \t\n",
      "Epoch 29475 \t\t Training Loss: 0.0005808259011246264 \t\n",
      "Epoch 29476 \t\t Training Loss: 0.0005808259011246264 \t\n",
      "Epoch 29477 \t\t Training Loss: 0.0005808259011246264 \t\n",
      "Epoch 29478 \t\t Training Loss: 0.0005808259011246264 \t\n",
      "Epoch 29479 \t\t Training Loss: 0.0005808259011246264 \t\n",
      "Epoch 29480 \t\t Training Loss: 0.0005808259011246264 \t\n",
      "Epoch 29481 \t\t Training Loss: 0.0005808259011246264 \t\n",
      "Epoch 29482 \t\t Training Loss: 0.0005808259011246264 \t\n",
      "Epoch 29483 \t\t Training Loss: 0.0005808257847093046 \t\n",
      "Epoch 29484 \t\t Training Loss: 0.0005808257847093046 \t\n",
      "Epoch 29485 \t\t Training Loss: 0.0005808257847093046 \t\n",
      "Epoch 29486 \t\t Training Loss: 0.0005808257847093046 \t\n",
      "Epoch 29487 \t\t Training Loss: 0.0005808257847093046 \t\n",
      "Epoch 29488 \t\t Training Loss: 0.0005808257847093046 \t\n",
      "Epoch 29489 \t\t Training Loss: 0.0005808257847093046 \t\n",
      "Epoch 29490 \t\t Training Loss: 0.0005808257847093046 \t\n",
      "Epoch 29491 \t\t Training Loss: 0.0005808257847093046 \t\n",
      "Epoch 29492 \t\t Training Loss: 0.0005808257847093046 \t\n",
      "Epoch 29493 \t\t Training Loss: 0.0005808257847093046 \t\n",
      "Epoch 29494 \t\t Training Loss: 0.0005808257847093046 \t\n",
      "Epoch 29495 \t\t Training Loss: 0.0005808257265016437 \t\n",
      "Epoch 29496 \t\t Training Loss: 0.0005808257847093046 \t\n",
      "Epoch 29497 \t\t Training Loss: 0.0005808257265016437 \t\n",
      "Epoch 29498 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29499 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29500 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29501 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29502 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29503 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29504 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29505 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29506 \t\t Training Loss: 0.0005808257265016437 \t\n",
      "Epoch 29507 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29508 \t\t Training Loss: 0.0005808257847093046 \t\n",
      "Epoch 29509 \t\t Training Loss: 0.0005808257265016437 \t\n",
      "Epoch 29510 \t\t Training Loss: 0.0005808257265016437 \t\n",
      "Epoch 29511 \t\t Training Loss: 0.0005808257265016437 \t\n",
      "Epoch 29512 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29513 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29514 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29515 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29516 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29517 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29518 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29519 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29520 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29521 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29522 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29523 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29524 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29525 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29526 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29527 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29528 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29529 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29530 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29531 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29532 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29533 \t\t Training Loss: 0.0005808256682939827 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29534 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29535 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29536 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29537 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29538 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29539 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29540 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29541 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29542 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29543 \t\t Training Loss: 0.0005808256100863218 \t\n",
      "Epoch 29544 \t\t Training Loss: 0.0005808256100863218 \t\n",
      "Epoch 29545 \t\t Training Loss: 0.0005808256100863218 \t\n",
      "Epoch 29546 \t\t Training Loss: 0.0005808256100863218 \t\n",
      "Epoch 29547 \t\t Training Loss: 0.0005808256100863218 \t\n",
      "Epoch 29548 \t\t Training Loss: 0.0005808256100863218 \t\n",
      "Epoch 29549 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29550 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29551 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29552 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29553 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29554 \t\t Training Loss: 0.0005808256682939827 \t\n",
      "Epoch 29555 \t\t Training Loss: 0.0005808256100863218 \t\n",
      "Epoch 29556 \t\t Training Loss: 0.0005808256100863218 \t\n",
      "Epoch 29557 \t\t Training Loss: 0.0005808256100863218 \t\n",
      "Epoch 29558 \t\t Training Loss: 0.0005808256100863218 \t\n",
      "Epoch 29559 \t\t Training Loss: 0.0005808256100863218 \t\n",
      "Epoch 29560 \t\t Training Loss: 0.0005808255518786609 \t\n",
      "Epoch 29561 \t\t Training Loss: 0.0005808255518786609 \t\n",
      "Epoch 29562 \t\t Training Loss: 0.0005808255518786609 \t\n",
      "Epoch 29563 \t\t Training Loss: 0.0005808255518786609 \t\n",
      "Epoch 29564 \t\t Training Loss: 0.0005808255518786609 \t\n",
      "Epoch 29565 \t\t Training Loss: 0.0005808255518786609 \t\n",
      "Epoch 29566 \t\t Training Loss: 0.0005808255518786609 \t\n",
      "Epoch 29567 \t\t Training Loss: 0.0005808255518786609 \t\n",
      "Epoch 29568 \t\t Training Loss: 0.0005808255518786609 \t\n",
      "Epoch 29569 \t\t Training Loss: 0.0005808255518786609 \t\n",
      "Epoch 29570 \t\t Training Loss: 0.000580825493671 \t\n",
      "Epoch 29571 \t\t Training Loss: 0.000580825493671 \t\n",
      "Epoch 29572 \t\t Training Loss: 0.0005808254354633391 \t\n",
      "Epoch 29573 \t\t Training Loss: 0.0005808255518786609 \t\n",
      "Epoch 29574 \t\t Training Loss: 0.000580825493671 \t\n",
      "Epoch 29575 \t\t Training Loss: 0.0005808254354633391 \t\n",
      "Epoch 29576 \t\t Training Loss: 0.0005808254354633391 \t\n",
      "Epoch 29577 \t\t Training Loss: 0.0005808254354633391 \t\n",
      "Epoch 29578 \t\t Training Loss: 0.0005808254354633391 \t\n",
      "Epoch 29579 \t\t Training Loss: 0.0005808254354633391 \t\n",
      "Epoch 29580 \t\t Training Loss: 0.0005808254354633391 \t\n",
      "Epoch 29581 \t\t Training Loss: 0.0005808253772556782 \t\n",
      "Epoch 29582 \t\t Training Loss: 0.0005808253772556782 \t\n",
      "Epoch 29583 \t\t Training Loss: 0.0005808253772556782 \t\n",
      "Epoch 29584 \t\t Training Loss: 0.0005808253772556782 \t\n",
      "Epoch 29585 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29586 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29587 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29588 \t\t Training Loss: 0.0005808253772556782 \t\n",
      "Epoch 29589 \t\t Training Loss: 0.0005808253772556782 \t\n",
      "Epoch 29590 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29591 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29592 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29593 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29594 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29595 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29596 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29597 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29598 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29599 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29600 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29601 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29602 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29603 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29604 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29605 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29606 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29607 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29608 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29609 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29610 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29611 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29612 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29613 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29614 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29615 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29616 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29617 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29618 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29619 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29620 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29621 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29622 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29623 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29624 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29625 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29626 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29627 \t\t Training Loss: 0.0005808253190480173 \t\n",
      "Epoch 29628 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29629 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29630 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29631 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29632 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29633 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29634 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29635 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29636 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29637 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29638 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29639 \t\t Training Loss: 0.0005808251444250345 \t\n",
      "Epoch 29640 \t\t Training Loss: 0.0005808252026326954 \t\n",
      "Epoch 29641 \t\t Training Loss: 0.0005808251444250345 \t\n",
      "Epoch 29642 \t\t Training Loss: 0.0005808250862173736 \t\n",
      "Epoch 29643 \t\t Training Loss: 0.0005808250862173736 \t\n",
      "Epoch 29644 \t\t Training Loss: 0.0005808250862173736 \t\n",
      "Epoch 29645 \t\t Training Loss: 0.0005808250862173736 \t\n",
      "Epoch 29646 \t\t Training Loss: 0.0005808250862173736 \t\n",
      "Epoch 29647 \t\t Training Loss: 0.0005808250862173736 \t\n",
      "Epoch 29648 \t\t Training Loss: 0.0005808250862173736 \t\n",
      "Epoch 29649 \t\t Training Loss: 0.0005808250862173736 \t\n",
      "Epoch 29650 \t\t Training Loss: 0.0005808250862173736 \t\n",
      "Epoch 29651 \t\t Training Loss: 0.0005808250862173736 \t\n",
      "Epoch 29652 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29653 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29654 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29655 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29656 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29657 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29658 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29659 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29660 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29661 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29662 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29663 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29664 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29665 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29666 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29667 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29668 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29669 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29670 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29671 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29672 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29673 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29674 \t\t Training Loss: 0.0005808250280097127 \t\n",
      "Epoch 29675 \t\t Training Loss: 0.0005808249698020518 \t\n",
      "Epoch 29676 \t\t Training Loss: 0.0005808249698020518 \t\n",
      "Epoch 29677 \t\t Training Loss: 0.0005808249698020518 \t\n",
      "Epoch 29678 \t\t Training Loss: 0.0005808249115943909 \t\n",
      "Epoch 29679 \t\t Training Loss: 0.0005808249115943909 \t\n",
      "Epoch 29680 \t\t Training Loss: 0.0005808249698020518 \t\n",
      "Epoch 29681 \t\t Training Loss: 0.0005808249115943909 \t\n",
      "Epoch 29682 \t\t Training Loss: 0.0005808249115943909 \t\n",
      "Epoch 29683 \t\t Training Loss: 0.0005808249115943909 \t\n",
      "Epoch 29684 \t\t Training Loss: 0.0005808249115943909 \t\n",
      "Epoch 29685 \t\t Training Loss: 0.0005808249115943909 \t\n",
      "Epoch 29686 \t\t Training Loss: 0.00058082485338673 \t\n",
      "Epoch 29687 \t\t Training Loss: 0.00058082485338673 \t\n",
      "Epoch 29688 \t\t Training Loss: 0.00058082485338673 \t\n",
      "Epoch 29689 \t\t Training Loss: 0.00058082485338673 \t\n",
      "Epoch 29690 \t\t Training Loss: 0.00058082485338673 \t\n",
      "Epoch 29691 \t\t Training Loss: 0.00058082485338673 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29692 \t\t Training Loss: 0.00058082485338673 \t\n",
      "Epoch 29693 \t\t Training Loss: 0.00058082485338673 \t\n",
      "Epoch 29694 \t\t Training Loss: 0.00058082485338673 \t\n",
      "Epoch 29695 \t\t Training Loss: 0.00058082485338673 \t\n",
      "Epoch 29696 \t\t Training Loss: 0.00058082485338673 \t\n",
      "Epoch 29697 \t\t Training Loss: 0.00058082485338673 \t\n",
      "Epoch 29698 \t\t Training Loss: 0.00058082485338673 \t\n",
      "Epoch 29699 \t\t Training Loss: 0.00058082485338673 \t\n",
      "Epoch 29700 \t\t Training Loss: 0.00058082485338673 \t\n",
      "Epoch 29701 \t\t Training Loss: 0.00058082485338673 \t\n",
      "Epoch 29702 \t\t Training Loss: 0.000580824795179069 \t\n",
      "Epoch 29703 \t\t Training Loss: 0.000580824795179069 \t\n",
      "Epoch 29704 \t\t Training Loss: 0.000580824795179069 \t\n",
      "Epoch 29705 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29706 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29707 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29708 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29709 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29710 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29711 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29712 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29713 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29714 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29715 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29716 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29717 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29718 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29719 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29720 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29721 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29722 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29723 \t\t Training Loss: 0.0005808246205560863 \t\n",
      "Epoch 29724 \t\t Training Loss: 0.0005808246205560863 \t\n",
      "Epoch 29725 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29726 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29727 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29728 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29729 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29730 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29731 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29732 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29733 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29734 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29735 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29736 \t\t Training Loss: 0.0005808247369714081 \t\n",
      "Epoch 29737 \t\t Training Loss: 0.0005808246205560863 \t\n",
      "Epoch 29738 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29739 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29740 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29741 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29742 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29743 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29744 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29745 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29746 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29747 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29748 \t\t Training Loss: 0.0005808244459331036 \t\n",
      "Epoch 29749 \t\t Training Loss: 0.0005808244459331036 \t\n",
      "Epoch 29750 \t\t Training Loss: 0.0005808244459331036 \t\n",
      "Epoch 29751 \t\t Training Loss: 0.0005808244459331036 \t\n",
      "Epoch 29752 \t\t Training Loss: 0.0005808244459331036 \t\n",
      "Epoch 29753 \t\t Training Loss: 0.0005808244459331036 \t\n",
      "Epoch 29754 \t\t Training Loss: 0.0005808244459331036 \t\n",
      "Epoch 29755 \t\t Training Loss: 0.0005808244459331036 \t\n",
      "Epoch 29756 \t\t Training Loss: 0.0005808244459331036 \t\n",
      "Epoch 29757 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29758 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29759 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29760 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29761 \t\t Training Loss: 0.0005808244459331036 \t\n",
      "Epoch 29762 \t\t Training Loss: 0.0005808244459331036 \t\n",
      "Epoch 29763 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29764 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29765 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29766 \t\t Training Loss: 0.0005808244459331036 \t\n",
      "Epoch 29767 \t\t Training Loss: 0.0005808245041407645 \t\n",
      "Epoch 29768 \t\t Training Loss: 0.0005808244459331036 \t\n",
      "Epoch 29769 \t\t Training Loss: 0.0005808244459331036 \t\n",
      "Epoch 29770 \t\t Training Loss: 0.0005808244459331036 \t\n",
      "Epoch 29771 \t\t Training Loss: 0.0005808244459331036 \t\n",
      "Epoch 29772 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29773 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29774 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29775 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29776 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29777 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29778 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29779 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29780 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29781 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29782 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29783 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29784 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29785 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29786 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29787 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29788 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29789 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29790 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29791 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29792 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29793 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29794 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29795 \t\t Training Loss: 0.0005808243877254426 \t\n",
      "Epoch 29796 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29797 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29798 \t\t Training Loss: 0.0005808242131024599 \t\n",
      "Epoch 29799 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29800 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29801 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29802 \t\t Training Loss: 0.0005808242131024599 \t\n",
      "Epoch 29803 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29804 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29805 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29806 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29807 \t\t Training Loss: 0.0005808242131024599 \t\n",
      "Epoch 29808 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29809 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29810 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29811 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29812 \t\t Training Loss: 0.0005808242713101208 \t\n",
      "Epoch 29813 \t\t Training Loss: 0.0005808242131024599 \t\n",
      "Epoch 29814 \t\t Training Loss: 0.0005808242131024599 \t\n",
      "Epoch 29815 \t\t Training Loss: 0.0005808242131024599 \t\n",
      "Epoch 29816 \t\t Training Loss: 0.0005808242131024599 \t\n",
      "Epoch 29817 \t\t Training Loss: 0.000580824154894799 \t\n",
      "Epoch 29818 \t\t Training Loss: 0.000580824154894799 \t\n",
      "Epoch 29819 \t\t Training Loss: 0.000580824154894799 \t\n",
      "Epoch 29820 \t\t Training Loss: 0.000580824154894799 \t\n",
      "Epoch 29821 \t\t Training Loss: 0.000580824154894799 \t\n",
      "Epoch 29822 \t\t Training Loss: 0.000580824154894799 \t\n",
      "Epoch 29823 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29824 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29825 \t\t Training Loss: 0.0005808240966871381 \t\n",
      "Epoch 29826 \t\t Training Loss: 0.0005808240966871381 \t\n",
      "Epoch 29827 \t\t Training Loss: 0.0005808240966871381 \t\n",
      "Epoch 29828 \t\t Training Loss: 0.0005808240966871381 \t\n",
      "Epoch 29829 \t\t Training Loss: 0.000580824154894799 \t\n",
      "Epoch 29830 \t\t Training Loss: 0.0005808240966871381 \t\n",
      "Epoch 29831 \t\t Training Loss: 0.000580824154894799 \t\n",
      "Epoch 29832 \t\t Training Loss: 0.0005808240966871381 \t\n",
      "Epoch 29833 \t\t Training Loss: 0.0005808240966871381 \t\n",
      "Epoch 29834 \t\t Training Loss: 0.000580824154894799 \t\n",
      "Epoch 29835 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29836 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29837 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29838 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29839 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29840 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29841 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29842 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29843 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29844 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29845 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29846 \t\t Training Loss: 0.0005808239802718163 \t\n",
      "Epoch 29847 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29848 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29849 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29850 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29851 \t\t Training Loss: 0.0005808240384794772 \t\n",
      "Epoch 29852 \t\t Training Loss: 0.0005808239802718163 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29853 \t\t Training Loss: 0.0005808239802718163 \t\n",
      "Epoch 29854 \t\t Training Loss: 0.0005808239802718163 \t\n",
      "Epoch 29855 \t\t Training Loss: 0.0005808239802718163 \t\n",
      "Epoch 29856 \t\t Training Loss: 0.0005808239220641553 \t\n",
      "Epoch 29857 \t\t Training Loss: 0.0005808238638564944 \t\n",
      "Epoch 29858 \t\t Training Loss: 0.0005808238638564944 \t\n",
      "Epoch 29859 \t\t Training Loss: 0.0005808238638564944 \t\n",
      "Epoch 29860 \t\t Training Loss: 0.0005808238638564944 \t\n",
      "Epoch 29861 \t\t Training Loss: 0.0005808238056488335 \t\n",
      "Epoch 29862 \t\t Training Loss: 0.0005808238056488335 \t\n",
      "Epoch 29863 \t\t Training Loss: 0.0005808238056488335 \t\n",
      "Epoch 29864 \t\t Training Loss: 0.0005808238056488335 \t\n",
      "Epoch 29865 \t\t Training Loss: 0.0005808238056488335 \t\n",
      "Epoch 29866 \t\t Training Loss: 0.0005808238056488335 \t\n",
      "Epoch 29867 \t\t Training Loss: 0.0005808238056488335 \t\n",
      "Epoch 29868 \t\t Training Loss: 0.0005808238056488335 \t\n",
      "Epoch 29869 \t\t Training Loss: 0.0005808238056488335 \t\n",
      "Epoch 29870 \t\t Training Loss: 0.0005808238056488335 \t\n",
      "Epoch 29871 \t\t Training Loss: 0.0005808238056488335 \t\n",
      "Epoch 29872 \t\t Training Loss: 0.0005808238056488335 \t\n",
      "Epoch 29873 \t\t Training Loss: 0.0005808238056488335 \t\n",
      "Epoch 29874 \t\t Training Loss: 0.0005808238056488335 \t\n",
      "Epoch 29875 \t\t Training Loss: 0.0005808238056488335 \t\n",
      "Epoch 29876 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29877 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29878 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29879 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29880 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29881 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29882 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29883 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29884 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29885 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29886 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29887 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29888 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29889 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29890 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29891 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29892 \t\t Training Loss: 0.0005808236892335117 \t\n",
      "Epoch 29893 \t\t Training Loss: 0.0005808235728181899 \t\n",
      "Epoch 29894 \t\t Training Loss: 0.0005808235728181899 \t\n",
      "Epoch 29895 \t\t Training Loss: 0.0005808235728181899 \t\n",
      "Epoch 29896 \t\t Training Loss: 0.0005808235728181899 \t\n",
      "Epoch 29897 \t\t Training Loss: 0.0005808235728181899 \t\n",
      "Epoch 29898 \t\t Training Loss: 0.0005808235728181899 \t\n",
      "Epoch 29899 \t\t Training Loss: 0.000580823456402868 \t\n",
      "Epoch 29900 \t\t Training Loss: 0.000580823456402868 \t\n",
      "Epoch 29901 \t\t Training Loss: 0.0005808235728181899 \t\n",
      "Epoch 29902 \t\t Training Loss: 0.000580823456402868 \t\n",
      "Epoch 29903 \t\t Training Loss: 0.000580823456402868 \t\n",
      "Epoch 29904 \t\t Training Loss: 0.000580823456402868 \t\n",
      "Epoch 29905 \t\t Training Loss: 0.000580823456402868 \t\n",
      "Epoch 29906 \t\t Training Loss: 0.000580823456402868 \t\n",
      "Epoch 29907 \t\t Training Loss: 0.000580823456402868 \t\n",
      "Epoch 29908 \t\t Training Loss: 0.000580823456402868 \t\n",
      "Epoch 29909 \t\t Training Loss: 0.000580823456402868 \t\n",
      "Epoch 29910 \t\t Training Loss: 0.000580823456402868 \t\n",
      "Epoch 29911 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29912 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29913 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29914 \t\t Training Loss: 0.000580823456402868 \t\n",
      "Epoch 29915 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29916 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29917 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29918 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29919 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29920 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29921 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29922 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29923 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29924 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29925 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29926 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29927 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29928 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29929 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29930 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29931 \t\t Training Loss: 0.0005808233981952071 \t\n",
      "Epoch 29932 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29933 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29934 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29935 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29936 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29937 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29938 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29939 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29940 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29941 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29942 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29943 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29944 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29945 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29946 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29947 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29948 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29949 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29950 \t\t Training Loss: 0.0005808232817798853 \t\n",
      "Epoch 29951 \t\t Training Loss: 0.0005808231653645635 \t\n",
      "Epoch 29952 \t\t Training Loss: 0.0005808231653645635 \t\n",
      "Epoch 29953 \t\t Training Loss: 0.0005808231071569026 \t\n",
      "Epoch 29954 \t\t Training Loss: 0.0005808231653645635 \t\n",
      "Epoch 29955 \t\t Training Loss: 0.0005808231071569026 \t\n",
      "Epoch 29956 \t\t Training Loss: 0.0005808231071569026 \t\n",
      "Epoch 29957 \t\t Training Loss: 0.0005808231071569026 \t\n",
      "Epoch 29958 \t\t Training Loss: 0.0005808231071569026 \t\n",
      "Epoch 29959 \t\t Training Loss: 0.0005808231071569026 \t\n",
      "Epoch 29960 \t\t Training Loss: 0.0005808231071569026 \t\n",
      "Epoch 29961 \t\t Training Loss: 0.0005808231653645635 \t\n",
      "Epoch 29962 \t\t Training Loss: 0.0005808231653645635 \t\n",
      "Epoch 29963 \t\t Training Loss: 0.0005808231071569026 \t\n",
      "Epoch 29964 \t\t Training Loss: 0.0005808231071569026 \t\n",
      "Epoch 29965 \t\t Training Loss: 0.0005808231071569026 \t\n",
      "Epoch 29966 \t\t Training Loss: 0.0005808231653645635 \t\n",
      "Epoch 29967 \t\t Training Loss: 0.0005808231653645635 \t\n",
      "Epoch 29968 \t\t Training Loss: 0.0005808231071569026 \t\n",
      "Epoch 29969 \t\t Training Loss: 0.0005808231653645635 \t\n",
      "Epoch 29970 \t\t Training Loss: 0.0005808231653645635 \t\n",
      "Epoch 29971 \t\t Training Loss: 0.0005808231071569026 \t\n",
      "Epoch 29972 \t\t Training Loss: 0.0005808231071569026 \t\n",
      "Epoch 29973 \t\t Training Loss: 0.0005808231071569026 \t\n",
      "Epoch 29974 \t\t Training Loss: 0.0005808229907415807 \t\n",
      "Epoch 29975 \t\t Training Loss: 0.0005808229907415807 \t\n",
      "Epoch 29976 \t\t Training Loss: 0.0005808229907415807 \t\n",
      "Epoch 29977 \t\t Training Loss: 0.0005808229907415807 \t\n",
      "Epoch 29978 \t\t Training Loss: 0.0005808229907415807 \t\n",
      "Epoch 29979 \t\t Training Loss: 0.0005808229907415807 \t\n",
      "Epoch 29980 \t\t Training Loss: 0.0005808229907415807 \t\n",
      "Epoch 29981 \t\t Training Loss: 0.0005808229907415807 \t\n",
      "Epoch 29982 \t\t Training Loss: 0.0005808229907415807 \t\n",
      "Epoch 29983 \t\t Training Loss: 0.0005808229907415807 \t\n",
      "Epoch 29984 \t\t Training Loss: 0.0005808229907415807 \t\n",
      "Epoch 29985 \t\t Training Loss: 0.0005808229907415807 \t\n",
      "Epoch 29986 \t\t Training Loss: 0.0005808229907415807 \t\n",
      "Epoch 29987 \t\t Training Loss: 0.0005808229907415807 \t\n",
      "Epoch 29988 \t\t Training Loss: 0.0005808229907415807 \t\n",
      "Epoch 29989 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 29990 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 29991 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 29992 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 29993 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 29994 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 29995 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 29996 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 29997 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 29998 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 29999 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30000 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30001 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30002 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30003 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30004 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30005 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30006 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30007 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30008 \t\t Training Loss: 0.0005808228743262589 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30009 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30010 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30011 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30012 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30013 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30014 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30015 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30016 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30017 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30018 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30019 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30020 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30021 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30022 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30023 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30024 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30025 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30026 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30027 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30028 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30029 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30030 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30031 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30032 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30033 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30034 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30035 \t\t Training Loss: 0.0005808228743262589 \t\n",
      "Epoch 30036 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30037 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30038 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30039 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30040 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30041 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30042 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30043 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30044 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30045 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30046 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30047 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30048 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30049 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30050 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30051 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30052 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30053 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30054 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30055 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30056 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30057 \t\t Training Loss: 0.0005808226997032762 \t\n",
      "Epoch 30058 \t\t Training Loss: 0.000580822816118598 \t\n",
      "Epoch 30059 \t\t Training Loss: 0.0005808226997032762 \t\n",
      "Epoch 30060 \t\t Training Loss: 0.0005808226997032762 \t\n",
      "Epoch 30061 \t\t Training Loss: 0.0005808226997032762 \t\n",
      "Epoch 30062 \t\t Training Loss: 0.0005808226997032762 \t\n",
      "Epoch 30063 \t\t Training Loss: 0.0005808226997032762 \t\n",
      "Epoch 30064 \t\t Training Loss: 0.0005808226997032762 \t\n",
      "Epoch 30065 \t\t Training Loss: 0.0005808226997032762 \t\n",
      "Epoch 30066 \t\t Training Loss: 0.0005808226997032762 \t\n",
      "Epoch 30067 \t\t Training Loss: 0.0005808226997032762 \t\n",
      "Epoch 30068 \t\t Training Loss: 0.0005808226997032762 \t\n",
      "Epoch 30069 \t\t Training Loss: 0.0005808226997032762 \t\n",
      "Epoch 30070 \t\t Training Loss: 0.0005808226997032762 \t\n",
      "Epoch 30071 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30072 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30073 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30074 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30075 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30076 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30077 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30078 \t\t Training Loss: 0.0005808226997032762 \t\n",
      "Epoch 30079 \t\t Training Loss: 0.0005808226997032762 \t\n",
      "Epoch 30080 \t\t Training Loss: 0.0005808226997032762 \t\n",
      "Epoch 30081 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30082 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30083 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30084 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30085 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30086 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30087 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30088 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30089 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30090 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30091 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30092 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30093 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30094 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30095 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30096 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30097 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30098 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30099 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30100 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30101 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30102 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30103 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30104 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30105 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30106 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30107 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30108 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30109 \t\t Training Loss: 0.0005808224668726325 \t\n",
      "Epoch 30110 \t\t Training Loss: 0.0005808224668726325 \t\n",
      "Epoch 30111 \t\t Training Loss: 0.0005808224668726325 \t\n",
      "Epoch 30112 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30113 \t\t Training Loss: 0.0005808225832879543 \t\n",
      "Epoch 30114 \t\t Training Loss: 0.0005808224668726325 \t\n",
      "Epoch 30115 \t\t Training Loss: 0.0005808224668726325 \t\n",
      "Epoch 30116 \t\t Training Loss: 0.0005808224668726325 \t\n",
      "Epoch 30117 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30118 \t\t Training Loss: 0.0005808224668726325 \t\n",
      "Epoch 30119 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30120 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30121 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30122 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30123 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30124 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30125 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30126 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30127 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30128 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30129 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30130 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30131 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30132 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30133 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30134 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30135 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30136 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30137 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30138 \t\t Training Loss: 0.0005808222922496498 \t\n",
      "Epoch 30139 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30140 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30141 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30142 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30143 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30144 \t\t Training Loss: 0.0005808224086649716 \t\n",
      "Epoch 30145 \t\t Training Loss: 0.0005808222922496498 \t\n",
      "Epoch 30146 \t\t Training Loss: 0.0005808222922496498 \t\n",
      "Epoch 30147 \t\t Training Loss: 0.0005808222922496498 \t\n",
      "Epoch 30148 \t\t Training Loss: 0.0005808222922496498 \t\n",
      "Epoch 30149 \t\t Training Loss: 0.0005808222922496498 \t\n",
      "Epoch 30150 \t\t Training Loss: 0.0005808222922496498 \t\n",
      "Epoch 30151 \t\t Training Loss: 0.0005808222922496498 \t\n",
      "Epoch 30152 \t\t Training Loss: 0.0005808222922496498 \t\n",
      "Epoch 30153 \t\t Training Loss: 0.0005808221758343279 \t\n",
      "Epoch 30154 \t\t Training Loss: 0.0005808221758343279 \t\n",
      "Epoch 30155 \t\t Training Loss: 0.0005808222922496498 \t\n",
      "Epoch 30156 \t\t Training Loss: 0.0005808222922496498 \t\n",
      "Epoch 30157 \t\t Training Loss: 0.0005808221758343279 \t\n",
      "Epoch 30158 \t\t Training Loss: 0.0005808221758343279 \t\n",
      "Epoch 30159 \t\t Training Loss: 0.0005808221758343279 \t\n",
      "Epoch 30160 \t\t Training Loss: 0.0005808221758343279 \t\n",
      "Epoch 30161 \t\t Training Loss: 0.0005808221758343279 \t\n",
      "Epoch 30162 \t\t Training Loss: 0.0005808221758343279 \t\n",
      "Epoch 30163 \t\t Training Loss: 0.0005808221758343279 \t\n",
      "Epoch 30164 \t\t Training Loss: 0.0005808221758343279 \t\n",
      "Epoch 30165 \t\t Training Loss: 0.0005808221758343279 \t\n",
      "Epoch 30166 \t\t Training Loss: 0.0005808221758343279 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30167 \t\t Training Loss: 0.0005808221758343279 \t\n",
      "Epoch 30168 \t\t Training Loss: 0.0005808221758343279 \t\n",
      "Epoch 30169 \t\t Training Loss: 0.000580822117626667 \t\n",
      "Epoch 30170 \t\t Training Loss: 0.000580822117626667 \t\n",
      "Epoch 30171 \t\t Training Loss: 0.0005808221758343279 \t\n",
      "Epoch 30172 \t\t Training Loss: 0.0005808221758343279 \t\n",
      "Epoch 30173 \t\t Training Loss: 0.000580822117626667 \t\n",
      "Epoch 30174 \t\t Training Loss: 0.000580822117626667 \t\n",
      "Epoch 30175 \t\t Training Loss: 0.000580822117626667 \t\n",
      "Epoch 30176 \t\t Training Loss: 0.000580822117626667 \t\n",
      "Epoch 30177 \t\t Training Loss: 0.000580822117626667 \t\n",
      "Epoch 30178 \t\t Training Loss: 0.000580822117626667 \t\n",
      "Epoch 30179 \t\t Training Loss: 0.000580822117626667 \t\n",
      "Epoch 30180 \t\t Training Loss: 0.0005808220012113452 \t\n",
      "Epoch 30181 \t\t Training Loss: 0.0005808220012113452 \t\n",
      "Epoch 30182 \t\t Training Loss: 0.0005808220012113452 \t\n",
      "Epoch 30183 \t\t Training Loss: 0.0005808220012113452 \t\n",
      "Epoch 30184 \t\t Training Loss: 0.0005808220012113452 \t\n",
      "Epoch 30185 \t\t Training Loss: 0.0005808220012113452 \t\n",
      "Epoch 30186 \t\t Training Loss: 0.0005808220012113452 \t\n",
      "Epoch 30187 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30188 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30189 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30190 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30191 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30192 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30193 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30194 \t\t Training Loss: 0.0005808220012113452 \t\n",
      "Epoch 30195 \t\t Training Loss: 0.0005808220012113452 \t\n",
      "Epoch 30196 \t\t Training Loss: 0.0005808220012113452 \t\n",
      "Epoch 30197 \t\t Training Loss: 0.0005808220012113452 \t\n",
      "Epoch 30198 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30199 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30200 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30201 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30202 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30203 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30204 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30205 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30206 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30207 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30208 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30209 \t\t Training Loss: 0.0005808218265883625 \t\n",
      "Epoch 30210 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30211 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30212 \t\t Training Loss: 0.0005808218847960234 \t\n",
      "Epoch 30213 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30214 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30215 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30216 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30217 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30218 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30219 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30220 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30221 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30222 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30223 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30224 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30225 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30226 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30227 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30228 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30229 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30230 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30231 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30232 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30233 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30234 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30235 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30236 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30237 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30238 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30239 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30240 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30241 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30242 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30243 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30244 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30245 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30246 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30247 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30248 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30249 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30250 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30251 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30252 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30253 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30254 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30255 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30256 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30257 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30258 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30259 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30260 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30261 \t\t Training Loss: 0.0005808217101730406 \t\n",
      "Epoch 30262 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30263 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30264 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30265 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30266 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30267 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30268 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30269 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30270 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30271 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30272 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30273 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30274 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30275 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30276 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30277 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30278 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30279 \t\t Training Loss: 0.0005808215937577188 \t\n",
      "Epoch 30280 \t\t Training Loss: 0.0005808215355500579 \t\n",
      "Epoch 30281 \t\t Training Loss: 0.0005808215355500579 \t\n",
      "Epoch 30282 \t\t Training Loss: 0.0005808215355500579 \t\n",
      "Epoch 30283 \t\t Training Loss: 0.0005808215355500579 \t\n",
      "Epoch 30284 \t\t Training Loss: 0.0005808215355500579 \t\n",
      "Epoch 30285 \t\t Training Loss: 0.0005808215355500579 \t\n",
      "Epoch 30286 \t\t Training Loss: 0.0005808215355500579 \t\n",
      "Epoch 30287 \t\t Training Loss: 0.0005808215355500579 \t\n",
      "Epoch 30288 \t\t Training Loss: 0.0005808215355500579 \t\n",
      "Epoch 30289 \t\t Training Loss: 0.0005808215355500579 \t\n",
      "Epoch 30290 \t\t Training Loss: 0.0005808215355500579 \t\n",
      "Epoch 30291 \t\t Training Loss: 0.0005808215355500579 \t\n",
      "Epoch 30292 \t\t Training Loss: 0.0005808215355500579 \t\n",
      "Epoch 30293 \t\t Training Loss: 0.0005808215355500579 \t\n",
      "Epoch 30294 \t\t Training Loss: 0.000580821477342397 \t\n",
      "Epoch 30295 \t\t Training Loss: 0.0005808215355500579 \t\n",
      "Epoch 30296 \t\t Training Loss: 0.000580821477342397 \t\n",
      "Epoch 30297 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30298 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30299 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30300 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30301 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30302 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30303 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30304 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30305 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30306 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30307 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30308 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30309 \t\t Training Loss: 0.0005808213027194142 \t\n",
      "Epoch 30310 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30311 \t\t Training Loss: 0.0005808213027194142 \t\n",
      "Epoch 30312 \t\t Training Loss: 0.0005808213027194142 \t\n",
      "Epoch 30313 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30314 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30315 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30316 \t\t Training Loss: 0.0005808214191347361 \t\n",
      "Epoch 30317 \t\t Training Loss: 0.0005808213027194142 \t\n",
      "Epoch 30318 \t\t Training Loss: 0.0005808213027194142 \t\n",
      "Epoch 30319 \t\t Training Loss: 0.0005808213027194142 \t\n",
      "Epoch 30320 \t\t Training Loss: 0.0005808213027194142 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30321 \t\t Training Loss: 0.0005808213027194142 \t\n",
      "Epoch 30322 \t\t Training Loss: 0.0005808212445117533 \t\n",
      "Epoch 30323 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30324 \t\t Training Loss: 0.0005808212445117533 \t\n",
      "Epoch 30325 \t\t Training Loss: 0.0005808213027194142 \t\n",
      "Epoch 30326 \t\t Training Loss: 0.0005808212445117533 \t\n",
      "Epoch 30327 \t\t Training Loss: 0.0005808212445117533 \t\n",
      "Epoch 30328 \t\t Training Loss: 0.0005808212445117533 \t\n",
      "Epoch 30329 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30330 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30331 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30332 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30333 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30334 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30335 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30336 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30337 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30338 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30339 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30340 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30341 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30342 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30343 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30344 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30345 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30346 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30347 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30348 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30349 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30350 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30351 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30352 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30353 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30354 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30355 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30356 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30357 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30358 \t\t Training Loss: 0.0005808211280964315 \t\n",
      "Epoch 30359 \t\t Training Loss: 0.0005808211280964315 \t\n",
      "Epoch 30360 \t\t Training Loss: 0.0005808211863040924 \t\n",
      "Epoch 30361 \t\t Training Loss: 0.0005808211280964315 \t\n",
      "Epoch 30362 \t\t Training Loss: 0.0005808211280964315 \t\n",
      "Epoch 30363 \t\t Training Loss: 0.0005808211280964315 \t\n",
      "Epoch 30364 \t\t Training Loss: 0.0005808211280964315 \t\n",
      "Epoch 30365 \t\t Training Loss: 0.0005808211280964315 \t\n",
      "Epoch 30366 \t\t Training Loss: 0.0005808211280964315 \t\n",
      "Epoch 30367 \t\t Training Loss: 0.0005808211280964315 \t\n",
      "Epoch 30368 \t\t Training Loss: 0.0005808210698887706 \t\n",
      "Epoch 30369 \t\t Training Loss: 0.0005808211280964315 \t\n",
      "Epoch 30370 \t\t Training Loss: 0.0005808210698887706 \t\n",
      "Epoch 30371 \t\t Training Loss: 0.0005808210698887706 \t\n",
      "Epoch 30372 \t\t Training Loss: 0.0005808210698887706 \t\n",
      "Epoch 30373 \t\t Training Loss: 0.0005808210698887706 \t\n",
      "Epoch 30374 \t\t Training Loss: 0.0005808210698887706 \t\n",
      "Epoch 30375 \t\t Training Loss: 0.0005808210116811097 \t\n",
      "Epoch 30376 \t\t Training Loss: 0.0005808210116811097 \t\n",
      "Epoch 30377 \t\t Training Loss: 0.0005808210698887706 \t\n",
      "Epoch 30378 \t\t Training Loss: 0.0005808210698887706 \t\n",
      "Epoch 30379 \t\t Training Loss: 0.0005808210698887706 \t\n",
      "Epoch 30380 \t\t Training Loss: 0.0005808210698887706 \t\n",
      "Epoch 30381 \t\t Training Loss: 0.0005808210698887706 \t\n",
      "Epoch 30382 \t\t Training Loss: 0.0005808210116811097 \t\n",
      "Epoch 30383 \t\t Training Loss: 0.0005808210116811097 \t\n",
      "Epoch 30384 \t\t Training Loss: 0.0005808210116811097 \t\n",
      "Epoch 30385 \t\t Training Loss: 0.0005808210116811097 \t\n",
      "Epoch 30386 \t\t Training Loss: 0.0005808210116811097 \t\n",
      "Epoch 30387 \t\t Training Loss: 0.0005808210116811097 \t\n",
      "Epoch 30388 \t\t Training Loss: 0.0005808208952657878 \t\n",
      "Epoch 30389 \t\t Training Loss: 0.0005808208952657878 \t\n",
      "Epoch 30390 \t\t Training Loss: 0.0005808208952657878 \t\n",
      "Epoch 30391 \t\t Training Loss: 0.0005808208952657878 \t\n",
      "Epoch 30392 \t\t Training Loss: 0.0005808208952657878 \t\n",
      "Epoch 30393 \t\t Training Loss: 0.0005808208952657878 \t\n",
      "Epoch 30394 \t\t Training Loss: 0.0005808208952657878 \t\n",
      "Epoch 30395 \t\t Training Loss: 0.0005808208952657878 \t\n",
      "Epoch 30396 \t\t Training Loss: 0.0005808208952657878 \t\n",
      "Epoch 30397 \t\t Training Loss: 0.0005808208952657878 \t\n",
      "Epoch 30398 \t\t Training Loss: 0.0005808208952657878 \t\n",
      "Epoch 30399 \t\t Training Loss: 0.0005808208952657878 \t\n",
      "Epoch 30400 \t\t Training Loss: 0.0005808208952657878 \t\n",
      "Epoch 30401 \t\t Training Loss: 0.0005808208952657878 \t\n",
      "Epoch 30402 \t\t Training Loss: 0.0005808208370581269 \t\n",
      "Epoch 30403 \t\t Training Loss: 0.0005808208952657878 \t\n",
      "Epoch 30404 \t\t Training Loss: 0.0005808208370581269 \t\n",
      "Epoch 30405 \t\t Training Loss: 0.0005808208370581269 \t\n",
      "Epoch 30406 \t\t Training Loss: 0.0005808208370581269 \t\n",
      "Epoch 30407 \t\t Training Loss: 0.0005808208370581269 \t\n",
      "Epoch 30408 \t\t Training Loss: 0.0005808207206428051 \t\n",
      "Epoch 30409 \t\t Training Loss: 0.0005808208370581269 \t\n",
      "Epoch 30410 \t\t Training Loss: 0.0005808208370581269 \t\n",
      "Epoch 30411 \t\t Training Loss: 0.0005808208370581269 \t\n",
      "Epoch 30412 \t\t Training Loss: 0.0005808207206428051 \t\n",
      "Epoch 30413 \t\t Training Loss: 0.0005808207206428051 \t\n",
      "Epoch 30414 \t\t Training Loss: 0.0005808207206428051 \t\n",
      "Epoch 30415 \t\t Training Loss: 0.0005808207206428051 \t\n",
      "Epoch 30416 \t\t Training Loss: 0.0005808207206428051 \t\n",
      "Epoch 30417 \t\t Training Loss: 0.0005808207206428051 \t\n",
      "Epoch 30418 \t\t Training Loss: 0.0005808207206428051 \t\n",
      "Epoch 30419 \t\t Training Loss: 0.0005808207206428051 \t\n",
      "Epoch 30420 \t\t Training Loss: 0.0005808207206428051 \t\n",
      "Epoch 30421 \t\t Training Loss: 0.0005808207206428051 \t\n",
      "Epoch 30422 \t\t Training Loss: 0.0005808207206428051 \t\n",
      "Epoch 30423 \t\t Training Loss: 0.0005808207206428051 \t\n",
      "Epoch 30424 \t\t Training Loss: 0.0005808207206428051 \t\n",
      "Epoch 30425 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30426 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30427 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30428 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30429 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30430 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30431 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30432 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30433 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30434 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30435 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30436 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30437 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30438 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30439 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30440 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30441 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30442 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30443 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30444 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30445 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30446 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30447 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30448 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30449 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30450 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30451 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30452 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30453 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30454 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30455 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30456 \t\t Training Loss: 0.0005808206042274833 \t\n",
      "Epoch 30457 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30458 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30459 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30460 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30461 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30462 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30463 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30464 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30465 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30466 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30467 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30468 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30469 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30470 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30471 \t\t Training Loss: 0.0005808205460198224 \t\n",
      "Epoch 30472 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30473 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30474 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30475 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30476 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30477 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30478 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30479 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30480 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30481 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30482 \t\t Training Loss: 0.0005808204878121614 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30483 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30484 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30485 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30486 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30487 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30488 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30489 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30490 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30491 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30492 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30493 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30494 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30495 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30496 \t\t Training Loss: 0.0005808204878121614 \t\n",
      "Epoch 30497 \t\t Training Loss: 0.0005808203713968396 \t\n",
      "Epoch 30498 \t\t Training Loss: 0.0005808203713968396 \t\n",
      "Epoch 30499 \t\t Training Loss: 0.0005808203713968396 \t\n",
      "Epoch 30500 \t\t Training Loss: 0.0005808203713968396 \t\n",
      "Epoch 30501 \t\t Training Loss: 0.0005808203131891787 \t\n",
      "Epoch 30502 \t\t Training Loss: 0.0005808203131891787 \t\n",
      "Epoch 30503 \t\t Training Loss: 0.0005808203131891787 \t\n",
      "Epoch 30504 \t\t Training Loss: 0.0005808203131891787 \t\n",
      "Epoch 30505 \t\t Training Loss: 0.0005808203131891787 \t\n",
      "Epoch 30506 \t\t Training Loss: 0.0005808203131891787 \t\n",
      "Epoch 30507 \t\t Training Loss: 0.0005808203131891787 \t\n",
      "Epoch 30508 \t\t Training Loss: 0.0005808203131891787 \t\n",
      "Epoch 30509 \t\t Training Loss: 0.0005808203131891787 \t\n",
      "Epoch 30510 \t\t Training Loss: 0.0005808203131891787 \t\n",
      "Epoch 30511 \t\t Training Loss: 0.0005808203131891787 \t\n",
      "Epoch 30512 \t\t Training Loss: 0.0005808203131891787 \t\n",
      "Epoch 30513 \t\t Training Loss: 0.0005808203131891787 \t\n",
      "Epoch 30514 \t\t Training Loss: 0.0005808203131891787 \t\n",
      "Epoch 30515 \t\t Training Loss: 0.0005808203131891787 \t\n",
      "Epoch 30516 \t\t Training Loss: 0.0005808203131891787 \t\n",
      "Epoch 30517 \t\t Training Loss: 0.0005808202549815178 \t\n",
      "Epoch 30518 \t\t Training Loss: 0.0005808202549815178 \t\n",
      "Epoch 30519 \t\t Training Loss: 0.0005808202549815178 \t\n",
      "Epoch 30520 \t\t Training Loss: 0.0005808202549815178 \t\n",
      "Epoch 30521 \t\t Training Loss: 0.0005808202549815178 \t\n",
      "Epoch 30522 \t\t Training Loss: 0.0005808202549815178 \t\n",
      "Epoch 30523 \t\t Training Loss: 0.0005808201967738569 \t\n",
      "Epoch 30524 \t\t Training Loss: 0.0005808202549815178 \t\n",
      "Epoch 30525 \t\t Training Loss: 0.0005808202549815178 \t\n",
      "Epoch 30526 \t\t Training Loss: 0.0005808202549815178 \t\n",
      "Epoch 30527 \t\t Training Loss: 0.0005808202549815178 \t\n",
      "Epoch 30528 \t\t Training Loss: 0.0005808202549815178 \t\n",
      "Epoch 30529 \t\t Training Loss: 0.0005808202549815178 \t\n",
      "Epoch 30530 \t\t Training Loss: 0.0005808202549815178 \t\n",
      "Epoch 30531 \t\t Training Loss: 0.0005808202549815178 \t\n",
      "Epoch 30532 \t\t Training Loss: 0.000580820138566196 \t\n",
      "Epoch 30533 \t\t Training Loss: 0.000580820138566196 \t\n",
      "Epoch 30534 \t\t Training Loss: 0.0005808201967738569 \t\n",
      "Epoch 30535 \t\t Training Loss: 0.0005808201967738569 \t\n",
      "Epoch 30536 \t\t Training Loss: 0.0005808201967738569 \t\n",
      "Epoch 30537 \t\t Training Loss: 0.000580820138566196 \t\n",
      "Epoch 30538 \t\t Training Loss: 0.000580820138566196 \t\n",
      "Epoch 30539 \t\t Training Loss: 0.000580820138566196 \t\n",
      "Epoch 30540 \t\t Training Loss: 0.000580820138566196 \t\n",
      "Epoch 30541 \t\t Training Loss: 0.000580820138566196 \t\n",
      "Epoch 30542 \t\t Training Loss: 0.000580820138566196 \t\n",
      "Epoch 30543 \t\t Training Loss: 0.000580820138566196 \t\n",
      "Epoch 30544 \t\t Training Loss: 0.000580820138566196 \t\n",
      "Epoch 30545 \t\t Training Loss: 0.000580820080358535 \t\n",
      "Epoch 30546 \t\t Training Loss: 0.000580820138566196 \t\n",
      "Epoch 30547 \t\t Training Loss: 0.000580820080358535 \t\n",
      "Epoch 30548 \t\t Training Loss: 0.000580820080358535 \t\n",
      "Epoch 30549 \t\t Training Loss: 0.000580820080358535 \t\n",
      "Epoch 30550 \t\t Training Loss: 0.000580820080358535 \t\n",
      "Epoch 30551 \t\t Training Loss: 0.000580820080358535 \t\n",
      "Epoch 30552 \t\t Training Loss: 0.0005808200221508741 \t\n",
      "Epoch 30553 \t\t Training Loss: 0.0005808200221508741 \t\n",
      "Epoch 30554 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30555 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30556 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30557 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30558 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30559 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30560 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30561 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30562 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30563 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30564 \t\t Training Loss: 0.000580820080358535 \t\n",
      "Epoch 30565 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30566 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30567 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30568 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30569 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30570 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30571 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30572 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30573 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30574 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30575 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30576 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30577 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30578 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30579 \t\t Training Loss: 0.0005808199639432132 \t\n",
      "Epoch 30580 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30581 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30582 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30583 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30584 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30585 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30586 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30587 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30588 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30589 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30590 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30591 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30592 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30593 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30594 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30595 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30596 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30597 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30598 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30599 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30600 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30601 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30602 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30603 \t\t Training Loss: 0.0005808198475278914 \t\n",
      "Epoch 30604 \t\t Training Loss: 0.0005808198475278914 \t\n",
      "Epoch 30605 \t\t Training Loss: 0.0005808198475278914 \t\n",
      "Epoch 30606 \t\t Training Loss: 0.0005808198475278914 \t\n",
      "Epoch 30607 \t\t Training Loss: 0.0005808198475278914 \t\n",
      "Epoch 30608 \t\t Training Loss: 0.0005808198475278914 \t\n",
      "Epoch 30609 \t\t Training Loss: 0.0005808199057355523 \t\n",
      "Epoch 30610 \t\t Training Loss: 0.0005808198475278914 \t\n",
      "Epoch 30611 \t\t Training Loss: 0.0005808198475278914 \t\n",
      "Epoch 30612 \t\t Training Loss: 0.0005808198475278914 \t\n",
      "Epoch 30613 \t\t Training Loss: 0.0005808198475278914 \t\n",
      "Epoch 30614 \t\t Training Loss: 0.0005808198475278914 \t\n",
      "Epoch 30615 \t\t Training Loss: 0.0005808198475278914 \t\n",
      "Epoch 30616 \t\t Training Loss: 0.0005808198475278914 \t\n",
      "Epoch 30617 \t\t Training Loss: 0.0005808198475278914 \t\n",
      "Epoch 30618 \t\t Training Loss: 0.0005808197893202305 \t\n",
      "Epoch 30619 \t\t Training Loss: 0.0005808197893202305 \t\n",
      "Epoch 30620 \t\t Training Loss: 0.0005808198475278914 \t\n",
      "Epoch 30621 \t\t Training Loss: 0.0005808197893202305 \t\n",
      "Epoch 30622 \t\t Training Loss: 0.0005808197893202305 \t\n",
      "Epoch 30623 \t\t Training Loss: 0.0005808197893202305 \t\n",
      "Epoch 30624 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30625 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30626 \t\t Training Loss: 0.0005808197893202305 \t\n",
      "Epoch 30627 \t\t Training Loss: 0.0005808197893202305 \t\n",
      "Epoch 30628 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30629 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30630 \t\t Training Loss: 0.0005808197893202305 \t\n",
      "Epoch 30631 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30632 \t\t Training Loss: 0.0005808197893202305 \t\n",
      "Epoch 30633 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30634 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30635 \t\t Training Loss: 0.0005808197311125696 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30636 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30637 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30638 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30639 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30640 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30641 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30642 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30643 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30644 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30645 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30646 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30647 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30648 \t\t Training Loss: 0.0005808197311125696 \t\n",
      "Epoch 30649 \t\t Training Loss: 0.0005808196146972477 \t\n",
      "Epoch 30650 \t\t Training Loss: 0.0005808196146972477 \t\n",
      "Epoch 30651 \t\t Training Loss: 0.0005808196146972477 \t\n",
      "Epoch 30652 \t\t Training Loss: 0.0005808196146972477 \t\n",
      "Epoch 30653 \t\t Training Loss: 0.0005808196146972477 \t\n",
      "Epoch 30654 \t\t Training Loss: 0.0005808196146972477 \t\n",
      "Epoch 30655 \t\t Training Loss: 0.0005808195564895868 \t\n",
      "Epoch 30656 \t\t Training Loss: 0.0005808195564895868 \t\n",
      "Epoch 30657 \t\t Training Loss: 0.0005808195564895868 \t\n",
      "Epoch 30658 \t\t Training Loss: 0.0005808195564895868 \t\n",
      "Epoch 30659 \t\t Training Loss: 0.0005808195564895868 \t\n",
      "Epoch 30660 \t\t Training Loss: 0.0005808195564895868 \t\n",
      "Epoch 30661 \t\t Training Loss: 0.0005808195564895868 \t\n",
      "Epoch 30662 \t\t Training Loss: 0.0005808195564895868 \t\n",
      "Epoch 30663 \t\t Training Loss: 0.0005808195564895868 \t\n",
      "Epoch 30664 \t\t Training Loss: 0.0005808194982819259 \t\n",
      "Epoch 30665 \t\t Training Loss: 0.0005808195564895868 \t\n",
      "Epoch 30666 \t\t Training Loss: 0.0005808194982819259 \t\n",
      "Epoch 30667 \t\t Training Loss: 0.0005808194982819259 \t\n",
      "Epoch 30668 \t\t Training Loss: 0.000580819440074265 \t\n",
      "Epoch 30669 \t\t Training Loss: 0.000580819440074265 \t\n",
      "Epoch 30670 \t\t Training Loss: 0.000580819440074265 \t\n",
      "Epoch 30671 \t\t Training Loss: 0.000580819440074265 \t\n",
      "Epoch 30672 \t\t Training Loss: 0.000580819440074265 \t\n",
      "Epoch 30673 \t\t Training Loss: 0.000580819440074265 \t\n",
      "Epoch 30674 \t\t Training Loss: 0.000580819440074265 \t\n",
      "Epoch 30675 \t\t Training Loss: 0.000580819440074265 \t\n",
      "Epoch 30676 \t\t Training Loss: 0.000580819440074265 \t\n",
      "Epoch 30677 \t\t Training Loss: 0.0005808193818666041 \t\n",
      "Epoch 30678 \t\t Training Loss: 0.0005808193818666041 \t\n",
      "Epoch 30679 \t\t Training Loss: 0.0005808193818666041 \t\n",
      "Epoch 30680 \t\t Training Loss: 0.0005808193818666041 \t\n",
      "Epoch 30681 \t\t Training Loss: 0.0005808193818666041 \t\n",
      "Epoch 30682 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30683 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30684 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30685 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30686 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30687 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30688 \t\t Training Loss: 0.0005808193818666041 \t\n",
      "Epoch 30689 \t\t Training Loss: 0.0005808193818666041 \t\n",
      "Epoch 30690 \t\t Training Loss: 0.0005808193818666041 \t\n",
      "Epoch 30691 \t\t Training Loss: 0.0005808193818666041 \t\n",
      "Epoch 30692 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30693 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30694 \t\t Training Loss: 0.0005808193818666041 \t\n",
      "Epoch 30695 \t\t Training Loss: 0.0005808193818666041 \t\n",
      "Epoch 30696 \t\t Training Loss: 0.0005808193818666041 \t\n",
      "Epoch 30697 \t\t Training Loss: 0.0005808193818666041 \t\n",
      "Epoch 30698 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30699 \t\t Training Loss: 0.0005808193818666041 \t\n",
      "Epoch 30700 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30701 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30702 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30703 \t\t Training Loss: 0.0005808192654512823 \t\n",
      "Epoch 30704 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30705 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30706 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30707 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30708 \t\t Training Loss: 0.0005808192654512823 \t\n",
      "Epoch 30709 \t\t Training Loss: 0.0005808192654512823 \t\n",
      "Epoch 30710 \t\t Training Loss: 0.0005808192654512823 \t\n",
      "Epoch 30711 \t\t Training Loss: 0.0005808193236589432 \t\n",
      "Epoch 30712 \t\t Training Loss: 0.0005808192654512823 \t\n",
      "Epoch 30713 \t\t Training Loss: 0.0005808192654512823 \t\n",
      "Epoch 30714 \t\t Training Loss: 0.0005808192654512823 \t\n",
      "Epoch 30715 \t\t Training Loss: 0.0005808192654512823 \t\n",
      "Epoch 30716 \t\t Training Loss: 0.0005808192654512823 \t\n",
      "Epoch 30717 \t\t Training Loss: 0.0005808192654512823 \t\n",
      "Epoch 30718 \t\t Training Loss: 0.0005808192654512823 \t\n",
      "Epoch 30719 \t\t Training Loss: 0.0005808192654512823 \t\n",
      "Epoch 30720 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30721 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30722 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30723 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30724 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30725 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30726 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30727 \t\t Training Loss: 0.0005808192654512823 \t\n",
      "Epoch 30728 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30729 \t\t Training Loss: 0.0005808192654512823 \t\n",
      "Epoch 30730 \t\t Training Loss: 0.0005808192654512823 \t\n",
      "Epoch 30731 \t\t Training Loss: 0.0005808192654512823 \t\n",
      "Epoch 30732 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30733 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30734 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30735 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30736 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30737 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30738 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30739 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30740 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30741 \t\t Training Loss: 0.0005808192072436213 \t\n",
      "Epoch 30742 \t\t Training Loss: 0.0005808191490359604 \t\n",
      "Epoch 30743 \t\t Training Loss: 0.0005808191490359604 \t\n",
      "Epoch 30744 \t\t Training Loss: 0.0005808191490359604 \t\n",
      "Epoch 30745 \t\t Training Loss: 0.0005808191490359604 \t\n",
      "Epoch 30746 \t\t Training Loss: 0.0005808191490359604 \t\n",
      "Epoch 30747 \t\t Training Loss: 0.0005808190908282995 \t\n",
      "Epoch 30748 \t\t Training Loss: 0.0005808190908282995 \t\n",
      "Epoch 30749 \t\t Training Loss: 0.0005808190908282995 \t\n",
      "Epoch 30750 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30751 \t\t Training Loss: 0.0005808190908282995 \t\n",
      "Epoch 30752 \t\t Training Loss: 0.0005808190326206386 \t\n",
      "Epoch 30753 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30754 \t\t Training Loss: 0.0005808190908282995 \t\n",
      "Epoch 30755 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30756 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30757 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30758 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30759 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30760 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30761 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30762 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30763 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30764 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30765 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30766 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30767 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30768 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30769 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30770 \t\t Training Loss: 0.0005808189162053168 \t\n",
      "Epoch 30771 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30772 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30773 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30774 \t\t Training Loss: 0.0005808189162053168 \t\n",
      "Epoch 30775 \t\t Training Loss: 0.0005808189744129777 \t\n",
      "Epoch 30776 \t\t Training Loss: 0.0005808189162053168 \t\n",
      "Epoch 30777 \t\t Training Loss: 0.0005808189162053168 \t\n",
      "Epoch 30778 \t\t Training Loss: 0.0005808189162053168 \t\n",
      "Epoch 30779 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30780 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30781 \t\t Training Loss: 0.0005808189162053168 \t\n",
      "Epoch 30782 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30783 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30784 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30785 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30786 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30787 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30788 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30789 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30790 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30791 \t\t Training Loss: 0.0005808188579976559 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30792 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30793 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30794 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30795 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30796 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30797 \t\t Training Loss: 0.0005808188579976559 \t\n",
      "Epoch 30798 \t\t Training Loss: 0.000580818799789995 \t\n",
      "Epoch 30799 \t\t Training Loss: 0.000580818799789995 \t\n",
      "Epoch 30800 \t\t Training Loss: 0.000580818799789995 \t\n",
      "Epoch 30801 \t\t Training Loss: 0.000580818799789995 \t\n",
      "Epoch 30802 \t\t Training Loss: 0.000580818799789995 \t\n",
      "Epoch 30803 \t\t Training Loss: 0.000580818799789995 \t\n",
      "Epoch 30804 \t\t Training Loss: 0.000580818799789995 \t\n",
      "Epoch 30805 \t\t Training Loss: 0.000580818741582334 \t\n",
      "Epoch 30806 \t\t Training Loss: 0.000580818741582334 \t\n",
      "Epoch 30807 \t\t Training Loss: 0.000580818741582334 \t\n",
      "Epoch 30808 \t\t Training Loss: 0.000580818741582334 \t\n",
      "Epoch 30809 \t\t Training Loss: 0.000580818741582334 \t\n",
      "Epoch 30810 \t\t Training Loss: 0.0005808186833746731 \t\n",
      "Epoch 30811 \t\t Training Loss: 0.000580818741582334 \t\n",
      "Epoch 30812 \t\t Training Loss: 0.0005808186833746731 \t\n",
      "Epoch 30813 \t\t Training Loss: 0.0005808186833746731 \t\n",
      "Epoch 30814 \t\t Training Loss: 0.0005808186833746731 \t\n",
      "Epoch 30815 \t\t Training Loss: 0.0005808186833746731 \t\n",
      "Epoch 30816 \t\t Training Loss: 0.0005808186833746731 \t\n",
      "Epoch 30817 \t\t Training Loss: 0.000580818741582334 \t\n",
      "Epoch 30818 \t\t Training Loss: 0.0005808186833746731 \t\n",
      "Epoch 30819 \t\t Training Loss: 0.0005808186833746731 \t\n",
      "Epoch 30820 \t\t Training Loss: 0.0005808186833746731 \t\n",
      "Epoch 30821 \t\t Training Loss: 0.0005808186833746731 \t\n",
      "Epoch 30822 \t\t Training Loss: 0.0005808186833746731 \t\n",
      "Epoch 30823 \t\t Training Loss: 0.0005808186833746731 \t\n",
      "Epoch 30824 \t\t Training Loss: 0.0005808186251670122 \t\n",
      "Epoch 30825 \t\t Training Loss: 0.0005808186833746731 \t\n",
      "Epoch 30826 \t\t Training Loss: 0.0005808186833746731 \t\n",
      "Epoch 30827 \t\t Training Loss: 0.0005808186833746731 \t\n",
      "Epoch 30828 \t\t Training Loss: 0.0005808186251670122 \t\n",
      "Epoch 30829 \t\t Training Loss: 0.0005808186251670122 \t\n",
      "Epoch 30830 \t\t Training Loss: 0.0005808186251670122 \t\n",
      "Epoch 30831 \t\t Training Loss: 0.0005808186251670122 \t\n",
      "Epoch 30832 \t\t Training Loss: 0.0005808186251670122 \t\n",
      "Epoch 30833 \t\t Training Loss: 0.0005808186251670122 \t\n",
      "Epoch 30834 \t\t Training Loss: 0.0005808186251670122 \t\n",
      "Epoch 30835 \t\t Training Loss: 0.0005808186251670122 \t\n",
      "Epoch 30836 \t\t Training Loss: 0.0005808185669593513 \t\n",
      "Epoch 30837 \t\t Training Loss: 0.0005808185669593513 \t\n",
      "Epoch 30838 \t\t Training Loss: 0.0005808185669593513 \t\n",
      "Epoch 30839 \t\t Training Loss: 0.0005808185669593513 \t\n",
      "Epoch 30840 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30841 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30842 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30843 \t\t Training Loss: 0.0005808185669593513 \t\n",
      "Epoch 30844 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30845 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30846 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30847 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30848 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30849 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30850 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30851 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30852 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30853 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30854 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30855 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30856 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30857 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30858 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30859 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30860 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30861 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30862 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30863 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30864 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30865 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30866 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30867 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30868 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30869 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30870 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30871 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30872 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30873 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30874 \t\t Training Loss: 0.0005808185087516904 \t\n",
      "Epoch 30875 \t\t Training Loss: 0.0005808183923363686 \t\n",
      "Epoch 30876 \t\t Training Loss: 0.0005808183923363686 \t\n",
      "Epoch 30877 \t\t Training Loss: 0.0005808183923363686 \t\n",
      "Epoch 30878 \t\t Training Loss: 0.0005808183923363686 \t\n",
      "Epoch 30879 \t\t Training Loss: 0.0005808183923363686 \t\n",
      "Epoch 30880 \t\t Training Loss: 0.0005808183341287076 \t\n",
      "Epoch 30881 \t\t Training Loss: 0.0005808183341287076 \t\n",
      "Epoch 30882 \t\t Training Loss: 0.0005808183341287076 \t\n",
      "Epoch 30883 \t\t Training Loss: 0.0005808183341287076 \t\n",
      "Epoch 30884 \t\t Training Loss: 0.0005808183341287076 \t\n",
      "Epoch 30885 \t\t Training Loss: 0.0005808183923363686 \t\n",
      "Epoch 30886 \t\t Training Loss: 0.0005808183923363686 \t\n",
      "Epoch 30887 \t\t Training Loss: 0.0005808183923363686 \t\n",
      "Epoch 30888 \t\t Training Loss: 0.0005808183923363686 \t\n",
      "Epoch 30889 \t\t Training Loss: 0.0005808183923363686 \t\n",
      "Epoch 30890 \t\t Training Loss: 0.0005808182759210467 \t\n",
      "Epoch 30891 \t\t Training Loss: 0.0005808183341287076 \t\n",
      "Epoch 30892 \t\t Training Loss: 0.0005808183923363686 \t\n",
      "Epoch 30893 \t\t Training Loss: 0.0005808183341287076 \t\n",
      "Epoch 30894 \t\t Training Loss: 0.0005808183923363686 \t\n",
      "Epoch 30895 \t\t Training Loss: 0.0005808183923363686 \t\n",
      "Epoch 30896 \t\t Training Loss: 0.0005808182759210467 \t\n",
      "Epoch 30897 \t\t Training Loss: 0.0005808182759210467 \t\n",
      "Epoch 30898 \t\t Training Loss: 0.0005808182759210467 \t\n",
      "Epoch 30899 \t\t Training Loss: 0.0005808182759210467 \t\n",
      "Epoch 30900 \t\t Training Loss: 0.0005808182759210467 \t\n",
      "Epoch 30901 \t\t Training Loss: 0.0005808182759210467 \t\n",
      "Epoch 30902 \t\t Training Loss: 0.0005808182759210467 \t\n",
      "Epoch 30903 \t\t Training Loss: 0.0005808182759210467 \t\n",
      "Epoch 30904 \t\t Training Loss: 0.0005808182759210467 \t\n",
      "Epoch 30905 \t\t Training Loss: 0.0005808182759210467 \t\n",
      "Epoch 30906 \t\t Training Loss: 0.0005808182759210467 \t\n",
      "Epoch 30907 \t\t Training Loss: 0.0005808182759210467 \t\n",
      "Epoch 30908 \t\t Training Loss: 0.0005808182759210467 \t\n",
      "Epoch 30909 \t\t Training Loss: 0.0005808182759210467 \t\n",
      "Epoch 30910 \t\t Training Loss: 0.0005808182759210467 \t\n",
      "Epoch 30911 \t\t Training Loss: 0.0005808182177133858 \t\n",
      "Epoch 30912 \t\t Training Loss: 0.0005808182177133858 \t\n",
      "Epoch 30913 \t\t Training Loss: 0.0005808182177133858 \t\n",
      "Epoch 30914 \t\t Training Loss: 0.0005808182177133858 \t\n",
      "Epoch 30915 \t\t Training Loss: 0.0005808182177133858 \t\n",
      "Epoch 30916 \t\t Training Loss: 0.0005808181595057249 \t\n",
      "Epoch 30917 \t\t Training Loss: 0.0005808181595057249 \t\n",
      "Epoch 30918 \t\t Training Loss: 0.0005808181595057249 \t\n",
      "Epoch 30919 \t\t Training Loss: 0.0005808181595057249 \t\n",
      "Epoch 30920 \t\t Training Loss: 0.0005808181595057249 \t\n",
      "Epoch 30921 \t\t Training Loss: 0.0005808180430904031 \t\n",
      "Epoch 30922 \t\t Training Loss: 0.0005808180430904031 \t\n",
      "Epoch 30923 \t\t Training Loss: 0.0005808180430904031 \t\n",
      "Epoch 30924 \t\t Training Loss: 0.0005808180430904031 \t\n",
      "Epoch 30925 \t\t Training Loss: 0.0005808180430904031 \t\n",
      "Epoch 30926 \t\t Training Loss: 0.0005808180430904031 \t\n",
      "Epoch 30927 \t\t Training Loss: 0.0005808180430904031 \t\n",
      "Epoch 30928 \t\t Training Loss: 0.0005808179848827422 \t\n",
      "Epoch 30929 \t\t Training Loss: 0.0005808179848827422 \t\n",
      "Epoch 30930 \t\t Training Loss: 0.0005808179848827422 \t\n",
      "Epoch 30931 \t\t Training Loss: 0.0005808179266750813 \t\n",
      "Epoch 30932 \t\t Training Loss: 0.0005808179848827422 \t\n",
      "Epoch 30933 \t\t Training Loss: 0.0005808179266750813 \t\n",
      "Epoch 30934 \t\t Training Loss: 0.0005808179266750813 \t\n",
      "Epoch 30935 \t\t Training Loss: 0.0005808179848827422 \t\n",
      "Epoch 30936 \t\t Training Loss: 0.0005808179266750813 \t\n",
      "Epoch 30937 \t\t Training Loss: 0.0005808179266750813 \t\n",
      "Epoch 30938 \t\t Training Loss: 0.0005808179266750813 \t\n",
      "Epoch 30939 \t\t Training Loss: 0.0005808179848827422 \t\n",
      "Epoch 30940 \t\t Training Loss: 0.0005808179266750813 \t\n",
      "Epoch 30941 \t\t Training Loss: 0.0005808179266750813 \t\n",
      "Epoch 30942 \t\t Training Loss: 0.0005808179266750813 \t\n",
      "Epoch 30943 \t\t Training Loss: 0.0005808178102597594 \t\n",
      "Epoch 30944 \t\t Training Loss: 0.0005808178102597594 \t\n",
      "Epoch 30945 \t\t Training Loss: 0.0005808178102597594 \t\n",
      "Epoch 30946 \t\t Training Loss: 0.0005808178102597594 \t\n",
      "Epoch 30947 \t\t Training Loss: 0.0005808178102597594 \t\n",
      "Epoch 30948 \t\t Training Loss: 0.0005808178102597594 \t\n",
      "Epoch 30949 \t\t Training Loss: 0.0005808178102597594 \t\n",
      "Epoch 30950 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30951 \t\t Training Loss: 0.0005808177520520985 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30952 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30953 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30954 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30955 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30956 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30957 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30958 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30959 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30960 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30961 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30962 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30963 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30964 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30965 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30966 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30967 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30968 \t\t Training Loss: 0.0005808177520520985 \t\n",
      "Epoch 30969 \t\t Training Loss: 0.0005808176356367767 \t\n",
      "Epoch 30970 \t\t Training Loss: 0.0005808176356367767 \t\n",
      "Epoch 30971 \t\t Training Loss: 0.0005808176356367767 \t\n",
      "Epoch 30972 \t\t Training Loss: 0.0005808176356367767 \t\n",
      "Epoch 30973 \t\t Training Loss: 0.0005808176356367767 \t\n",
      "Epoch 30974 \t\t Training Loss: 0.0005808176356367767 \t\n",
      "Epoch 30975 \t\t Training Loss: 0.0005808176356367767 \t\n",
      "Epoch 30976 \t\t Training Loss: 0.0005808176356367767 \t\n",
      "Epoch 30977 \t\t Training Loss: 0.0005808176356367767 \t\n",
      "Epoch 30978 \t\t Training Loss: 0.0005808176356367767 \t\n",
      "Epoch 30979 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 30980 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 30981 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 30982 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 30983 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 30984 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 30985 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 30986 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 30987 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 30988 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 30989 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 30990 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 30991 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 30992 \t\t Training Loss: 0.0005808175192214549 \t\n",
      "Epoch 30993 \t\t Training Loss: 0.0005808175192214549 \t\n",
      "Epoch 30994 \t\t Training Loss: 0.0005808175192214549 \t\n",
      "Epoch 30995 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 30996 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 30997 \t\t Training Loss: 0.0005808175192214549 \t\n",
      "Epoch 30998 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 30999 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 31000 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 31001 \t\t Training Loss: 0.0005808175192214549 \t\n",
      "Epoch 31002 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 31003 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 31004 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 31005 \t\t Training Loss: 0.0005808175774291158 \t\n",
      "Epoch 31006 \t\t Training Loss: 0.0005808175192214549 \t\n",
      "Epoch 31007 \t\t Training Loss: 0.0005808175192214549 \t\n",
      "Epoch 31008 \t\t Training Loss: 0.0005808175192214549 \t\n",
      "Epoch 31009 \t\t Training Loss: 0.0005808175192214549 \t\n",
      "Epoch 31010 \t\t Training Loss: 0.0005808175192214549 \t\n",
      "Epoch 31011 \t\t Training Loss: 0.0005808175192214549 \t\n",
      "Epoch 31012 \t\t Training Loss: 0.0005808175192214549 \t\n",
      "Epoch 31013 \t\t Training Loss: 0.0005808175192214549 \t\n",
      "Epoch 31014 \t\t Training Loss: 0.0005808175192214549 \t\n",
      "Epoch 31015 \t\t Training Loss: 0.0005808175192214549 \t\n",
      "Epoch 31016 \t\t Training Loss: 0.0005808174610137939 \t\n",
      "Epoch 31017 \t\t Training Loss: 0.0005808175192214549 \t\n",
      "Epoch 31018 \t\t Training Loss: 0.0005808174610137939 \t\n",
      "Epoch 31019 \t\t Training Loss: 0.0005808174610137939 \t\n",
      "Epoch 31020 \t\t Training Loss: 0.0005808174610137939 \t\n",
      "Epoch 31021 \t\t Training Loss: 0.0005808174610137939 \t\n",
      "Epoch 31022 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31023 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31024 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31025 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31026 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31027 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31028 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31029 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31030 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31031 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31032 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31033 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31034 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31035 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31036 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31037 \t\t Training Loss: 0.0005808174610137939 \t\n",
      "Epoch 31038 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31039 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31040 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31041 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31042 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31043 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31044 \t\t Training Loss: 0.0005808174610137939 \t\n",
      "Epoch 31045 \t\t Training Loss: 0.0005808174610137939 \t\n",
      "Epoch 31046 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31047 \t\t Training Loss: 0.0005808174610137939 \t\n",
      "Epoch 31048 \t\t Training Loss: 0.0005808174610137939 \t\n",
      "Epoch 31049 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31050 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31051 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31052 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31053 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31054 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31055 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31056 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31057 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31058 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31059 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31060 \t\t Training Loss: 0.0005808173445984721 \t\n",
      "Epoch 31061 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31062 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31063 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31064 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31065 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31066 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31067 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31068 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31069 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31070 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31071 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31072 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31073 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31074 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31075 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31076 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31077 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31078 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31079 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31080 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31081 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31082 \t\t Training Loss: 0.0005808172281831503 \t\n",
      "Epoch 31083 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31084 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31085 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31086 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31087 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31088 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31089 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31090 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31091 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31092 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31093 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31094 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31095 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31096 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31097 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31098 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31099 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31100 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31101 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31102 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31103 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31104 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31105 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31106 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31107 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31108 \t\t Training Loss: 0.0005808171699754894 \t\n",
      "Epoch 31109 \t\t Training Loss: 0.0005808171699754894 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31110 \t\t Training Loss: 0.0005808170535601676 \t\n",
      "Epoch 31111 \t\t Training Loss: 0.0005808170535601676 \t\n",
      "Epoch 31112 \t\t Training Loss: 0.0005808170535601676 \t\n",
      "Epoch 31113 \t\t Training Loss: 0.0005808170535601676 \t\n",
      "Epoch 31114 \t\t Training Loss: 0.0005808170535601676 \t\n",
      "Epoch 31115 \t\t Training Loss: 0.0005808170535601676 \t\n",
      "Epoch 31116 \t\t Training Loss: 0.0005808170535601676 \t\n",
      "Epoch 31117 \t\t Training Loss: 0.0005808170535601676 \t\n",
      "Epoch 31118 \t\t Training Loss: 0.0005808170535601676 \t\n",
      "Epoch 31119 \t\t Training Loss: 0.0005808170535601676 \t\n",
      "Epoch 31120 \t\t Training Loss: 0.0005808170535601676 \t\n",
      "Epoch 31121 \t\t Training Loss: 0.0005808170535601676 \t\n",
      "Epoch 31122 \t\t Training Loss: 0.0005808170535601676 \t\n",
      "Epoch 31123 \t\t Training Loss: 0.0005808169371448457 \t\n",
      "Epoch 31124 \t\t Training Loss: 0.0005808169371448457 \t\n",
      "Epoch 31125 \t\t Training Loss: 0.0005808169371448457 \t\n",
      "Epoch 31126 \t\t Training Loss: 0.0005808169371448457 \t\n",
      "Epoch 31127 \t\t Training Loss: 0.0005808169371448457 \t\n",
      "Epoch 31128 \t\t Training Loss: 0.0005808169371448457 \t\n",
      "Epoch 31129 \t\t Training Loss: 0.0005808169371448457 \t\n",
      "Epoch 31130 \t\t Training Loss: 0.0005808169371448457 \t\n",
      "Epoch 31131 \t\t Training Loss: 0.0005808169371448457 \t\n",
      "Epoch 31132 \t\t Training Loss: 0.0005808169371448457 \t\n",
      "Epoch 31133 \t\t Training Loss: 0.0005808169371448457 \t\n",
      "Epoch 31134 \t\t Training Loss: 0.0005808169371448457 \t\n",
      "Epoch 31135 \t\t Training Loss: 0.0005808169371448457 \t\n",
      "Epoch 31136 \t\t Training Loss: 0.0005808169371448457 \t\n",
      "Epoch 31137 \t\t Training Loss: 0.0005808168789371848 \t\n",
      "Epoch 31138 \t\t Training Loss: 0.0005808168789371848 \t\n",
      "Epoch 31139 \t\t Training Loss: 0.0005808168789371848 \t\n",
      "Epoch 31140 \t\t Training Loss: 0.0005808168789371848 \t\n",
      "Epoch 31141 \t\t Training Loss: 0.0005808168789371848 \t\n",
      "Epoch 31142 \t\t Training Loss: 0.000580816762521863 \t\n",
      "Epoch 31143 \t\t Training Loss: 0.000580816762521863 \t\n",
      "Epoch 31144 \t\t Training Loss: 0.000580816762521863 \t\n",
      "Epoch 31145 \t\t Training Loss: 0.000580816762521863 \t\n",
      "Epoch 31146 \t\t Training Loss: 0.000580816762521863 \t\n",
      "Epoch 31147 \t\t Training Loss: 0.000580816762521863 \t\n",
      "Epoch 31148 \t\t Training Loss: 0.0005808166461065412 \t\n",
      "Epoch 31149 \t\t Training Loss: 0.0005808166461065412 \t\n",
      "Epoch 31150 \t\t Training Loss: 0.0005808166461065412 \t\n",
      "Epoch 31151 \t\t Training Loss: 0.0005808166461065412 \t\n",
      "Epoch 31152 \t\t Training Loss: 0.0005808166461065412 \t\n",
      "Epoch 31153 \t\t Training Loss: 0.0005808166461065412 \t\n",
      "Epoch 31154 \t\t Training Loss: 0.0005808166461065412 \t\n",
      "Epoch 31155 \t\t Training Loss: 0.0005808166461065412 \t\n",
      "Epoch 31156 \t\t Training Loss: 0.0005808166461065412 \t\n",
      "Epoch 31157 \t\t Training Loss: 0.0005808166461065412 \t\n",
      "Epoch 31158 \t\t Training Loss: 0.0005808166461065412 \t\n",
      "Epoch 31159 \t\t Training Loss: 0.0005808166461065412 \t\n",
      "Epoch 31160 \t\t Training Loss: 0.0005808166461065412 \t\n",
      "Epoch 31161 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31162 \t\t Training Loss: 0.0005808166461065412 \t\n",
      "Epoch 31163 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31164 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31165 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31166 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31167 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31168 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31169 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31170 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31171 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31172 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31173 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31174 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31175 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31176 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31177 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31178 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31179 \t\t Training Loss: 0.0005808165296912193 \t\n",
      "Epoch 31180 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31181 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31182 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31183 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31184 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31185 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31186 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31187 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31188 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31189 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31190 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31191 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31192 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31193 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31194 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31195 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31196 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31197 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31198 \t\t Training Loss: 0.0005808164714835584 \t\n",
      "Epoch 31199 \t\t Training Loss: 0.0005808163550682366 \t\n",
      "Epoch 31200 \t\t Training Loss: 0.0005808163550682366 \t\n",
      "Epoch 31201 \t\t Training Loss: 0.0005808163550682366 \t\n",
      "Epoch 31202 \t\t Training Loss: 0.0005808163550682366 \t\n",
      "Epoch 31203 \t\t Training Loss: 0.0005808163550682366 \t\n",
      "Epoch 31204 \t\t Training Loss: 0.0005808163550682366 \t\n",
      "Epoch 31205 \t\t Training Loss: 0.0005808163550682366 \t\n",
      "Epoch 31206 \t\t Training Loss: 0.0005808163550682366 \t\n",
      "Epoch 31207 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31208 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31209 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31210 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31211 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31212 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31213 \t\t Training Loss: 0.0005808163550682366 \t\n",
      "Epoch 31214 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31215 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31216 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31217 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31218 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31219 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31220 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31221 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31222 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31223 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31224 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31225 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31226 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31227 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31228 \t\t Training Loss: 0.000580816064029932 \t\n",
      "Epoch 31229 \t\t Training Loss: 0.0005808161804452538 \t\n",
      "Epoch 31230 \t\t Training Loss: 0.000580816064029932 \t\n",
      "Epoch 31231 \t\t Training Loss: 0.000580816064029932 \t\n",
      "Epoch 31232 \t\t Training Loss: 0.000580816064029932 \t\n",
      "Epoch 31233 \t\t Training Loss: 0.000580816064029932 \t\n",
      "Epoch 31234 \t\t Training Loss: 0.000580816064029932 \t\n",
      "Epoch 31235 \t\t Training Loss: 0.000580816064029932 \t\n",
      "Epoch 31236 \t\t Training Loss: 0.000580816064029932 \t\n",
      "Epoch 31237 \t\t Training Loss: 0.000580816064029932 \t\n",
      "Epoch 31238 \t\t Training Loss: 0.000580816064029932 \t\n",
      "Epoch 31239 \t\t Training Loss: 0.000580816064029932 \t\n",
      "Epoch 31240 \t\t Training Loss: 0.000580816064029932 \t\n",
      "Epoch 31241 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31242 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31243 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31244 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31245 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31246 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31247 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31248 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31249 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31250 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31251 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31252 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31253 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31254 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31255 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31256 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31257 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31258 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31259 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31260 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31261 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31262 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31263 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31264 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31265 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31266 \t\t Training Loss: 0.0005808159476146102 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31267 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31268 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31269 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31270 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31271 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31272 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31273 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31274 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31275 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31276 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31277 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31278 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31279 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31280 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31281 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31282 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31283 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31284 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31285 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31286 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31287 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31288 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31289 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31290 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31291 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31292 \t\t Training Loss: 0.0005808159476146102 \t\n",
      "Epoch 31293 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31294 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31295 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31296 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31297 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31298 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31299 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31300 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31301 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31302 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31303 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31304 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31305 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31306 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31307 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31308 \t\t Training Loss: 0.0005808158894069493 \t\n",
      "Epoch 31309 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31310 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31311 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31312 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31313 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31314 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31315 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31316 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31317 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31318 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31319 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31320 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31321 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31322 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31323 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31324 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31325 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31326 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31327 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31328 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31329 \t\t Training Loss: 0.0005808157729916275 \t\n",
      "Epoch 31330 \t\t Training Loss: 0.0005808156565763056 \t\n",
      "Epoch 31331 \t\t Training Loss: 0.0005808156565763056 \t\n",
      "Epoch 31332 \t\t Training Loss: 0.0005808156565763056 \t\n",
      "Epoch 31333 \t\t Training Loss: 0.0005808156565763056 \t\n",
      "Epoch 31334 \t\t Training Loss: 0.0005808155983686447 \t\n",
      "Epoch 31335 \t\t Training Loss: 0.0005808155983686447 \t\n",
      "Epoch 31336 \t\t Training Loss: 0.0005808155983686447 \t\n",
      "Epoch 31337 \t\t Training Loss: 0.0005808155983686447 \t\n",
      "Epoch 31338 \t\t Training Loss: 0.0005808155983686447 \t\n",
      "Epoch 31339 \t\t Training Loss: 0.0005808155983686447 \t\n",
      "Epoch 31340 \t\t Training Loss: 0.0005808155401609838 \t\n",
      "Epoch 31341 \t\t Training Loss: 0.0005808155401609838 \t\n",
      "Epoch 31342 \t\t Training Loss: 0.0005808155401609838 \t\n",
      "Epoch 31343 \t\t Training Loss: 0.0005808155401609838 \t\n",
      "Epoch 31344 \t\t Training Loss: 0.0005808155401609838 \t\n",
      "Epoch 31345 \t\t Training Loss: 0.0005808155401609838 \t\n",
      "Epoch 31346 \t\t Training Loss: 0.0005808155401609838 \t\n",
      "Epoch 31347 \t\t Training Loss: 0.0005808154819533229 \t\n",
      "Epoch 31348 \t\t Training Loss: 0.0005808154819533229 \t\n",
      "Epoch 31349 \t\t Training Loss: 0.0005808154819533229 \t\n",
      "Epoch 31350 \t\t Training Loss: 0.0005808154819533229 \t\n",
      "Epoch 31351 \t\t Training Loss: 0.0005808154819533229 \t\n",
      "Epoch 31352 \t\t Training Loss: 0.0005808154819533229 \t\n",
      "Epoch 31353 \t\t Training Loss: 0.0005808154819533229 \t\n",
      "Epoch 31354 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31355 \t\t Training Loss: 0.0005808154819533229 \t\n",
      "Epoch 31356 \t\t Training Loss: 0.0005808154819533229 \t\n",
      "Epoch 31357 \t\t Training Loss: 0.0005808154819533229 \t\n",
      "Epoch 31358 \t\t Training Loss: 0.0005808154819533229 \t\n",
      "Epoch 31359 \t\t Training Loss: 0.0005808154819533229 \t\n",
      "Epoch 31360 \t\t Training Loss: 0.0005808154819533229 \t\n",
      "Epoch 31361 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31362 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31363 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31364 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31365 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31366 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31367 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31368 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31369 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31370 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31371 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31372 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31373 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31374 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31375 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31376 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31377 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31378 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31379 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31380 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31381 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31382 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31383 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31384 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31385 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31386 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31387 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31388 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31389 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31390 \t\t Training Loss: 0.0005808153073303401 \t\n",
      "Epoch 31391 \t\t Training Loss: 0.0005808153655380011 \t\n",
      "Epoch 31392 \t\t Training Loss: 0.0005808153073303401 \t\n",
      "Epoch 31393 \t\t Training Loss: 0.0005808153073303401 \t\n",
      "Epoch 31394 \t\t Training Loss: 0.0005808153073303401 \t\n",
      "Epoch 31395 \t\t Training Loss: 0.0005808153073303401 \t\n",
      "Epoch 31396 \t\t Training Loss: 0.0005808153073303401 \t\n",
      "Epoch 31397 \t\t Training Loss: 0.0005808153073303401 \t\n",
      "Epoch 31398 \t\t Training Loss: 0.0005808153073303401 \t\n",
      "Epoch 31399 \t\t Training Loss: 0.0005808153073303401 \t\n",
      "Epoch 31400 \t\t Training Loss: 0.0005808152491226792 \t\n",
      "Epoch 31401 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31402 \t\t Training Loss: 0.0005808152491226792 \t\n",
      "Epoch 31403 \t\t Training Loss: 0.0005808152491226792 \t\n",
      "Epoch 31404 \t\t Training Loss: 0.0005808152491226792 \t\n",
      "Epoch 31405 \t\t Training Loss: 0.0005808152491226792 \t\n",
      "Epoch 31406 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31407 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31408 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31409 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31410 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31411 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31412 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31413 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31414 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31415 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31416 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31417 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31418 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31419 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31420 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31421 \t\t Training Loss: 0.0005808151909150183 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31422 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31423 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31424 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31425 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31426 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31427 \t\t Training Loss: 0.0005808151909150183 \t\n",
      "Epoch 31428 \t\t Training Loss: 0.0005808150744996965 \t\n",
      "Epoch 31429 \t\t Training Loss: 0.0005808150744996965 \t\n",
      "Epoch 31430 \t\t Training Loss: 0.0005808150744996965 \t\n",
      "Epoch 31431 \t\t Training Loss: 0.0005808150744996965 \t\n",
      "Epoch 31432 \t\t Training Loss: 0.0005808150744996965 \t\n",
      "Epoch 31433 \t\t Training Loss: 0.0005808150744996965 \t\n",
      "Epoch 31434 \t\t Training Loss: 0.0005808150744996965 \t\n",
      "Epoch 31435 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31436 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31437 \t\t Training Loss: 0.0005808150744996965 \t\n",
      "Epoch 31438 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31439 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31440 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31441 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31442 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31443 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31444 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31445 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31446 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31447 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31448 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31449 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31450 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31451 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31452 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31453 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31454 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31455 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31456 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31457 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31458 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31459 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31460 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31461 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31462 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31463 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31464 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31465 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31466 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31467 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31468 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31469 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31470 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31471 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31472 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31473 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31474 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31475 \t\t Training Loss: 0.0005808149580843747 \t\n",
      "Epoch 31476 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31477 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31478 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31479 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31480 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31481 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31482 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31483 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31484 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31485 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31486 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31487 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31488 \t\t Training Loss: 0.0005808148998767138 \t\n",
      "Epoch 31489 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31490 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31491 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31492 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31493 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31494 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31495 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31496 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31497 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31498 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31499 \t\t Training Loss: 0.0005808147834613919 \t\n",
      "Epoch 31500 \t\t Training Loss: 0.0005808147834613919 \t\n",
      "Epoch 31501 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31502 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31503 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31504 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31505 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31506 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31507 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31508 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31509 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31510 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31511 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31512 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31513 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31514 \t\t Training Loss: 0.0005808148416690528 \t\n",
      "Epoch 31515 \t\t Training Loss: 0.0005808147834613919 \t\n",
      "Epoch 31516 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31517 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31518 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31519 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31520 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31521 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31522 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31523 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31524 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31525 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31526 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31527 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31528 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31529 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31530 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31531 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31532 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31533 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31534 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31535 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31536 \t\t Training Loss: 0.0005808146670460701 \t\n",
      "Epoch 31537 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31538 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31539 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31540 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31541 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31542 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31543 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31544 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31545 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31546 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31547 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31548 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31549 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31550 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31551 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31552 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31553 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31554 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31555 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31556 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31557 \t\t Training Loss: 0.0005808146088384092 \t\n",
      "Epoch 31558 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31559 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31560 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31561 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31562 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31563 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31564 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31565 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31566 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31567 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31568 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31569 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31570 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31571 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31572 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31573 \t\t Training Loss: 0.0005808144342154264 \t\n",
      "Epoch 31574 \t\t Training Loss: 0.0005808143760077655 \t\n",
      "Epoch 31575 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31576 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31577 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31578 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31579 \t\t Training Loss: 0.0005808144342154264 \t\n",
      "Epoch 31580 \t\t Training Loss: 0.0005808144924230874 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31581 \t\t Training Loss: 0.0005808143760077655 \t\n",
      "Epoch 31582 \t\t Training Loss: 0.0005808144924230874 \t\n",
      "Epoch 31583 \t\t Training Loss: 0.0005808143760077655 \t\n",
      "Epoch 31584 \t\t Training Loss: 0.0005808143760077655 \t\n",
      "Epoch 31585 \t\t Training Loss: 0.0005808144342154264 \t\n",
      "Epoch 31586 \t\t Training Loss: 0.0005808143760077655 \t\n",
      "Epoch 31587 \t\t Training Loss: 0.0005808143760077655 \t\n",
      "Epoch 31588 \t\t Training Loss: 0.0005808143760077655 \t\n",
      "Epoch 31589 \t\t Training Loss: 0.0005808143760077655 \t\n",
      "Epoch 31590 \t\t Training Loss: 0.0005808143760077655 \t\n",
      "Epoch 31591 \t\t Training Loss: 0.0005808143760077655 \t\n",
      "Epoch 31592 \t\t Training Loss: 0.0005808143760077655 \t\n",
      "Epoch 31593 \t\t Training Loss: 0.0005808143760077655 \t\n",
      "Epoch 31594 \t\t Training Loss: 0.0005808143760077655 \t\n",
      "Epoch 31595 \t\t Training Loss: 0.0005808143760077655 \t\n",
      "Epoch 31596 \t\t Training Loss: 0.0005808143760077655 \t\n",
      "Epoch 31597 \t\t Training Loss: 0.0005808143178001046 \t\n",
      "Epoch 31598 \t\t Training Loss: 0.0005808143178001046 \t\n",
      "Epoch 31599 \t\t Training Loss: 0.0005808143178001046 \t\n",
      "Epoch 31600 \t\t Training Loss: 0.0005808143178001046 \t\n",
      "Epoch 31601 \t\t Training Loss: 0.0005808143178001046 \t\n",
      "Epoch 31602 \t\t Training Loss: 0.0005808143178001046 \t\n",
      "Epoch 31603 \t\t Training Loss: 0.0005808143178001046 \t\n",
      "Epoch 31604 \t\t Training Loss: 0.0005808143178001046 \t\n",
      "Epoch 31605 \t\t Training Loss: 0.0005808143178001046 \t\n",
      "Epoch 31606 \t\t Training Loss: 0.0005808143178001046 \t\n",
      "Epoch 31607 \t\t Training Loss: 0.0005808143178001046 \t\n",
      "Epoch 31608 \t\t Training Loss: 0.0005808143178001046 \t\n",
      "Epoch 31609 \t\t Training Loss: 0.0005808143178001046 \t\n",
      "Epoch 31610 \t\t Training Loss: 0.0005808143178001046 \t\n",
      "Epoch 31611 \t\t Training Loss: 0.0005808142595924437 \t\n",
      "Epoch 31612 \t\t Training Loss: 0.0005808142595924437 \t\n",
      "Epoch 31613 \t\t Training Loss: 0.0005808142595924437 \t\n",
      "Epoch 31614 \t\t Training Loss: 0.0005808142595924437 \t\n",
      "Epoch 31615 \t\t Training Loss: 0.0005808142595924437 \t\n",
      "Epoch 31616 \t\t Training Loss: 0.0005808142595924437 \t\n",
      "Epoch 31617 \t\t Training Loss: 0.0005808142013847828 \t\n",
      "Epoch 31618 \t\t Training Loss: 0.0005808142013847828 \t\n",
      "Epoch 31619 \t\t Training Loss: 0.0005808142013847828 \t\n",
      "Epoch 31620 \t\t Training Loss: 0.0005808142013847828 \t\n",
      "Epoch 31621 \t\t Training Loss: 0.0005808142013847828 \t\n",
      "Epoch 31622 \t\t Training Loss: 0.0005808142013847828 \t\n",
      "Epoch 31623 \t\t Training Loss: 0.0005808142013847828 \t\n",
      "Epoch 31624 \t\t Training Loss: 0.0005808142013847828 \t\n",
      "Epoch 31625 \t\t Training Loss: 0.0005808142013847828 \t\n",
      "Epoch 31626 \t\t Training Loss: 0.0005808142013847828 \t\n",
      "Epoch 31627 \t\t Training Loss: 0.0005808142013847828 \t\n",
      "Epoch 31628 \t\t Training Loss: 0.0005808141431771219 \t\n",
      "Epoch 31629 \t\t Training Loss: 0.0005808141431771219 \t\n",
      "Epoch 31630 \t\t Training Loss: 0.000580814084969461 \t\n",
      "Epoch 31631 \t\t Training Loss: 0.0005808140267618 \t\n",
      "Epoch 31632 \t\t Training Loss: 0.0005808140267618 \t\n",
      "Epoch 31633 \t\t Training Loss: 0.0005808140267618 \t\n",
      "Epoch 31634 \t\t Training Loss: 0.0005808140267618 \t\n",
      "Epoch 31635 \t\t Training Loss: 0.0005808140267618 \t\n",
      "Epoch 31636 \t\t Training Loss: 0.0005808140267618 \t\n",
      "Epoch 31637 \t\t Training Loss: 0.0005808140267618 \t\n",
      "Epoch 31638 \t\t Training Loss: 0.0005808140267618 \t\n",
      "Epoch 31639 \t\t Training Loss: 0.000580814084969461 \t\n",
      "Epoch 31640 \t\t Training Loss: 0.000580814084969461 \t\n",
      "Epoch 31641 \t\t Training Loss: 0.0005808140267618 \t\n",
      "Epoch 31642 \t\t Training Loss: 0.0005808140267618 \t\n",
      "Epoch 31643 \t\t Training Loss: 0.0005808139685541391 \t\n",
      "Epoch 31644 \t\t Training Loss: 0.0005808139685541391 \t\n",
      "Epoch 31645 \t\t Training Loss: 0.0005808139685541391 \t\n",
      "Epoch 31646 \t\t Training Loss: 0.0005808139685541391 \t\n",
      "Epoch 31647 \t\t Training Loss: 0.0005808139685541391 \t\n",
      "Epoch 31648 \t\t Training Loss: 0.0005808140267618 \t\n",
      "Epoch 31649 \t\t Training Loss: 0.0005808139685541391 \t\n",
      "Epoch 31650 \t\t Training Loss: 0.0005808139103464782 \t\n",
      "Epoch 31651 \t\t Training Loss: 0.0005808139103464782 \t\n",
      "Epoch 31652 \t\t Training Loss: 0.0005808139103464782 \t\n",
      "Epoch 31653 \t\t Training Loss: 0.0005808139103464782 \t\n",
      "Epoch 31654 \t\t Training Loss: 0.0005808138521388173 \t\n",
      "Epoch 31655 \t\t Training Loss: 0.0005808138521388173 \t\n",
      "Epoch 31656 \t\t Training Loss: 0.0005808138521388173 \t\n",
      "Epoch 31657 \t\t Training Loss: 0.0005808139103464782 \t\n",
      "Epoch 31658 \t\t Training Loss: 0.0005808139103464782 \t\n",
      "Epoch 31659 \t\t Training Loss: 0.0005808139103464782 \t\n",
      "Epoch 31660 \t\t Training Loss: 0.0005808138521388173 \t\n",
      "Epoch 31661 \t\t Training Loss: 0.0005808138521388173 \t\n",
      "Epoch 31662 \t\t Training Loss: 0.0005808138521388173 \t\n",
      "Epoch 31663 \t\t Training Loss: 0.0005808138521388173 \t\n",
      "Epoch 31664 \t\t Training Loss: 0.0005808138521388173 \t\n",
      "Epoch 31665 \t\t Training Loss: 0.0005808138521388173 \t\n",
      "Epoch 31666 \t\t Training Loss: 0.0005808138521388173 \t\n",
      "Epoch 31667 \t\t Training Loss: 0.0005808139103464782 \t\n",
      "Epoch 31668 \t\t Training Loss: 0.0005808138521388173 \t\n",
      "Epoch 31669 \t\t Training Loss: 0.0005808138521388173 \t\n",
      "Epoch 31670 \t\t Training Loss: 0.0005808138521388173 \t\n",
      "Epoch 31671 \t\t Training Loss: 0.0005808137939311564 \t\n",
      "Epoch 31672 \t\t Training Loss: 0.0005808138521388173 \t\n",
      "Epoch 31673 \t\t Training Loss: 0.0005808137939311564 \t\n",
      "Epoch 31674 \t\t Training Loss: 0.0005808137939311564 \t\n",
      "Epoch 31675 \t\t Training Loss: 0.0005808137939311564 \t\n",
      "Epoch 31676 \t\t Training Loss: 0.0005808137939311564 \t\n",
      "Epoch 31677 \t\t Training Loss: 0.0005808137939311564 \t\n",
      "Epoch 31678 \t\t Training Loss: 0.0005808137939311564 \t\n",
      "Epoch 31679 \t\t Training Loss: 0.0005808136775158346 \t\n",
      "Epoch 31680 \t\t Training Loss: 0.0005808136775158346 \t\n",
      "Epoch 31681 \t\t Training Loss: 0.0005808136775158346 \t\n",
      "Epoch 31682 \t\t Training Loss: 0.0005808136775158346 \t\n",
      "Epoch 31683 \t\t Training Loss: 0.0005808136775158346 \t\n",
      "Epoch 31684 \t\t Training Loss: 0.0005808136775158346 \t\n",
      "Epoch 31685 \t\t Training Loss: 0.0005808136775158346 \t\n",
      "Epoch 31686 \t\t Training Loss: 0.0005808136775158346 \t\n",
      "Epoch 31687 \t\t Training Loss: 0.0005808136775158346 \t\n",
      "Epoch 31688 \t\t Training Loss: 0.0005808136775158346 \t\n",
      "Epoch 31689 \t\t Training Loss: 0.0005808136775158346 \t\n",
      "Epoch 31690 \t\t Training Loss: 0.0005808136775158346 \t\n",
      "Epoch 31691 \t\t Training Loss: 0.0005808136193081737 \t\n",
      "Epoch 31692 \t\t Training Loss: 0.0005808136193081737 \t\n",
      "Epoch 31693 \t\t Training Loss: 0.0005808136193081737 \t\n",
      "Epoch 31694 \t\t Training Loss: 0.0005808136193081737 \t\n",
      "Epoch 31695 \t\t Training Loss: 0.0005808136193081737 \t\n",
      "Epoch 31696 \t\t Training Loss: 0.0005808136193081737 \t\n",
      "Epoch 31697 \t\t Training Loss: 0.0005808136193081737 \t\n",
      "Epoch 31698 \t\t Training Loss: 0.0005808136193081737 \t\n",
      "Epoch 31699 \t\t Training Loss: 0.0005808135611005127 \t\n",
      "Epoch 31700 \t\t Training Loss: 0.0005808135611005127 \t\n",
      "Epoch 31701 \t\t Training Loss: 0.0005808135611005127 \t\n",
      "Epoch 31702 \t\t Training Loss: 0.0005808135611005127 \t\n",
      "Epoch 31703 \t\t Training Loss: 0.0005808135611005127 \t\n",
      "Epoch 31704 \t\t Training Loss: 0.0005808135611005127 \t\n",
      "Epoch 31705 \t\t Training Loss: 0.0005808135611005127 \t\n",
      "Epoch 31706 \t\t Training Loss: 0.0005808135611005127 \t\n",
      "Epoch 31707 \t\t Training Loss: 0.0005808135611005127 \t\n",
      "Epoch 31708 \t\t Training Loss: 0.0005808135611005127 \t\n",
      "Epoch 31709 \t\t Training Loss: 0.0005808135028928518 \t\n",
      "Epoch 31710 \t\t Training Loss: 0.0005808135611005127 \t\n",
      "Epoch 31711 \t\t Training Loss: 0.0005808135028928518 \t\n",
      "Epoch 31712 \t\t Training Loss: 0.0005808135028928518 \t\n",
      "Epoch 31713 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31714 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31715 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31716 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31717 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31718 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31719 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31720 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31721 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31722 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31723 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31724 \t\t Training Loss: 0.0005808135611005127 \t\n",
      "Epoch 31725 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31726 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31727 \t\t Training Loss: 0.0005808135028928518 \t\n",
      "Epoch 31728 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31729 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31730 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31731 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31732 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31733 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31734 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31735 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31736 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31737 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31738 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31739 \t\t Training Loss: 0.0005808134446851909 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31740 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31741 \t\t Training Loss: 0.0005808135028928518 \t\n",
      "Epoch 31742 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31743 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31744 \t\t Training Loss: 0.0005808135028928518 \t\n",
      "Epoch 31745 \t\t Training Loss: 0.0005808135028928518 \t\n",
      "Epoch 31746 \t\t Training Loss: 0.0005808135028928518 \t\n",
      "Epoch 31747 \t\t Training Loss: 0.0005808135028928518 \t\n",
      "Epoch 31748 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31749 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31750 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31751 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31752 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31753 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31754 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31755 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31756 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31757 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31758 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31759 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31760 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31761 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31762 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31763 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31764 \t\t Training Loss: 0.0005808132700622082 \t\n",
      "Epoch 31765 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31766 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31767 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31768 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31769 \t\t Training Loss: 0.0005808134446851909 \t\n",
      "Epoch 31770 \t\t Training Loss: 0.0005808132700622082 \t\n",
      "Epoch 31771 \t\t Training Loss: 0.00058081338647753 \t\n",
      "Epoch 31772 \t\t Training Loss: 0.0005808132700622082 \t\n",
      "Epoch 31773 \t\t Training Loss: 0.0005808132700622082 \t\n",
      "Epoch 31774 \t\t Training Loss: 0.0005808132700622082 \t\n",
      "Epoch 31775 \t\t Training Loss: 0.0005808132700622082 \t\n",
      "Epoch 31776 \t\t Training Loss: 0.0005808132700622082 \t\n",
      "Epoch 31777 \t\t Training Loss: 0.0005808132700622082 \t\n",
      "Epoch 31778 \t\t Training Loss: 0.0005808132700622082 \t\n",
      "Epoch 31779 \t\t Training Loss: 0.0005808132700622082 \t\n",
      "Epoch 31780 \t\t Training Loss: 0.0005808132700622082 \t\n",
      "Epoch 31781 \t\t Training Loss: 0.0005808132700622082 \t\n",
      "Epoch 31782 \t\t Training Loss: 0.0005808132700622082 \t\n",
      "Epoch 31783 \t\t Training Loss: 0.0005808132118545473 \t\n",
      "Epoch 31784 \t\t Training Loss: 0.0005808132700622082 \t\n",
      "Epoch 31785 \t\t Training Loss: 0.0005808132118545473 \t\n",
      "Epoch 31786 \t\t Training Loss: 0.0005808132118545473 \t\n",
      "Epoch 31787 \t\t Training Loss: 0.0005808132118545473 \t\n",
      "Epoch 31788 \t\t Training Loss: 0.0005808132118545473 \t\n",
      "Epoch 31789 \t\t Training Loss: 0.0005808132118545473 \t\n",
      "Epoch 31790 \t\t Training Loss: 0.0005808132118545473 \t\n",
      "Epoch 31791 \t\t Training Loss: 0.0005808131536468863 \t\n",
      "Epoch 31792 \t\t Training Loss: 0.0005808131536468863 \t\n",
      "Epoch 31793 \t\t Training Loss: 0.0005808131536468863 \t\n",
      "Epoch 31794 \t\t Training Loss: 0.0005808131536468863 \t\n",
      "Epoch 31795 \t\t Training Loss: 0.0005808131536468863 \t\n",
      "Epoch 31796 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31797 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31798 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31799 \t\t Training Loss: 0.0005808130372315645 \t\n",
      "Epoch 31800 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31801 \t\t Training Loss: 0.0005808130372315645 \t\n",
      "Epoch 31802 \t\t Training Loss: 0.0005808130372315645 \t\n",
      "Epoch 31803 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31804 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31805 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31806 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31807 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31808 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31809 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31810 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31811 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31812 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31813 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31814 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31815 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31816 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31817 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31818 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31819 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31820 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31821 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31822 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31823 \t\t Training Loss: 0.0005808130954392254 \t\n",
      "Epoch 31824 \t\t Training Loss: 0.0005808130372315645 \t\n",
      "Epoch 31825 \t\t Training Loss: 0.0005808130372315645 \t\n",
      "Epoch 31826 \t\t Training Loss: 0.0005808130372315645 \t\n",
      "Epoch 31827 \t\t Training Loss: 0.0005808130372315645 \t\n",
      "Epoch 31828 \t\t Training Loss: 0.0005808130372315645 \t\n",
      "Epoch 31829 \t\t Training Loss: 0.0005808130372315645 \t\n",
      "Epoch 31830 \t\t Training Loss: 0.0005808130372315645 \t\n",
      "Epoch 31831 \t\t Training Loss: 0.0005808130372315645 \t\n",
      "Epoch 31832 \t\t Training Loss: 0.0005808130372315645 \t\n",
      "Epoch 31833 \t\t Training Loss: 0.0005808130372315645 \t\n",
      "Epoch 31834 \t\t Training Loss: 0.0005808130372315645 \t\n",
      "Epoch 31835 \t\t Training Loss: 0.0005808130372315645 \t\n",
      "Epoch 31836 \t\t Training Loss: 0.0005808129208162427 \t\n",
      "Epoch 31837 \t\t Training Loss: 0.0005808129208162427 \t\n",
      "Epoch 31838 \t\t Training Loss: 0.0005808129208162427 \t\n",
      "Epoch 31839 \t\t Training Loss: 0.0005808129208162427 \t\n",
      "Epoch 31840 \t\t Training Loss: 0.0005808129208162427 \t\n",
      "Epoch 31841 \t\t Training Loss: 0.0005808129208162427 \t\n",
      "Epoch 31842 \t\t Training Loss: 0.0005808129208162427 \t\n",
      "Epoch 31843 \t\t Training Loss: 0.0005808129208162427 \t\n",
      "Epoch 31844 \t\t Training Loss: 0.0005808129208162427 \t\n",
      "Epoch 31845 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31846 \t\t Training Loss: 0.0005808129208162427 \t\n",
      "Epoch 31847 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31848 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31849 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31850 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31851 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31852 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31853 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31854 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31855 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31856 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31857 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31858 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31859 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31860 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31861 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31862 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31863 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31864 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31865 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31866 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31867 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31868 \t\t Training Loss: 0.0005808128626085818 \t\n",
      "Epoch 31869 \t\t Training Loss: 0.00058081274619326 \t\n",
      "Epoch 31870 \t\t Training Loss: 0.00058081274619326 \t\n",
      "Epoch 31871 \t\t Training Loss: 0.00058081274619326 \t\n",
      "Epoch 31872 \t\t Training Loss: 0.00058081274619326 \t\n",
      "Epoch 31873 \t\t Training Loss: 0.00058081274619326 \t\n",
      "Epoch 31874 \t\t Training Loss: 0.00058081274619326 \t\n",
      "Epoch 31875 \t\t Training Loss: 0.00058081274619326 \t\n",
      "Epoch 31876 \t\t Training Loss: 0.00058081274619326 \t\n",
      "Epoch 31877 \t\t Training Loss: 0.00058081274619326 \t\n",
      "Epoch 31878 \t\t Training Loss: 0.00058081274619326 \t\n",
      "Epoch 31879 \t\t Training Loss: 0.00058081274619326 \t\n",
      "Epoch 31880 \t\t Training Loss: 0.00058081274619326 \t\n",
      "Epoch 31881 \t\t Training Loss: 0.00058081274619326 \t\n",
      "Epoch 31882 \t\t Training Loss: 0.00058081274619326 \t\n",
      "Epoch 31883 \t\t Training Loss: 0.00058081274619326 \t\n",
      "Epoch 31884 \t\t Training Loss: 0.00058081274619326 \t\n",
      "Epoch 31885 \t\t Training Loss: 0.000580812687985599 \t\n",
      "Epoch 31886 \t\t Training Loss: 0.000580812687985599 \t\n",
      "Epoch 31887 \t\t Training Loss: 0.0005808126297779381 \t\n",
      "Epoch 31888 \t\t Training Loss: 0.0005808126297779381 \t\n",
      "Epoch 31889 \t\t Training Loss: 0.0005808126297779381 \t\n",
      "Epoch 31890 \t\t Training Loss: 0.0005808126297779381 \t\n",
      "Epoch 31891 \t\t Training Loss: 0.0005808126297779381 \t\n",
      "Epoch 31892 \t\t Training Loss: 0.0005808126297779381 \t\n",
      "Epoch 31893 \t\t Training Loss: 0.0005808126297779381 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31894 \t\t Training Loss: 0.0005808126297779381 \t\n",
      "Epoch 31895 \t\t Training Loss: 0.0005808126297779381 \t\n",
      "Epoch 31896 \t\t Training Loss: 0.0005808126297779381 \t\n",
      "Epoch 31897 \t\t Training Loss: 0.0005808126297779381 \t\n",
      "Epoch 31898 \t\t Training Loss: 0.0005808126297779381 \t\n",
      "Epoch 31899 \t\t Training Loss: 0.0005808126297779381 \t\n",
      "Epoch 31900 \t\t Training Loss: 0.0005808125715702772 \t\n",
      "Epoch 31901 \t\t Training Loss: 0.0005808125715702772 \t\n",
      "Epoch 31902 \t\t Training Loss: 0.0005808125715702772 \t\n",
      "Epoch 31903 \t\t Training Loss: 0.0005808125715702772 \t\n",
      "Epoch 31904 \t\t Training Loss: 0.0005808125715702772 \t\n",
      "Epoch 31905 \t\t Training Loss: 0.0005808125715702772 \t\n",
      "Epoch 31906 \t\t Training Loss: 0.0005808125715702772 \t\n",
      "Epoch 31907 \t\t Training Loss: 0.0005808125715702772 \t\n",
      "Epoch 31908 \t\t Training Loss: 0.0005808125715702772 \t\n",
      "Epoch 31909 \t\t Training Loss: 0.0005808125715702772 \t\n",
      "Epoch 31910 \t\t Training Loss: 0.0005808125133626163 \t\n",
      "Epoch 31911 \t\t Training Loss: 0.0005808124551549554 \t\n",
      "Epoch 31912 \t\t Training Loss: 0.0005808123969472945 \t\n",
      "Epoch 31913 \t\t Training Loss: 0.0005808123969472945 \t\n",
      "Epoch 31914 \t\t Training Loss: 0.0005808123969472945 \t\n",
      "Epoch 31915 \t\t Training Loss: 0.0005808123969472945 \t\n",
      "Epoch 31916 \t\t Training Loss: 0.0005808123387396336 \t\n",
      "Epoch 31917 \t\t Training Loss: 0.0005808122805319726 \t\n",
      "Epoch 31918 \t\t Training Loss: 0.0005808123387396336 \t\n",
      "Epoch 31919 \t\t Training Loss: 0.0005808123387396336 \t\n",
      "Epoch 31920 \t\t Training Loss: 0.0005808123387396336 \t\n",
      "Epoch 31921 \t\t Training Loss: 0.0005808123387396336 \t\n",
      "Epoch 31922 \t\t Training Loss: 0.0005808122805319726 \t\n",
      "Epoch 31923 \t\t Training Loss: 0.0005808123387396336 \t\n",
      "Epoch 31924 \t\t Training Loss: 0.0005808123969472945 \t\n",
      "Epoch 31925 \t\t Training Loss: 0.0005808123387396336 \t\n",
      "Epoch 31926 \t\t Training Loss: 0.0005808122805319726 \t\n",
      "Epoch 31927 \t\t Training Loss: 0.0005808123387396336 \t\n",
      "Epoch 31928 \t\t Training Loss: 0.0005808122805319726 \t\n",
      "Epoch 31929 \t\t Training Loss: 0.0005808122805319726 \t\n",
      "Epoch 31930 \t\t Training Loss: 0.0005808122805319726 \t\n",
      "Epoch 31931 \t\t Training Loss: 0.0005808122805319726 \t\n",
      "Epoch 31932 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31933 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31934 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31935 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31936 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31937 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31938 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31939 \t\t Training Loss: 0.0005808122805319726 \t\n",
      "Epoch 31940 \t\t Training Loss: 0.0005808122805319726 \t\n",
      "Epoch 31941 \t\t Training Loss: 0.0005808122805319726 \t\n",
      "Epoch 31942 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31943 \t\t Training Loss: 0.0005808122805319726 \t\n",
      "Epoch 31944 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31945 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31946 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31947 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31948 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31949 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31950 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31951 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31952 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31953 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31954 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31955 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31956 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31957 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31958 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31959 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31960 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31961 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31962 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31963 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31964 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31965 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31966 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31967 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31968 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31969 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31970 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31971 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31972 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31973 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31974 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31975 \t\t Training Loss: 0.0005808121059089899 \t\n",
      "Epoch 31976 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31977 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31978 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31979 \t\t Training Loss: 0.0005808121059089899 \t\n",
      "Epoch 31980 \t\t Training Loss: 0.0005808121059089899 \t\n",
      "Epoch 31981 \t\t Training Loss: 0.0005808121059089899 \t\n",
      "Epoch 31982 \t\t Training Loss: 0.0005808121059089899 \t\n",
      "Epoch 31983 \t\t Training Loss: 0.0005808122223243117 \t\n",
      "Epoch 31984 \t\t Training Loss: 0.0005808121059089899 \t\n",
      "Epoch 31985 \t\t Training Loss: 0.0005808121059089899 \t\n",
      "Epoch 31986 \t\t Training Loss: 0.0005808121059089899 \t\n",
      "Epoch 31987 \t\t Training Loss: 0.0005808121059089899 \t\n",
      "Epoch 31988 \t\t Training Loss: 0.0005808121059089899 \t\n",
      "Epoch 31989 \t\t Training Loss: 0.0005808121059089899 \t\n",
      "Epoch 31990 \t\t Training Loss: 0.000580812047701329 \t\n",
      "Epoch 31991 \t\t Training Loss: 0.000580812047701329 \t\n",
      "Epoch 31992 \t\t Training Loss: 0.000580812047701329 \t\n",
      "Epoch 31993 \t\t Training Loss: 0.000580812047701329 \t\n",
      "Epoch 31994 \t\t Training Loss: 0.000580812047701329 \t\n",
      "Epoch 31995 \t\t Training Loss: 0.000580812047701329 \t\n",
      "Epoch 31996 \t\t Training Loss: 0.000580812047701329 \t\n",
      "Epoch 31997 \t\t Training Loss: 0.000580812047701329 \t\n",
      "Epoch 31998 \t\t Training Loss: 0.000580812047701329 \t\n",
      "Epoch 31999 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32000 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32001 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32002 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32003 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32004 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32005 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32006 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32007 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32008 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32009 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32010 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32011 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32012 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32013 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32014 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32015 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32016 \t\t Training Loss: 0.0005808119894936681 \t\n",
      "Epoch 32017 \t\t Training Loss: 0.0005808119312860072 \t\n",
      "Epoch 32018 \t\t Training Loss: 0.0005808119312860072 \t\n",
      "Epoch 32019 \t\t Training Loss: 0.0005808119312860072 \t\n",
      "Epoch 32020 \t\t Training Loss: 0.0005808119312860072 \t\n",
      "Epoch 32021 \t\t Training Loss: 0.0005808118730783463 \t\n",
      "Epoch 32022 \t\t Training Loss: 0.0005808118730783463 \t\n",
      "Epoch 32023 \t\t Training Loss: 0.0005808118730783463 \t\n",
      "Epoch 32024 \t\t Training Loss: 0.0005808118148706853 \t\n",
      "Epoch 32025 \t\t Training Loss: 0.0005808118148706853 \t\n",
      "Epoch 32026 \t\t Training Loss: 0.0005808118148706853 \t\n",
      "Epoch 32027 \t\t Training Loss: 0.0005808118148706853 \t\n",
      "Epoch 32028 \t\t Training Loss: 0.0005808118148706853 \t\n",
      "Epoch 32029 \t\t Training Loss: 0.0005808118148706853 \t\n",
      "Epoch 32030 \t\t Training Loss: 0.0005808118148706853 \t\n",
      "Epoch 32031 \t\t Training Loss: 0.0005808118148706853 \t\n",
      "Epoch 32032 \t\t Training Loss: 0.0005808118148706853 \t\n",
      "Epoch 32033 \t\t Training Loss: 0.0005808118148706853 \t\n",
      "Epoch 32034 \t\t Training Loss: 0.0005808118148706853 \t\n",
      "Epoch 32035 \t\t Training Loss: 0.0005808116984553635 \t\n",
      "Epoch 32036 \t\t Training Loss: 0.0005808116984553635 \t\n",
      "Epoch 32037 \t\t Training Loss: 0.0005808116984553635 \t\n",
      "Epoch 32038 \t\t Training Loss: 0.0005808116984553635 \t\n",
      "Epoch 32039 \t\t Training Loss: 0.0005808116984553635 \t\n",
      "Epoch 32040 \t\t Training Loss: 0.0005808116984553635 \t\n",
      "Epoch 32041 \t\t Training Loss: 0.0005808116984553635 \t\n",
      "Epoch 32042 \t\t Training Loss: 0.0005808116984553635 \t\n",
      "Epoch 32043 \t\t Training Loss: 0.0005808116402477026 \t\n",
      "Epoch 32044 \t\t Training Loss: 0.0005808116984553635 \t\n",
      "Epoch 32045 \t\t Training Loss: 0.0005808115820400417 \t\n",
      "Epoch 32046 \t\t Training Loss: 0.0005808115820400417 \t\n",
      "Epoch 32047 \t\t Training Loss: 0.0005808115820400417 \t\n",
      "Epoch 32048 \t\t Training Loss: 0.0005808115820400417 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32049 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32050 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32051 \t\t Training Loss: 0.0005808115820400417 \t\n",
      "Epoch 32052 \t\t Training Loss: 0.0005808115820400417 \t\n",
      "Epoch 32053 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32054 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32055 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32056 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32057 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32058 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32059 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32060 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32061 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32062 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32063 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32064 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32065 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32066 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32067 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32068 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32069 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32070 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32071 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32072 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32073 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32074 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32075 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32076 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32077 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32078 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32079 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32080 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32081 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32082 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32083 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32084 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32085 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32086 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32087 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32088 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32089 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32090 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32091 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32092 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32093 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32094 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32095 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32096 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32097 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32098 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32099 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32100 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32101 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32102 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32103 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32104 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32105 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32106 \t\t Training Loss: 0.0005808115238323808 \t\n",
      "Epoch 32107 \t\t Training Loss: 0.0005808114074170589 \t\n",
      "Epoch 32108 \t\t Training Loss: 0.0005808114074170589 \t\n",
      "Epoch 32109 \t\t Training Loss: 0.0005808114074170589 \t\n",
      "Epoch 32110 \t\t Training Loss: 0.0005808114074170589 \t\n",
      "Epoch 32111 \t\t Training Loss: 0.0005808114074170589 \t\n",
      "Epoch 32112 \t\t Training Loss: 0.0005808114074170589 \t\n",
      "Epoch 32113 \t\t Training Loss: 0.0005808114074170589 \t\n",
      "Epoch 32114 \t\t Training Loss: 0.000580811349209398 \t\n",
      "Epoch 32115 \t\t Training Loss: 0.000580811349209398 \t\n",
      "Epoch 32116 \t\t Training Loss: 0.000580811349209398 \t\n",
      "Epoch 32117 \t\t Training Loss: 0.0005808114074170589 \t\n",
      "Epoch 32118 \t\t Training Loss: 0.000580811349209398 \t\n",
      "Epoch 32119 \t\t Training Loss: 0.000580811349209398 \t\n",
      "Epoch 32120 \t\t Training Loss: 0.000580811349209398 \t\n",
      "Epoch 32121 \t\t Training Loss: 0.000580811349209398 \t\n",
      "Epoch 32122 \t\t Training Loss: 0.000580811349209398 \t\n",
      "Epoch 32123 \t\t Training Loss: 0.000580811349209398 \t\n",
      "Epoch 32124 \t\t Training Loss: 0.000580811349209398 \t\n",
      "Epoch 32125 \t\t Training Loss: 0.000580811349209398 \t\n",
      "Epoch 32126 \t\t Training Loss: 0.000580811349209398 \t\n",
      "Epoch 32127 \t\t Training Loss: 0.000580811349209398 \t\n",
      "Epoch 32128 \t\t Training Loss: 0.0005808112910017371 \t\n",
      "Epoch 32129 \t\t Training Loss: 0.0005808112327940762 \t\n",
      "Epoch 32130 \t\t Training Loss: 0.0005808112327940762 \t\n",
      "Epoch 32131 \t\t Training Loss: 0.0005808112327940762 \t\n",
      "Epoch 32132 \t\t Training Loss: 0.0005808112327940762 \t\n",
      "Epoch 32133 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32134 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32135 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32136 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32137 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32138 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32139 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32140 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32141 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32142 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32143 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32144 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32145 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32146 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32147 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32148 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32149 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32150 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32151 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32152 \t\t Training Loss: 0.0005808111163787544 \t\n",
      "Epoch 32153 \t\t Training Loss: 0.0005808109999634326 \t\n",
      "Epoch 32154 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32155 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32156 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32157 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32158 \t\t Training Loss: 0.0005808109999634326 \t\n",
      "Epoch 32159 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32160 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32161 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32162 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32163 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32164 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32165 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32166 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32167 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32168 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32169 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32170 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32171 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32172 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32173 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32174 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32175 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32176 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32177 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32178 \t\t Training Loss: 0.0005808109417557716 \t\n",
      "Epoch 32179 \t\t Training Loss: 0.0005808108253404498 \t\n",
      "Epoch 32180 \t\t Training Loss: 0.0005808108253404498 \t\n",
      "Epoch 32181 \t\t Training Loss: 0.0005808108253404498 \t\n",
      "Epoch 32182 \t\t Training Loss: 0.0005808108253404498 \t\n",
      "Epoch 32183 \t\t Training Loss: 0.0005808108253404498 \t\n",
      "Epoch 32184 \t\t Training Loss: 0.0005808108253404498 \t\n",
      "Epoch 32185 \t\t Training Loss: 0.0005808108253404498 \t\n",
      "Epoch 32186 \t\t Training Loss: 0.000580810708925128 \t\n",
      "Epoch 32187 \t\t Training Loss: 0.000580810708925128 \t\n",
      "Epoch 32188 \t\t Training Loss: 0.0005808106507174671 \t\n",
      "Epoch 32189 \t\t Training Loss: 0.000580810708925128 \t\n",
      "Epoch 32190 \t\t Training Loss: 0.000580810708925128 \t\n",
      "Epoch 32191 \t\t Training Loss: 0.000580810708925128 \t\n",
      "Epoch 32192 \t\t Training Loss: 0.000580810708925128 \t\n",
      "Epoch 32193 \t\t Training Loss: 0.000580810708925128 \t\n",
      "Epoch 32194 \t\t Training Loss: 0.000580810708925128 \t\n",
      "Epoch 32195 \t\t Training Loss: 0.0005808106507174671 \t\n",
      "Epoch 32196 \t\t Training Loss: 0.0005808106507174671 \t\n",
      "Epoch 32197 \t\t Training Loss: 0.0005808106507174671 \t\n",
      "Epoch 32198 \t\t Training Loss: 0.0005808106507174671 \t\n",
      "Epoch 32199 \t\t Training Loss: 0.0005808106507174671 \t\n",
      "Epoch 32200 \t\t Training Loss: 0.0005808106507174671 \t\n",
      "Epoch 32201 \t\t Training Loss: 0.0005808106507174671 \t\n",
      "Epoch 32202 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32203 \t\t Training Loss: 0.0005808106507174671 \t\n",
      "Epoch 32204 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32205 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32206 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32207 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32208 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32209 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32210 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32211 \t\t Training Loss: 0.0005808105343021452 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32212 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32213 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32214 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32215 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32216 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32217 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32218 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32219 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32220 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32221 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32222 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32223 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32224 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32225 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32226 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32227 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32228 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32229 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32230 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32231 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32232 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32233 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32234 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32235 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32236 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32237 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32238 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32239 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32240 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32241 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32242 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32243 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32244 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32245 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32246 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32247 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32248 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32249 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32250 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32251 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32252 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32253 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32254 \t\t Training Loss: 0.0005808104178868234 \t\n",
      "Epoch 32255 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32256 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32257 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32258 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32259 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32260 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32261 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32262 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32263 \t\t Training Loss: 0.0005808104178868234 \t\n",
      "Epoch 32264 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32265 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32266 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32267 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32268 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32269 \t\t Training Loss: 0.0005808104178868234 \t\n",
      "Epoch 32270 \t\t Training Loss: 0.0005808104178868234 \t\n",
      "Epoch 32271 \t\t Training Loss: 0.0005808104178868234 \t\n",
      "Epoch 32272 \t\t Training Loss: 0.0005808104178868234 \t\n",
      "Epoch 32273 \t\t Training Loss: 0.0005808104178868234 \t\n",
      "Epoch 32274 \t\t Training Loss: 0.0005808105343021452 \t\n",
      "Epoch 32275 \t\t Training Loss: 0.0005808104178868234 \t\n",
      "Epoch 32276 \t\t Training Loss: 0.0005808104178868234 \t\n",
      "Epoch 32277 \t\t Training Loss: 0.0005808104178868234 \t\n",
      "Epoch 32278 \t\t Training Loss: 0.0005808104178868234 \t\n",
      "Epoch 32279 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32280 \t\t Training Loss: 0.0005808104178868234 \t\n",
      "Epoch 32281 \t\t Training Loss: 0.0005808104178868234 \t\n",
      "Epoch 32282 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32283 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32284 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32285 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32286 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32287 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32288 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32289 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32290 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32291 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32292 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32293 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32294 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32295 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32296 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32297 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32298 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32299 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32300 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32301 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32302 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32303 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32304 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32305 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32306 \t\t Training Loss: 0.0005808103014715016 \t\n",
      "Epoch 32307 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32308 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32309 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32310 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32311 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32312 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32313 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32314 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32315 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32316 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32317 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32318 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32319 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32320 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32321 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32322 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32323 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32324 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32325 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32326 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32327 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32328 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32329 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32330 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32331 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32332 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32333 \t\t Training Loss: 0.0005808102432638407 \t\n",
      "Epoch 32334 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32335 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32336 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32337 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32338 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32339 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32340 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32341 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32342 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32343 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32344 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32345 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32346 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32347 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32348 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32349 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32350 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32351 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32352 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32353 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32354 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32355 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32356 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32357 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32358 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32359 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32360 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32361 \t\t Training Loss: 0.0005808101268485188 \t\n",
      "Epoch 32362 \t\t Training Loss: 0.000580810010433197 \t\n",
      "Epoch 32363 \t\t Training Loss: 0.000580810010433197 \t\n",
      "Epoch 32364 \t\t Training Loss: 0.000580810010433197 \t\n",
      "Epoch 32365 \t\t Training Loss: 0.000580810010433197 \t\n",
      "Epoch 32366 \t\t Training Loss: 0.000580810010433197 \t\n",
      "Epoch 32367 \t\t Training Loss: 0.000580810010433197 \t\n",
      "Epoch 32368 \t\t Training Loss: 0.000580810010433197 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32369 \t\t Training Loss: 0.000580810010433197 \t\n",
      "Epoch 32370 \t\t Training Loss: 0.000580810010433197 \t\n",
      "Epoch 32371 \t\t Training Loss: 0.000580810010433197 \t\n",
      "Epoch 32372 \t\t Training Loss: 0.000580810010433197 \t\n",
      "Epoch 32373 \t\t Training Loss: 0.000580810010433197 \t\n",
      "Epoch 32374 \t\t Training Loss: 0.000580810010433197 \t\n",
      "Epoch 32375 \t\t Training Loss: 0.000580810010433197 \t\n",
      "Epoch 32376 \t\t Training Loss: 0.000580810010433197 \t\n",
      "Epoch 32377 \t\t Training Loss: 0.000580810010433197 \t\n",
      "Epoch 32378 \t\t Training Loss: 0.000580810010433197 \t\n",
      "Epoch 32379 \t\t Training Loss: 0.0005808099522255361 \t\n",
      "Epoch 32380 \t\t Training Loss: 0.0005808099522255361 \t\n",
      "Epoch 32381 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32382 \t\t Training Loss: 0.0005808099522255361 \t\n",
      "Epoch 32383 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32384 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32385 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32386 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32387 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32388 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32389 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32390 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32391 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32392 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32393 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32394 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32395 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32396 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32397 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32398 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32399 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32400 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32401 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32402 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32403 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32404 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32405 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32406 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32407 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32408 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32409 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32410 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32411 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32412 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32413 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32414 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32415 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32416 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32417 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32418 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32419 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32420 \t\t Training Loss: 0.0005808098358102143 \t\n",
      "Epoch 32421 \t\t Training Loss: 0.0005808097193948925 \t\n",
      "Epoch 32422 \t\t Training Loss: 0.0005808097193948925 \t\n",
      "Epoch 32423 \t\t Training Loss: 0.0005808097193948925 \t\n",
      "Epoch 32424 \t\t Training Loss: 0.0005808097193948925 \t\n",
      "Epoch 32425 \t\t Training Loss: 0.0005808097193948925 \t\n",
      "Epoch 32426 \t\t Training Loss: 0.0005808097193948925 \t\n",
      "Epoch 32427 \t\t Training Loss: 0.0005808097193948925 \t\n",
      "Epoch 32428 \t\t Training Loss: 0.0005808097193948925 \t\n",
      "Epoch 32429 \t\t Training Loss: 0.0005808097193948925 \t\n",
      "Epoch 32430 \t\t Training Loss: 0.0005808097193948925 \t\n",
      "Epoch 32431 \t\t Training Loss: 0.0005808097193948925 \t\n",
      "Epoch 32432 \t\t Training Loss: 0.0005808097193948925 \t\n",
      "Epoch 32433 \t\t Training Loss: 0.0005808097193948925 \t\n",
      "Epoch 32434 \t\t Training Loss: 0.0005808097193948925 \t\n",
      "Epoch 32435 \t\t Training Loss: 0.0005808097193948925 \t\n",
      "Epoch 32436 \t\t Training Loss: 0.0005808096611872315 \t\n",
      "Epoch 32437 \t\t Training Loss: 0.0005808096611872315 \t\n",
      "Epoch 32438 \t\t Training Loss: 0.0005808096611872315 \t\n",
      "Epoch 32439 \t\t Training Loss: 0.0005808097193948925 \t\n",
      "Epoch 32440 \t\t Training Loss: 0.0005808096611872315 \t\n",
      "Epoch 32441 \t\t Training Loss: 0.0005808095447719097 \t\n",
      "Epoch 32442 \t\t Training Loss: 0.0005808095447719097 \t\n",
      "Epoch 32443 \t\t Training Loss: 0.0005808095447719097 \t\n",
      "Epoch 32444 \t\t Training Loss: 0.0005808095447719097 \t\n",
      "Epoch 32445 \t\t Training Loss: 0.0005808095447719097 \t\n",
      "Epoch 32446 \t\t Training Loss: 0.0005808095447719097 \t\n",
      "Epoch 32447 \t\t Training Loss: 0.0005808095447719097 \t\n",
      "Epoch 32448 \t\t Training Loss: 0.0005808095447719097 \t\n",
      "Epoch 32449 \t\t Training Loss: 0.0005808095447719097 \t\n",
      "Epoch 32450 \t\t Training Loss: 0.0005808095447719097 \t\n",
      "Epoch 32451 \t\t Training Loss: 0.0005808095447719097 \t\n",
      "Epoch 32452 \t\t Training Loss: 0.0005808095447719097 \t\n",
      "Epoch 32453 \t\t Training Loss: 0.0005808095447719097 \t\n",
      "Epoch 32454 \t\t Training Loss: 0.0005808095447719097 \t\n",
      "Epoch 32455 \t\t Training Loss: 0.0005808095447719097 \t\n",
      "Epoch 32456 \t\t Training Loss: 0.0005808095447719097 \t\n",
      "Epoch 32457 \t\t Training Loss: 0.0005808094283565879 \t\n",
      "Epoch 32458 \t\t Training Loss: 0.0005808094283565879 \t\n",
      "Epoch 32459 \t\t Training Loss: 0.0005808094283565879 \t\n",
      "Epoch 32460 \t\t Training Loss: 0.0005808094283565879 \t\n",
      "Epoch 32461 \t\t Training Loss: 0.0005808094283565879 \t\n",
      "Epoch 32462 \t\t Training Loss: 0.0005808094283565879 \t\n",
      "Epoch 32463 \t\t Training Loss: 0.0005808094283565879 \t\n",
      "Epoch 32464 \t\t Training Loss: 0.0005808094283565879 \t\n",
      "Epoch 32465 \t\t Training Loss: 0.0005808094283565879 \t\n",
      "Epoch 32466 \t\t Training Loss: 0.0005808094283565879 \t\n",
      "Epoch 32467 \t\t Training Loss: 0.0005808094283565879 \t\n",
      "Epoch 32468 \t\t Training Loss: 0.0005808094283565879 \t\n",
      "Epoch 32469 \t\t Training Loss: 0.0005808094283565879 \t\n",
      "Epoch 32470 \t\t Training Loss: 0.0005808094283565879 \t\n",
      "Epoch 32471 \t\t Training Loss: 0.0005808094283565879 \t\n",
      "Epoch 32472 \t\t Training Loss: 0.000580809370148927 \t\n",
      "Epoch 32473 \t\t Training Loss: 0.000580809370148927 \t\n",
      "Epoch 32474 \t\t Training Loss: 0.000580809370148927 \t\n",
      "Epoch 32475 \t\t Training Loss: 0.000580809370148927 \t\n",
      "Epoch 32476 \t\t Training Loss: 0.000580809370148927 \t\n",
      "Epoch 32477 \t\t Training Loss: 0.000580809370148927 \t\n",
      "Epoch 32478 \t\t Training Loss: 0.000580809370148927 \t\n",
      "Epoch 32479 \t\t Training Loss: 0.000580809370148927 \t\n",
      "Epoch 32480 \t\t Training Loss: 0.000580809370148927 \t\n",
      "Epoch 32481 \t\t Training Loss: 0.000580809370148927 \t\n",
      "Epoch 32482 \t\t Training Loss: 0.0005808093119412661 \t\n",
      "Epoch 32483 \t\t Training Loss: 0.0005808093119412661 \t\n",
      "Epoch 32484 \t\t Training Loss: 0.0005808093119412661 \t\n",
      "Epoch 32485 \t\t Training Loss: 0.0005808093119412661 \t\n",
      "Epoch 32486 \t\t Training Loss: 0.0005808093119412661 \t\n",
      "Epoch 32487 \t\t Training Loss: 0.0005808093119412661 \t\n",
      "Epoch 32488 \t\t Training Loss: 0.000580809370148927 \t\n",
      "Epoch 32489 \t\t Training Loss: 0.0005808093119412661 \t\n",
      "Epoch 32490 \t\t Training Loss: 0.0005808093119412661 \t\n",
      "Epoch 32491 \t\t Training Loss: 0.0005808093119412661 \t\n",
      "Epoch 32492 \t\t Training Loss: 0.0005808093119412661 \t\n",
      "Epoch 32493 \t\t Training Loss: 0.0005808093119412661 \t\n",
      "Epoch 32494 \t\t Training Loss: 0.000580809370148927 \t\n",
      "Epoch 32495 \t\t Training Loss: 0.0005808093119412661 \t\n",
      "Epoch 32496 \t\t Training Loss: 0.0005808093119412661 \t\n",
      "Epoch 32497 \t\t Training Loss: 0.0005808093119412661 \t\n",
      "Epoch 32498 \t\t Training Loss: 0.0005808093119412661 \t\n",
      "Epoch 32499 \t\t Training Loss: 0.0005808093119412661 \t\n",
      "Epoch 32500 \t\t Training Loss: 0.0005808092537336051 \t\n",
      "Epoch 32501 \t\t Training Loss: 0.0005808092537336051 \t\n",
      "Epoch 32502 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32503 \t\t Training Loss: 0.0005808092537336051 \t\n",
      "Epoch 32504 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32505 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32506 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32507 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32508 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32509 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32510 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32511 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32512 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32513 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32514 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32515 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32516 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32517 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32518 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32519 \t\t Training Loss: 0.0005808090791106224 \t\n",
      "Epoch 32520 \t\t Training Loss: 0.0005808090791106224 \t\n",
      "Epoch 32521 \t\t Training Loss: 0.0005808091373182833 \t\n",
      "Epoch 32522 \t\t Training Loss: 0.0005808090791106224 \t\n",
      "Epoch 32523 \t\t Training Loss: 0.0005808090209029615 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32524 \t\t Training Loss: 0.0005808090209029615 \t\n",
      "Epoch 32525 \t\t Training Loss: 0.0005808090209029615 \t\n",
      "Epoch 32526 \t\t Training Loss: 0.0005808090209029615 \t\n",
      "Epoch 32527 \t\t Training Loss: 0.0005808090209029615 \t\n",
      "Epoch 32528 \t\t Training Loss: 0.0005808090209029615 \t\n",
      "Epoch 32529 \t\t Training Loss: 0.0005808090209029615 \t\n",
      "Epoch 32530 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32531 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32532 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32533 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32534 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32535 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32536 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32537 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32538 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32539 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32540 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32541 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32542 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32543 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32544 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32545 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32546 \t\t Training Loss: 0.0005808089626953006 \t\n",
      "Epoch 32547 \t\t Training Loss: 0.0005808089044876397 \t\n",
      "Epoch 32548 \t\t Training Loss: 0.0005808089044876397 \t\n",
      "Epoch 32549 \t\t Training Loss: 0.0005808088462799788 \t\n",
      "Epoch 32550 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32551 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32552 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32553 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32554 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32555 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32556 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32557 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32558 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32559 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32560 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32561 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32562 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32563 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32564 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32565 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32566 \t\t Training Loss: 0.000580808671656996 \t\n",
      "Epoch 32567 \t\t Training Loss: 0.000580808671656996 \t\n",
      "Epoch 32568 \t\t Training Loss: 0.000580808671656996 \t\n",
      "Epoch 32569 \t\t Training Loss: 0.000580808671656996 \t\n",
      "Epoch 32570 \t\t Training Loss: 0.000580808671656996 \t\n",
      "Epoch 32571 \t\t Training Loss: 0.0005808087298646569 \t\n",
      "Epoch 32572 \t\t Training Loss: 0.000580808671656996 \t\n",
      "Epoch 32573 \t\t Training Loss: 0.0005808086134493351 \t\n",
      "Epoch 32574 \t\t Training Loss: 0.000580808671656996 \t\n",
      "Epoch 32575 \t\t Training Loss: 0.000580808671656996 \t\n",
      "Epoch 32576 \t\t Training Loss: 0.000580808671656996 \t\n",
      "Epoch 32577 \t\t Training Loss: 0.000580808671656996 \t\n",
      "Epoch 32578 \t\t Training Loss: 0.000580808671656996 \t\n",
      "Epoch 32579 \t\t Training Loss: 0.0005808086134493351 \t\n",
      "Epoch 32580 \t\t Training Loss: 0.000580808671656996 \t\n",
      "Epoch 32581 \t\t Training Loss: 0.000580808671656996 \t\n",
      "Epoch 32582 \t\t Training Loss: 0.0005808086134493351 \t\n",
      "Epoch 32583 \t\t Training Loss: 0.0005808086134493351 \t\n",
      "Epoch 32584 \t\t Training Loss: 0.0005808086134493351 \t\n",
      "Epoch 32585 \t\t Training Loss: 0.0005808086134493351 \t\n",
      "Epoch 32586 \t\t Training Loss: 0.0005808086134493351 \t\n",
      "Epoch 32587 \t\t Training Loss: 0.0005808086134493351 \t\n",
      "Epoch 32588 \t\t Training Loss: 0.0005808085552416742 \t\n",
      "Epoch 32589 \t\t Training Loss: 0.0005808085552416742 \t\n",
      "Epoch 32590 \t\t Training Loss: 0.0005808086134493351 \t\n",
      "Epoch 32591 \t\t Training Loss: 0.0005808086134493351 \t\n",
      "Epoch 32592 \t\t Training Loss: 0.0005808086134493351 \t\n",
      "Epoch 32593 \t\t Training Loss: 0.0005808085552416742 \t\n",
      "Epoch 32594 \t\t Training Loss: 0.0005808085552416742 \t\n",
      "Epoch 32595 \t\t Training Loss: 0.0005808085552416742 \t\n",
      "Epoch 32596 \t\t Training Loss: 0.0005808085552416742 \t\n",
      "Epoch 32597 \t\t Training Loss: 0.0005808085552416742 \t\n",
      "Epoch 32598 \t\t Training Loss: 0.0005808085552416742 \t\n",
      "Epoch 32599 \t\t Training Loss: 0.0005808085552416742 \t\n",
      "Epoch 32600 \t\t Training Loss: 0.0005808085552416742 \t\n",
      "Epoch 32601 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32602 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32603 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32604 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32605 \t\t Training Loss: 0.0005808085552416742 \t\n",
      "Epoch 32606 \t\t Training Loss: 0.0005808085552416742 \t\n",
      "Epoch 32607 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32608 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32609 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32610 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32611 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32612 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32613 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32614 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32615 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32616 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32617 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32618 \t\t Training Loss: 0.0005808083806186914 \t\n",
      "Epoch 32619 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32620 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32621 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32622 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32623 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32624 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32625 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32626 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32627 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32628 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32629 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32630 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32631 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32632 \t\t Training Loss: 0.0005808083806186914 \t\n",
      "Epoch 32633 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32634 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32635 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32636 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32637 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32638 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32639 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32640 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32641 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32642 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32643 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32644 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32645 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32646 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32647 \t\t Training Loss: 0.0005808083806186914 \t\n",
      "Epoch 32648 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32649 \t\t Training Loss: 0.0005808084388263524 \t\n",
      "Epoch 32650 \t\t Training Loss: 0.0005808082642033696 \t\n",
      "Epoch 32651 \t\t Training Loss: 0.0005808082642033696 \t\n",
      "Epoch 32652 \t\t Training Loss: 0.0005808082642033696 \t\n",
      "Epoch 32653 \t\t Training Loss: 0.0005808082642033696 \t\n",
      "Epoch 32654 \t\t Training Loss: 0.0005808082642033696 \t\n",
      "Epoch 32655 \t\t Training Loss: 0.0005808082642033696 \t\n",
      "Epoch 32656 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32657 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32658 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32659 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32660 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32661 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32662 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32663 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32664 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32665 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32666 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32667 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32668 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32669 \t\t Training Loss: 0.0005808082642033696 \t\n",
      "Epoch 32670 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32671 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32672 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32673 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32674 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32675 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32676 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32677 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32678 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32679 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32680 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32681 \t\t Training Loss: 0.0005808081477880478 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32682 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32683 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32684 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32685 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32686 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32687 \t\t Training Loss: 0.0005808082059957087 \t\n",
      "Epoch 32688 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32689 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32690 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32691 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32692 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32693 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32694 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32695 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32696 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32697 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32698 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32699 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32700 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32701 \t\t Training Loss: 0.0005808081477880478 \t\n",
      "Epoch 32702 \t\t Training Loss: 0.0005808080895803869 \t\n",
      "Epoch 32703 \t\t Training Loss: 0.0005808080895803869 \t\n",
      "Epoch 32704 \t\t Training Loss: 0.0005808080895803869 \t\n",
      "Epoch 32705 \t\t Training Loss: 0.0005808080895803869 \t\n",
      "Epoch 32706 \t\t Training Loss: 0.0005808080895803869 \t\n",
      "Epoch 32707 \t\t Training Loss: 0.0005808080895803869 \t\n",
      "Epoch 32708 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32709 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32710 \t\t Training Loss: 0.0005808080895803869 \t\n",
      "Epoch 32711 \t\t Training Loss: 0.0005808080895803869 \t\n",
      "Epoch 32712 \t\t Training Loss: 0.0005808080895803869 \t\n",
      "Epoch 32713 \t\t Training Loss: 0.0005808080895803869 \t\n",
      "Epoch 32714 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32715 \t\t Training Loss: 0.0005808080895803869 \t\n",
      "Epoch 32716 \t\t Training Loss: 0.0005808080895803869 \t\n",
      "Epoch 32717 \t\t Training Loss: 0.0005808080895803869 \t\n",
      "Epoch 32718 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32719 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32720 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32721 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32722 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32723 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32724 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32725 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32726 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32727 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32728 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32729 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32730 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32731 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32732 \t\t Training Loss: 0.000580807973165065 \t\n",
      "Epoch 32733 \t\t Training Loss: 0.000580808031372726 \t\n",
      "Epoch 32734 \t\t Training Loss: 0.000580807973165065 \t\n",
      "Epoch 32735 \t\t Training Loss: 0.000580807973165065 \t\n",
      "Epoch 32736 \t\t Training Loss: 0.000580807973165065 \t\n",
      "Epoch 32737 \t\t Training Loss: 0.000580807973165065 \t\n",
      "Epoch 32738 \t\t Training Loss: 0.000580807973165065 \t\n",
      "Epoch 32739 \t\t Training Loss: 0.0005808079149574041 \t\n",
      "Epoch 32740 \t\t Training Loss: 0.0005808079149574041 \t\n",
      "Epoch 32741 \t\t Training Loss: 0.0005808079149574041 \t\n",
      "Epoch 32742 \t\t Training Loss: 0.0005808079149574041 \t\n",
      "Epoch 32743 \t\t Training Loss: 0.0005808079149574041 \t\n",
      "Epoch 32744 \t\t Training Loss: 0.0005808078567497432 \t\n",
      "Epoch 32745 \t\t Training Loss: 0.0005808078567497432 \t\n",
      "Epoch 32746 \t\t Training Loss: 0.0005808078567497432 \t\n",
      "Epoch 32747 \t\t Training Loss: 0.0005808078567497432 \t\n",
      "Epoch 32748 \t\t Training Loss: 0.0005808078567497432 \t\n",
      "Epoch 32749 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32750 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32751 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32752 \t\t Training Loss: 0.0005808078567497432 \t\n",
      "Epoch 32753 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32754 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32755 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32756 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32757 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32758 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32759 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32760 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32761 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32762 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32763 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32764 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32765 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32766 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32767 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32768 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32769 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32770 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32771 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32772 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32773 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32774 \t\t Training Loss: 0.0005808077985420823 \t\n",
      "Epoch 32775 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32776 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32777 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32778 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32779 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32780 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32781 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32782 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32783 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32784 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32785 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32786 \t\t Training Loss: 0.0005808075657114387 \t\n",
      "Epoch 32787 \t\t Training Loss: 0.0005808075657114387 \t\n",
      "Epoch 32788 \t\t Training Loss: 0.0005808076821267605 \t\n",
      "Epoch 32789 \t\t Training Loss: 0.0005808075657114387 \t\n",
      "Epoch 32790 \t\t Training Loss: 0.0005808075657114387 \t\n",
      "Epoch 32791 \t\t Training Loss: 0.0005808075657114387 \t\n",
      "Epoch 32792 \t\t Training Loss: 0.0005808075657114387 \t\n",
      "Epoch 32793 \t\t Training Loss: 0.0005808076239190996 \t\n",
      "Epoch 32794 \t\t Training Loss: 0.0005808076239190996 \t\n",
      "Epoch 32795 \t\t Training Loss: 0.0005808076239190996 \t\n",
      "Epoch 32796 \t\t Training Loss: 0.0005808076239190996 \t\n",
      "Epoch 32797 \t\t Training Loss: 0.0005808075657114387 \t\n",
      "Epoch 32798 \t\t Training Loss: 0.0005808075657114387 \t\n",
      "Epoch 32799 \t\t Training Loss: 0.0005808075657114387 \t\n",
      "Epoch 32800 \t\t Training Loss: 0.0005808075657114387 \t\n",
      "Epoch 32801 \t\t Training Loss: 0.0005808075075037777 \t\n",
      "Epoch 32802 \t\t Training Loss: 0.0005808075075037777 \t\n",
      "Epoch 32803 \t\t Training Loss: 0.0005808075075037777 \t\n",
      "Epoch 32804 \t\t Training Loss: 0.0005808075075037777 \t\n",
      "Epoch 32805 \t\t Training Loss: 0.0005808075075037777 \t\n",
      "Epoch 32806 \t\t Training Loss: 0.0005808075075037777 \t\n",
      "Epoch 32807 \t\t Training Loss: 0.0005808075075037777 \t\n",
      "Epoch 32808 \t\t Training Loss: 0.0005808075075037777 \t\n",
      "Epoch 32809 \t\t Training Loss: 0.0005808075075037777 \t\n",
      "Epoch 32810 \t\t Training Loss: 0.0005808075075037777 \t\n",
      "Epoch 32811 \t\t Training Loss: 0.0005808074492961168 \t\n",
      "Epoch 32812 \t\t Training Loss: 0.0005808074492961168 \t\n",
      "Epoch 32813 \t\t Training Loss: 0.0005808074492961168 \t\n",
      "Epoch 32814 \t\t Training Loss: 0.0005808074492961168 \t\n",
      "Epoch 32815 \t\t Training Loss: 0.0005808074492961168 \t\n",
      "Epoch 32816 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32817 \t\t Training Loss: 0.0005808074492961168 \t\n",
      "Epoch 32818 \t\t Training Loss: 0.0005808074492961168 \t\n",
      "Epoch 32819 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32820 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32821 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32822 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32823 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32824 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32825 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32826 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32827 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32828 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32829 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32830 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32831 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32832 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32833 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32834 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32835 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32836 \t\t Training Loss: 0.0005808073910884559 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32837 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32838 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32839 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32840 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32841 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32842 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32843 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32844 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32845 \t\t Training Loss: 0.0005808073910884559 \t\n",
      "Epoch 32846 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32847 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32848 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32849 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32850 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32851 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32852 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32853 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32854 \t\t Training Loss: 0.0005808072746731341 \t\n",
      "Epoch 32855 \t\t Training Loss: 0.0005808072164654732 \t\n",
      "Epoch 32856 \t\t Training Loss: 0.0005808072164654732 \t\n",
      "Epoch 32857 \t\t Training Loss: 0.0005808072164654732 \t\n",
      "Epoch 32858 \t\t Training Loss: 0.0005808072746731341 \t\n",
      "Epoch 32859 \t\t Training Loss: 0.0005808072746731341 \t\n",
      "Epoch 32860 \t\t Training Loss: 0.0005808072746731341 \t\n",
      "Epoch 32861 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32862 \t\t Training Loss: 0.0005808072746731341 \t\n",
      "Epoch 32863 \t\t Training Loss: 0.0005808072746731341 \t\n",
      "Epoch 32864 \t\t Training Loss: 0.0005808072746731341 \t\n",
      "Epoch 32865 \t\t Training Loss: 0.0005808072746731341 \t\n",
      "Epoch 32866 \t\t Training Loss: 0.0005808072746731341 \t\n",
      "Epoch 32867 \t\t Training Loss: 0.0005808072746731341 \t\n",
      "Epoch 32868 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32869 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32870 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32871 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32872 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32873 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32874 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32875 \t\t Training Loss: 0.0005808072746731341 \t\n",
      "Epoch 32876 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32877 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32878 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32879 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32880 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32881 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32882 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32883 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32884 \t\t Training Loss: 0.000580807332880795 \t\n",
      "Epoch 32885 \t\t Training Loss: 0.0005808072164654732 \t\n",
      "Epoch 32886 \t\t Training Loss: 0.0005808072164654732 \t\n",
      "Epoch 32887 \t\t Training Loss: 0.0005808072164654732 \t\n",
      "Epoch 32888 \t\t Training Loss: 0.0005808072164654732 \t\n",
      "Epoch 32889 \t\t Training Loss: 0.0005808071000501513 \t\n",
      "Epoch 32890 \t\t Training Loss: 0.0005808072164654732 \t\n",
      "Epoch 32891 \t\t Training Loss: 0.0005808071000501513 \t\n",
      "Epoch 32892 \t\t Training Loss: 0.0005808071000501513 \t\n",
      "Epoch 32893 \t\t Training Loss: 0.0005808071000501513 \t\n",
      "Epoch 32894 \t\t Training Loss: 0.0005808071000501513 \t\n",
      "Epoch 32895 \t\t Training Loss: 0.0005808071000501513 \t\n",
      "Epoch 32896 \t\t Training Loss: 0.0005808071000501513 \t\n",
      "Epoch 32897 \t\t Training Loss: 0.0005808071000501513 \t\n",
      "Epoch 32898 \t\t Training Loss: 0.0005808071000501513 \t\n",
      "Epoch 32899 \t\t Training Loss: 0.0005808071000501513 \t\n",
      "Epoch 32900 \t\t Training Loss: 0.0005808070418424904 \t\n",
      "Epoch 32901 \t\t Training Loss: 0.0005808070418424904 \t\n",
      "Epoch 32902 \t\t Training Loss: 0.0005808070418424904 \t\n",
      "Epoch 32903 \t\t Training Loss: 0.0005808070418424904 \t\n",
      "Epoch 32904 \t\t Training Loss: 0.0005808069836348295 \t\n",
      "Epoch 32905 \t\t Training Loss: 0.0005808069836348295 \t\n",
      "Epoch 32906 \t\t Training Loss: 0.0005808069836348295 \t\n",
      "Epoch 32907 \t\t Training Loss: 0.0005808069836348295 \t\n",
      "Epoch 32908 \t\t Training Loss: 0.0005808069836348295 \t\n",
      "Epoch 32909 \t\t Training Loss: 0.0005808069836348295 \t\n",
      "Epoch 32910 \t\t Training Loss: 0.0005808069836348295 \t\n",
      "Epoch 32911 \t\t Training Loss: 0.0005808069836348295 \t\n",
      "Epoch 32912 \t\t Training Loss: 0.0005808069254271686 \t\n",
      "Epoch 32913 \t\t Training Loss: 0.0005808069836348295 \t\n",
      "Epoch 32914 \t\t Training Loss: 0.0005808069254271686 \t\n",
      "Epoch 32915 \t\t Training Loss: 0.0005808069254271686 \t\n",
      "Epoch 32916 \t\t Training Loss: 0.0005808069254271686 \t\n",
      "Epoch 32917 \t\t Training Loss: 0.0005808069254271686 \t\n",
      "Epoch 32918 \t\t Training Loss: 0.0005808068672195077 \t\n",
      "Epoch 32919 \t\t Training Loss: 0.0005808069254271686 \t\n",
      "Epoch 32920 \t\t Training Loss: 0.0005808069254271686 \t\n",
      "Epoch 32921 \t\t Training Loss: 0.0005808069254271686 \t\n",
      "Epoch 32922 \t\t Training Loss: 0.0005808069254271686 \t\n",
      "Epoch 32923 \t\t Training Loss: 0.0005808069254271686 \t\n",
      "Epoch 32924 \t\t Training Loss: 0.0005808069254271686 \t\n",
      "Epoch 32925 \t\t Training Loss: 0.0005808069254271686 \t\n",
      "Epoch 32926 \t\t Training Loss: 0.0005808069254271686 \t\n",
      "Epoch 32927 \t\t Training Loss: 0.0005808069254271686 \t\n",
      "Epoch 32928 \t\t Training Loss: 0.0005808069254271686 \t\n",
      "Epoch 32929 \t\t Training Loss: 0.0005808068672195077 \t\n",
      "Epoch 32930 \t\t Training Loss: 0.0005808068090118468 \t\n",
      "Epoch 32931 \t\t Training Loss: 0.0005808068090118468 \t\n",
      "Epoch 32932 \t\t Training Loss: 0.0005808068090118468 \t\n",
      "Epoch 32933 \t\t Training Loss: 0.0005808068090118468 \t\n",
      "Epoch 32934 \t\t Training Loss: 0.0005808068090118468 \t\n",
      "Epoch 32935 \t\t Training Loss: 0.0005808068090118468 \t\n",
      "Epoch 32936 \t\t Training Loss: 0.0005808068090118468 \t\n",
      "Epoch 32937 \t\t Training Loss: 0.0005808067508041859 \t\n",
      "Epoch 32938 \t\t Training Loss: 0.0005808067508041859 \t\n",
      "Epoch 32939 \t\t Training Loss: 0.0005808067508041859 \t\n",
      "Epoch 32940 \t\t Training Loss: 0.0005808067508041859 \t\n",
      "Epoch 32941 \t\t Training Loss: 0.0005808067508041859 \t\n",
      "Epoch 32942 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32943 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32944 \t\t Training Loss: 0.0005808065179735422 \t\n",
      "Epoch 32945 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32946 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32947 \t\t Training Loss: 0.0005808065179735422 \t\n",
      "Epoch 32948 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32949 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32950 \t\t Training Loss: 0.000580806692596525 \t\n",
      "Epoch 32951 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32952 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32953 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32954 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32955 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32956 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32957 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32958 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32959 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32960 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32961 \t\t Training Loss: 0.000580806634388864 \t\n",
      "Epoch 32962 \t\t Training Loss: 0.0005808065179735422 \t\n",
      "Epoch 32963 \t\t Training Loss: 0.0005808065179735422 \t\n",
      "Epoch 32964 \t\t Training Loss: 0.0005808065179735422 \t\n",
      "Epoch 32965 \t\t Training Loss: 0.0005808065179735422 \t\n",
      "Epoch 32966 \t\t Training Loss: 0.0005808065179735422 \t\n",
      "Epoch 32967 \t\t Training Loss: 0.0005808065179735422 \t\n",
      "Epoch 32968 \t\t Training Loss: 0.0005808065179735422 \t\n",
      "Epoch 32969 \t\t Training Loss: 0.0005808065179735422 \t\n",
      "Epoch 32970 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32971 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32972 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32973 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32974 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32975 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32976 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32977 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32978 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32979 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32980 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32981 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32982 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32983 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32984 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32985 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32986 \t\t Training Loss: 0.0005808064015582204 \t\n",
      "Epoch 32987 \t\t Training Loss: 0.0005808064597658813 \t\n",
      "Epoch 32988 \t\t Training Loss: 0.0005808064015582204 \t\n",
      "Epoch 32989 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 32990 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 32991 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 32992 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 32993 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 32994 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 32995 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 32996 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 32997 \t\t Training Loss: 0.0005808063433505595 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32998 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 32999 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33000 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33001 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33002 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33003 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33004 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33005 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33006 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33007 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33008 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33009 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33010 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33011 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33012 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33013 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33014 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33015 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33016 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33017 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33018 \t\t Training Loss: 0.0005808063433505595 \t\n",
      "Epoch 33019 \t\t Training Loss: 0.0005808062851428986 \t\n",
      "Epoch 33020 \t\t Training Loss: 0.0005808062269352376 \t\n",
      "Epoch 33021 \t\t Training Loss: 0.0005808062269352376 \t\n",
      "Epoch 33022 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33023 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33024 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33025 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33026 \t\t Training Loss: 0.0005808062269352376 \t\n",
      "Epoch 33027 \t\t Training Loss: 0.0005808062269352376 \t\n",
      "Epoch 33028 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33029 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33030 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33031 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33032 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33033 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33034 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33035 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33036 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33037 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33038 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33039 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33040 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33041 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33042 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33043 \t\t Training Loss: 0.0005808061687275767 \t\n",
      "Epoch 33044 \t\t Training Loss: 0.0005808061105199158 \t\n",
      "Epoch 33045 \t\t Training Loss: 0.0005808061105199158 \t\n",
      "Epoch 33046 \t\t Training Loss: 0.0005808061105199158 \t\n",
      "Epoch 33047 \t\t Training Loss: 0.0005808061105199158 \t\n",
      "Epoch 33048 \t\t Training Loss: 0.0005808061105199158 \t\n",
      "Epoch 33049 \t\t Training Loss: 0.0005808060523122549 \t\n",
      "Epoch 33050 \t\t Training Loss: 0.0005808060523122549 \t\n",
      "Epoch 33051 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33052 \t\t Training Loss: 0.0005808060523122549 \t\n",
      "Epoch 33053 \t\t Training Loss: 0.0005808060523122549 \t\n",
      "Epoch 33054 \t\t Training Loss: 0.0005808060523122549 \t\n",
      "Epoch 33055 \t\t Training Loss: 0.0005808060523122549 \t\n",
      "Epoch 33056 \t\t Training Loss: 0.0005808060523122549 \t\n",
      "Epoch 33057 \t\t Training Loss: 0.0005808060523122549 \t\n",
      "Epoch 33058 \t\t Training Loss: 0.0005808061105199158 \t\n",
      "Epoch 33059 \t\t Training Loss: 0.0005808061105199158 \t\n",
      "Epoch 33060 \t\t Training Loss: 0.0005808061105199158 \t\n",
      "Epoch 33061 \t\t Training Loss: 0.0005808061105199158 \t\n",
      "Epoch 33062 \t\t Training Loss: 0.0005808060523122549 \t\n",
      "Epoch 33063 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33064 \t\t Training Loss: 0.0005808060523122549 \t\n",
      "Epoch 33065 \t\t Training Loss: 0.0005808060523122549 \t\n",
      "Epoch 33066 \t\t Training Loss: 0.0005808060523122549 \t\n",
      "Epoch 33067 \t\t Training Loss: 0.0005808060523122549 \t\n",
      "Epoch 33068 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33069 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33070 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33071 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33072 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33073 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33074 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33075 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33076 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33077 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33078 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33079 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33080 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33081 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33082 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33083 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33084 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33085 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33086 \t\t Training Loss: 0.0005808058776892722 \t\n",
      "Epoch 33087 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33088 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33089 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33090 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33091 \t\t Training Loss: 0.0005808058776892722 \t\n",
      "Epoch 33092 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33093 \t\t Training Loss: 0.0005808058776892722 \t\n",
      "Epoch 33094 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33095 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33096 \t\t Training Loss: 0.000580805994104594 \t\n",
      "Epoch 33097 \t\t Training Loss: 0.0005808058776892722 \t\n",
      "Epoch 33098 \t\t Training Loss: 0.0005808058776892722 \t\n",
      "Epoch 33099 \t\t Training Loss: 0.0005808058194816113 \t\n",
      "Epoch 33100 \t\t Training Loss: 0.0005808058194816113 \t\n",
      "Epoch 33101 \t\t Training Loss: 0.0005808058194816113 \t\n",
      "Epoch 33102 \t\t Training Loss: 0.0005808057612739503 \t\n",
      "Epoch 33103 \t\t Training Loss: 0.0005808058776892722 \t\n",
      "Epoch 33104 \t\t Training Loss: 0.0005808058194816113 \t\n",
      "Epoch 33105 \t\t Training Loss: 0.0005808058194816113 \t\n",
      "Epoch 33106 \t\t Training Loss: 0.0005808058194816113 \t\n",
      "Epoch 33107 \t\t Training Loss: 0.0005808057612739503 \t\n",
      "Epoch 33108 \t\t Training Loss: 0.0005808058194816113 \t\n",
      "Epoch 33109 \t\t Training Loss: 0.0005808057612739503 \t\n",
      "Epoch 33110 \t\t Training Loss: 0.0005808057612739503 \t\n",
      "Epoch 33111 \t\t Training Loss: 0.0005808057612739503 \t\n",
      "Epoch 33112 \t\t Training Loss: 0.0005808057030662894 \t\n",
      "Epoch 33113 \t\t Training Loss: 0.0005808057030662894 \t\n",
      "Epoch 33114 \t\t Training Loss: 0.0005808057030662894 \t\n",
      "Epoch 33115 \t\t Training Loss: 0.0005808057030662894 \t\n",
      "Epoch 33116 \t\t Training Loss: 0.0005808057030662894 \t\n",
      "Epoch 33117 \t\t Training Loss: 0.0005808057030662894 \t\n",
      "Epoch 33118 \t\t Training Loss: 0.0005808057030662894 \t\n",
      "Epoch 33119 \t\t Training Loss: 0.0005808057030662894 \t\n",
      "Epoch 33120 \t\t Training Loss: 0.0005808057030662894 \t\n",
      "Epoch 33121 \t\t Training Loss: 0.0005808057030662894 \t\n",
      "Epoch 33122 \t\t Training Loss: 0.0005808057030662894 \t\n",
      "Epoch 33123 \t\t Training Loss: 0.0005808057030662894 \t\n",
      "Epoch 33124 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33125 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33126 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33127 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33128 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33129 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33130 \t\t Training Loss: 0.0005808057030662894 \t\n",
      "Epoch 33131 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33132 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33133 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33134 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33135 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33136 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33137 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33138 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33139 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33140 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33141 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33142 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33143 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33144 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33145 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33146 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33147 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33148 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33149 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33150 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33151 \t\t Training Loss: 0.0005808056448586285 \t\n",
      "Epoch 33152 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33153 \t\t Training Loss: 0.0005808056448586285 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33154 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33155 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33156 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33157 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33158 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33159 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33160 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33161 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33162 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33163 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33164 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33165 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33166 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33167 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33168 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33169 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33170 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33171 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33172 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33173 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33174 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33175 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33176 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33177 \t\t Training Loss: 0.0005808054702356458 \t\n",
      "Epoch 33178 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33179 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33180 \t\t Training Loss: 0.0005808055866509676 \t\n",
      "Epoch 33181 \t\t Training Loss: 0.0005808054702356458 \t\n",
      "Epoch 33182 \t\t Training Loss: 0.0005808054702356458 \t\n",
      "Epoch 33183 \t\t Training Loss: 0.0005808054702356458 \t\n",
      "Epoch 33184 \t\t Training Loss: 0.0005808054702356458 \t\n",
      "Epoch 33185 \t\t Training Loss: 0.0005808054702356458 \t\n",
      "Epoch 33186 \t\t Training Loss: 0.0005808054702356458 \t\n",
      "Epoch 33187 \t\t Training Loss: 0.0005808054702356458 \t\n",
      "Epoch 33188 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33189 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33190 \t\t Training Loss: 0.0005808054120279849 \t\n",
      "Epoch 33191 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33192 \t\t Training Loss: 0.0005808054702356458 \t\n",
      "Epoch 33193 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33194 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33195 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33196 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33197 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33198 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33199 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33200 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33201 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33202 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33203 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33204 \t\t Training Loss: 0.000580805295612663 \t\n",
      "Epoch 33205 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33206 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33207 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33208 \t\t Training Loss: 0.000580805295612663 \t\n",
      "Epoch 33209 \t\t Training Loss: 0.000580805295612663 \t\n",
      "Epoch 33210 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33211 \t\t Training Loss: 0.0005808053538203239 \t\n",
      "Epoch 33212 \t\t Training Loss: 0.000580805295612663 \t\n",
      "Epoch 33213 \t\t Training Loss: 0.000580805295612663 \t\n",
      "Epoch 33214 \t\t Training Loss: 0.000580805295612663 \t\n",
      "Epoch 33215 \t\t Training Loss: 0.000580805295612663 \t\n",
      "Epoch 33216 \t\t Training Loss: 0.000580805295612663 \t\n",
      "Epoch 33217 \t\t Training Loss: 0.000580805295612663 \t\n",
      "Epoch 33218 \t\t Training Loss: 0.000580805295612663 \t\n",
      "Epoch 33219 \t\t Training Loss: 0.000580805295612663 \t\n",
      "Epoch 33220 \t\t Training Loss: 0.000580805295612663 \t\n",
      "Epoch 33221 \t\t Training Loss: 0.0005808051791973412 \t\n",
      "Epoch 33222 \t\t Training Loss: 0.0005808051791973412 \t\n",
      "Epoch 33223 \t\t Training Loss: 0.0005808051791973412 \t\n",
      "Epoch 33224 \t\t Training Loss: 0.0005808051209896803 \t\n",
      "Epoch 33225 \t\t Training Loss: 0.0005808051209896803 \t\n",
      "Epoch 33226 \t\t Training Loss: 0.0005808051209896803 \t\n",
      "Epoch 33227 \t\t Training Loss: 0.0005808051209896803 \t\n",
      "Epoch 33228 \t\t Training Loss: 0.0005808050045743585 \t\n",
      "Epoch 33229 \t\t Training Loss: 0.0005808050045743585 \t\n",
      "Epoch 33230 \t\t Training Loss: 0.0005808050045743585 \t\n",
      "Epoch 33231 \t\t Training Loss: 0.0005808050045743585 \t\n",
      "Epoch 33232 \t\t Training Loss: 0.0005808050045743585 \t\n",
      "Epoch 33233 \t\t Training Loss: 0.0005808050045743585 \t\n",
      "Epoch 33234 \t\t Training Loss: 0.0005808050045743585 \t\n",
      "Epoch 33235 \t\t Training Loss: 0.0005808050045743585 \t\n",
      "Epoch 33236 \t\t Training Loss: 0.0005808050045743585 \t\n",
      "Epoch 33237 \t\t Training Loss: 0.0005808050045743585 \t\n",
      "Epoch 33238 \t\t Training Loss: 0.0005808050045743585 \t\n",
      "Epoch 33239 \t\t Training Loss: 0.0005808050045743585 \t\n",
      "Epoch 33240 \t\t Training Loss: 0.0005808048881590366 \t\n",
      "Epoch 33241 \t\t Training Loss: 0.0005808048881590366 \t\n",
      "Epoch 33242 \t\t Training Loss: 0.0005808048881590366 \t\n",
      "Epoch 33243 \t\t Training Loss: 0.0005808048881590366 \t\n",
      "Epoch 33244 \t\t Training Loss: 0.0005808048881590366 \t\n",
      "Epoch 33245 \t\t Training Loss: 0.0005808048881590366 \t\n",
      "Epoch 33246 \t\t Training Loss: 0.0005808048881590366 \t\n",
      "Epoch 33247 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33248 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33249 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33250 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33251 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33252 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33253 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33254 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33255 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33256 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33257 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33258 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33259 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33260 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33261 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33262 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33263 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33264 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33265 \t\t Training Loss: 0.0005808048881590366 \t\n",
      "Epoch 33266 \t\t Training Loss: 0.0005808048881590366 \t\n",
      "Epoch 33267 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33268 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33269 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33270 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33271 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33272 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33273 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33274 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33275 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33276 \t\t Training Loss: 0.0005808047717437148 \t\n",
      "Epoch 33277 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33278 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33279 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33280 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33281 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33282 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33283 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33284 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33285 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33286 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33287 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33288 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33289 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33290 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33291 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33292 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33293 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33294 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33295 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33296 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33297 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33298 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33299 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33300 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33301 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33302 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33303 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33304 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33305 \t\t Training Loss: 0.0005808047135360539 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33306 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33307 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33308 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33309 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33310 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33311 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33312 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33313 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33314 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33315 \t\t Training Loss: 0.0005808047135360539 \t\n",
      "Epoch 33316 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33317 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33318 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33319 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33320 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33321 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33322 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33323 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33324 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33325 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33326 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33327 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33328 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33329 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33330 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33331 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33332 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33333 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33334 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33335 \t\t Training Loss: 0.0005808045971207321 \t\n",
      "Epoch 33336 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33337 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33338 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33339 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33340 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33341 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33342 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33343 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33344 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33345 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33346 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33347 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33348 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33349 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33350 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33351 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33352 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33353 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33354 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33355 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33356 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33357 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33358 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33359 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33360 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33361 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33362 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33363 \t\t Training Loss: 0.0005808044807054102 \t\n",
      "Epoch 33364 \t\t Training Loss: 0.0005808044224977493 \t\n",
      "Epoch 33365 \t\t Training Loss: 0.0005808044224977493 \t\n",
      "Epoch 33366 \t\t Training Loss: 0.0005808044224977493 \t\n",
      "Epoch 33367 \t\t Training Loss: 0.0005808044224977493 \t\n",
      "Epoch 33368 \t\t Training Loss: 0.0005808044224977493 \t\n",
      "Epoch 33369 \t\t Training Loss: 0.0005808044224977493 \t\n",
      "Epoch 33370 \t\t Training Loss: 0.0005808044224977493 \t\n",
      "Epoch 33371 \t\t Training Loss: 0.0005808044224977493 \t\n",
      "Epoch 33372 \t\t Training Loss: 0.0005808043060824275 \t\n",
      "Epoch 33373 \t\t Training Loss: 0.0005808043060824275 \t\n",
      "Epoch 33374 \t\t Training Loss: 0.0005808043060824275 \t\n",
      "Epoch 33375 \t\t Training Loss: 0.0005808044224977493 \t\n",
      "Epoch 33376 \t\t Training Loss: 0.0005808043060824275 \t\n",
      "Epoch 33377 \t\t Training Loss: 0.0005808043060824275 \t\n",
      "Epoch 33378 \t\t Training Loss: 0.0005808043060824275 \t\n",
      "Epoch 33379 \t\t Training Loss: 0.0005808043060824275 \t\n",
      "Epoch 33380 \t\t Training Loss: 0.0005808043060824275 \t\n",
      "Epoch 33381 \t\t Training Loss: 0.0005808043060824275 \t\n",
      "Epoch 33382 \t\t Training Loss: 0.0005808043060824275 \t\n",
      "Epoch 33383 \t\t Training Loss: 0.0005808043060824275 \t\n",
      "Epoch 33384 \t\t Training Loss: 0.0005808041896671057 \t\n",
      "Epoch 33385 \t\t Training Loss: 0.0005808041896671057 \t\n",
      "Epoch 33386 \t\t Training Loss: 0.0005808041896671057 \t\n",
      "Epoch 33387 \t\t Training Loss: 0.0005808041896671057 \t\n",
      "Epoch 33388 \t\t Training Loss: 0.0005808041896671057 \t\n",
      "Epoch 33389 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33390 \t\t Training Loss: 0.0005808041896671057 \t\n",
      "Epoch 33391 \t\t Training Loss: 0.0005808041896671057 \t\n",
      "Epoch 33392 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33393 \t\t Training Loss: 0.0005808041896671057 \t\n",
      "Epoch 33394 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33395 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33396 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33397 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33398 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33399 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33400 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33401 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33402 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33403 \t\t Training Loss: 0.0005808041896671057 \t\n",
      "Epoch 33404 \t\t Training Loss: 0.0005808041896671057 \t\n",
      "Epoch 33405 \t\t Training Loss: 0.0005808041896671057 \t\n",
      "Epoch 33406 \t\t Training Loss: 0.0005808041896671057 \t\n",
      "Epoch 33407 \t\t Training Loss: 0.0005808041896671057 \t\n",
      "Epoch 33408 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33409 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33410 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33411 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33412 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33413 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33414 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33415 \t\t Training Loss: 0.0005808041896671057 \t\n",
      "Epoch 33416 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33417 \t\t Training Loss: 0.0005808041896671057 \t\n",
      "Epoch 33418 \t\t Training Loss: 0.0005808041896671057 \t\n",
      "Epoch 33419 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33420 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33421 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33422 \t\t Training Loss: 0.0005808040732517838 \t\n",
      "Epoch 33423 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33424 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33425 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33426 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33427 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33428 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33429 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33430 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33431 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33432 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33433 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33434 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33435 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33436 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33437 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33438 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33439 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33440 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33441 \t\t Training Loss: 0.0005808040150441229 \t\n",
      "Epoch 33442 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33443 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33444 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33445 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33446 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33447 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33448 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33449 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33450 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33451 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33452 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33453 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33454 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33455 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33456 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33457 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33458 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33459 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33460 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33461 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33462 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33463 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33464 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33465 \t\t Training Loss: 0.0005808038986288011 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33466 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33467 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33468 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33469 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33470 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33471 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33472 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33473 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33474 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33475 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33476 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33477 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33478 \t\t Training Loss: 0.0005808038986288011 \t\n",
      "Epoch 33479 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33480 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33481 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33482 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33483 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33484 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33485 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33486 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33487 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33488 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33489 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33490 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33491 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33492 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33493 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33494 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33495 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33496 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33497 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33498 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33499 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33500 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33501 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33502 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33503 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33504 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33505 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33506 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33507 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33508 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33509 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33510 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33511 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33512 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33513 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33514 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33515 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33516 \t\t Training Loss: 0.0005808037240058184 \t\n",
      "Epoch 33517 \t\t Training Loss: 0.0005808037822134793 \t\n",
      "Epoch 33518 \t\t Training Loss: 0.0005808036075904965 \t\n",
      "Epoch 33519 \t\t Training Loss: 0.0005808036075904965 \t\n",
      "Epoch 33520 \t\t Training Loss: 0.0005808036075904965 \t\n",
      "Epoch 33521 \t\t Training Loss: 0.0005808036075904965 \t\n",
      "Epoch 33522 \t\t Training Loss: 0.0005808036075904965 \t\n",
      "Epoch 33523 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33524 \t\t Training Loss: 0.0005808036075904965 \t\n",
      "Epoch 33525 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33526 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33527 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33528 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33529 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33530 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33531 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33532 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33533 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33534 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33535 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33536 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33537 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33538 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33539 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33540 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33541 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33542 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33543 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33544 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33545 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33546 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33547 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33548 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33549 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33550 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33551 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33552 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33553 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33554 \t\t Training Loss: 0.0005808034911751747 \t\n",
      "Epoch 33555 \t\t Training Loss: 0.0005808034329675138 \t\n",
      "Epoch 33556 \t\t Training Loss: 0.0005808034329675138 \t\n",
      "Epoch 33557 \t\t Training Loss: 0.0005808034329675138 \t\n",
      "Epoch 33558 \t\t Training Loss: 0.0005808034329675138 \t\n",
      "Epoch 33559 \t\t Training Loss: 0.0005808034329675138 \t\n",
      "Epoch 33560 \t\t Training Loss: 0.0005808034329675138 \t\n",
      "Epoch 33561 \t\t Training Loss: 0.0005808034329675138 \t\n",
      "Epoch 33562 \t\t Training Loss: 0.0005808034329675138 \t\n",
      "Epoch 33563 \t\t Training Loss: 0.0005808033747598529 \t\n",
      "Epoch 33564 \t\t Training Loss: 0.0005808033747598529 \t\n",
      "Epoch 33565 \t\t Training Loss: 0.0005808033747598529 \t\n",
      "Epoch 33566 \t\t Training Loss: 0.0005808033747598529 \t\n",
      "Epoch 33567 \t\t Training Loss: 0.0005808033747598529 \t\n",
      "Epoch 33568 \t\t Training Loss: 0.0005808033747598529 \t\n",
      "Epoch 33569 \t\t Training Loss: 0.000580803316552192 \t\n",
      "Epoch 33570 \t\t Training Loss: 0.000580803316552192 \t\n",
      "Epoch 33571 \t\t Training Loss: 0.000580803316552192 \t\n",
      "Epoch 33572 \t\t Training Loss: 0.000580803316552192 \t\n",
      "Epoch 33573 \t\t Training Loss: 0.000580803316552192 \t\n",
      "Epoch 33574 \t\t Training Loss: 0.000580803316552192 \t\n",
      "Epoch 33575 \t\t Training Loss: 0.000580803316552192 \t\n",
      "Epoch 33576 \t\t Training Loss: 0.000580803316552192 \t\n",
      "Epoch 33577 \t\t Training Loss: 0.000580803316552192 \t\n",
      "Epoch 33578 \t\t Training Loss: 0.000580803316552192 \t\n",
      "Epoch 33579 \t\t Training Loss: 0.000580803316552192 \t\n",
      "Epoch 33580 \t\t Training Loss: 0.000580803316552192 \t\n",
      "Epoch 33581 \t\t Training Loss: 0.000580803316552192 \t\n",
      "Epoch 33582 \t\t Training Loss: 0.000580803316552192 \t\n",
      "Epoch 33583 \t\t Training Loss: 0.000580803316552192 \t\n",
      "Epoch 33584 \t\t Training Loss: 0.0005808032001368701 \t\n",
      "Epoch 33585 \t\t Training Loss: 0.0005808032001368701 \t\n",
      "Epoch 33586 \t\t Training Loss: 0.0005808032001368701 \t\n",
      "Epoch 33587 \t\t Training Loss: 0.0005808032001368701 \t\n",
      "Epoch 33588 \t\t Training Loss: 0.0005808032001368701 \t\n",
      "Epoch 33589 \t\t Training Loss: 0.0005808032001368701 \t\n",
      "Epoch 33590 \t\t Training Loss: 0.0005808032001368701 \t\n",
      "Epoch 33591 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33592 \t\t Training Loss: 0.0005808031419292092 \t\n",
      "Epoch 33593 \t\t Training Loss: 0.0005808032001368701 \t\n",
      "Epoch 33594 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33595 \t\t Training Loss: 0.0005808032001368701 \t\n",
      "Epoch 33596 \t\t Training Loss: 0.0005808032001368701 \t\n",
      "Epoch 33597 \t\t Training Loss: 0.0005808032001368701 \t\n",
      "Epoch 33598 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33599 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33600 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33601 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33602 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33603 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33604 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33605 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33606 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33607 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33608 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33609 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33610 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33611 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33612 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33613 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33614 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33615 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33616 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33617 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33618 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33619 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33620 \t\t Training Loss: 0.0005808030255138874 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33621 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33622 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33623 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33624 \t\t Training Loss: 0.0005808030837215483 \t\n",
      "Epoch 33625 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33626 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33627 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33628 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33629 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33630 \t\t Training Loss: 0.0005808030255138874 \t\n",
      "Epoch 33631 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33632 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33633 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33634 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33635 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33636 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33637 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33638 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33639 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33640 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33641 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33642 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33643 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33644 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33645 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33646 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33647 \t\t Training Loss: 0.0005808029090985656 \t\n",
      "Epoch 33648 \t\t Training Loss: 0.0005808028508909047 \t\n",
      "Epoch 33649 \t\t Training Loss: 0.0005808028508909047 \t\n",
      "Epoch 33650 \t\t Training Loss: 0.0005808028508909047 \t\n",
      "Epoch 33651 \t\t Training Loss: 0.0005808027926832438 \t\n",
      "Epoch 33652 \t\t Training Loss: 0.0005808028508909047 \t\n",
      "Epoch 33653 \t\t Training Loss: 0.0005808027926832438 \t\n",
      "Epoch 33654 \t\t Training Loss: 0.0005808028508909047 \t\n",
      "Epoch 33655 \t\t Training Loss: 0.0005808028508909047 \t\n",
      "Epoch 33656 \t\t Training Loss: 0.0005808028508909047 \t\n",
      "Epoch 33657 \t\t Training Loss: 0.0005808028508909047 \t\n",
      "Epoch 33658 \t\t Training Loss: 0.0005808028508909047 \t\n",
      "Epoch 33659 \t\t Training Loss: 0.0005808028508909047 \t\n",
      "Epoch 33660 \t\t Training Loss: 0.0005808028508909047 \t\n",
      "Epoch 33661 \t\t Training Loss: 0.0005808027926832438 \t\n",
      "Epoch 33662 \t\t Training Loss: 0.0005808027926832438 \t\n",
      "Epoch 33663 \t\t Training Loss: 0.0005808027926832438 \t\n",
      "Epoch 33664 \t\t Training Loss: 0.0005808027926832438 \t\n",
      "Epoch 33665 \t\t Training Loss: 0.0005808027926832438 \t\n",
      "Epoch 33666 \t\t Training Loss: 0.0005808027926832438 \t\n",
      "Epoch 33667 \t\t Training Loss: 0.0005808027926832438 \t\n",
      "Epoch 33668 \t\t Training Loss: 0.0005808027926832438 \t\n",
      "Epoch 33669 \t\t Training Loss: 0.0005808027926832438 \t\n",
      "Epoch 33670 \t\t Training Loss: 0.0005808027926832438 \t\n",
      "Epoch 33671 \t\t Training Loss: 0.0005808027344755828 \t\n",
      "Epoch 33672 \t\t Training Loss: 0.0005808027344755828 \t\n",
      "Epoch 33673 \t\t Training Loss: 0.0005808027344755828 \t\n",
      "Epoch 33674 \t\t Training Loss: 0.0005808027344755828 \t\n",
      "Epoch 33675 \t\t Training Loss: 0.0005808027344755828 \t\n",
      "Epoch 33676 \t\t Training Loss: 0.0005808027344755828 \t\n",
      "Epoch 33677 \t\t Training Loss: 0.0005808027344755828 \t\n",
      "Epoch 33678 \t\t Training Loss: 0.0005808027344755828 \t\n",
      "Epoch 33679 \t\t Training Loss: 0.0005808027344755828 \t\n",
      "Epoch 33680 \t\t Training Loss: 0.0005808027344755828 \t\n",
      "Epoch 33681 \t\t Training Loss: 0.0005808026762679219 \t\n",
      "Epoch 33682 \t\t Training Loss: 0.0005808026762679219 \t\n",
      "Epoch 33683 \t\t Training Loss: 0.0005808026762679219 \t\n",
      "Epoch 33684 \t\t Training Loss: 0.0005808026762679219 \t\n",
      "Epoch 33685 \t\t Training Loss: 0.0005808026762679219 \t\n",
      "Epoch 33686 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33687 \t\t Training Loss: 0.0005808026762679219 \t\n",
      "Epoch 33688 \t\t Training Loss: 0.0005808026762679219 \t\n",
      "Epoch 33689 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33690 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33691 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33692 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33693 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33694 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33695 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33696 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33697 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33698 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33699 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33700 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33701 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33702 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33703 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33704 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33705 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33706 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33707 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33708 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33709 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33710 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33711 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33712 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33713 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33714 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33715 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33716 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33717 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33718 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33719 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33720 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33721 \t\t Training Loss: 0.0005808023852296174 \t\n",
      "Epoch 33722 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33723 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33724 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33725 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33726 \t\t Training Loss: 0.0005808025016449392 \t\n",
      "Epoch 33727 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33728 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33729 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33730 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33731 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33732 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33733 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33734 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33735 \t\t Training Loss: 0.0005808024434372783 \t\n",
      "Epoch 33736 \t\t Training Loss: 0.0005808023852296174 \t\n",
      "Epoch 33737 \t\t Training Loss: 0.0005808023852296174 \t\n",
      "Epoch 33738 \t\t Training Loss: 0.0005808023852296174 \t\n",
      "Epoch 33739 \t\t Training Loss: 0.0005808023852296174 \t\n",
      "Epoch 33740 \t\t Training Loss: 0.0005808023852296174 \t\n",
      "Epoch 33741 \t\t Training Loss: 0.0005808023852296174 \t\n",
      "Epoch 33742 \t\t Training Loss: 0.0005808023852296174 \t\n",
      "Epoch 33743 \t\t Training Loss: 0.0005808023852296174 \t\n",
      "Epoch 33744 \t\t Training Loss: 0.0005808023852296174 \t\n",
      "Epoch 33745 \t\t Training Loss: 0.0005808023852296174 \t\n",
      "Epoch 33746 \t\t Training Loss: 0.0005808023852296174 \t\n",
      "Epoch 33747 \t\t Training Loss: 0.0005808023852296174 \t\n",
      "Epoch 33748 \t\t Training Loss: 0.0005808023852296174 \t\n",
      "Epoch 33749 \t\t Training Loss: 0.0005808023852296174 \t\n",
      "Epoch 33750 \t\t Training Loss: 0.0005808023852296174 \t\n",
      "Epoch 33751 \t\t Training Loss: 0.0005808023270219564 \t\n",
      "Epoch 33752 \t\t Training Loss: 0.0005808023270219564 \t\n",
      "Epoch 33753 \t\t Training Loss: 0.0005808023270219564 \t\n",
      "Epoch 33754 \t\t Training Loss: 0.0005808023270219564 \t\n",
      "Epoch 33755 \t\t Training Loss: 0.0005808023270219564 \t\n",
      "Epoch 33756 \t\t Training Loss: 0.0005808023270219564 \t\n",
      "Epoch 33757 \t\t Training Loss: 0.0005808023270219564 \t\n",
      "Epoch 33758 \t\t Training Loss: 0.0005808023270219564 \t\n",
      "Epoch 33759 \t\t Training Loss: 0.0005808023270219564 \t\n",
      "Epoch 33760 \t\t Training Loss: 0.0005808022688142955 \t\n",
      "Epoch 33761 \t\t Training Loss: 0.0005808023270219564 \t\n",
      "Epoch 33762 \t\t Training Loss: 0.0005808022688142955 \t\n",
      "Epoch 33763 \t\t Training Loss: 0.0005808022688142955 \t\n",
      "Epoch 33764 \t\t Training Loss: 0.0005808022688142955 \t\n",
      "Epoch 33765 \t\t Training Loss: 0.0005808022688142955 \t\n",
      "Epoch 33766 \t\t Training Loss: 0.0005808022688142955 \t\n",
      "Epoch 33767 \t\t Training Loss: 0.0005808022688142955 \t\n",
      "Epoch 33768 \t\t Training Loss: 0.0005808022688142955 \t\n",
      "Epoch 33769 \t\t Training Loss: 0.0005808022688142955 \t\n",
      "Epoch 33770 \t\t Training Loss: 0.0005808022688142955 \t\n",
      "Epoch 33771 \t\t Training Loss: 0.0005808022106066346 \t\n",
      "Epoch 33772 \t\t Training Loss: 0.0005808022688142955 \t\n",
      "Epoch 33773 \t\t Training Loss: 0.0005808022688142955 \t\n",
      "Epoch 33774 \t\t Training Loss: 0.0005808022688142955 \t\n",
      "Epoch 33775 \t\t Training Loss: 0.0005808022688142955 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33776 \t\t Training Loss: 0.0005808022688142955 \t\n",
      "Epoch 33777 \t\t Training Loss: 0.0005808022688142955 \t\n",
      "Epoch 33778 \t\t Training Loss: 0.0005808021523989737 \t\n",
      "Epoch 33779 \t\t Training Loss: 0.0005808022106066346 \t\n",
      "Epoch 33780 \t\t Training Loss: 0.0005808022106066346 \t\n",
      "Epoch 33781 \t\t Training Loss: 0.0005808022106066346 \t\n",
      "Epoch 33782 \t\t Training Loss: 0.0005808021523989737 \t\n",
      "Epoch 33783 \t\t Training Loss: 0.0005808021523989737 \t\n",
      "Epoch 33784 \t\t Training Loss: 0.0005808021523989737 \t\n",
      "Epoch 33785 \t\t Training Loss: 0.0005808021523989737 \t\n",
      "Epoch 33786 \t\t Training Loss: 0.0005808021523989737 \t\n",
      "Epoch 33787 \t\t Training Loss: 0.0005808021523989737 \t\n",
      "Epoch 33788 \t\t Training Loss: 0.0005808021523989737 \t\n",
      "Epoch 33789 \t\t Training Loss: 0.0005808021523989737 \t\n",
      "Epoch 33790 \t\t Training Loss: 0.0005808021523989737 \t\n",
      "Epoch 33791 \t\t Training Loss: 0.0005808020941913128 \t\n",
      "Epoch 33792 \t\t Training Loss: 0.0005808021523989737 \t\n",
      "Epoch 33793 \t\t Training Loss: 0.0005808021523989737 \t\n",
      "Epoch 33794 \t\t Training Loss: 0.0005808021523989737 \t\n",
      "Epoch 33795 \t\t Training Loss: 0.0005808020941913128 \t\n",
      "Epoch 33796 \t\t Training Loss: 0.0005808020941913128 \t\n",
      "Epoch 33797 \t\t Training Loss: 0.0005808020359836519 \t\n",
      "Epoch 33798 \t\t Training Loss: 0.0005808020359836519 \t\n",
      "Epoch 33799 \t\t Training Loss: 0.0005808020359836519 \t\n",
      "Epoch 33800 \t\t Training Loss: 0.00058080191956833 \t\n",
      "Epoch 33801 \t\t Training Loss: 0.00058080191956833 \t\n",
      "Epoch 33802 \t\t Training Loss: 0.0005808018613606691 \t\n",
      "Epoch 33803 \t\t Training Loss: 0.0005808018613606691 \t\n",
      "Epoch 33804 \t\t Training Loss: 0.0005808018613606691 \t\n",
      "Epoch 33805 \t\t Training Loss: 0.0005808018613606691 \t\n",
      "Epoch 33806 \t\t Training Loss: 0.0005808018613606691 \t\n",
      "Epoch 33807 \t\t Training Loss: 0.0005808018613606691 \t\n",
      "Epoch 33808 \t\t Training Loss: 0.0005808018613606691 \t\n",
      "Epoch 33809 \t\t Training Loss: 0.0005808018031530082 \t\n",
      "Epoch 33810 \t\t Training Loss: 0.0005808018031530082 \t\n",
      "Epoch 33811 \t\t Training Loss: 0.0005808018031530082 \t\n",
      "Epoch 33812 \t\t Training Loss: 0.0005808018031530082 \t\n",
      "Epoch 33813 \t\t Training Loss: 0.0005808018031530082 \t\n",
      "Epoch 33814 \t\t Training Loss: 0.0005808017449453473 \t\n",
      "Epoch 33815 \t\t Training Loss: 0.0005808017449453473 \t\n",
      "Epoch 33816 \t\t Training Loss: 0.0005808017449453473 \t\n",
      "Epoch 33817 \t\t Training Loss: 0.0005808017449453473 \t\n",
      "Epoch 33818 \t\t Training Loss: 0.0005808017449453473 \t\n",
      "Epoch 33819 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33820 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33821 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33822 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33823 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33824 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33825 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33826 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33827 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33828 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33829 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33830 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33831 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33832 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33833 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33834 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33835 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33836 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33837 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33838 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33839 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33840 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33841 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33842 \t\t Training Loss: 0.0005808017449453473 \t\n",
      "Epoch 33843 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33844 \t\t Training Loss: 0.0005808017449453473 \t\n",
      "Epoch 33845 \t\t Training Loss: 0.0005808017449453473 \t\n",
      "Epoch 33846 \t\t Training Loss: 0.0005808017449453473 \t\n",
      "Epoch 33847 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33848 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33849 \t\t Training Loss: 0.0005808017449453473 \t\n",
      "Epoch 33850 \t\t Training Loss: 0.0005808017449453473 \t\n",
      "Epoch 33851 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33852 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33853 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33854 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33855 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33856 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33857 \t\t Training Loss: 0.0005808016867376864 \t\n",
      "Epoch 33858 \t\t Training Loss: 0.0005808016285300255 \t\n",
      "Epoch 33859 \t\t Training Loss: 0.0005808016285300255 \t\n",
      "Epoch 33860 \t\t Training Loss: 0.0005808016285300255 \t\n",
      "Epoch 33861 \t\t Training Loss: 0.0005808016285300255 \t\n",
      "Epoch 33862 \t\t Training Loss: 0.0005808016285300255 \t\n",
      "Epoch 33863 \t\t Training Loss: 0.0005808016285300255 \t\n",
      "Epoch 33864 \t\t Training Loss: 0.0005808016285300255 \t\n",
      "Epoch 33865 \t\t Training Loss: 0.0005808016285300255 \t\n",
      "Epoch 33866 \t\t Training Loss: 0.0005808016285300255 \t\n",
      "Epoch 33867 \t\t Training Loss: 0.0005808015703223646 \t\n",
      "Epoch 33868 \t\t Training Loss: 0.0005808015703223646 \t\n",
      "Epoch 33869 \t\t Training Loss: 0.0005808015703223646 \t\n",
      "Epoch 33870 \t\t Training Loss: 0.0005808015703223646 \t\n",
      "Epoch 33871 \t\t Training Loss: 0.0005808015121147037 \t\n",
      "Epoch 33872 \t\t Training Loss: 0.0005808015121147037 \t\n",
      "Epoch 33873 \t\t Training Loss: 0.0005808015703223646 \t\n",
      "Epoch 33874 \t\t Training Loss: 0.0005808015121147037 \t\n",
      "Epoch 33875 \t\t Training Loss: 0.0005808015121147037 \t\n",
      "Epoch 33876 \t\t Training Loss: 0.0005808015121147037 \t\n",
      "Epoch 33877 \t\t Training Loss: 0.0005808015121147037 \t\n",
      "Epoch 33878 \t\t Training Loss: 0.0005808015121147037 \t\n",
      "Epoch 33879 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33880 \t\t Training Loss: 0.0005808015121147037 \t\n",
      "Epoch 33881 \t\t Training Loss: 0.0005808015121147037 \t\n",
      "Epoch 33882 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33883 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33884 \t\t Training Loss: 0.0005808015121147037 \t\n",
      "Epoch 33885 \t\t Training Loss: 0.0005808015121147037 \t\n",
      "Epoch 33886 \t\t Training Loss: 0.0005808015121147037 \t\n",
      "Epoch 33887 \t\t Training Loss: 0.0005808015121147037 \t\n",
      "Epoch 33888 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33889 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33890 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33891 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33892 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33893 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33894 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33895 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33896 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33897 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33898 \t\t Training Loss: 0.0005808013374917209 \t\n",
      "Epoch 33899 \t\t Training Loss: 0.0005808013374917209 \t\n",
      "Epoch 33900 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33901 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33902 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33903 \t\t Training Loss: 0.0005808014539070427 \t\n",
      "Epoch 33904 \t\t Training Loss: 0.0005808013956993818 \t\n",
      "Epoch 33905 \t\t Training Loss: 0.0005808013956993818 \t\n",
      "Epoch 33906 \t\t Training Loss: 0.0005808013956993818 \t\n",
      "Epoch 33907 \t\t Training Loss: 0.0005808013374917209 \t\n",
      "Epoch 33908 \t\t Training Loss: 0.0005808013374917209 \t\n",
      "Epoch 33909 \t\t Training Loss: 0.0005808013374917209 \t\n",
      "Epoch 33910 \t\t Training Loss: 0.0005808013374917209 \t\n",
      "Epoch 33911 \t\t Training Loss: 0.00058080127928406 \t\n",
      "Epoch 33912 \t\t Training Loss: 0.00058080127928406 \t\n",
      "Epoch 33913 \t\t Training Loss: 0.00058080127928406 \t\n",
      "Epoch 33914 \t\t Training Loss: 0.00058080127928406 \t\n",
      "Epoch 33915 \t\t Training Loss: 0.00058080127928406 \t\n",
      "Epoch 33916 \t\t Training Loss: 0.0005808012210763991 \t\n",
      "Epoch 33917 \t\t Training Loss: 0.00058080127928406 \t\n",
      "Epoch 33918 \t\t Training Loss: 0.0005808012210763991 \t\n",
      "Epoch 33919 \t\t Training Loss: 0.0005808012210763991 \t\n",
      "Epoch 33920 \t\t Training Loss: 0.00058080127928406 \t\n",
      "Epoch 33921 \t\t Training Loss: 0.0005808012210763991 \t\n",
      "Epoch 33922 \t\t Training Loss: 0.0005808012210763991 \t\n",
      "Epoch 33923 \t\t Training Loss: 0.0005808012210763991 \t\n",
      "Epoch 33924 \t\t Training Loss: 0.0005808012210763991 \t\n",
      "Epoch 33925 \t\t Training Loss: 0.0005808012210763991 \t\n",
      "Epoch 33926 \t\t Training Loss: 0.0005808011628687382 \t\n",
      "Epoch 33927 \t\t Training Loss: 0.0005808011628687382 \t\n",
      "Epoch 33928 \t\t Training Loss: 0.0005808011628687382 \t\n",
      "Epoch 33929 \t\t Training Loss: 0.0005808011628687382 \t\n",
      "Epoch 33930 \t\t Training Loss: 0.0005808011628687382 \t\n",
      "Epoch 33931 \t\t Training Loss: 0.0005808011628687382 \t\n",
      "Epoch 33932 \t\t Training Loss: 0.0005808011628687382 \t\n",
      "Epoch 33933 \t\t Training Loss: 0.0005808011046610773 \t\n",
      "Epoch 33934 \t\t Training Loss: 0.0005808011046610773 \t\n",
      "Epoch 33935 \t\t Training Loss: 0.0005808011046610773 \t\n",
      "Epoch 33936 \t\t Training Loss: 0.0005808011046610773 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33937 \t\t Training Loss: 0.0005808011046610773 \t\n",
      "Epoch 33938 \t\t Training Loss: 0.0005808011046610773 \t\n",
      "Epoch 33939 \t\t Training Loss: 0.0005808011046610773 \t\n",
      "Epoch 33940 \t\t Training Loss: 0.0005808011046610773 \t\n",
      "Epoch 33941 \t\t Training Loss: 0.0005808011046610773 \t\n",
      "Epoch 33942 \t\t Training Loss: 0.0005808011046610773 \t\n",
      "Epoch 33943 \t\t Training Loss: 0.0005808011046610773 \t\n",
      "Epoch 33944 \t\t Training Loss: 0.0005808011046610773 \t\n",
      "Epoch 33945 \t\t Training Loss: 0.0005808011046610773 \t\n",
      "Epoch 33946 \t\t Training Loss: 0.0005808011046610773 \t\n",
      "Epoch 33947 \t\t Training Loss: 0.0005808011046610773 \t\n",
      "Epoch 33948 \t\t Training Loss: 0.0005808010464534163 \t\n",
      "Epoch 33949 \t\t Training Loss: 0.0005808010464534163 \t\n",
      "Epoch 33950 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33951 \t\t Training Loss: 0.0005808010464534163 \t\n",
      "Epoch 33952 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33953 \t\t Training Loss: 0.0005808010464534163 \t\n",
      "Epoch 33954 \t\t Training Loss: 0.0005808010464534163 \t\n",
      "Epoch 33955 \t\t Training Loss: 0.0005808010464534163 \t\n",
      "Epoch 33956 \t\t Training Loss: 0.0005808010464534163 \t\n",
      "Epoch 33957 \t\t Training Loss: 0.0005808010464534163 \t\n",
      "Epoch 33958 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33959 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33960 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33961 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33962 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33963 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33964 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33965 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33966 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33967 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33968 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33969 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33970 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33971 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33972 \t\t Training Loss: 0.0005808009882457554 \t\n",
      "Epoch 33973 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33974 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33975 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33976 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33977 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33978 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33979 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33980 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33981 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 33982 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 33983 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 33984 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 33985 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33986 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33987 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33988 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33989 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33990 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33991 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33992 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33993 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33994 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33995 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33996 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 33997 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33998 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 33999 \t\t Training Loss: 0.0005808008136227727 \t\n",
      "Epoch 34000 \t\t Training Loss: 0.0005808008136227727 \t\n",
      "Epoch 34001 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 34002 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 34003 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34004 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34005 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34006 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34007 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34008 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34009 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34010 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 34011 \t\t Training Loss: 0.0005808008136227727 \t\n",
      "Epoch 34012 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34013 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34014 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 34015 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 34016 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 34017 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 34018 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 34019 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 34020 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 34021 \t\t Training Loss: 0.0005808008718304336 \t\n",
      "Epoch 34022 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34023 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34024 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34025 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34026 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34027 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34028 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34029 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34030 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34031 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34032 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34033 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34034 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34035 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34036 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34037 \t\t Training Loss: 0.0005808007554151118 \t\n",
      "Epoch 34038 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34039 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34040 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34041 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34042 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34043 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34044 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34045 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34046 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34047 \t\t Training Loss: 0.0005808006972074509 \t\n",
      "Epoch 34048 \t\t Training Loss: 0.00058080063899979 \t\n",
      "Epoch 34049 \t\t Training Loss: 0.00058080063899979 \t\n",
      "Epoch 34050 \t\t Training Loss: 0.00058080063899979 \t\n",
      "Epoch 34051 \t\t Training Loss: 0.00058080063899979 \t\n",
      "Epoch 34052 \t\t Training Loss: 0.000580800580792129 \t\n",
      "Epoch 34053 \t\t Training Loss: 0.000580800580792129 \t\n",
      "Epoch 34054 \t\t Training Loss: 0.000580800580792129 \t\n",
      "Epoch 34055 \t\t Training Loss: 0.000580800580792129 \t\n",
      "Epoch 34056 \t\t Training Loss: 0.000580800580792129 \t\n",
      "Epoch 34057 \t\t Training Loss: 0.000580800580792129 \t\n",
      "Epoch 34058 \t\t Training Loss: 0.000580800580792129 \t\n",
      "Epoch 34059 \t\t Training Loss: 0.0005808004643768072 \t\n",
      "Epoch 34060 \t\t Training Loss: 0.000580800580792129 \t\n",
      "Epoch 34061 \t\t Training Loss: 0.000580800580792129 \t\n",
      "Epoch 34062 \t\t Training Loss: 0.000580800580792129 \t\n",
      "Epoch 34063 \t\t Training Loss: 0.000580800580792129 \t\n",
      "Epoch 34064 \t\t Training Loss: 0.0005808004643768072 \t\n",
      "Epoch 34065 \t\t Training Loss: 0.0005808004643768072 \t\n",
      "Epoch 34066 \t\t Training Loss: 0.0005808004643768072 \t\n",
      "Epoch 34067 \t\t Training Loss: 0.0005808004643768072 \t\n",
      "Epoch 34068 \t\t Training Loss: 0.0005808004643768072 \t\n",
      "Epoch 34069 \t\t Training Loss: 0.0005808004643768072 \t\n",
      "Epoch 34070 \t\t Training Loss: 0.0005808004643768072 \t\n",
      "Epoch 34071 \t\t Training Loss: 0.0005808004643768072 \t\n",
      "Epoch 34072 \t\t Training Loss: 0.0005808004643768072 \t\n",
      "Epoch 34073 \t\t Training Loss: 0.0005808004643768072 \t\n",
      "Epoch 34074 \t\t Training Loss: 0.0005808004643768072 \t\n",
      "Epoch 34075 \t\t Training Loss: 0.0005808004061691463 \t\n",
      "Epoch 34076 \t\t Training Loss: 0.0005808004061691463 \t\n",
      "Epoch 34077 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34078 \t\t Training Loss: 0.0005808004061691463 \t\n",
      "Epoch 34079 \t\t Training Loss: 0.0005808004061691463 \t\n",
      "Epoch 34080 \t\t Training Loss: 0.0005808003479614854 \t\n",
      "Epoch 34081 \t\t Training Loss: 0.0005808003479614854 \t\n",
      "Epoch 34082 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34083 \t\t Training Loss: 0.0005808003479614854 \t\n",
      "Epoch 34084 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34085 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34086 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34087 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34088 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34089 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34090 \t\t Training Loss: 0.0005808003479614854 \t\n",
      "Epoch 34091 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34092 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34093 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34094 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34095 \t\t Training Loss: 0.0005808002897538245 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34096 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34097 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34098 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34099 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34100 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34101 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34102 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34103 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34104 \t\t Training Loss: 0.0005808002315461636 \t\n",
      "Epoch 34105 \t\t Training Loss: 0.0005808002315461636 \t\n",
      "Epoch 34106 \t\t Training Loss: 0.0005808002315461636 \t\n",
      "Epoch 34107 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34108 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34109 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34110 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34111 \t\t Training Loss: 0.0005808002315461636 \t\n",
      "Epoch 34112 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34113 \t\t Training Loss: 0.0005808002315461636 \t\n",
      "Epoch 34114 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34115 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34116 \t\t Training Loss: 0.0005808002897538245 \t\n",
      "Epoch 34117 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34118 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34119 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34120 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34121 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34122 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34123 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34124 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34125 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34126 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34127 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34128 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34129 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34130 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34131 \t\t Training Loss: 0.0005808001151308417 \t\n",
      "Epoch 34132 \t\t Training Loss: 0.0005808001151308417 \t\n",
      "Epoch 34133 \t\t Training Loss: 0.0005808001151308417 \t\n",
      "Epoch 34134 \t\t Training Loss: 0.0005808001151308417 \t\n",
      "Epoch 34135 \t\t Training Loss: 0.0005808001151308417 \t\n",
      "Epoch 34136 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34137 \t\t Training Loss: 0.0005808001151308417 \t\n",
      "Epoch 34138 \t\t Training Loss: 0.0005808001151308417 \t\n",
      "Epoch 34139 \t\t Training Loss: 0.0005808001151308417 \t\n",
      "Epoch 34140 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34141 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34142 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34143 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34144 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34145 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34146 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34147 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34148 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34149 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34150 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34151 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34152 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34153 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34154 \t\t Training Loss: 0.0005808001151308417 \t\n",
      "Epoch 34155 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34156 \t\t Training Loss: 0.0005808001151308417 \t\n",
      "Epoch 34157 \t\t Training Loss: 0.0005808001733385026 \t\n",
      "Epoch 34158 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34159 \t\t Training Loss: 0.0005808001151308417 \t\n",
      "Epoch 34160 \t\t Training Loss: 0.0005808001151308417 \t\n",
      "Epoch 34161 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34162 \t\t Training Loss: 0.0005808001151308417 \t\n",
      "Epoch 34163 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34164 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34165 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34166 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34167 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34168 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34169 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34170 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34171 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34172 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34173 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34174 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34175 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34176 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34177 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34178 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34179 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34180 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34181 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34182 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34183 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34184 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34185 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34186 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34187 \t\t Training Loss: 0.000580799940507859 \t\n",
      "Epoch 34188 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34189 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34190 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34191 \t\t Training Loss: 0.0005808000569231808 \t\n",
      "Epoch 34192 \t\t Training Loss: 0.000580799940507859 \t\n",
      "Epoch 34193 \t\t Training Loss: 0.000580799940507859 \t\n",
      "Epoch 34194 \t\t Training Loss: 0.000580799940507859 \t\n",
      "Epoch 34195 \t\t Training Loss: 0.000580799940507859 \t\n",
      "Epoch 34196 \t\t Training Loss: 0.000580799940507859 \t\n",
      "Epoch 34197 \t\t Training Loss: 0.000580799940507859 \t\n",
      "Epoch 34198 \t\t Training Loss: 0.000580799940507859 \t\n",
      "Epoch 34199 \t\t Training Loss: 0.000580799940507859 \t\n",
      "Epoch 34200 \t\t Training Loss: 0.000580799940507859 \t\n",
      "Epoch 34201 \t\t Training Loss: 0.0005807998823001981 \t\n",
      "Epoch 34202 \t\t Training Loss: 0.0005807998823001981 \t\n",
      "Epoch 34203 \t\t Training Loss: 0.0005807998823001981 \t\n",
      "Epoch 34204 \t\t Training Loss: 0.0005807998823001981 \t\n",
      "Epoch 34205 \t\t Training Loss: 0.0005807998823001981 \t\n",
      "Epoch 34206 \t\t Training Loss: 0.0005807998823001981 \t\n",
      "Epoch 34207 \t\t Training Loss: 0.0005807998240925372 \t\n",
      "Epoch 34208 \t\t Training Loss: 0.0005807998240925372 \t\n",
      "Epoch 34209 \t\t Training Loss: 0.0005807998240925372 \t\n",
      "Epoch 34210 \t\t Training Loss: 0.0005807998240925372 \t\n",
      "Epoch 34211 \t\t Training Loss: 0.0005807998240925372 \t\n",
      "Epoch 34212 \t\t Training Loss: 0.0005807998240925372 \t\n",
      "Epoch 34213 \t\t Training Loss: 0.0005807998240925372 \t\n",
      "Epoch 34214 \t\t Training Loss: 0.0005807998240925372 \t\n",
      "Epoch 34215 \t\t Training Loss: 0.0005807998240925372 \t\n",
      "Epoch 34216 \t\t Training Loss: 0.0005807998240925372 \t\n",
      "Epoch 34217 \t\t Training Loss: 0.0005807998240925372 \t\n",
      "Epoch 34218 \t\t Training Loss: 0.0005807998240925372 \t\n",
      "Epoch 34219 \t\t Training Loss: 0.0005807997658848763 \t\n",
      "Epoch 34220 \t\t Training Loss: 0.0005807997658848763 \t\n",
      "Epoch 34221 \t\t Training Loss: 0.0005807997658848763 \t\n",
      "Epoch 34222 \t\t Training Loss: 0.0005807997658848763 \t\n",
      "Epoch 34223 \t\t Training Loss: 0.0005807997658848763 \t\n",
      "Epoch 34224 \t\t Training Loss: 0.0005807997658848763 \t\n",
      "Epoch 34225 \t\t Training Loss: 0.0005807997658848763 \t\n",
      "Epoch 34226 \t\t Training Loss: 0.0005807997658848763 \t\n",
      "Epoch 34227 \t\t Training Loss: 0.0005807996494695544 \t\n",
      "Epoch 34228 \t\t Training Loss: 0.0005807996494695544 \t\n",
      "Epoch 34229 \t\t Training Loss: 0.0005807996494695544 \t\n",
      "Epoch 34230 \t\t Training Loss: 0.0005807995912618935 \t\n",
      "Epoch 34231 \t\t Training Loss: 0.0005807995912618935 \t\n",
      "Epoch 34232 \t\t Training Loss: 0.0005807996494695544 \t\n",
      "Epoch 34233 \t\t Training Loss: 0.0005807995912618935 \t\n",
      "Epoch 34234 \t\t Training Loss: 0.0005807995912618935 \t\n",
      "Epoch 34235 \t\t Training Loss: 0.0005807995912618935 \t\n",
      "Epoch 34236 \t\t Training Loss: 0.0005807995912618935 \t\n",
      "Epoch 34237 \t\t Training Loss: 0.0005807995912618935 \t\n",
      "Epoch 34238 \t\t Training Loss: 0.0005807995912618935 \t\n",
      "Epoch 34239 \t\t Training Loss: 0.0005807995912618935 \t\n",
      "Epoch 34240 \t\t Training Loss: 0.0005807995330542326 \t\n",
      "Epoch 34241 \t\t Training Loss: 0.0005807995330542326 \t\n",
      "Epoch 34242 \t\t Training Loss: 0.0005807994748465717 \t\n",
      "Epoch 34243 \t\t Training Loss: 0.0005807995330542326 \t\n",
      "Epoch 34244 \t\t Training Loss: 0.0005807995330542326 \t\n",
      "Epoch 34245 \t\t Training Loss: 0.0005807995330542326 \t\n",
      "Epoch 34246 \t\t Training Loss: 0.0005807995330542326 \t\n",
      "Epoch 34247 \t\t Training Loss: 0.0005807995330542326 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34248 \t\t Training Loss: 0.0005807995330542326 \t\n",
      "Epoch 34249 \t\t Training Loss: 0.0005807995330542326 \t\n",
      "Epoch 34250 \t\t Training Loss: 0.0005807995330542326 \t\n",
      "Epoch 34251 \t\t Training Loss: 0.0005807994748465717 \t\n",
      "Epoch 34252 \t\t Training Loss: 0.0005807994748465717 \t\n",
      "Epoch 34253 \t\t Training Loss: 0.0005807994748465717 \t\n",
      "Epoch 34254 \t\t Training Loss: 0.0005807994748465717 \t\n",
      "Epoch 34255 \t\t Training Loss: 0.0005807994748465717 \t\n",
      "Epoch 34256 \t\t Training Loss: 0.0005807994748465717 \t\n",
      "Epoch 34257 \t\t Training Loss: 0.0005807994748465717 \t\n",
      "Epoch 34258 \t\t Training Loss: 0.0005807994748465717 \t\n",
      "Epoch 34259 \t\t Training Loss: 0.0005807994748465717 \t\n",
      "Epoch 34260 \t\t Training Loss: 0.0005807994748465717 \t\n",
      "Epoch 34261 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34262 \t\t Training Loss: 0.0005807994166389108 \t\n",
      "Epoch 34263 \t\t Training Loss: 0.0005807994166389108 \t\n",
      "Epoch 34264 \t\t Training Loss: 0.0005807994166389108 \t\n",
      "Epoch 34265 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34266 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34267 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34268 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34269 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34270 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34271 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34272 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34273 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34274 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34275 \t\t Training Loss: 0.0005807994166389108 \t\n",
      "Epoch 34276 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34277 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34278 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34279 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34280 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34281 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34282 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34283 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34284 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34285 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34286 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34287 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34288 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34289 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34290 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34291 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34292 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34293 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34294 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34295 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34296 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34297 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34298 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34299 \t\t Training Loss: 0.000580799242015928 \t\n",
      "Epoch 34300 \t\t Training Loss: 0.000580799242015928 \t\n",
      "Epoch 34301 \t\t Training Loss: 0.000580799242015928 \t\n",
      "Epoch 34302 \t\t Training Loss: 0.000580799242015928 \t\n",
      "Epoch 34303 \t\t Training Loss: 0.000580799242015928 \t\n",
      "Epoch 34304 \t\t Training Loss: 0.000580799242015928 \t\n",
      "Epoch 34305 \t\t Training Loss: 0.000580799242015928 \t\n",
      "Epoch 34306 \t\t Training Loss: 0.0005807993584312499 \t\n",
      "Epoch 34307 \t\t Training Loss: 0.000580799242015928 \t\n",
      "Epoch 34308 \t\t Training Loss: 0.000580799242015928 \t\n",
      "Epoch 34309 \t\t Training Loss: 0.000580799242015928 \t\n",
      "Epoch 34310 \t\t Training Loss: 0.000580799242015928 \t\n",
      "Epoch 34311 \t\t Training Loss: 0.000580799242015928 \t\n",
      "Epoch 34312 \t\t Training Loss: 0.0005807991256006062 \t\n",
      "Epoch 34313 \t\t Training Loss: 0.0005807991256006062 \t\n",
      "Epoch 34314 \t\t Training Loss: 0.0005807991256006062 \t\n",
      "Epoch 34315 \t\t Training Loss: 0.0005807991256006062 \t\n",
      "Epoch 34316 \t\t Training Loss: 0.0005807991256006062 \t\n",
      "Epoch 34317 \t\t Training Loss: 0.0005807991256006062 \t\n",
      "Epoch 34318 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34319 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34320 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34321 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34322 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34323 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34324 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34325 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34326 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34327 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34328 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34329 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34330 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34331 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34332 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34333 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34334 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34335 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34336 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34337 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34338 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34339 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34340 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34341 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34342 \t\t Training Loss: 0.0005807989509776235 \t\n",
      "Epoch 34343 \t\t Training Loss: 0.0005807990673929453 \t\n",
      "Epoch 34344 \t\t Training Loss: 0.0005807989509776235 \t\n",
      "Epoch 34345 \t\t Training Loss: 0.0005807989509776235 \t\n",
      "Epoch 34346 \t\t Training Loss: 0.0005807989509776235 \t\n",
      "Epoch 34347 \t\t Training Loss: 0.0005807989509776235 \t\n",
      "Epoch 34348 \t\t Training Loss: 0.0005807989509776235 \t\n",
      "Epoch 34349 \t\t Training Loss: 0.0005807989509776235 \t\n",
      "Epoch 34350 \t\t Training Loss: 0.0005807989509776235 \t\n",
      "Epoch 34351 \t\t Training Loss: 0.0005807989509776235 \t\n",
      "Epoch 34352 \t\t Training Loss: 0.0005807989509776235 \t\n",
      "Epoch 34353 \t\t Training Loss: 0.0005807989509776235 \t\n",
      "Epoch 34354 \t\t Training Loss: 0.0005807989509776235 \t\n",
      "Epoch 34355 \t\t Training Loss: 0.0005807989509776235 \t\n",
      "Epoch 34356 \t\t Training Loss: 0.0005807989509776235 \t\n",
      "Epoch 34357 \t\t Training Loss: 0.0005807989509776235 \t\n",
      "Epoch 34358 \t\t Training Loss: 0.0005807988345623016 \t\n",
      "Epoch 34359 \t\t Training Loss: 0.0005807988345623016 \t\n",
      "Epoch 34360 \t\t Training Loss: 0.0005807988345623016 \t\n",
      "Epoch 34361 \t\t Training Loss: 0.0005807988345623016 \t\n",
      "Epoch 34362 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34363 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34364 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34365 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34366 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34367 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34368 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34369 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34370 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34371 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34372 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34373 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34374 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34375 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34376 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34377 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34378 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34379 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34380 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34381 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34382 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34383 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34384 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34385 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34386 \t\t Training Loss: 0.0005807987763546407 \t\n",
      "Epoch 34387 \t\t Training Loss: 0.0005807986599393189 \t\n",
      "Epoch 34388 \t\t Training Loss: 0.0005807986599393189 \t\n",
      "Epoch 34389 \t\t Training Loss: 0.0005807985435239971 \t\n",
      "Epoch 34390 \t\t Training Loss: 0.0005807985435239971 \t\n",
      "Epoch 34391 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34392 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34393 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34394 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34395 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34396 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34397 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34398 \t\t Training Loss: 0.0005807985435239971 \t\n",
      "Epoch 34399 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34400 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34401 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34402 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34403 \t\t Training Loss: 0.0005807984853163362 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34404 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34405 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34406 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34407 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34408 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34409 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34410 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34411 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34412 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34413 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34414 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34415 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34416 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34417 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34418 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34419 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34420 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34421 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34422 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34423 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34424 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34425 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34426 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34427 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34428 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34429 \t\t Training Loss: 0.0005807984853163362 \t\n",
      "Epoch 34430 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34431 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34432 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34433 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34434 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34435 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34436 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34437 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34438 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34439 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34440 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34441 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34442 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34443 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34444 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34445 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34446 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34447 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34448 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34449 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34450 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34451 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34452 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34453 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34454 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34455 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34456 \t\t Training Loss: 0.0005807983689010143 \t\n",
      "Epoch 34457 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34458 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34459 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34460 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34461 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34462 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34463 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34464 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34465 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34466 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34467 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34468 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34469 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34470 \t\t Training Loss: 0.0005807982524856925 \t\n",
      "Epoch 34471 \t\t Training Loss: 0.0005807981942780316 \t\n",
      "Epoch 34472 \t\t Training Loss: 0.0005807981942780316 \t\n",
      "Epoch 34473 \t\t Training Loss: 0.0005807980778627098 \t\n",
      "Epoch 34474 \t\t Training Loss: 0.0005807980778627098 \t\n",
      "Epoch 34475 \t\t Training Loss: 0.0005807980778627098 \t\n",
      "Epoch 34476 \t\t Training Loss: 0.0005807980778627098 \t\n",
      "Epoch 34477 \t\t Training Loss: 0.0005807980778627098 \t\n",
      "Epoch 34478 \t\t Training Loss: 0.0005807980778627098 \t\n",
      "Epoch 34479 \t\t Training Loss: 0.0005807980778627098 \t\n",
      "Epoch 34480 \t\t Training Loss: 0.0005807980778627098 \t\n",
      "Epoch 34481 \t\t Training Loss: 0.0005807980778627098 \t\n",
      "Epoch 34482 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34483 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34484 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34485 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34486 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34487 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34488 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34489 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34490 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34491 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34492 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34493 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34494 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34495 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34496 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34497 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34498 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34499 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34500 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34501 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34502 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34503 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34504 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34505 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34506 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34507 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34508 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34509 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34510 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34511 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34512 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34513 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34514 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34515 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34516 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34517 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34518 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34519 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34520 \t\t Training Loss: 0.0005807979614473879 \t\n",
      "Epoch 34521 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34522 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34523 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34524 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34525 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34526 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34527 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34528 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34529 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34530 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34531 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34532 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34533 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34534 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34535 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34536 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34537 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34538 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34539 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34540 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34541 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34542 \t\t Training Loss: 0.0005807978450320661 \t\n",
      "Epoch 34543 \t\t Training Loss: 0.0005807977868244052 \t\n",
      "Epoch 34544 \t\t Training Loss: 0.0005807977868244052 \t\n",
      "Epoch 34545 \t\t Training Loss: 0.0005807977868244052 \t\n",
      "Epoch 34546 \t\t Training Loss: 0.0005807977868244052 \t\n",
      "Epoch 34547 \t\t Training Loss: 0.0005807977868244052 \t\n",
      "Epoch 34548 \t\t Training Loss: 0.0005807977868244052 \t\n",
      "Epoch 34549 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34550 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34551 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34552 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34553 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34554 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34555 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34556 \t\t Training Loss: 0.0005807976704090834 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34557 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34558 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34559 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34560 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34561 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34562 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34563 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34564 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34565 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34566 \t\t Training Loss: 0.0005807975539937615 \t\n",
      "Epoch 34567 \t\t Training Loss: 0.0005807975539937615 \t\n",
      "Epoch 34568 \t\t Training Loss: 0.0005807975539937615 \t\n",
      "Epoch 34569 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34570 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34571 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34572 \t\t Training Loss: 0.0005807976704090834 \t\n",
      "Epoch 34573 \t\t Training Loss: 0.0005807975539937615 \t\n",
      "Epoch 34574 \t\t Training Loss: 0.0005807975539937615 \t\n",
      "Epoch 34575 \t\t Training Loss: 0.0005807975539937615 \t\n",
      "Epoch 34576 \t\t Training Loss: 0.0005807975539937615 \t\n",
      "Epoch 34577 \t\t Training Loss: 0.0005807975539937615 \t\n",
      "Epoch 34578 \t\t Training Loss: 0.0005807975539937615 \t\n",
      "Epoch 34579 \t\t Training Loss: 0.0005807975539937615 \t\n",
      "Epoch 34580 \t\t Training Loss: 0.0005807975539937615 \t\n",
      "Epoch 34581 \t\t Training Loss: 0.0005807974957861006 \t\n",
      "Epoch 34582 \t\t Training Loss: 0.0005807975539937615 \t\n",
      "Epoch 34583 \t\t Training Loss: 0.0005807974957861006 \t\n",
      "Epoch 34584 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34585 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34586 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34587 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34588 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34589 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34590 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34591 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34592 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34593 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34594 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34595 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34596 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34597 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34598 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34599 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34600 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34601 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34602 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34603 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34604 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34605 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34606 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34607 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34608 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34609 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34610 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34611 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34612 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34613 \t\t Training Loss: 0.0005807973793707788 \t\n",
      "Epoch 34614 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34615 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34616 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34617 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34618 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34619 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34620 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34621 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34622 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34623 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34624 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34625 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34626 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34627 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34628 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34629 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34630 \t\t Training Loss: 0.0005807972047477961 \t\n",
      "Epoch 34631 \t\t Training Loss: 0.000580797262955457 \t\n",
      "Epoch 34632 \t\t Training Loss: 0.0005807972047477961 \t\n",
      "Epoch 34633 \t\t Training Loss: 0.0005807972047477961 \t\n",
      "Epoch 34634 \t\t Training Loss: 0.0005807972047477961 \t\n",
      "Epoch 34635 \t\t Training Loss: 0.0005807972047477961 \t\n",
      "Epoch 34636 \t\t Training Loss: 0.0005807972047477961 \t\n",
      "Epoch 34637 \t\t Training Loss: 0.0005807972047477961 \t\n",
      "Epoch 34638 \t\t Training Loss: 0.0005807972047477961 \t\n",
      "Epoch 34639 \t\t Training Loss: 0.0005807972047477961 \t\n",
      "Epoch 34640 \t\t Training Loss: 0.0005807972047477961 \t\n",
      "Epoch 34641 \t\t Training Loss: 0.0005807972047477961 \t\n",
      "Epoch 34642 \t\t Training Loss: 0.0005807972047477961 \t\n",
      "Epoch 34643 \t\t Training Loss: 0.0005807972047477961 \t\n",
      "Epoch 34644 \t\t Training Loss: 0.0005807972047477961 \t\n",
      "Epoch 34645 \t\t Training Loss: 0.0005807971465401351 \t\n",
      "Epoch 34646 \t\t Training Loss: 0.0005807971465401351 \t\n",
      "Epoch 34647 \t\t Training Loss: 0.0005807971465401351 \t\n",
      "Epoch 34648 \t\t Training Loss: 0.0005807971465401351 \t\n",
      "Epoch 34649 \t\t Training Loss: 0.0005807971465401351 \t\n",
      "Epoch 34650 \t\t Training Loss: 0.0005807970883324742 \t\n",
      "Epoch 34651 \t\t Training Loss: 0.0005807970883324742 \t\n",
      "Epoch 34652 \t\t Training Loss: 0.0005807970883324742 \t\n",
      "Epoch 34653 \t\t Training Loss: 0.0005807970883324742 \t\n",
      "Epoch 34654 \t\t Training Loss: 0.0005807971465401351 \t\n",
      "Epoch 34655 \t\t Training Loss: 0.0005807970883324742 \t\n",
      "Epoch 34656 \t\t Training Loss: 0.0005807970883324742 \t\n",
      "Epoch 34657 \t\t Training Loss: 0.0005807970883324742 \t\n",
      "Epoch 34658 \t\t Training Loss: 0.0005807970883324742 \t\n",
      "Epoch 34659 \t\t Training Loss: 0.0005807970883324742 \t\n",
      "Epoch 34660 \t\t Training Loss: 0.0005807970883324742 \t\n",
      "Epoch 34661 \t\t Training Loss: 0.0005807969719171524 \t\n",
      "Epoch 34662 \t\t Training Loss: 0.0005807970883324742 \t\n",
      "Epoch 34663 \t\t Training Loss: 0.0005807970883324742 \t\n",
      "Epoch 34664 \t\t Training Loss: 0.0005807970883324742 \t\n",
      "Epoch 34665 \t\t Training Loss: 0.0005807969719171524 \t\n",
      "Epoch 34666 \t\t Training Loss: 0.0005807969719171524 \t\n",
      "Epoch 34667 \t\t Training Loss: 0.0005807969719171524 \t\n",
      "Epoch 34668 \t\t Training Loss: 0.0005807969719171524 \t\n",
      "Epoch 34669 \t\t Training Loss: 0.0005807969719171524 \t\n",
      "Epoch 34670 \t\t Training Loss: 0.0005807969719171524 \t\n",
      "Epoch 34671 \t\t Training Loss: 0.0005807969719171524 \t\n",
      "Epoch 34672 \t\t Training Loss: 0.0005807969719171524 \t\n",
      "Epoch 34673 \t\t Training Loss: 0.0005807969719171524 \t\n",
      "Epoch 34674 \t\t Training Loss: 0.0005807969719171524 \t\n",
      "Epoch 34675 \t\t Training Loss: 0.0005807969719171524 \t\n",
      "Epoch 34676 \t\t Training Loss: 0.0005807969719171524 \t\n",
      "Epoch 34677 \t\t Training Loss: 0.0005807969719171524 \t\n",
      "Epoch 34678 \t\t Training Loss: 0.0005807969719171524 \t\n",
      "Epoch 34679 \t\t Training Loss: 0.0005807969137094915 \t\n",
      "Epoch 34680 \t\t Training Loss: 0.0005807969137094915 \t\n",
      "Epoch 34681 \t\t Training Loss: 0.0005807969137094915 \t\n",
      "Epoch 34682 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34683 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34684 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34685 \t\t Training Loss: 0.0005807969137094915 \t\n",
      "Epoch 34686 \t\t Training Loss: 0.0005807969137094915 \t\n",
      "Epoch 34687 \t\t Training Loss: 0.0005807969137094915 \t\n",
      "Epoch 34688 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34689 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34690 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34691 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34692 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34693 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34694 \t\t Training Loss: 0.0005807969137094915 \t\n",
      "Epoch 34695 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34696 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34697 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34698 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34699 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34700 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34701 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34702 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34703 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34704 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34705 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34706 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34707 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34708 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34709 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34710 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34711 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34712 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34713 \t\t Training Loss: 0.0005807967972941697 \t\n",
      "Epoch 34714 \t\t Training Loss: 0.0005807967972941697 \t\n",
      "Epoch 34715 \t\t Training Loss: 0.0005807967972941697 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34716 \t\t Training Loss: 0.0005807967972941697 \t\n",
      "Epoch 34717 \t\t Training Loss: 0.0005807968555018306 \t\n",
      "Epoch 34718 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34719 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34720 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34721 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34722 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34723 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34724 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34725 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34726 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34727 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34728 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34729 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34730 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34731 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34732 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34733 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34734 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34735 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34736 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34737 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34738 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34739 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34740 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34741 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34742 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34743 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34744 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34745 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34746 \t\t Training Loss: 0.0005807967390865088 \t\n",
      "Epoch 34747 \t\t Training Loss: 0.0005807966808788478 \t\n",
      "Epoch 34748 \t\t Training Loss: 0.0005807966808788478 \t\n",
      "Epoch 34749 \t\t Training Loss: 0.0005807966808788478 \t\n",
      "Epoch 34750 \t\t Training Loss: 0.0005807966808788478 \t\n",
      "Epoch 34751 \t\t Training Loss: 0.0005807966226711869 \t\n",
      "Epoch 34752 \t\t Training Loss: 0.0005807966226711869 \t\n",
      "Epoch 34753 \t\t Training Loss: 0.0005807966226711869 \t\n",
      "Epoch 34754 \t\t Training Loss: 0.0005807966808788478 \t\n",
      "Epoch 34755 \t\t Training Loss: 0.0005807966808788478 \t\n",
      "Epoch 34756 \t\t Training Loss: 0.0005807966808788478 \t\n",
      "Epoch 34757 \t\t Training Loss: 0.0005807966808788478 \t\n",
      "Epoch 34758 \t\t Training Loss: 0.0005807966808788478 \t\n",
      "Epoch 34759 \t\t Training Loss: 0.0005807966226711869 \t\n",
      "Epoch 34760 \t\t Training Loss: 0.0005807966808788478 \t\n",
      "Epoch 34761 \t\t Training Loss: 0.0005807966226711869 \t\n",
      "Epoch 34762 \t\t Training Loss: 0.0005807966226711869 \t\n",
      "Epoch 34763 \t\t Training Loss: 0.0005807966226711869 \t\n",
      "Epoch 34764 \t\t Training Loss: 0.0005807966226711869 \t\n",
      "Epoch 34765 \t\t Training Loss: 0.0005807966226711869 \t\n",
      "Epoch 34766 \t\t Training Loss: 0.000580796564463526 \t\n",
      "Epoch 34767 \t\t Training Loss: 0.000580796564463526 \t\n",
      "Epoch 34768 \t\t Training Loss: 0.000580796564463526 \t\n",
      "Epoch 34769 \t\t Training Loss: 0.000580796564463526 \t\n",
      "Epoch 34770 \t\t Training Loss: 0.000580796564463526 \t\n",
      "Epoch 34771 \t\t Training Loss: 0.0005807965062558651 \t\n",
      "Epoch 34772 \t\t Training Loss: 0.0005807966226711869 \t\n",
      "Epoch 34773 \t\t Training Loss: 0.000580796564463526 \t\n",
      "Epoch 34774 \t\t Training Loss: 0.0005807965062558651 \t\n",
      "Epoch 34775 \t\t Training Loss: 0.0005807965062558651 \t\n",
      "Epoch 34776 \t\t Training Loss: 0.0005807965062558651 \t\n",
      "Epoch 34777 \t\t Training Loss: 0.0005807965062558651 \t\n",
      "Epoch 34778 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34779 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34780 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34781 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34782 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34783 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34784 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34785 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34786 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34787 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34788 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34789 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34790 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34791 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34792 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34793 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34794 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34795 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34796 \t\t Training Loss: 0.0005807964480482042 \t\n",
      "Epoch 34797 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34798 \t\t Training Loss: 0.0005807965062558651 \t\n",
      "Epoch 34799 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34800 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34801 \t\t Training Loss: 0.0005807965062558651 \t\n",
      "Epoch 34802 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34803 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34804 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34805 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34806 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34807 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34808 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34809 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34810 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34811 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34812 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34813 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34814 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34815 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34816 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34817 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34818 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34819 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34820 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34821 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34822 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34823 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34824 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34825 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34826 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34827 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34828 \t\t Training Loss: 0.0005807963898405433 \t\n",
      "Epoch 34829 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34830 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34831 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34832 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34833 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34834 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34835 \t\t Training Loss: 0.0005807962734252214 \t\n",
      "Epoch 34836 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34837 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34838 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34839 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34840 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34841 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34842 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34843 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34844 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34845 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34846 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34847 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34848 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34849 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34850 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34851 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34852 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34853 \t\t Training Loss: 0.0005807961570098996 \t\n",
      "Epoch 34854 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34855 \t\t Training Loss: 0.0005807961570098996 \t\n",
      "Epoch 34856 \t\t Training Loss: 0.0005807961570098996 \t\n",
      "Epoch 34857 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34858 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34859 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34860 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34861 \t\t Training Loss: 0.0005807962152175605 \t\n",
      "Epoch 34862 \t\t Training Loss: 0.0005807961570098996 \t\n",
      "Epoch 34863 \t\t Training Loss: 0.0005807961570098996 \t\n",
      "Epoch 34864 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34865 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34866 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34867 \t\t Training Loss: 0.0005807960988022387 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34868 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34869 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34870 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34871 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34872 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34873 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34874 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34875 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34876 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34877 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34878 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34879 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34880 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34881 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34882 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34883 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34884 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34885 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34886 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34887 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34888 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34889 \t\t Training Loss: 0.0005807960988022387 \t\n",
      "Epoch 34890 \t\t Training Loss: 0.0005807960405945778 \t\n",
      "Epoch 34891 \t\t Training Loss: 0.0005807960405945778 \t\n",
      "Epoch 34892 \t\t Training Loss: 0.0005807960405945778 \t\n",
      "Epoch 34893 \t\t Training Loss: 0.0005807959823869169 \t\n",
      "Epoch 34894 \t\t Training Loss: 0.0005807959823869169 \t\n",
      "Epoch 34895 \t\t Training Loss: 0.0005807959823869169 \t\n",
      "Epoch 34896 \t\t Training Loss: 0.000580795924179256 \t\n",
      "Epoch 34897 \t\t Training Loss: 0.0005807959823869169 \t\n",
      "Epoch 34898 \t\t Training Loss: 0.000580795924179256 \t\n",
      "Epoch 34899 \t\t Training Loss: 0.000580795924179256 \t\n",
      "Epoch 34900 \t\t Training Loss: 0.000580795924179256 \t\n",
      "Epoch 34901 \t\t Training Loss: 0.000580795924179256 \t\n",
      "Epoch 34902 \t\t Training Loss: 0.000580795924179256 \t\n",
      "Epoch 34903 \t\t Training Loss: 0.000580795924179256 \t\n",
      "Epoch 34904 \t\t Training Loss: 0.000580795924179256 \t\n",
      "Epoch 34905 \t\t Training Loss: 0.000580795924179256 \t\n",
      "Epoch 34906 \t\t Training Loss: 0.000580795924179256 \t\n",
      "Epoch 34907 \t\t Training Loss: 0.000580795924179256 \t\n",
      "Epoch 34908 \t\t Training Loss: 0.000580795924179256 \t\n",
      "Epoch 34909 \t\t Training Loss: 0.000580795924179256 \t\n",
      "Epoch 34910 \t\t Training Loss: 0.000580795924179256 \t\n",
      "Epoch 34911 \t\t Training Loss: 0.000580795924179256 \t\n",
      "Epoch 34912 \t\t Training Loss: 0.000580795924179256 \t\n",
      "Epoch 34913 \t\t Training Loss: 0.0005807958077639341 \t\n",
      "Epoch 34914 \t\t Training Loss: 0.0005807958077639341 \t\n",
      "Epoch 34915 \t\t Training Loss: 0.0005807958077639341 \t\n",
      "Epoch 34916 \t\t Training Loss: 0.0005807957495562732 \t\n",
      "Epoch 34917 \t\t Training Loss: 0.0005807957495562732 \t\n",
      "Epoch 34918 \t\t Training Loss: 0.0005807957495562732 \t\n",
      "Epoch 34919 \t\t Training Loss: 0.0005807957495562732 \t\n",
      "Epoch 34920 \t\t Training Loss: 0.0005807958077639341 \t\n",
      "Epoch 34921 \t\t Training Loss: 0.0005807957495562732 \t\n",
      "Epoch 34922 \t\t Training Loss: 0.0005807957495562732 \t\n",
      "Epoch 34923 \t\t Training Loss: 0.0005807956913486123 \t\n",
      "Epoch 34924 \t\t Training Loss: 0.0005807956913486123 \t\n",
      "Epoch 34925 \t\t Training Loss: 0.0005807956913486123 \t\n",
      "Epoch 34926 \t\t Training Loss: 0.0005807956913486123 \t\n",
      "Epoch 34927 \t\t Training Loss: 0.0005807956913486123 \t\n",
      "Epoch 34928 \t\t Training Loss: 0.0005807956913486123 \t\n",
      "Epoch 34929 \t\t Training Loss: 0.0005807956913486123 \t\n",
      "Epoch 34930 \t\t Training Loss: 0.0005807956913486123 \t\n",
      "Epoch 34931 \t\t Training Loss: 0.0005807957495562732 \t\n",
      "Epoch 34932 \t\t Training Loss: 0.0005807956913486123 \t\n",
      "Epoch 34933 \t\t Training Loss: 0.0005807956913486123 \t\n",
      "Epoch 34934 \t\t Training Loss: 0.0005807956913486123 \t\n",
      "Epoch 34935 \t\t Training Loss: 0.0005807956913486123 \t\n",
      "Epoch 34936 \t\t Training Loss: 0.0005807956913486123 \t\n",
      "Epoch 34937 \t\t Training Loss: 0.0005807956913486123 \t\n",
      "Epoch 34938 \t\t Training Loss: 0.0005807956913486123 \t\n",
      "Epoch 34939 \t\t Training Loss: 0.0005807956913486123 \t\n",
      "Epoch 34940 \t\t Training Loss: 0.0005807956331409514 \t\n",
      "Epoch 34941 \t\t Training Loss: 0.0005807956331409514 \t\n",
      "Epoch 34942 \t\t Training Loss: 0.0005807956331409514 \t\n",
      "Epoch 34943 \t\t Training Loss: 0.0005807956331409514 \t\n",
      "Epoch 34944 \t\t Training Loss: 0.0005807956331409514 \t\n",
      "Epoch 34945 \t\t Training Loss: 0.0005807955749332905 \t\n",
      "Epoch 34946 \t\t Training Loss: 0.0005807955749332905 \t\n",
      "Epoch 34947 \t\t Training Loss: 0.0005807955749332905 \t\n",
      "Epoch 34948 \t\t Training Loss: 0.0005807955749332905 \t\n",
      "Epoch 34949 \t\t Training Loss: 0.0005807955749332905 \t\n",
      "Epoch 34950 \t\t Training Loss: 0.0005807955749332905 \t\n",
      "Epoch 34951 \t\t Training Loss: 0.0005807955749332905 \t\n",
      "Epoch 34952 \t\t Training Loss: 0.0005807955749332905 \t\n",
      "Epoch 34953 \t\t Training Loss: 0.0005807955749332905 \t\n",
      "Epoch 34954 \t\t Training Loss: 0.0005807955749332905 \t\n",
      "Epoch 34955 \t\t Training Loss: 0.0005807955167256296 \t\n",
      "Epoch 34956 \t\t Training Loss: 0.0005807955749332905 \t\n",
      "Epoch 34957 \t\t Training Loss: 0.0005807955749332905 \t\n",
      "Epoch 34958 \t\t Training Loss: 0.0005807955749332905 \t\n",
      "Epoch 34959 \t\t Training Loss: 0.0005807955749332905 \t\n",
      "Epoch 34960 \t\t Training Loss: 0.0005807955167256296 \t\n",
      "Epoch 34961 \t\t Training Loss: 0.0005807955749332905 \t\n",
      "Epoch 34962 \t\t Training Loss: 0.0005807955167256296 \t\n",
      "Epoch 34963 \t\t Training Loss: 0.0005807955167256296 \t\n",
      "Epoch 34964 \t\t Training Loss: 0.0005807955167256296 \t\n",
      "Epoch 34965 \t\t Training Loss: 0.0005807955167256296 \t\n",
      "Epoch 34966 \t\t Training Loss: 0.0005807955167256296 \t\n",
      "Epoch 34967 \t\t Training Loss: 0.0005807955167256296 \t\n",
      "Epoch 34968 \t\t Training Loss: 0.0005807955167256296 \t\n",
      "Epoch 34969 \t\t Training Loss: 0.0005807954585179687 \t\n",
      "Epoch 34970 \t\t Training Loss: 0.0005807954585179687 \t\n",
      "Epoch 34971 \t\t Training Loss: 0.0005807954585179687 \t\n",
      "Epoch 34972 \t\t Training Loss: 0.0005807955167256296 \t\n",
      "Epoch 34973 \t\t Training Loss: 0.0005807955167256296 \t\n",
      "Epoch 34974 \t\t Training Loss: 0.0005807955167256296 \t\n",
      "Epoch 34975 \t\t Training Loss: 0.0005807954585179687 \t\n",
      "Epoch 34976 \t\t Training Loss: 0.0005807954003103077 \t\n",
      "Epoch 34977 \t\t Training Loss: 0.0005807954003103077 \t\n",
      "Epoch 34978 \t\t Training Loss: 0.0005807954003103077 \t\n",
      "Epoch 34979 \t\t Training Loss: 0.0005807954003103077 \t\n",
      "Epoch 34980 \t\t Training Loss: 0.0005807953421026468 \t\n",
      "Epoch 34981 \t\t Training Loss: 0.0005807954003103077 \t\n",
      "Epoch 34982 \t\t Training Loss: 0.0005807953421026468 \t\n",
      "Epoch 34983 \t\t Training Loss: 0.0005807953421026468 \t\n",
      "Epoch 34984 \t\t Training Loss: 0.0005807954003103077 \t\n",
      "Epoch 34985 \t\t Training Loss: 0.0005807954003103077 \t\n",
      "Epoch 34986 \t\t Training Loss: 0.0005807954003103077 \t\n",
      "Epoch 34987 \t\t Training Loss: 0.0005807954003103077 \t\n",
      "Epoch 34988 \t\t Training Loss: 0.0005807953421026468 \t\n",
      "Epoch 34989 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 34990 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 34991 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 34992 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 34993 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 34994 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 34995 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 34996 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 34997 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 34998 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 34999 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 35000 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 35001 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 35002 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 35003 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 35004 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 35005 \t\t Training Loss: 0.0005807951674796641 \t\n",
      "Epoch 35006 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 35007 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 35008 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 35009 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 35010 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 35011 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 35012 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 35013 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 35014 \t\t Training Loss: 0.000580795225687325 \t\n",
      "Epoch 35015 \t\t Training Loss: 0.0005807951674796641 \t\n",
      "Epoch 35016 \t\t Training Loss: 0.0005807951674796641 \t\n",
      "Epoch 35017 \t\t Training Loss: 0.0005807951674796641 \t\n",
      "Epoch 35018 \t\t Training Loss: 0.0005807951674796641 \t\n",
      "Epoch 35019 \t\t Training Loss: 0.0005807951674796641 \t\n",
      "Epoch 35020 \t\t Training Loss: 0.0005807951092720032 \t\n",
      "Epoch 35021 \t\t Training Loss: 0.0005807950510643423 \t\n",
      "Epoch 35022 \t\t Training Loss: 0.0005807950510643423 \t\n",
      "Epoch 35023 \t\t Training Loss: 0.0005807950510643423 \t\n",
      "Epoch 35024 \t\t Training Loss: 0.0005807950510643423 \t\n",
      "Epoch 35025 \t\t Training Loss: 0.0005807950510643423 \t\n",
      "Epoch 35026 \t\t Training Loss: 0.0005807950510643423 \t\n",
      "Epoch 35027 \t\t Training Loss: 0.0005807949928566813 \t\n",
      "Epoch 35028 \t\t Training Loss: 0.0005807949928566813 \t\n",
      "Epoch 35029 \t\t Training Loss: 0.0005807949928566813 \t\n",
      "Epoch 35030 \t\t Training Loss: 0.0005807949928566813 \t\n",
      "Epoch 35031 \t\t Training Loss: 0.0005807949928566813 \t\n",
      "Epoch 35032 \t\t Training Loss: 0.0005807950510643423 \t\n",
      "Epoch 35033 \t\t Training Loss: 0.0005807949928566813 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35034 \t\t Training Loss: 0.0005807949928566813 \t\n",
      "Epoch 35035 \t\t Training Loss: 0.0005807949928566813 \t\n",
      "Epoch 35036 \t\t Training Loss: 0.0005807949928566813 \t\n",
      "Epoch 35037 \t\t Training Loss: 0.0005807949928566813 \t\n",
      "Epoch 35038 \t\t Training Loss: 0.0005807949928566813 \t\n",
      "Epoch 35039 \t\t Training Loss: 0.0005807949928566813 \t\n",
      "Epoch 35040 \t\t Training Loss: 0.0005807949928566813 \t\n",
      "Epoch 35041 \t\t Training Loss: 0.0005807949346490204 \t\n",
      "Epoch 35042 \t\t Training Loss: 0.0005807949346490204 \t\n",
      "Epoch 35043 \t\t Training Loss: 0.0005807949346490204 \t\n",
      "Epoch 35044 \t\t Training Loss: 0.0005807948764413595 \t\n",
      "Epoch 35045 \t\t Training Loss: 0.0005807948764413595 \t\n",
      "Epoch 35046 \t\t Training Loss: 0.0005807948764413595 \t\n",
      "Epoch 35047 \t\t Training Loss: 0.0005807948764413595 \t\n",
      "Epoch 35048 \t\t Training Loss: 0.0005807948764413595 \t\n",
      "Epoch 35049 \t\t Training Loss: 0.0005807948764413595 \t\n",
      "Epoch 35050 \t\t Training Loss: 0.0005807948764413595 \t\n",
      "Epoch 35051 \t\t Training Loss: 0.0005807948764413595 \t\n",
      "Epoch 35052 \t\t Training Loss: 0.0005807947600260377 \t\n",
      "Epoch 35053 \t\t Training Loss: 0.0005807947600260377 \t\n",
      "Epoch 35054 \t\t Training Loss: 0.0005807947600260377 \t\n",
      "Epoch 35055 \t\t Training Loss: 0.0005807947600260377 \t\n",
      "Epoch 35056 \t\t Training Loss: 0.0005807947600260377 \t\n",
      "Epoch 35057 \t\t Training Loss: 0.0005807947600260377 \t\n",
      "Epoch 35058 \t\t Training Loss: 0.0005807947600260377 \t\n",
      "Epoch 35059 \t\t Training Loss: 0.0005807947600260377 \t\n",
      "Epoch 35060 \t\t Training Loss: 0.0005807947600260377 \t\n",
      "Epoch 35061 \t\t Training Loss: 0.0005807947600260377 \t\n",
      "Epoch 35062 \t\t Training Loss: 0.0005807947600260377 \t\n",
      "Epoch 35063 \t\t Training Loss: 0.0005807947600260377 \t\n",
      "Epoch 35064 \t\t Training Loss: 0.0005807947018183768 \t\n",
      "Epoch 35065 \t\t Training Loss: 0.0005807947018183768 \t\n",
      "Epoch 35066 \t\t Training Loss: 0.0005807947600260377 \t\n",
      "Epoch 35067 \t\t Training Loss: 0.0005807947018183768 \t\n",
      "Epoch 35068 \t\t Training Loss: 0.0005807947018183768 \t\n",
      "Epoch 35069 \t\t Training Loss: 0.0005807947018183768 \t\n",
      "Epoch 35070 \t\t Training Loss: 0.0005807947018183768 \t\n",
      "Epoch 35071 \t\t Training Loss: 0.0005807947018183768 \t\n",
      "Epoch 35072 \t\t Training Loss: 0.0005807946436107159 \t\n",
      "Epoch 35073 \t\t Training Loss: 0.0005807946436107159 \t\n",
      "Epoch 35074 \t\t Training Loss: 0.0005807946436107159 \t\n",
      "Epoch 35075 \t\t Training Loss: 0.0005807946436107159 \t\n",
      "Epoch 35076 \t\t Training Loss: 0.0005807946436107159 \t\n",
      "Epoch 35077 \t\t Training Loss: 0.0005807946436107159 \t\n",
      "Epoch 35078 \t\t Training Loss: 0.0005807946436107159 \t\n",
      "Epoch 35079 \t\t Training Loss: 0.0005807946436107159 \t\n",
      "Epoch 35080 \t\t Training Loss: 0.0005807946436107159 \t\n",
      "Epoch 35081 \t\t Training Loss: 0.0005807946436107159 \t\n",
      "Epoch 35082 \t\t Training Loss: 0.0005807946436107159 \t\n",
      "Epoch 35083 \t\t Training Loss: 0.0005807946436107159 \t\n",
      "Epoch 35084 \t\t Training Loss: 0.0005807946436107159 \t\n",
      "Epoch 35085 \t\t Training Loss: 0.0005807946436107159 \t\n",
      "Epoch 35086 \t\t Training Loss: 0.0005807946436107159 \t\n",
      "Epoch 35087 \t\t Training Loss: 0.000580794585403055 \t\n",
      "Epoch 35088 \t\t Training Loss: 0.000580794585403055 \t\n",
      "Epoch 35089 \t\t Training Loss: 0.000580794585403055 \t\n",
      "Epoch 35090 \t\t Training Loss: 0.000580794585403055 \t\n",
      "Epoch 35091 \t\t Training Loss: 0.000580794585403055 \t\n",
      "Epoch 35092 \t\t Training Loss: 0.000580794585403055 \t\n",
      "Epoch 35093 \t\t Training Loss: 0.000580794585403055 \t\n",
      "Epoch 35094 \t\t Training Loss: 0.000580794585403055 \t\n",
      "Epoch 35095 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35096 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35097 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35098 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35099 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35100 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35101 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35102 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35103 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35104 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35105 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35106 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35107 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35108 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35109 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35110 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35111 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35112 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35113 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35114 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35115 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35116 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35117 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35118 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35119 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35120 \t\t Training Loss: 0.000580794585403055 \t\n",
      "Epoch 35121 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35122 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35123 \t\t Training Loss: 0.000580794527195394 \t\n",
      "Epoch 35124 \t\t Training Loss: 0.0005807944689877331 \t\n",
      "Epoch 35125 \t\t Training Loss: 0.0005807944689877331 \t\n",
      "Epoch 35126 \t\t Training Loss: 0.0005807944689877331 \t\n",
      "Epoch 35127 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35128 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35129 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35130 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35131 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35132 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35133 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35134 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35135 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35136 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35137 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35138 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35139 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35140 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35141 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35142 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35143 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35144 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35145 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35146 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35147 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35148 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35149 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35150 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35151 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35152 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35153 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35154 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35155 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35156 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35157 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35158 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35159 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35160 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35161 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35162 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35163 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35164 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35165 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35166 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35167 \t\t Training Loss: 0.0005807944107800722 \t\n",
      "Epoch 35168 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35169 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35170 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35171 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35172 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35173 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35174 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35175 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35176 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35177 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35178 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35179 \t\t Training Loss: 0.0005807941779494286 \t\n",
      "Epoch 35180 \t\t Training Loss: 0.0005807942943647504 \t\n",
      "Epoch 35181 \t\t Training Loss: 0.0005807941779494286 \t\n",
      "Epoch 35182 \t\t Training Loss: 0.0005807941779494286 \t\n",
      "Epoch 35183 \t\t Training Loss: 0.0005807941779494286 \t\n",
      "Epoch 35184 \t\t Training Loss: 0.0005807941779494286 \t\n",
      "Epoch 35185 \t\t Training Loss: 0.0005807941779494286 \t\n",
      "Epoch 35186 \t\t Training Loss: 0.0005807941779494286 \t\n",
      "Epoch 35187 \t\t Training Loss: 0.0005807941779494286 \t\n",
      "Epoch 35188 \t\t Training Loss: 0.0005807941779494286 \t\n",
      "Epoch 35189 \t\t Training Loss: 0.0005807941779494286 \t\n",
      "Epoch 35190 \t\t Training Loss: 0.0005807941779494286 \t\n",
      "Epoch 35191 \t\t Training Loss: 0.0005807941779494286 \t\n",
      "Epoch 35192 \t\t Training Loss: 0.0005807941779494286 \t\n",
      "Epoch 35193 \t\t Training Loss: 0.0005807941779494286 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35194 \t\t Training Loss: 0.0005807941197417676 \t\n",
      "Epoch 35195 \t\t Training Loss: 0.0005807941197417676 \t\n",
      "Epoch 35196 \t\t Training Loss: 0.0005807941197417676 \t\n",
      "Epoch 35197 \t\t Training Loss: 0.0005807941197417676 \t\n",
      "Epoch 35198 \t\t Training Loss: 0.0005807941197417676 \t\n",
      "Epoch 35199 \t\t Training Loss: 0.0005807941197417676 \t\n",
      "Epoch 35200 \t\t Training Loss: 0.0005807941197417676 \t\n",
      "Epoch 35201 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35202 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35203 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35204 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35205 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35206 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35207 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35208 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35209 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35210 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35211 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35212 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35213 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35214 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35215 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35216 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35217 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35218 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35219 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35220 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35221 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35222 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35223 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35224 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35225 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35226 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35227 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35228 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35229 \t\t Training Loss: 0.0005807940033264458 \t\n",
      "Epoch 35230 \t\t Training Loss: 0.000580793886911124 \t\n",
      "Epoch 35231 \t\t Training Loss: 0.000580793886911124 \t\n",
      "Epoch 35232 \t\t Training Loss: 0.000580793886911124 \t\n",
      "Epoch 35233 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35234 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35235 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35236 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35237 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35238 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35239 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35240 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35241 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35242 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35243 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35244 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35245 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35246 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35247 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35248 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35249 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35250 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35251 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35252 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35253 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35254 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35255 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35256 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35257 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35258 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35259 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35260 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35261 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35262 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35263 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35264 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35265 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35266 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35267 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35268 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35269 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35270 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35271 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35272 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35273 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35274 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35275 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35276 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35277 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35278 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35279 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35280 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35281 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35282 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35283 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35284 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35285 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35286 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35287 \t\t Training Loss: 0.0005807936540804803 \t\n",
      "Epoch 35288 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35289 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35290 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35291 \t\t Training Loss: 0.0005807938287034631 \t\n",
      "Epoch 35292 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35293 \t\t Training Loss: 0.0005807937122881413 \t\n",
      "Epoch 35294 \t\t Training Loss: 0.0005807935958728194 \t\n",
      "Epoch 35295 \t\t Training Loss: 0.0005807935958728194 \t\n",
      "Epoch 35296 \t\t Training Loss: 0.0005807935958728194 \t\n",
      "Epoch 35297 \t\t Training Loss: 0.0005807935958728194 \t\n",
      "Epoch 35298 \t\t Training Loss: 0.0005807935958728194 \t\n",
      "Epoch 35299 \t\t Training Loss: 0.0005807935958728194 \t\n",
      "Epoch 35300 \t\t Training Loss: 0.0005807935958728194 \t\n",
      "Epoch 35301 \t\t Training Loss: 0.0005807935958728194 \t\n",
      "Epoch 35302 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35303 \t\t Training Loss: 0.0005807935958728194 \t\n",
      "Epoch 35304 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35305 \t\t Training Loss: 0.0005807935958728194 \t\n",
      "Epoch 35306 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35307 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35308 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35309 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35310 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35311 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35312 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35313 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35314 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35315 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35316 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35317 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35318 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35319 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35320 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35321 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35322 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35323 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35324 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35325 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35326 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35327 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35328 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35329 \t\t Training Loss: 0.0005807935958728194 \t\n",
      "Epoch 35330 \t\t Training Loss: 0.0005807935958728194 \t\n",
      "Epoch 35331 \t\t Training Loss: 0.0005807935958728194 \t\n",
      "Epoch 35332 \t\t Training Loss: 0.0005807935958728194 \t\n",
      "Epoch 35333 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35334 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35335 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35336 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35337 \t\t Training Loss: 0.0005807935376651585 \t\n",
      "Epoch 35338 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35339 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35340 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35341 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35342 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35343 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35344 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35345 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35346 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35347 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35348 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35349 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35350 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35351 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35352 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35353 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35354 \t\t Training Loss: 0.0005807933048345149 \t\n",
      "Epoch 35355 \t\t Training Loss: 0.0005807934212498367 \t\n",
      "Epoch 35356 \t\t Training Loss: 0.0005807933048345149 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35357 \t\t Training Loss: 0.0005807933048345149 \t\n",
      "Epoch 35358 \t\t Training Loss: 0.0005807933048345149 \t\n",
      "Epoch 35359 \t\t Training Loss: 0.0005807933048345149 \t\n",
      "Epoch 35360 \t\t Training Loss: 0.0005807933048345149 \t\n",
      "Epoch 35361 \t\t Training Loss: 0.0005807933048345149 \t\n",
      "Epoch 35362 \t\t Training Loss: 0.0005807933048345149 \t\n",
      "Epoch 35363 \t\t Training Loss: 0.0005807932466268539 \t\n",
      "Epoch 35364 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35365 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35366 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35367 \t\t Training Loss: 0.0005807932466268539 \t\n",
      "Epoch 35368 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35369 \t\t Training Loss: 0.0005807932466268539 \t\n",
      "Epoch 35370 \t\t Training Loss: 0.0005807932466268539 \t\n",
      "Epoch 35371 \t\t Training Loss: 0.0005807932466268539 \t\n",
      "Epoch 35372 \t\t Training Loss: 0.0005807932466268539 \t\n",
      "Epoch 35373 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35374 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35375 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35376 \t\t Training Loss: 0.0005807932466268539 \t\n",
      "Epoch 35377 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35378 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35379 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35380 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35381 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35382 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35383 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35384 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35385 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35386 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35387 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35388 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35389 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35390 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35391 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35392 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35393 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35394 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35395 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35396 \t\t Training Loss: 0.000580793188419193 \t\n",
      "Epoch 35397 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35398 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35399 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35400 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35401 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35402 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35403 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35404 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35405 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35406 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35407 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35408 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35409 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35410 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35411 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35412 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35413 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35414 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35415 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35416 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35417 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35418 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35419 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35420 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35421 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35422 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35423 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35424 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35425 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35426 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35427 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35428 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35429 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35430 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35431 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35432 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35433 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35434 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35435 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35436 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35437 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35438 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35439 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35440 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35441 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35442 \t\t Training Loss: 0.0005807931302115321 \t\n",
      "Epoch 35443 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35444 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35445 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35446 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35447 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35448 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35449 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35450 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35451 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35452 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35453 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35454 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35455 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35456 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35457 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35458 \t\t Training Loss: 0.0005807929555885494 \t\n",
      "Epoch 35459 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35460 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35461 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35462 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35463 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35464 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35465 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35466 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35467 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35468 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35469 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35470 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35471 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35472 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35473 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35474 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35475 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35476 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35477 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35478 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35479 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35480 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35481 \t\t Training Loss: 0.0005807930137962103 \t\n",
      "Epoch 35482 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35483 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35484 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35485 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35486 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35487 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35488 \t\t Training Loss: 0.0005807928973808885 \t\n",
      "Epoch 35489 \t\t Training Loss: 0.0005807928391732275 \t\n",
      "Epoch 35490 \t\t Training Loss: 0.0005807928391732275 \t\n",
      "Epoch 35491 \t\t Training Loss: 0.0005807928391732275 \t\n",
      "Epoch 35492 \t\t Training Loss: 0.0005807928391732275 \t\n",
      "Epoch 35493 \t\t Training Loss: 0.0005807928391732275 \t\n",
      "Epoch 35494 \t\t Training Loss: 0.0005807928391732275 \t\n",
      "Epoch 35495 \t\t Training Loss: 0.0005807928391732275 \t\n",
      "Epoch 35496 \t\t Training Loss: 0.0005807928391732275 \t\n",
      "Epoch 35497 \t\t Training Loss: 0.0005807928391732275 \t\n",
      "Epoch 35498 \t\t Training Loss: 0.0005807928391732275 \t\n",
      "Epoch 35499 \t\t Training Loss: 0.0005807928391732275 \t\n",
      "Epoch 35500 \t\t Training Loss: 0.0005807928391732275 \t\n",
      "Epoch 35501 \t\t Training Loss: 0.0005807928391732275 \t\n",
      "Epoch 35502 \t\t Training Loss: 0.0005807928391732275 \t\n",
      "Epoch 35503 \t\t Training Loss: 0.0005807927227579057 \t\n",
      "Epoch 35504 \t\t Training Loss: 0.0005807927227579057 \t\n",
      "Epoch 35505 \t\t Training Loss: 0.0005807927227579057 \t\n",
      "Epoch 35506 \t\t Training Loss: 0.0005807927227579057 \t\n",
      "Epoch 35507 \t\t Training Loss: 0.0005807927227579057 \t\n",
      "Epoch 35508 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35509 \t\t Training Loss: 0.0005807927227579057 \t\n",
      "Epoch 35510 \t\t Training Loss: 0.0005807927227579057 \t\n",
      "Epoch 35511 \t\t Training Loss: 0.0005807927227579057 \t\n",
      "Epoch 35512 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35513 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35514 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35515 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35516 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35517 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35518 \t\t Training Loss: 0.000580792548134923 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35519 \t\t Training Loss: 0.000580792548134923 \t\n",
      "Epoch 35520 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35521 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35522 \t\t Training Loss: 0.000580792548134923 \t\n",
      "Epoch 35523 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35524 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35525 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35526 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35527 \t\t Training Loss: 0.000580792548134923 \t\n",
      "Epoch 35528 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35529 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35530 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35531 \t\t Training Loss: 0.0005807926063425839 \t\n",
      "Epoch 35532 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35533 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35534 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35535 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35536 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35537 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35538 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35539 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35540 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35541 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35542 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35543 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35544 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35545 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35546 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35547 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35548 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35549 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35550 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35551 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35552 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35553 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35554 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35555 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35556 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35557 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35558 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35559 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35560 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35561 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35562 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35563 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35564 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35565 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35566 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35567 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35568 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35569 \t\t Training Loss: 0.0005807924317196012 \t\n",
      "Epoch 35570 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35571 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35572 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35573 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35574 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35575 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35576 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35577 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35578 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35579 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35580 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35581 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35582 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35583 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35584 \t\t Training Loss: 0.0005807923153042793 \t\n",
      "Epoch 35585 \t\t Training Loss: 0.0005807922570966184 \t\n",
      "Epoch 35586 \t\t Training Loss: 0.0005807922570966184 \t\n",
      "Epoch 35587 \t\t Training Loss: 0.0005807922570966184 \t\n",
      "Epoch 35588 \t\t Training Loss: 0.0005807922570966184 \t\n",
      "Epoch 35589 \t\t Training Loss: 0.0005807922570966184 \t\n",
      "Epoch 35590 \t\t Training Loss: 0.0005807922570966184 \t\n",
      "Epoch 35591 \t\t Training Loss: 0.0005807922570966184 \t\n",
      "Epoch 35592 \t\t Training Loss: 0.0005807922570966184 \t\n",
      "Epoch 35593 \t\t Training Loss: 0.0005807922570966184 \t\n",
      "Epoch 35594 \t\t Training Loss: 0.0005807922570966184 \t\n",
      "Epoch 35595 \t\t Training Loss: 0.0005807922570966184 \t\n",
      "Epoch 35596 \t\t Training Loss: 0.0005807921406812966 \t\n",
      "Epoch 35597 \t\t Training Loss: 0.0005807921406812966 \t\n",
      "Epoch 35598 \t\t Training Loss: 0.0005807921406812966 \t\n",
      "Epoch 35599 \t\t Training Loss: 0.0005807921406812966 \t\n",
      "Epoch 35600 \t\t Training Loss: 0.0005807921406812966 \t\n",
      "Epoch 35601 \t\t Training Loss: 0.0005807921406812966 \t\n",
      "Epoch 35602 \t\t Training Loss: 0.0005807921406812966 \t\n",
      "Epoch 35603 \t\t Training Loss: 0.0005807921406812966 \t\n",
      "Epoch 35604 \t\t Training Loss: 0.0005807921406812966 \t\n",
      "Epoch 35605 \t\t Training Loss: 0.0005807921406812966 \t\n",
      "Epoch 35606 \t\t Training Loss: 0.0005807921406812966 \t\n",
      "Epoch 35607 \t\t Training Loss: 0.0005807921406812966 \t\n",
      "Epoch 35608 \t\t Training Loss: 0.0005807921406812966 \t\n",
      "Epoch 35609 \t\t Training Loss: 0.0005807921406812966 \t\n",
      "Epoch 35610 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35611 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35612 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35613 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35614 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35615 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35616 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35617 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35618 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35619 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35620 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35621 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35622 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35623 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35624 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35625 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35626 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35627 \t\t Training Loss: 0.0005807920242659748 \t\n",
      "Epoch 35628 \t\t Training Loss: 0.000580791849642992 \t\n",
      "Epoch 35629 \t\t Training Loss: 0.000580791849642992 \t\n",
      "Epoch 35630 \t\t Training Loss: 0.000580791849642992 \t\n",
      "Epoch 35631 \t\t Training Loss: 0.000580791849642992 \t\n",
      "Epoch 35632 \t\t Training Loss: 0.000580791849642992 \t\n",
      "Epoch 35633 \t\t Training Loss: 0.000580791849642992 \t\n",
      "Epoch 35634 \t\t Training Loss: 0.000580791849642992 \t\n",
      "Epoch 35635 \t\t Training Loss: 0.0005807917332276702 \t\n",
      "Epoch 35636 \t\t Training Loss: 0.0005807917332276702 \t\n",
      "Epoch 35637 \t\t Training Loss: 0.0005807917332276702 \t\n",
      "Epoch 35638 \t\t Training Loss: 0.0005807917332276702 \t\n",
      "Epoch 35639 \t\t Training Loss: 0.0005807917332276702 \t\n",
      "Epoch 35640 \t\t Training Loss: 0.0005807917332276702 \t\n",
      "Epoch 35641 \t\t Training Loss: 0.0005807917332276702 \t\n",
      "Epoch 35642 \t\t Training Loss: 0.0005807917332276702 \t\n",
      "Epoch 35643 \t\t Training Loss: 0.0005807917332276702 \t\n",
      "Epoch 35644 \t\t Training Loss: 0.0005807917332276702 \t\n",
      "Epoch 35645 \t\t Training Loss: 0.0005807917332276702 \t\n",
      "Epoch 35646 \t\t Training Loss: 0.0005807917332276702 \t\n",
      "Epoch 35647 \t\t Training Loss: 0.0005807917332276702 \t\n",
      "Epoch 35648 \t\t Training Loss: 0.0005807917332276702 \t\n",
      "Epoch 35649 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35650 \t\t Training Loss: 0.0005807917332276702 \t\n",
      "Epoch 35651 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35652 \t\t Training Loss: 0.0005807917332276702 \t\n",
      "Epoch 35653 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35654 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35655 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35656 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35657 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35658 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35659 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35660 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35661 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35662 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35663 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35664 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35665 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35666 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35667 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35668 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35669 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35670 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35671 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35672 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35673 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35674 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35675 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35676 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35677 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35678 \t\t Training Loss: 0.0005807916168123484 \t\n",
      "Epoch 35679 \t\t Training Loss: 0.0005807915586046875 \t\n",
      "Epoch 35680 \t\t Training Loss: 0.0005807915586046875 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35681 \t\t Training Loss: 0.0005807915586046875 \t\n",
      "Epoch 35682 \t\t Training Loss: 0.0005807915586046875 \t\n",
      "Epoch 35683 \t\t Training Loss: 0.0005807915586046875 \t\n",
      "Epoch 35684 \t\t Training Loss: 0.0005807915586046875 \t\n",
      "Epoch 35685 \t\t Training Loss: 0.0005807915586046875 \t\n",
      "Epoch 35686 \t\t Training Loss: 0.0005807915586046875 \t\n",
      "Epoch 35687 \t\t Training Loss: 0.0005807915586046875 \t\n",
      "Epoch 35688 \t\t Training Loss: 0.0005807915586046875 \t\n",
      "Epoch 35689 \t\t Training Loss: 0.0005807915586046875 \t\n",
      "Epoch 35690 \t\t Training Loss: 0.0005807915586046875 \t\n",
      "Epoch 35691 \t\t Training Loss: 0.0005807915586046875 \t\n",
      "Epoch 35692 \t\t Training Loss: 0.0005807915586046875 \t\n",
      "Epoch 35693 \t\t Training Loss: 0.0005807914421893656 \t\n",
      "Epoch 35694 \t\t Training Loss: 0.0005807914421893656 \t\n",
      "Epoch 35695 \t\t Training Loss: 0.0005807914421893656 \t\n",
      "Epoch 35696 \t\t Training Loss: 0.0005807914421893656 \t\n",
      "Epoch 35697 \t\t Training Loss: 0.0005807914421893656 \t\n",
      "Epoch 35698 \t\t Training Loss: 0.0005807914421893656 \t\n",
      "Epoch 35699 \t\t Training Loss: 0.0005807914421893656 \t\n",
      "Epoch 35700 \t\t Training Loss: 0.0005807914421893656 \t\n",
      "Epoch 35701 \t\t Training Loss: 0.0005807914421893656 \t\n",
      "Epoch 35702 \t\t Training Loss: 0.0005807913257740438 \t\n",
      "Epoch 35703 \t\t Training Loss: 0.0005807914421893656 \t\n",
      "Epoch 35704 \t\t Training Loss: 0.0005807913257740438 \t\n",
      "Epoch 35705 \t\t Training Loss: 0.0005807913257740438 \t\n",
      "Epoch 35706 \t\t Training Loss: 0.0005807913257740438 \t\n",
      "Epoch 35707 \t\t Training Loss: 0.0005807913257740438 \t\n",
      "Epoch 35708 \t\t Training Loss: 0.0005807913257740438 \t\n",
      "Epoch 35709 \t\t Training Loss: 0.0005807913257740438 \t\n",
      "Epoch 35710 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35711 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35712 \t\t Training Loss: 0.0005807913257740438 \t\n",
      "Epoch 35713 \t\t Training Loss: 0.0005807913257740438 \t\n",
      "Epoch 35714 \t\t Training Loss: 0.0005807913257740438 \t\n",
      "Epoch 35715 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35716 \t\t Training Loss: 0.0005807913257740438 \t\n",
      "Epoch 35717 \t\t Training Loss: 0.0005807913257740438 \t\n",
      "Epoch 35718 \t\t Training Loss: 0.0005807913257740438 \t\n",
      "Epoch 35719 \t\t Training Loss: 0.0005807913257740438 \t\n",
      "Epoch 35720 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35721 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35722 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35723 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35724 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35725 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35726 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35727 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35728 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35729 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35730 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35731 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35732 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35733 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35734 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35735 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35736 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35737 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35738 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35739 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35740 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35741 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35742 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35743 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35744 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35745 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35746 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35747 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35748 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35749 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35750 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35751 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35752 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35753 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35754 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35755 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35756 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35757 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35758 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35759 \t\t Training Loss: 0.000580791209358722 \t\n",
      "Epoch 35760 \t\t Training Loss: 0.000580791209358722 \t\n",
      "Epoch 35761 \t\t Training Loss: 0.000580791209358722 \t\n",
      "Epoch 35762 \t\t Training Loss: 0.000580791209358722 \t\n",
      "Epoch 35763 \t\t Training Loss: 0.000580791209358722 \t\n",
      "Epoch 35764 \t\t Training Loss: 0.0005807912675663829 \t\n",
      "Epoch 35765 \t\t Training Loss: 0.000580791209358722 \t\n",
      "Epoch 35766 \t\t Training Loss: 0.000580791209358722 \t\n",
      "Epoch 35767 \t\t Training Loss: 0.000580791209358722 \t\n",
      "Epoch 35768 \t\t Training Loss: 0.000580791209358722 \t\n",
      "Epoch 35769 \t\t Training Loss: 0.000580791209358722 \t\n",
      "Epoch 35770 \t\t Training Loss: 0.000580791209358722 \t\n",
      "Epoch 35771 \t\t Training Loss: 0.000580791209358722 \t\n",
      "Epoch 35772 \t\t Training Loss: 0.000580791209358722 \t\n",
      "Epoch 35773 \t\t Training Loss: 0.000580791209358722 \t\n",
      "Epoch 35774 \t\t Training Loss: 0.000580791209358722 \t\n",
      "Epoch 35775 \t\t Training Loss: 0.0005807911511510611 \t\n",
      "Epoch 35776 \t\t Training Loss: 0.0005807911511510611 \t\n",
      "Epoch 35777 \t\t Training Loss: 0.0005807911511510611 \t\n",
      "Epoch 35778 \t\t Training Loss: 0.0005807911511510611 \t\n",
      "Epoch 35779 \t\t Training Loss: 0.0005807911511510611 \t\n",
      "Epoch 35780 \t\t Training Loss: 0.0005807911511510611 \t\n",
      "Epoch 35781 \t\t Training Loss: 0.0005807911511510611 \t\n",
      "Epoch 35782 \t\t Training Loss: 0.0005807910347357392 \t\n",
      "Epoch 35783 \t\t Training Loss: 0.0005807911511510611 \t\n",
      "Epoch 35784 \t\t Training Loss: 0.0005807911511510611 \t\n",
      "Epoch 35785 \t\t Training Loss: 0.0005807910347357392 \t\n",
      "Epoch 35786 \t\t Training Loss: 0.0005807910347357392 \t\n",
      "Epoch 35787 \t\t Training Loss: 0.0005807910347357392 \t\n",
      "Epoch 35788 \t\t Training Loss: 0.0005807910347357392 \t\n",
      "Epoch 35789 \t\t Training Loss: 0.0005807910347357392 \t\n",
      "Epoch 35790 \t\t Training Loss: 0.0005807910347357392 \t\n",
      "Epoch 35791 \t\t Training Loss: 0.0005807910347357392 \t\n",
      "Epoch 35792 \t\t Training Loss: 0.0005807910347357392 \t\n",
      "Epoch 35793 \t\t Training Loss: 0.0005807910347357392 \t\n",
      "Epoch 35794 \t\t Training Loss: 0.0005807910347357392 \t\n",
      "Epoch 35795 \t\t Training Loss: 0.0005807910347357392 \t\n",
      "Epoch 35796 \t\t Training Loss: 0.0005807910347357392 \t\n",
      "Epoch 35797 \t\t Training Loss: 0.0005807910347357392 \t\n",
      "Epoch 35798 \t\t Training Loss: 0.0005807909765280783 \t\n",
      "Epoch 35799 \t\t Training Loss: 0.0005807909765280783 \t\n",
      "Epoch 35800 \t\t Training Loss: 0.0005807909765280783 \t\n",
      "Epoch 35801 \t\t Training Loss: 0.0005807909765280783 \t\n",
      "Epoch 35802 \t\t Training Loss: 0.0005807909765280783 \t\n",
      "Epoch 35803 \t\t Training Loss: 0.0005807909765280783 \t\n",
      "Epoch 35804 \t\t Training Loss: 0.0005807909765280783 \t\n",
      "Epoch 35805 \t\t Training Loss: 0.0005807909765280783 \t\n",
      "Epoch 35806 \t\t Training Loss: 0.0005807909765280783 \t\n",
      "Epoch 35807 \t\t Training Loss: 0.0005807909765280783 \t\n",
      "Epoch 35808 \t\t Training Loss: 0.0005807909765280783 \t\n",
      "Epoch 35809 \t\t Training Loss: 0.0005807909765280783 \t\n",
      "Epoch 35810 \t\t Training Loss: 0.0005807909765280783 \t\n",
      "Epoch 35811 \t\t Training Loss: 0.0005807909765280783 \t\n",
      "Epoch 35812 \t\t Training Loss: 0.0005807908601127565 \t\n",
      "Epoch 35813 \t\t Training Loss: 0.0005807908601127565 \t\n",
      "Epoch 35814 \t\t Training Loss: 0.0005807908601127565 \t\n",
      "Epoch 35815 \t\t Training Loss: 0.0005807908601127565 \t\n",
      "Epoch 35816 \t\t Training Loss: 0.0005807907436974347 \t\n",
      "Epoch 35817 \t\t Training Loss: 0.0005807907436974347 \t\n",
      "Epoch 35818 \t\t Training Loss: 0.0005807907436974347 \t\n",
      "Epoch 35819 \t\t Training Loss: 0.0005807907436974347 \t\n",
      "Epoch 35820 \t\t Training Loss: 0.0005807907436974347 \t\n",
      "Epoch 35821 \t\t Training Loss: 0.0005807907436974347 \t\n",
      "Epoch 35822 \t\t Training Loss: 0.0005807907436974347 \t\n",
      "Epoch 35823 \t\t Training Loss: 0.0005807907436974347 \t\n",
      "Epoch 35824 \t\t Training Loss: 0.0005807907436974347 \t\n",
      "Epoch 35825 \t\t Training Loss: 0.0005807907436974347 \t\n",
      "Epoch 35826 \t\t Training Loss: 0.0005807907436974347 \t\n",
      "Epoch 35827 \t\t Training Loss: 0.0005807907436974347 \t\n",
      "Epoch 35828 \t\t Training Loss: 0.0005807907436974347 \t\n",
      "Epoch 35829 \t\t Training Loss: 0.0005807906854897738 \t\n",
      "Epoch 35830 \t\t Training Loss: 0.0005807906854897738 \t\n",
      "Epoch 35831 \t\t Training Loss: 0.0005807906854897738 \t\n",
      "Epoch 35832 \t\t Training Loss: 0.0005807906854897738 \t\n",
      "Epoch 35833 \t\t Training Loss: 0.0005807906854897738 \t\n",
      "Epoch 35834 \t\t Training Loss: 0.0005807906854897738 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35835 \t\t Training Loss: 0.0005807906854897738 \t\n",
      "Epoch 35836 \t\t Training Loss: 0.0005807906854897738 \t\n",
      "Epoch 35837 \t\t Training Loss: 0.0005807906272821128 \t\n",
      "Epoch 35838 \t\t Training Loss: 0.0005807906272821128 \t\n",
      "Epoch 35839 \t\t Training Loss: 0.0005807906272821128 \t\n",
      "Epoch 35840 \t\t Training Loss: 0.0005807906272821128 \t\n",
      "Epoch 35841 \t\t Training Loss: 0.0005807906272821128 \t\n",
      "Epoch 35842 \t\t Training Loss: 0.0005807906272821128 \t\n",
      "Epoch 35843 \t\t Training Loss: 0.0005807905690744519 \t\n",
      "Epoch 35844 \t\t Training Loss: 0.0005807905690744519 \t\n",
      "Epoch 35845 \t\t Training Loss: 0.0005807905690744519 \t\n",
      "Epoch 35846 \t\t Training Loss: 0.0005807905690744519 \t\n",
      "Epoch 35847 \t\t Training Loss: 0.0005807905690744519 \t\n",
      "Epoch 35848 \t\t Training Loss: 0.0005807905690744519 \t\n",
      "Epoch 35849 \t\t Training Loss: 0.0005807905690744519 \t\n",
      "Epoch 35850 \t\t Training Loss: 0.0005807905690744519 \t\n",
      "Epoch 35851 \t\t Training Loss: 0.0005807905690744519 \t\n",
      "Epoch 35852 \t\t Training Loss: 0.0005807905690744519 \t\n",
      "Epoch 35853 \t\t Training Loss: 0.0005807905690744519 \t\n",
      "Epoch 35854 \t\t Training Loss: 0.0005807905690744519 \t\n",
      "Epoch 35855 \t\t Training Loss: 0.0005807905690744519 \t\n",
      "Epoch 35856 \t\t Training Loss: 0.0005807905690744519 \t\n",
      "Epoch 35857 \t\t Training Loss: 0.000580790510866791 \t\n",
      "Epoch 35858 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35859 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35860 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35861 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35862 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35863 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35864 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35865 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35866 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35867 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35868 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35869 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35870 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35871 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35872 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35873 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35874 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35875 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35876 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35877 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35878 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35879 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35880 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35881 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35882 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35883 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35884 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35885 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35886 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35887 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35888 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35889 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35890 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35891 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35892 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35893 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35894 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35895 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35896 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35897 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35898 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35899 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35900 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35901 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35902 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35903 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35904 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35905 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35906 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35907 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35908 \t\t Training Loss: 0.0005807904526591301 \t\n",
      "Epoch 35909 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35910 \t\t Training Loss: 0.0005807903944514692 \t\n",
      "Epoch 35911 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35912 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35913 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35914 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35915 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35916 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35917 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35918 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35919 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35920 \t\t Training Loss: 0.0005807902198284864 \t\n",
      "Epoch 35921 \t\t Training Loss: 0.0005807902198284864 \t\n",
      "Epoch 35922 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35923 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35924 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35925 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35926 \t\t Training Loss: 0.0005807902780361474 \t\n",
      "Epoch 35927 \t\t Training Loss: 0.0005807902198284864 \t\n",
      "Epoch 35928 \t\t Training Loss: 0.0005807902198284864 \t\n",
      "Epoch 35929 \t\t Training Loss: 0.0005807902198284864 \t\n",
      "Epoch 35930 \t\t Training Loss: 0.0005807902198284864 \t\n",
      "Epoch 35931 \t\t Training Loss: 0.0005807902198284864 \t\n",
      "Epoch 35932 \t\t Training Loss: 0.0005807902198284864 \t\n",
      "Epoch 35933 \t\t Training Loss: 0.0005807902198284864 \t\n",
      "Epoch 35934 \t\t Training Loss: 0.0005807902198284864 \t\n",
      "Epoch 35935 \t\t Training Loss: 0.0005807902198284864 \t\n",
      "Epoch 35936 \t\t Training Loss: 0.0005807902198284864 \t\n",
      "Epoch 35937 \t\t Training Loss: 0.0005807902198284864 \t\n",
      "Epoch 35938 \t\t Training Loss: 0.0005807901616208255 \t\n",
      "Epoch 35939 \t\t Training Loss: 0.0005807901616208255 \t\n",
      "Epoch 35940 \t\t Training Loss: 0.0005807902198284864 \t\n",
      "Epoch 35941 \t\t Training Loss: 0.0005807901616208255 \t\n",
      "Epoch 35942 \t\t Training Loss: 0.0005807901616208255 \t\n",
      "Epoch 35943 \t\t Training Loss: 0.0005807901616208255 \t\n",
      "Epoch 35944 \t\t Training Loss: 0.0005807901616208255 \t\n",
      "Epoch 35945 \t\t Training Loss: 0.0005807901034131646 \t\n",
      "Epoch 35946 \t\t Training Loss: 0.0005807901034131646 \t\n",
      "Epoch 35947 \t\t Training Loss: 0.0005807901616208255 \t\n",
      "Epoch 35948 \t\t Training Loss: 0.0005807901034131646 \t\n",
      "Epoch 35949 \t\t Training Loss: 0.0005807900452055037 \t\n",
      "Epoch 35950 \t\t Training Loss: 0.0005807900452055037 \t\n",
      "Epoch 35951 \t\t Training Loss: 0.0005807900452055037 \t\n",
      "Epoch 35952 \t\t Training Loss: 0.0005807900452055037 \t\n",
      "Epoch 35953 \t\t Training Loss: 0.0005807900452055037 \t\n",
      "Epoch 35954 \t\t Training Loss: 0.0005807900452055037 \t\n",
      "Epoch 35955 \t\t Training Loss: 0.0005807900452055037 \t\n",
      "Epoch 35956 \t\t Training Loss: 0.0005807900452055037 \t\n",
      "Epoch 35957 \t\t Training Loss: 0.0005807900452055037 \t\n",
      "Epoch 35958 \t\t Training Loss: 0.0005807900452055037 \t\n",
      "Epoch 35959 \t\t Training Loss: 0.0005807900452055037 \t\n",
      "Epoch 35960 \t\t Training Loss: 0.0005807900452055037 \t\n",
      "Epoch 35961 \t\t Training Loss: 0.0005807900452055037 \t\n",
      "Epoch 35962 \t\t Training Loss: 0.0005807900452055037 \t\n",
      "Epoch 35963 \t\t Training Loss: 0.0005807899869978428 \t\n",
      "Epoch 35964 \t\t Training Loss: 0.0005807899869978428 \t\n",
      "Epoch 35965 \t\t Training Loss: 0.0005807899869978428 \t\n",
      "Epoch 35966 \t\t Training Loss: 0.0005807899869978428 \t\n",
      "Epoch 35967 \t\t Training Loss: 0.0005807899287901819 \t\n",
      "Epoch 35968 \t\t Training Loss: 0.0005807899287901819 \t\n",
      "Epoch 35969 \t\t Training Loss: 0.0005807899287901819 \t\n",
      "Epoch 35970 \t\t Training Loss: 0.0005807899287901819 \t\n",
      "Epoch 35971 \t\t Training Loss: 0.0005807899287901819 \t\n",
      "Epoch 35972 \t\t Training Loss: 0.0005807899869978428 \t\n",
      "Epoch 35973 \t\t Training Loss: 0.0005807899869978428 \t\n",
      "Epoch 35974 \t\t Training Loss: 0.0005807899287901819 \t\n",
      "Epoch 35975 \t\t Training Loss: 0.0005807899287901819 \t\n",
      "Epoch 35976 \t\t Training Loss: 0.0005807899287901819 \t\n",
      "Epoch 35977 \t\t Training Loss: 0.000580789870582521 \t\n",
      "Epoch 35978 \t\t Training Loss: 0.0005807899287901819 \t\n",
      "Epoch 35979 \t\t Training Loss: 0.0005807899287901819 \t\n",
      "Epoch 35980 \t\t Training Loss: 0.000580789870582521 \t\n",
      "Epoch 35981 \t\t Training Loss: 0.0005807899287901819 \t\n",
      "Epoch 35982 \t\t Training Loss: 0.0005807899287901819 \t\n",
      "Epoch 35983 \t\t Training Loss: 0.0005807899287901819 \t\n",
      "Epoch 35984 \t\t Training Loss: 0.0005807899287901819 \t\n",
      "Epoch 35985 \t\t Training Loss: 0.000580789870582521 \t\n",
      "Epoch 35986 \t\t Training Loss: 0.000580789870582521 \t\n",
      "Epoch 35987 \t\t Training Loss: 0.000580789870582521 \t\n",
      "Epoch 35988 \t\t Training Loss: 0.000580789870582521 \t\n",
      "Epoch 35989 \t\t Training Loss: 0.000580789870582521 \t\n",
      "Epoch 35990 \t\t Training Loss: 0.000580789870582521 \t\n",
      "Epoch 35991 \t\t Training Loss: 0.000580789870582521 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35992 \t\t Training Loss: 0.000580789870582521 \t\n",
      "Epoch 35993 \t\t Training Loss: 0.000580789870582521 \t\n",
      "Epoch 35994 \t\t Training Loss: 0.00058078981237486 \t\n",
      "Epoch 35995 \t\t Training Loss: 0.00058078981237486 \t\n",
      "Epoch 35996 \t\t Training Loss: 0.00058078981237486 \t\n",
      "Epoch 35997 \t\t Training Loss: 0.00058078981237486 \t\n",
      "Epoch 35998 \t\t Training Loss: 0.00058078981237486 \t\n",
      "Epoch 35999 \t\t Training Loss: 0.00058078981237486 \t\n",
      "Epoch 36000 \t\t Training Loss: 0.00058078981237486 \t\n",
      "Epoch 36001 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36002 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36003 \t\t Training Loss: 0.00058078981237486 \t\n",
      "Epoch 36004 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36005 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36006 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36007 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36008 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36009 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36010 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36011 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36012 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36013 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36014 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36015 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36016 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36017 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36018 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36019 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36020 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36021 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36022 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36023 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36024 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36025 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36026 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36027 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36028 \t\t Training Loss: 0.0005807896959595382 \t\n",
      "Epoch 36029 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36030 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36031 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36032 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36033 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36034 \t\t Training Loss: 0.0005807895795442164 \t\n",
      "Epoch 36035 \t\t Training Loss: 0.0005807895795442164 \t\n",
      "Epoch 36036 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36037 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36038 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36039 \t\t Training Loss: 0.0005807896377518773 \t\n",
      "Epoch 36040 \t\t Training Loss: 0.0005807895795442164 \t\n",
      "Epoch 36041 \t\t Training Loss: 0.0005807895795442164 \t\n",
      "Epoch 36042 \t\t Training Loss: 0.0005807895795442164 \t\n",
      "Epoch 36043 \t\t Training Loss: 0.0005807895795442164 \t\n",
      "Epoch 36044 \t\t Training Loss: 0.0005807895795442164 \t\n",
      "Epoch 36045 \t\t Training Loss: 0.0005807895795442164 \t\n",
      "Epoch 36046 \t\t Training Loss: 0.0005807895795442164 \t\n",
      "Epoch 36047 \t\t Training Loss: 0.0005807895795442164 \t\n",
      "Epoch 36048 \t\t Training Loss: 0.0005807895795442164 \t\n",
      "Epoch 36049 \t\t Training Loss: 0.0005807895795442164 \t\n",
      "Epoch 36050 \t\t Training Loss: 0.0005807895795442164 \t\n",
      "Epoch 36051 \t\t Training Loss: 0.0005807895795442164 \t\n",
      "Epoch 36052 \t\t Training Loss: 0.0005807895795442164 \t\n",
      "Epoch 36053 \t\t Training Loss: 0.0005807895213365555 \t\n",
      "Epoch 36054 \t\t Training Loss: 0.0005807895213365555 \t\n",
      "Epoch 36055 \t\t Training Loss: 0.0005807895213365555 \t\n",
      "Epoch 36056 \t\t Training Loss: 0.0005807895213365555 \t\n",
      "Epoch 36057 \t\t Training Loss: 0.0005807895213365555 \t\n",
      "Epoch 36058 \t\t Training Loss: 0.0005807895213365555 \t\n",
      "Epoch 36059 \t\t Training Loss: 0.0005807894631288946 \t\n",
      "Epoch 36060 \t\t Training Loss: 0.0005807894631288946 \t\n",
      "Epoch 36061 \t\t Training Loss: 0.0005807894631288946 \t\n",
      "Epoch 36062 \t\t Training Loss: 0.0005807894631288946 \t\n",
      "Epoch 36063 \t\t Training Loss: 0.0005807894631288946 \t\n",
      "Epoch 36064 \t\t Training Loss: 0.0005807894631288946 \t\n",
      "Epoch 36065 \t\t Training Loss: 0.0005807894631288946 \t\n",
      "Epoch 36066 \t\t Training Loss: 0.0005807894631288946 \t\n",
      "Epoch 36067 \t\t Training Loss: 0.0005807894631288946 \t\n",
      "Epoch 36068 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36069 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36070 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36071 \t\t Training Loss: 0.0005807894631288946 \t\n",
      "Epoch 36072 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36073 \t\t Training Loss: 0.0005807894631288946 \t\n",
      "Epoch 36074 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36075 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36076 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36077 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36078 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36079 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36080 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36081 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36082 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36083 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36084 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36085 \t\t Training Loss: 0.0005807893467135727 \t\n",
      "Epoch 36086 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36087 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36088 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36089 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36090 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36091 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36092 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36093 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36094 \t\t Training Loss: 0.0005807894049212337 \t\n",
      "Epoch 36095 \t\t Training Loss: 0.0005807893467135727 \t\n",
      "Epoch 36096 \t\t Training Loss: 0.0005807893467135727 \t\n",
      "Epoch 36097 \t\t Training Loss: 0.0005807893467135727 \t\n",
      "Epoch 36098 \t\t Training Loss: 0.0005807893467135727 \t\n",
      "Epoch 36099 \t\t Training Loss: 0.0005807893467135727 \t\n",
      "Epoch 36100 \t\t Training Loss: 0.0005807893467135727 \t\n",
      "Epoch 36101 \t\t Training Loss: 0.0005807893467135727 \t\n",
      "Epoch 36102 \t\t Training Loss: 0.0005807893467135727 \t\n",
      "Epoch 36103 \t\t Training Loss: 0.0005807893467135727 \t\n",
      "Epoch 36104 \t\t Training Loss: 0.0005807893467135727 \t\n",
      "Epoch 36105 \t\t Training Loss: 0.0005807893467135727 \t\n",
      "Epoch 36106 \t\t Training Loss: 0.0005807893467135727 \t\n",
      "Epoch 36107 \t\t Training Loss: 0.0005807892885059118 \t\n",
      "Epoch 36108 \t\t Training Loss: 0.0005807892885059118 \t\n",
      "Epoch 36109 \t\t Training Loss: 0.0005807892302982509 \t\n",
      "Epoch 36110 \t\t Training Loss: 0.0005807892302982509 \t\n",
      "Epoch 36111 \t\t Training Loss: 0.0005807892302982509 \t\n",
      "Epoch 36112 \t\t Training Loss: 0.0005807892302982509 \t\n",
      "Epoch 36113 \t\t Training Loss: 0.0005807892302982509 \t\n",
      "Epoch 36114 \t\t Training Loss: 0.0005807892302982509 \t\n",
      "Epoch 36115 \t\t Training Loss: 0.0005807892302982509 \t\n",
      "Epoch 36116 \t\t Training Loss: 0.0005807892302982509 \t\n",
      "Epoch 36117 \t\t Training Loss: 0.0005807892302982509 \t\n",
      "Epoch 36118 \t\t Training Loss: 0.0005807892302982509 \t\n",
      "Epoch 36119 \t\t Training Loss: 0.0005807892302982509 \t\n",
      "Epoch 36120 \t\t Training Loss: 0.0005807892302982509 \t\n",
      "Epoch 36121 \t\t Training Loss: 0.0005807892302982509 \t\n",
      "Epoch 36122 \t\t Training Loss: 0.00058078917209059 \t\n",
      "Epoch 36123 \t\t Training Loss: 0.00058078917209059 \t\n",
      "Epoch 36124 \t\t Training Loss: 0.0005807891138829291 \t\n",
      "Epoch 36125 \t\t Training Loss: 0.00058078917209059 \t\n",
      "Epoch 36126 \t\t Training Loss: 0.00058078917209059 \t\n",
      "Epoch 36127 \t\t Training Loss: 0.0005807891138829291 \t\n",
      "Epoch 36128 \t\t Training Loss: 0.0005807891138829291 \t\n",
      "Epoch 36129 \t\t Training Loss: 0.0005807891138829291 \t\n",
      "Epoch 36130 \t\t Training Loss: 0.0005807891138829291 \t\n",
      "Epoch 36131 \t\t Training Loss: 0.0005807891138829291 \t\n",
      "Epoch 36132 \t\t Training Loss: 0.0005807891138829291 \t\n",
      "Epoch 36133 \t\t Training Loss: 0.0005807891138829291 \t\n",
      "Epoch 36134 \t\t Training Loss: 0.0005807892302982509 \t\n",
      "Epoch 36135 \t\t Training Loss: 0.0005807891138829291 \t\n",
      "Epoch 36136 \t\t Training Loss: 0.0005807891138829291 \t\n",
      "Epoch 36137 \t\t Training Loss: 0.0005807891138829291 \t\n",
      "Epoch 36138 \t\t Training Loss: 0.0005807891138829291 \t\n",
      "Epoch 36139 \t\t Training Loss: 0.0005807891138829291 \t\n",
      "Epoch 36140 \t\t Training Loss: 0.0005807891138829291 \t\n",
      "Epoch 36141 \t\t Training Loss: 0.0005807890556752682 \t\n",
      "Epoch 36142 \t\t Training Loss: 0.0005807890556752682 \t\n",
      "Epoch 36143 \t\t Training Loss: 0.0005807890556752682 \t\n",
      "Epoch 36144 \t\t Training Loss: 0.0005807890556752682 \t\n",
      "Epoch 36145 \t\t Training Loss: 0.0005807890556752682 \t\n",
      "Epoch 36146 \t\t Training Loss: 0.0005807890556752682 \t\n",
      "Epoch 36147 \t\t Training Loss: 0.0005807890556752682 \t\n",
      "Epoch 36148 \t\t Training Loss: 0.0005807890556752682 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36149 \t\t Training Loss: 0.0005807890556752682 \t\n",
      "Epoch 36150 \t\t Training Loss: 0.0005807890556752682 \t\n",
      "Epoch 36151 \t\t Training Loss: 0.0005807890556752682 \t\n",
      "Epoch 36152 \t\t Training Loss: 0.0005807889974676073 \t\n",
      "Epoch 36153 \t\t Training Loss: 0.0005807889974676073 \t\n",
      "Epoch 36154 \t\t Training Loss: 0.0005807889974676073 \t\n",
      "Epoch 36155 \t\t Training Loss: 0.0005807889974676073 \t\n",
      "Epoch 36156 \t\t Training Loss: 0.0005807889974676073 \t\n",
      "Epoch 36157 \t\t Training Loss: 0.0005807889974676073 \t\n",
      "Epoch 36158 \t\t Training Loss: 0.0005807889974676073 \t\n",
      "Epoch 36159 \t\t Training Loss: 0.0005807889974676073 \t\n",
      "Epoch 36160 \t\t Training Loss: 0.0005807889974676073 \t\n",
      "Epoch 36161 \t\t Training Loss: 0.0005807889974676073 \t\n",
      "Epoch 36162 \t\t Training Loss: 0.0005807889974676073 \t\n",
      "Epoch 36163 \t\t Training Loss: 0.0005807889974676073 \t\n",
      "Epoch 36164 \t\t Training Loss: 0.0005807889392599463 \t\n",
      "Epoch 36165 \t\t Training Loss: 0.0005807889392599463 \t\n",
      "Epoch 36166 \t\t Training Loss: 0.0005807888810522854 \t\n",
      "Epoch 36167 \t\t Training Loss: 0.0005807888810522854 \t\n",
      "Epoch 36168 \t\t Training Loss: 0.0005807888810522854 \t\n",
      "Epoch 36169 \t\t Training Loss: 0.0005807888810522854 \t\n",
      "Epoch 36170 \t\t Training Loss: 0.0005807888810522854 \t\n",
      "Epoch 36171 \t\t Training Loss: 0.0005807888810522854 \t\n",
      "Epoch 36172 \t\t Training Loss: 0.0005807888810522854 \t\n",
      "Epoch 36173 \t\t Training Loss: 0.0005807888810522854 \t\n",
      "Epoch 36174 \t\t Training Loss: 0.0005807888810522854 \t\n",
      "Epoch 36175 \t\t Training Loss: 0.0005807888810522854 \t\n",
      "Epoch 36176 \t\t Training Loss: 0.0005807888810522854 \t\n",
      "Epoch 36177 \t\t Training Loss: 0.0005807888228446245 \t\n",
      "Epoch 36178 \t\t Training Loss: 0.0005807888228446245 \t\n",
      "Epoch 36179 \t\t Training Loss: 0.0005807888228446245 \t\n",
      "Epoch 36180 \t\t Training Loss: 0.0005807888228446245 \t\n",
      "Epoch 36181 \t\t Training Loss: 0.0005807888228446245 \t\n",
      "Epoch 36182 \t\t Training Loss: 0.0005807888228446245 \t\n",
      "Epoch 36183 \t\t Training Loss: 0.0005807888228446245 \t\n",
      "Epoch 36184 \t\t Training Loss: 0.0005807888228446245 \t\n",
      "Epoch 36185 \t\t Training Loss: 0.0005807887646369636 \t\n",
      "Epoch 36186 \t\t Training Loss: 0.0005807887646369636 \t\n",
      "Epoch 36187 \t\t Training Loss: 0.0005807888228446245 \t\n",
      "Epoch 36188 \t\t Training Loss: 0.0005807887646369636 \t\n",
      "Epoch 36189 \t\t Training Loss: 0.0005807887646369636 \t\n",
      "Epoch 36190 \t\t Training Loss: 0.0005807887646369636 \t\n",
      "Epoch 36191 \t\t Training Loss: 0.0005807887646369636 \t\n",
      "Epoch 36192 \t\t Training Loss: 0.0005807887646369636 \t\n",
      "Epoch 36193 \t\t Training Loss: 0.0005807887646369636 \t\n",
      "Epoch 36194 \t\t Training Loss: 0.0005807887646369636 \t\n",
      "Epoch 36195 \t\t Training Loss: 0.0005807887064293027 \t\n",
      "Epoch 36196 \t\t Training Loss: 0.0005807887646369636 \t\n",
      "Epoch 36197 \t\t Training Loss: 0.0005807886482216418 \t\n",
      "Epoch 36198 \t\t Training Loss: 0.0005807886482216418 \t\n",
      "Epoch 36199 \t\t Training Loss: 0.0005807885900139809 \t\n",
      "Epoch 36200 \t\t Training Loss: 0.0005807886482216418 \t\n",
      "Epoch 36201 \t\t Training Loss: 0.0005807885900139809 \t\n",
      "Epoch 36202 \t\t Training Loss: 0.0005807885900139809 \t\n",
      "Epoch 36203 \t\t Training Loss: 0.0005807885900139809 \t\n",
      "Epoch 36204 \t\t Training Loss: 0.0005807885900139809 \t\n",
      "Epoch 36205 \t\t Training Loss: 0.0005807885900139809 \t\n",
      "Epoch 36206 \t\t Training Loss: 0.0005807885900139809 \t\n",
      "Epoch 36207 \t\t Training Loss: 0.0005807885900139809 \t\n",
      "Epoch 36208 \t\t Training Loss: 0.0005807885900139809 \t\n",
      "Epoch 36209 \t\t Training Loss: 0.0005807885900139809 \t\n",
      "Epoch 36210 \t\t Training Loss: 0.0005807885900139809 \t\n",
      "Epoch 36211 \t\t Training Loss: 0.0005807885900139809 \t\n",
      "Epoch 36212 \t\t Training Loss: 0.00058078853180632 \t\n",
      "Epoch 36213 \t\t Training Loss: 0.00058078853180632 \t\n",
      "Epoch 36214 \t\t Training Loss: 0.00058078853180632 \t\n",
      "Epoch 36215 \t\t Training Loss: 0.00058078853180632 \t\n",
      "Epoch 36216 \t\t Training Loss: 0.00058078853180632 \t\n",
      "Epoch 36217 \t\t Training Loss: 0.000580788473598659 \t\n",
      "Epoch 36218 \t\t Training Loss: 0.000580788473598659 \t\n",
      "Epoch 36219 \t\t Training Loss: 0.000580788473598659 \t\n",
      "Epoch 36220 \t\t Training Loss: 0.0005807883571833372 \t\n",
      "Epoch 36221 \t\t Training Loss: 0.0005807883571833372 \t\n",
      "Epoch 36222 \t\t Training Loss: 0.0005807883571833372 \t\n",
      "Epoch 36223 \t\t Training Loss: 0.0005807883571833372 \t\n",
      "Epoch 36224 \t\t Training Loss: 0.0005807883571833372 \t\n",
      "Epoch 36225 \t\t Training Loss: 0.0005807883571833372 \t\n",
      "Epoch 36226 \t\t Training Loss: 0.0005807882989756763 \t\n",
      "Epoch 36227 \t\t Training Loss: 0.0005807882989756763 \t\n",
      "Epoch 36228 \t\t Training Loss: 0.0005807883571833372 \t\n",
      "Epoch 36229 \t\t Training Loss: 0.0005807882989756763 \t\n",
      "Epoch 36230 \t\t Training Loss: 0.0005807882989756763 \t\n",
      "Epoch 36231 \t\t Training Loss: 0.0005807882989756763 \t\n",
      "Epoch 36232 \t\t Training Loss: 0.0005807882989756763 \t\n",
      "Epoch 36233 \t\t Training Loss: 0.0005807882989756763 \t\n",
      "Epoch 36234 \t\t Training Loss: 0.0005807882989756763 \t\n",
      "Epoch 36235 \t\t Training Loss: 0.0005807882989756763 \t\n",
      "Epoch 36236 \t\t Training Loss: 0.0005807882407680154 \t\n",
      "Epoch 36237 \t\t Training Loss: 0.0005807882989756763 \t\n",
      "Epoch 36238 \t\t Training Loss: 0.0005807881825603545 \t\n",
      "Epoch 36239 \t\t Training Loss: 0.0005807881825603545 \t\n",
      "Epoch 36240 \t\t Training Loss: 0.0005807881825603545 \t\n",
      "Epoch 36241 \t\t Training Loss: 0.0005807881825603545 \t\n",
      "Epoch 36242 \t\t Training Loss: 0.0005807880661450326 \t\n",
      "Epoch 36243 \t\t Training Loss: 0.0005807880661450326 \t\n",
      "Epoch 36244 \t\t Training Loss: 0.0005807880661450326 \t\n",
      "Epoch 36245 \t\t Training Loss: 0.0005807881825603545 \t\n",
      "Epoch 36246 \t\t Training Loss: 0.0005807881825603545 \t\n",
      "Epoch 36247 \t\t Training Loss: 0.0005807881825603545 \t\n",
      "Epoch 36248 \t\t Training Loss: 0.0005807881825603545 \t\n",
      "Epoch 36249 \t\t Training Loss: 0.0005807880661450326 \t\n",
      "Epoch 36250 \t\t Training Loss: 0.0005807880661450326 \t\n",
      "Epoch 36251 \t\t Training Loss: 0.0005807880661450326 \t\n",
      "Epoch 36252 \t\t Training Loss: 0.0005807880661450326 \t\n",
      "Epoch 36253 \t\t Training Loss: 0.0005807880661450326 \t\n",
      "Epoch 36254 \t\t Training Loss: 0.0005807880661450326 \t\n",
      "Epoch 36255 \t\t Training Loss: 0.0005807880661450326 \t\n",
      "Epoch 36256 \t\t Training Loss: 0.0005807880661450326 \t\n",
      "Epoch 36257 \t\t Training Loss: 0.0005807880661450326 \t\n",
      "Epoch 36258 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36259 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36260 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36261 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36262 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36263 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36264 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36265 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36266 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36267 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36268 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36269 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36270 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36271 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36272 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36273 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36274 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36275 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36276 \t\t Training Loss: 0.0005807880079373717 \t\n",
      "Epoch 36277 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36278 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36279 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36280 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36281 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36282 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36283 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36284 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36285 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36286 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36287 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36288 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36289 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36290 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36291 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36292 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36293 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36294 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36295 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36296 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36297 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36298 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36299 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36300 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36301 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36302 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36303 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36304 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36305 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36306 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36307 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36308 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36309 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36310 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36311 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36312 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36313 \t\t Training Loss: 0.0005807879497297108 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36314 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36315 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36316 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36317 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36318 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36319 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36320 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36321 \t\t Training Loss: 0.0005807879497297108 \t\n",
      "Epoch 36322 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36323 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36324 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36325 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36326 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36327 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36328 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36329 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36330 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36331 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36332 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36333 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36334 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36335 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36336 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36337 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36338 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36339 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36340 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36341 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36342 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36343 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36344 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36345 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36346 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36347 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36348 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36349 \t\t Training Loss: 0.0005807877751067281 \t\n",
      "Epoch 36350 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36351 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36352 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36353 \t\t Training Loss: 0.0005807878915220499 \t\n",
      "Epoch 36354 \t\t Training Loss: 0.0005807877751067281 \t\n",
      "Epoch 36355 \t\t Training Loss: 0.0005807877751067281 \t\n",
      "Epoch 36356 \t\t Training Loss: 0.0005807877751067281 \t\n",
      "Epoch 36357 \t\t Training Loss: 0.0005807877751067281 \t\n",
      "Epoch 36358 \t\t Training Loss: 0.0005807877751067281 \t\n",
      "Epoch 36359 \t\t Training Loss: 0.0005807877751067281 \t\n",
      "Epoch 36360 \t\t Training Loss: 0.0005807877751067281 \t\n",
      "Epoch 36361 \t\t Training Loss: 0.0005807877168990672 \t\n",
      "Epoch 36362 \t\t Training Loss: 0.0005807877168990672 \t\n",
      "Epoch 36363 \t\t Training Loss: 0.0005807877168990672 \t\n",
      "Epoch 36364 \t\t Training Loss: 0.0005807877168990672 \t\n",
      "Epoch 36365 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36366 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36367 \t\t Training Loss: 0.0005807877168990672 \t\n",
      "Epoch 36368 \t\t Training Loss: 0.0005807877168990672 \t\n",
      "Epoch 36369 \t\t Training Loss: 0.0005807877168990672 \t\n",
      "Epoch 36370 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36371 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36372 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36373 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36374 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36375 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36376 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36377 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36378 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36379 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36380 \t\t Training Loss: 0.0005807877168990672 \t\n",
      "Epoch 36381 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36382 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36383 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36384 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36385 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36386 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36387 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36388 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36389 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36390 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36391 \t\t Training Loss: 0.0005807876004837453 \t\n",
      "Epoch 36392 \t\t Training Loss: 0.0005807876004837453 \t\n",
      "Epoch 36393 \t\t Training Loss: 0.0005807876004837453 \t\n",
      "Epoch 36394 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36395 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36396 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36397 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36398 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36399 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36400 \t\t Training Loss: 0.0005807876004837453 \t\n",
      "Epoch 36401 \t\t Training Loss: 0.0005807876004837453 \t\n",
      "Epoch 36402 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36403 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36404 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36405 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36406 \t\t Training Loss: 0.0005807876004837453 \t\n",
      "Epoch 36407 \t\t Training Loss: 0.0005807876586914062 \t\n",
      "Epoch 36408 \t\t Training Loss: 0.0005807876004837453 \t\n",
      "Epoch 36409 \t\t Training Loss: 0.0005807876004837453 \t\n",
      "Epoch 36410 \t\t Training Loss: 0.0005807876004837453 \t\n",
      "Epoch 36411 \t\t Training Loss: 0.0005807876004837453 \t\n",
      "Epoch 36412 \t\t Training Loss: 0.0005807876004837453 \t\n",
      "Epoch 36413 \t\t Training Loss: 0.0005807876004837453 \t\n",
      "Epoch 36414 \t\t Training Loss: 0.0005807874840684235 \t\n",
      "Epoch 36415 \t\t Training Loss: 0.0005807874258607626 \t\n",
      "Epoch 36416 \t\t Training Loss: 0.0005807874258607626 \t\n",
      "Epoch 36417 \t\t Training Loss: 0.0005807874840684235 \t\n",
      "Epoch 36418 \t\t Training Loss: 0.0005807874258607626 \t\n",
      "Epoch 36419 \t\t Training Loss: 0.0005807874258607626 \t\n",
      "Epoch 36420 \t\t Training Loss: 0.0005807873676531017 \t\n",
      "Epoch 36421 \t\t Training Loss: 0.0005807874258607626 \t\n",
      "Epoch 36422 \t\t Training Loss: 0.0005807873676531017 \t\n",
      "Epoch 36423 \t\t Training Loss: 0.0005807873676531017 \t\n",
      "Epoch 36424 \t\t Training Loss: 0.0005807873676531017 \t\n",
      "Epoch 36425 \t\t Training Loss: 0.0005807873676531017 \t\n",
      "Epoch 36426 \t\t Training Loss: 0.0005807873676531017 \t\n",
      "Epoch 36427 \t\t Training Loss: 0.0005807873676531017 \t\n",
      "Epoch 36428 \t\t Training Loss: 0.0005807873676531017 \t\n",
      "Epoch 36429 \t\t Training Loss: 0.0005807873676531017 \t\n",
      "Epoch 36430 \t\t Training Loss: 0.0005807873676531017 \t\n",
      "Epoch 36431 \t\t Training Loss: 0.0005807873676531017 \t\n",
      "Epoch 36432 \t\t Training Loss: 0.0005807873676531017 \t\n",
      "Epoch 36433 \t\t Training Loss: 0.0005807873094454408 \t\n",
      "Epoch 36434 \t\t Training Loss: 0.0005807873094454408 \t\n",
      "Epoch 36435 \t\t Training Loss: 0.0005807873676531017 \t\n",
      "Epoch 36436 \t\t Training Loss: 0.0005807873094454408 \t\n",
      "Epoch 36437 \t\t Training Loss: 0.0005807873094454408 \t\n",
      "Epoch 36438 \t\t Training Loss: 0.0005807873676531017 \t\n",
      "Epoch 36439 \t\t Training Loss: 0.0005807873094454408 \t\n",
      "Epoch 36440 \t\t Training Loss: 0.0005807873094454408 \t\n",
      "Epoch 36441 \t\t Training Loss: 0.0005807873094454408 \t\n",
      "Epoch 36442 \t\t Training Loss: 0.0005807873094454408 \t\n",
      "Epoch 36443 \t\t Training Loss: 0.0005807873094454408 \t\n",
      "Epoch 36444 \t\t Training Loss: 0.0005807873094454408 \t\n",
      "Epoch 36445 \t\t Training Loss: 0.0005807873094454408 \t\n",
      "Epoch 36446 \t\t Training Loss: 0.0005807873094454408 \t\n",
      "Epoch 36447 \t\t Training Loss: 0.0005807873094454408 \t\n",
      "Epoch 36448 \t\t Training Loss: 0.0005807873094454408 \t\n",
      "Epoch 36449 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36450 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36451 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36452 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36453 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36454 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36455 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36456 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36457 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36458 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36459 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36460 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36461 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36462 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36463 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36464 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36465 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36466 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36467 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36468 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36469 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36470 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36471 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36472 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36473 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36474 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36475 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36476 \t\t Training Loss: 0.0005807871930301189 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36477 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36478 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36479 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36480 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36481 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36482 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36483 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36484 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36485 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36486 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36487 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36488 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36489 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36490 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36491 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36492 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36493 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36494 \t\t Training Loss: 0.0005807871930301189 \t\n",
      "Epoch 36495 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36496 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36497 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36498 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36499 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36500 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36501 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36502 \t\t Training Loss: 0.0005807870766147971 \t\n",
      "Epoch 36503 \t\t Training Loss: 0.0005807870184071362 \t\n",
      "Epoch 36504 \t\t Training Loss: 0.0005807870184071362 \t\n",
      "Epoch 36505 \t\t Training Loss: 0.0005807870184071362 \t\n",
      "Epoch 36506 \t\t Training Loss: 0.0005807870184071362 \t\n",
      "Epoch 36507 \t\t Training Loss: 0.0005807870184071362 \t\n",
      "Epoch 36508 \t\t Training Loss: 0.0005807870184071362 \t\n",
      "Epoch 36509 \t\t Training Loss: 0.0005807870184071362 \t\n",
      "Epoch 36510 \t\t Training Loss: 0.0005807870184071362 \t\n",
      "Epoch 36511 \t\t Training Loss: 0.0005807870184071362 \t\n",
      "Epoch 36512 \t\t Training Loss: 0.0005807870184071362 \t\n",
      "Epoch 36513 \t\t Training Loss: 0.0005807870184071362 \t\n",
      "Epoch 36514 \t\t Training Loss: 0.0005807870184071362 \t\n",
      "Epoch 36515 \t\t Training Loss: 0.0005807870184071362 \t\n",
      "Epoch 36516 \t\t Training Loss: 0.0005807869019918144 \t\n",
      "Epoch 36517 \t\t Training Loss: 0.0005807869019918144 \t\n",
      "Epoch 36518 \t\t Training Loss: 0.0005807869019918144 \t\n",
      "Epoch 36519 \t\t Training Loss: 0.0005807869019918144 \t\n",
      "Epoch 36520 \t\t Training Loss: 0.0005807869019918144 \t\n",
      "Epoch 36521 \t\t Training Loss: 0.0005807869019918144 \t\n",
      "Epoch 36522 \t\t Training Loss: 0.0005807869019918144 \t\n",
      "Epoch 36523 \t\t Training Loss: 0.0005807869019918144 \t\n",
      "Epoch 36524 \t\t Training Loss: 0.0005807869019918144 \t\n",
      "Epoch 36525 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36526 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36527 \t\t Training Loss: 0.0005807869019918144 \t\n",
      "Epoch 36528 \t\t Training Loss: 0.0005807869019918144 \t\n",
      "Epoch 36529 \t\t Training Loss: 0.0005807869019918144 \t\n",
      "Epoch 36530 \t\t Training Loss: 0.0005807869019918144 \t\n",
      "Epoch 36531 \t\t Training Loss: 0.0005807869019918144 \t\n",
      "Epoch 36532 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36533 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36534 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36535 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36536 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36537 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36538 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36539 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36540 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36541 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36542 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36543 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36544 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36545 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36546 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36547 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36548 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36549 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36550 \t\t Training Loss: 0.0005807867855764925 \t\n",
      "Epoch 36551 \t\t Training Loss: 0.0005807866691611707 \t\n",
      "Epoch 36552 \t\t Training Loss: 0.0005807866691611707 \t\n",
      "Epoch 36553 \t\t Training Loss: 0.0005807866691611707 \t\n",
      "Epoch 36554 \t\t Training Loss: 0.0005807866691611707 \t\n",
      "Epoch 36555 \t\t Training Loss: 0.0005807866691611707 \t\n",
      "Epoch 36556 \t\t Training Loss: 0.0005807866691611707 \t\n",
      "Epoch 36557 \t\t Training Loss: 0.0005807866691611707 \t\n",
      "Epoch 36558 \t\t Training Loss: 0.0005807866691611707 \t\n",
      "Epoch 36559 \t\t Training Loss: 0.0005807866691611707 \t\n",
      "Epoch 36560 \t\t Training Loss: 0.0005807866691611707 \t\n",
      "Epoch 36561 \t\t Training Loss: 0.0005807866109535098 \t\n",
      "Epoch 36562 \t\t Training Loss: 0.0005807866109535098 \t\n",
      "Epoch 36563 \t\t Training Loss: 0.0005807866109535098 \t\n",
      "Epoch 36564 \t\t Training Loss: 0.0005807866109535098 \t\n",
      "Epoch 36565 \t\t Training Loss: 0.0005807866109535098 \t\n",
      "Epoch 36566 \t\t Training Loss: 0.0005807866109535098 \t\n",
      "Epoch 36567 \t\t Training Loss: 0.0005807866109535098 \t\n",
      "Epoch 36568 \t\t Training Loss: 0.0005807866109535098 \t\n",
      "Epoch 36569 \t\t Training Loss: 0.0005807866109535098 \t\n",
      "Epoch 36570 \t\t Training Loss: 0.0005807866109535098 \t\n",
      "Epoch 36571 \t\t Training Loss: 0.0005807866109535098 \t\n",
      "Epoch 36572 \t\t Training Loss: 0.0005807866109535098 \t\n",
      "Epoch 36573 \t\t Training Loss: 0.0005807866109535098 \t\n",
      "Epoch 36574 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36575 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36576 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36577 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36578 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36579 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36580 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36581 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36582 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36583 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36584 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36585 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36586 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36587 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36588 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36589 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36590 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36591 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36592 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36593 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36594 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36595 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36596 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36597 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36598 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36599 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36600 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36601 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36602 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36603 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36604 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36605 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36606 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36607 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36608 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36609 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36610 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36611 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36612 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36613 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36614 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36615 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36616 \t\t Training Loss: 0.000580786494538188 \t\n",
      "Epoch 36617 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36618 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36619 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36620 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36621 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36622 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36623 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36624 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36625 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36626 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36627 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36628 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36629 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36630 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36631 \t\t Training Loss: 0.0005807863199152052 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36632 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36633 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36634 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36635 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36636 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36637 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36638 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36639 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36640 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36641 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36642 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36643 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36644 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36645 \t\t Training Loss: 0.0005807862034998834 \t\n",
      "Epoch 36646 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36647 \t\t Training Loss: 0.0005807863199152052 \t\n",
      "Epoch 36648 \t\t Training Loss: 0.0005807862034998834 \t\n",
      "Epoch 36649 \t\t Training Loss: 0.0005807862034998834 \t\n",
      "Epoch 36650 \t\t Training Loss: 0.0005807862034998834 \t\n",
      "Epoch 36651 \t\t Training Loss: 0.0005807862034998834 \t\n",
      "Epoch 36652 \t\t Training Loss: 0.0005807862034998834 \t\n",
      "Epoch 36653 \t\t Training Loss: 0.0005807862034998834 \t\n",
      "Epoch 36654 \t\t Training Loss: 0.0005807862034998834 \t\n",
      "Epoch 36655 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36656 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36657 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36658 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36659 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36660 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36661 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36662 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36663 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36664 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36665 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36666 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36667 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36668 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36669 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36670 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36671 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36672 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36673 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36674 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36675 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36676 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36677 \t\t Training Loss: 0.0005807860288769007 \t\n",
      "Epoch 36678 \t\t Training Loss: 0.0005807860288769007 \t\n",
      "Epoch 36679 \t\t Training Loss: 0.0005807860288769007 \t\n",
      "Epoch 36680 \t\t Training Loss: 0.0005807860288769007 \t\n",
      "Epoch 36681 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36682 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36683 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36684 \t\t Training Loss: 0.0005807860288769007 \t\n",
      "Epoch 36685 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36686 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36687 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36688 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36689 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36690 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36691 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36692 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36693 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36694 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36695 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36696 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36697 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36698 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36699 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36700 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36701 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36702 \t\t Training Loss: 0.0005807860288769007 \t\n",
      "Epoch 36703 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36704 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36705 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36706 \t\t Training Loss: 0.0005807860870845616 \t\n",
      "Epoch 36707 \t\t Training Loss: 0.0005807860288769007 \t\n",
      "Epoch 36708 \t\t Training Loss: 0.0005807860288769007 \t\n",
      "Epoch 36709 \t\t Training Loss: 0.0005807860288769007 \t\n",
      "Epoch 36710 \t\t Training Loss: 0.0005807860288769007 \t\n",
      "Epoch 36711 \t\t Training Loss: 0.0005807860288769007 \t\n",
      "Epoch 36712 \t\t Training Loss: 0.0005807860288769007 \t\n",
      "Epoch 36713 \t\t Training Loss: 0.0005807860288769007 \t\n",
      "Epoch 36714 \t\t Training Loss: 0.0005807860288769007 \t\n",
      "Epoch 36715 \t\t Training Loss: 0.0005807860288769007 \t\n",
      "Epoch 36716 \t\t Training Loss: 0.0005807859124615788 \t\n",
      "Epoch 36717 \t\t Training Loss: 0.0005807859124615788 \t\n",
      "Epoch 36718 \t\t Training Loss: 0.0005807859124615788 \t\n",
      "Epoch 36719 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36720 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36721 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36722 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36723 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36724 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36725 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36726 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36727 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36728 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36729 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36730 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36731 \t\t Training Loss: 0.0005807857378385961 \t\n",
      "Epoch 36732 \t\t Training Loss: 0.0005807857378385961 \t\n",
      "Epoch 36733 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36734 \t\t Training Loss: 0.0005807857378385961 \t\n",
      "Epoch 36735 \t\t Training Loss: 0.0005807857378385961 \t\n",
      "Epoch 36736 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36737 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36738 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36739 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36740 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36741 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36742 \t\t Training Loss: 0.0005807857378385961 \t\n",
      "Epoch 36743 \t\t Training Loss: 0.0005807857378385961 \t\n",
      "Epoch 36744 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36745 \t\t Training Loss: 0.000580785796046257 \t\n",
      "Epoch 36746 \t\t Training Loss: 0.0005807857378385961 \t\n",
      "Epoch 36747 \t\t Training Loss: 0.0005807857378385961 \t\n",
      "Epoch 36748 \t\t Training Loss: 0.0005807857378385961 \t\n",
      "Epoch 36749 \t\t Training Loss: 0.0005807857378385961 \t\n",
      "Epoch 36750 \t\t Training Loss: 0.0005807857378385961 \t\n",
      "Epoch 36751 \t\t Training Loss: 0.0005807856796309352 \t\n",
      "Epoch 36752 \t\t Training Loss: 0.0005807856796309352 \t\n",
      "Epoch 36753 \t\t Training Loss: 0.0005807856796309352 \t\n",
      "Epoch 36754 \t\t Training Loss: 0.0005807856796309352 \t\n",
      "Epoch 36755 \t\t Training Loss: 0.0005807856796309352 \t\n",
      "Epoch 36756 \t\t Training Loss: 0.0005807856796309352 \t\n",
      "Epoch 36757 \t\t Training Loss: 0.0005807856796309352 \t\n",
      "Epoch 36758 \t\t Training Loss: 0.0005807856214232743 \t\n",
      "Epoch 36759 \t\t Training Loss: 0.0005807856214232743 \t\n",
      "Epoch 36760 \t\t Training Loss: 0.0005807856796309352 \t\n",
      "Epoch 36761 \t\t Training Loss: 0.0005807856214232743 \t\n",
      "Epoch 36762 \t\t Training Loss: 0.0005807856796309352 \t\n",
      "Epoch 36763 \t\t Training Loss: 0.0005807856796309352 \t\n",
      "Epoch 36764 \t\t Training Loss: 0.0005807856214232743 \t\n",
      "Epoch 36765 \t\t Training Loss: 0.0005807856214232743 \t\n",
      "Epoch 36766 \t\t Training Loss: 0.0005807856214232743 \t\n",
      "Epoch 36767 \t\t Training Loss: 0.0005807856214232743 \t\n",
      "Epoch 36768 \t\t Training Loss: 0.0005807856214232743 \t\n",
      "Epoch 36769 \t\t Training Loss: 0.0005807856214232743 \t\n",
      "Epoch 36770 \t\t Training Loss: 0.0005807856214232743 \t\n",
      "Epoch 36771 \t\t Training Loss: 0.0005807856214232743 \t\n",
      "Epoch 36772 \t\t Training Loss: 0.0005807856214232743 \t\n",
      "Epoch 36773 \t\t Training Loss: 0.0005807856214232743 \t\n",
      "Epoch 36774 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36775 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36776 \t\t Training Loss: 0.0005807856214232743 \t\n",
      "Epoch 36777 \t\t Training Loss: 0.0005807856214232743 \t\n",
      "Epoch 36778 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36779 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36780 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36781 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36782 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36783 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36784 \t\t Training Loss: 0.0005807855050079525 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36785 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36786 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36787 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36788 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36789 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36790 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36791 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36792 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36793 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36794 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36795 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36796 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36797 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36798 \t\t Training Loss: 0.0005807853885926306 \t\n",
      "Epoch 36799 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36800 \t\t Training Loss: 0.0005807855050079525 \t\n",
      "Epoch 36801 \t\t Training Loss: 0.0005807853885926306 \t\n",
      "Epoch 36802 \t\t Training Loss: 0.0005807853885926306 \t\n",
      "Epoch 36803 \t\t Training Loss: 0.0005807853885926306 \t\n",
      "Epoch 36804 \t\t Training Loss: 0.0005807853885926306 \t\n",
      "Epoch 36805 \t\t Training Loss: 0.0005807853885926306 \t\n",
      "Epoch 36806 \t\t Training Loss: 0.0005807853885926306 \t\n",
      "Epoch 36807 \t\t Training Loss: 0.0005807853885926306 \t\n",
      "Epoch 36808 \t\t Training Loss: 0.0005807853885926306 \t\n",
      "Epoch 36809 \t\t Training Loss: 0.0005807853885926306 \t\n",
      "Epoch 36810 \t\t Training Loss: 0.0005807853885926306 \t\n",
      "Epoch 36811 \t\t Training Loss: 0.0005807853885926306 \t\n",
      "Epoch 36812 \t\t Training Loss: 0.0005807853885926306 \t\n",
      "Epoch 36813 \t\t Training Loss: 0.0005807853303849697 \t\n",
      "Epoch 36814 \t\t Training Loss: 0.0005807853303849697 \t\n",
      "Epoch 36815 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36816 \t\t Training Loss: 0.0005807853303849697 \t\n",
      "Epoch 36817 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36818 \t\t Training Loss: 0.0005807853303849697 \t\n",
      "Epoch 36819 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36820 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36821 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36822 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36823 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36824 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36825 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36826 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36827 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36828 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36829 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36830 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36831 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36832 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36833 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36834 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36835 \t\t Training Loss: 0.0005807852139696479 \t\n",
      "Epoch 36836 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36837 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36838 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36839 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36840 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36841 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36842 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36843 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36844 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36845 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36846 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36847 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36848 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36849 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36850 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36851 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36852 \t\t Training Loss: 0.0005807850975543261 \t\n",
      "Epoch 36853 \t\t Training Loss: 0.0005807850393466651 \t\n",
      "Epoch 36854 \t\t Training Loss: 0.0005807850393466651 \t\n",
      "Epoch 36855 \t\t Training Loss: 0.0005807850393466651 \t\n",
      "Epoch 36856 \t\t Training Loss: 0.0005807850393466651 \t\n",
      "Epoch 36857 \t\t Training Loss: 0.0005807850393466651 \t\n",
      "Epoch 36858 \t\t Training Loss: 0.0005807850393466651 \t\n",
      "Epoch 36859 \t\t Training Loss: 0.0005807850393466651 \t\n",
      "Epoch 36860 \t\t Training Loss: 0.0005807849229313433 \t\n",
      "Epoch 36861 \t\t Training Loss: 0.0005807849229313433 \t\n",
      "Epoch 36862 \t\t Training Loss: 0.0005807849229313433 \t\n",
      "Epoch 36863 \t\t Training Loss: 0.0005807850393466651 \t\n",
      "Epoch 36864 \t\t Training Loss: 0.0005807850393466651 \t\n",
      "Epoch 36865 \t\t Training Loss: 0.0005807850393466651 \t\n",
      "Epoch 36866 \t\t Training Loss: 0.0005807849229313433 \t\n",
      "Epoch 36867 \t\t Training Loss: 0.0005807849229313433 \t\n",
      "Epoch 36868 \t\t Training Loss: 0.0005807848065160215 \t\n",
      "Epoch 36869 \t\t Training Loss: 0.0005807848065160215 \t\n",
      "Epoch 36870 \t\t Training Loss: 0.0005807848065160215 \t\n",
      "Epoch 36871 \t\t Training Loss: 0.0005807848065160215 \t\n",
      "Epoch 36872 \t\t Training Loss: 0.0005807848065160215 \t\n",
      "Epoch 36873 \t\t Training Loss: 0.0005807848065160215 \t\n",
      "Epoch 36874 \t\t Training Loss: 0.0005807848065160215 \t\n",
      "Epoch 36875 \t\t Training Loss: 0.0005807848065160215 \t\n",
      "Epoch 36876 \t\t Training Loss: 0.0005807848065160215 \t\n",
      "Epoch 36877 \t\t Training Loss: 0.0005807848065160215 \t\n",
      "Epoch 36878 \t\t Training Loss: 0.0005807848065160215 \t\n",
      "Epoch 36879 \t\t Training Loss: 0.0005807848065160215 \t\n",
      "Epoch 36880 \t\t Training Loss: 0.0005807848065160215 \t\n",
      "Epoch 36881 \t\t Training Loss: 0.0005807848065160215 \t\n",
      "Epoch 36882 \t\t Training Loss: 0.0005807847483083606 \t\n",
      "Epoch 36883 \t\t Training Loss: 0.0005807847483083606 \t\n",
      "Epoch 36884 \t\t Training Loss: 0.0005807847483083606 \t\n",
      "Epoch 36885 \t\t Training Loss: 0.0005807847483083606 \t\n",
      "Epoch 36886 \t\t Training Loss: 0.0005807847483083606 \t\n",
      "Epoch 36887 \t\t Training Loss: 0.0005807846901006997 \t\n",
      "Epoch 36888 \t\t Training Loss: 0.0005807847483083606 \t\n",
      "Epoch 36889 \t\t Training Loss: 0.0005807847483083606 \t\n",
      "Epoch 36890 \t\t Training Loss: 0.0005807847483083606 \t\n",
      "Epoch 36891 \t\t Training Loss: 0.0005807846901006997 \t\n",
      "Epoch 36892 \t\t Training Loss: 0.0005807846901006997 \t\n",
      "Epoch 36893 \t\t Training Loss: 0.0005807847483083606 \t\n",
      "Epoch 36894 \t\t Training Loss: 0.0005807846901006997 \t\n",
      "Epoch 36895 \t\t Training Loss: 0.0005807846901006997 \t\n",
      "Epoch 36896 \t\t Training Loss: 0.0005807846901006997 \t\n",
      "Epoch 36897 \t\t Training Loss: 0.0005807846901006997 \t\n",
      "Epoch 36898 \t\t Training Loss: 0.0005807846901006997 \t\n",
      "Epoch 36899 \t\t Training Loss: 0.0005807846901006997 \t\n",
      "Epoch 36900 \t\t Training Loss: 0.0005807846901006997 \t\n",
      "Epoch 36901 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36902 \t\t Training Loss: 0.0005807846901006997 \t\n",
      "Epoch 36903 \t\t Training Loss: 0.0005807846901006997 \t\n",
      "Epoch 36904 \t\t Training Loss: 0.0005807846901006997 \t\n",
      "Epoch 36905 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36906 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36907 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36908 \t\t Training Loss: 0.0005807845736853778 \t\n",
      "Epoch 36909 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36910 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36911 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36912 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36913 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36914 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36915 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36916 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36917 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36918 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36919 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36920 \t\t Training Loss: 0.0005807845736853778 \t\n",
      "Epoch 36921 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36922 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36923 \t\t Training Loss: 0.0005807846318930387 \t\n",
      "Epoch 36924 \t\t Training Loss: 0.0005807845154777169 \t\n",
      "Epoch 36925 \t\t Training Loss: 0.0005807845736853778 \t\n",
      "Epoch 36926 \t\t Training Loss: 0.0005807845736853778 \t\n",
      "Epoch 36927 \t\t Training Loss: 0.0005807845154777169 \t\n",
      "Epoch 36928 \t\t Training Loss: 0.0005807845154777169 \t\n",
      "Epoch 36929 \t\t Training Loss: 0.0005807845154777169 \t\n",
      "Epoch 36930 \t\t Training Loss: 0.0005807845154777169 \t\n",
      "Epoch 36931 \t\t Training Loss: 0.0005807845154777169 \t\n",
      "Epoch 36932 \t\t Training Loss: 0.000580784457270056 \t\n",
      "Epoch 36933 \t\t Training Loss: 0.0005807845154777169 \t\n",
      "Epoch 36934 \t\t Training Loss: 0.000580784457270056 \t\n",
      "Epoch 36935 \t\t Training Loss: 0.000580784457270056 \t\n",
      "Epoch 36936 \t\t Training Loss: 0.000580784457270056 \t\n",
      "Epoch 36937 \t\t Training Loss: 0.000580784457270056 \t\n",
      "Epoch 36938 \t\t Training Loss: 0.0005807843990623951 \t\n",
      "Epoch 36939 \t\t Training Loss: 0.000580784457270056 \t\n",
      "Epoch 36940 \t\t Training Loss: 0.0005807843990623951 \t\n",
      "Epoch 36941 \t\t Training Loss: 0.000580784457270056 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36942 \t\t Training Loss: 0.0005807843990623951 \t\n",
      "Epoch 36943 \t\t Training Loss: 0.0005807843990623951 \t\n",
      "Epoch 36944 \t\t Training Loss: 0.0005807843990623951 \t\n",
      "Epoch 36945 \t\t Training Loss: 0.0005807843990623951 \t\n",
      "Epoch 36946 \t\t Training Loss: 0.0005807843990623951 \t\n",
      "Epoch 36947 \t\t Training Loss: 0.0005807843408547342 \t\n",
      "Epoch 36948 \t\t Training Loss: 0.0005807843408547342 \t\n",
      "Epoch 36949 \t\t Training Loss: 0.0005807843408547342 \t\n",
      "Epoch 36950 \t\t Training Loss: 0.0005807843408547342 \t\n",
      "Epoch 36951 \t\t Training Loss: 0.0005807843408547342 \t\n",
      "Epoch 36952 \t\t Training Loss: 0.0005807843408547342 \t\n",
      "Epoch 36953 \t\t Training Loss: 0.0005807843408547342 \t\n",
      "Epoch 36954 \t\t Training Loss: 0.0005807843408547342 \t\n",
      "Epoch 36955 \t\t Training Loss: 0.0005807842826470733 \t\n",
      "Epoch 36956 \t\t Training Loss: 0.0005807842826470733 \t\n",
      "Epoch 36957 \t\t Training Loss: 0.0005807842826470733 \t\n",
      "Epoch 36958 \t\t Training Loss: 0.0005807842244394124 \t\n",
      "Epoch 36959 \t\t Training Loss: 0.0005807842244394124 \t\n",
      "Epoch 36960 \t\t Training Loss: 0.0005807842244394124 \t\n",
      "Epoch 36961 \t\t Training Loss: 0.0005807841662317514 \t\n",
      "Epoch 36962 \t\t Training Loss: 0.0005807841662317514 \t\n",
      "Epoch 36963 \t\t Training Loss: 0.0005807841662317514 \t\n",
      "Epoch 36964 \t\t Training Loss: 0.0005807841662317514 \t\n",
      "Epoch 36965 \t\t Training Loss: 0.0005807841662317514 \t\n",
      "Epoch 36966 \t\t Training Loss: 0.0005807841662317514 \t\n",
      "Epoch 36967 \t\t Training Loss: 0.0005807841662317514 \t\n",
      "Epoch 36968 \t\t Training Loss: 0.0005807841662317514 \t\n",
      "Epoch 36969 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36970 \t\t Training Loss: 0.0005807841662317514 \t\n",
      "Epoch 36971 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36972 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36973 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36974 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36975 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36976 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36977 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36978 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36979 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36980 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36981 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36982 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36983 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36984 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36985 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36986 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36987 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36988 \t\t Training Loss: 0.0005807841662317514 \t\n",
      "Epoch 36989 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36990 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36991 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36992 \t\t Training Loss: 0.0005807841662317514 \t\n",
      "Epoch 36993 \t\t Training Loss: 0.0005807841662317514 \t\n",
      "Epoch 36994 \t\t Training Loss: 0.0005807841662317514 \t\n",
      "Epoch 36995 \t\t Training Loss: 0.0005807841662317514 \t\n",
      "Epoch 36996 \t\t Training Loss: 0.0005807841662317514 \t\n",
      "Epoch 36997 \t\t Training Loss: 0.0005807841662317514 \t\n",
      "Epoch 36998 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 36999 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 37000 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 37001 \t\t Training Loss: 0.0005807841080240905 \t\n",
      "Epoch 37002 \t\t Training Loss: 0.0005807840498164296 \t\n",
      "Epoch 37003 \t\t Training Loss: 0.0005807840498164296 \t\n",
      "Epoch 37004 \t\t Training Loss: 0.0005807840498164296 \t\n",
      "Epoch 37005 \t\t Training Loss: 0.0005807840498164296 \t\n",
      "Epoch 37006 \t\t Training Loss: 0.0005807840498164296 \t\n",
      "Epoch 37007 \t\t Training Loss: 0.0005807840498164296 \t\n",
      "Epoch 37008 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37009 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37010 \t\t Training Loss: 0.0005807840498164296 \t\n",
      "Epoch 37011 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37012 \t\t Training Loss: 0.0005807840498164296 \t\n",
      "Epoch 37013 \t\t Training Loss: 0.0005807840498164296 \t\n",
      "Epoch 37014 \t\t Training Loss: 0.0005807840498164296 \t\n",
      "Epoch 37015 \t\t Training Loss: 0.0005807840498164296 \t\n",
      "Epoch 37016 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37017 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37018 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37019 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37020 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37021 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37022 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37023 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37024 \t\t Training Loss: 0.0005807840498164296 \t\n",
      "Epoch 37025 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37026 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37027 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37028 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37029 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37030 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37031 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37032 \t\t Training Loss: 0.0005807839916087687 \t\n",
      "Epoch 37033 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37034 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37035 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37036 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37037 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37038 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37039 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37040 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37041 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37042 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37043 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37044 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37045 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37046 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37047 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37048 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37049 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37050 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37051 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37052 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37053 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37054 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37055 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37056 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37057 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37058 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37059 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37060 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37061 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37062 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37063 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37064 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37065 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37066 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37067 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37068 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37069 \t\t Training Loss: 0.0005807839334011078 \t\n",
      "Epoch 37070 \t\t Training Loss: 0.0005807838751934469 \t\n",
      "Epoch 37071 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37072 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37073 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37074 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37075 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37076 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37077 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37078 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37079 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37080 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37081 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37082 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37083 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37084 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37085 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37086 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37087 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37088 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37089 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37090 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37091 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37092 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37093 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37094 \t\t Training Loss: 0.000580783816985786 \t\n",
      "Epoch 37095 \t\t Training Loss: 0.0005807837005704641 \t\n",
      "Epoch 37096 \t\t Training Loss: 0.0005807837005704641 \t\n",
      "Epoch 37097 \t\t Training Loss: 0.0005807837005704641 \t\n",
      "Epoch 37098 \t\t Training Loss: 0.0005807837005704641 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37099 \t\t Training Loss: 0.0005807837005704641 \t\n",
      "Epoch 37100 \t\t Training Loss: 0.0005807837005704641 \t\n",
      "Epoch 37101 \t\t Training Loss: 0.0005807837005704641 \t\n",
      "Epoch 37102 \t\t Training Loss: 0.0005807837005704641 \t\n",
      "Epoch 37103 \t\t Training Loss: 0.0005807837005704641 \t\n",
      "Epoch 37104 \t\t Training Loss: 0.0005807837005704641 \t\n",
      "Epoch 37105 \t\t Training Loss: 0.0005807837005704641 \t\n",
      "Epoch 37106 \t\t Training Loss: 0.0005807835841551423 \t\n",
      "Epoch 37107 \t\t Training Loss: 0.0005807835841551423 \t\n",
      "Epoch 37108 \t\t Training Loss: 0.0005807837005704641 \t\n",
      "Epoch 37109 \t\t Training Loss: 0.0005807837005704641 \t\n",
      "Epoch 37110 \t\t Training Loss: 0.0005807837005704641 \t\n",
      "Epoch 37111 \t\t Training Loss: 0.0005807835841551423 \t\n",
      "Epoch 37112 \t\t Training Loss: 0.0005807836423628032 \t\n",
      "Epoch 37113 \t\t Training Loss: 0.0005807835841551423 \t\n",
      "Epoch 37114 \t\t Training Loss: 0.0005807835841551423 \t\n",
      "Epoch 37115 \t\t Training Loss: 0.0005807835841551423 \t\n",
      "Epoch 37116 \t\t Training Loss: 0.0005807835841551423 \t\n",
      "Epoch 37117 \t\t Training Loss: 0.0005807835841551423 \t\n",
      "Epoch 37118 \t\t Training Loss: 0.0005807835841551423 \t\n",
      "Epoch 37119 \t\t Training Loss: 0.0005807835259474814 \t\n",
      "Epoch 37120 \t\t Training Loss: 0.0005807835259474814 \t\n",
      "Epoch 37121 \t\t Training Loss: 0.0005807835841551423 \t\n",
      "Epoch 37122 \t\t Training Loss: 0.0005807835841551423 \t\n",
      "Epoch 37123 \t\t Training Loss: 0.0005807835841551423 \t\n",
      "Epoch 37124 \t\t Training Loss: 0.0005807835259474814 \t\n",
      "Epoch 37125 \t\t Training Loss: 0.0005807835259474814 \t\n",
      "Epoch 37126 \t\t Training Loss: 0.0005807835259474814 \t\n",
      "Epoch 37127 \t\t Training Loss: 0.0005807834677398205 \t\n",
      "Epoch 37128 \t\t Training Loss: 0.0005807834677398205 \t\n",
      "Epoch 37129 \t\t Training Loss: 0.0005807834677398205 \t\n",
      "Epoch 37130 \t\t Training Loss: 0.0005807834677398205 \t\n",
      "Epoch 37131 \t\t Training Loss: 0.0005807834677398205 \t\n",
      "Epoch 37132 \t\t Training Loss: 0.0005807834677398205 \t\n",
      "Epoch 37133 \t\t Training Loss: 0.0005807834677398205 \t\n",
      "Epoch 37134 \t\t Training Loss: 0.0005807834677398205 \t\n",
      "Epoch 37135 \t\t Training Loss: 0.0005807834677398205 \t\n",
      "Epoch 37136 \t\t Training Loss: 0.0005807834677398205 \t\n",
      "Epoch 37137 \t\t Training Loss: 0.0005807834677398205 \t\n",
      "Epoch 37138 \t\t Training Loss: 0.0005807834677398205 \t\n",
      "Epoch 37139 \t\t Training Loss: 0.0005807834677398205 \t\n",
      "Epoch 37140 \t\t Training Loss: 0.0005807834677398205 \t\n",
      "Epoch 37141 \t\t Training Loss: 0.0005807834677398205 \t\n",
      "Epoch 37142 \t\t Training Loss: 0.0005807834677398205 \t\n",
      "Epoch 37143 \t\t Training Loss: 0.0005807834095321596 \t\n",
      "Epoch 37144 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37145 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37146 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37147 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37148 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37149 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37150 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37151 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37152 \t\t Training Loss: 0.0005807832931168377 \t\n",
      "Epoch 37153 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37154 \t\t Training Loss: 0.0005807832931168377 \t\n",
      "Epoch 37155 \t\t Training Loss: 0.0005807832931168377 \t\n",
      "Epoch 37156 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37157 \t\t Training Loss: 0.0005807832931168377 \t\n",
      "Epoch 37158 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37159 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37160 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37161 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37162 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37163 \t\t Training Loss: 0.0005807832931168377 \t\n",
      "Epoch 37164 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37165 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37166 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37167 \t\t Training Loss: 0.0005807833513244987 \t\n",
      "Epoch 37168 \t\t Training Loss: 0.0005807832931168377 \t\n",
      "Epoch 37169 \t\t Training Loss: 0.0005807832931168377 \t\n",
      "Epoch 37170 \t\t Training Loss: 0.0005807832931168377 \t\n",
      "Epoch 37171 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37172 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37173 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37174 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37175 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37176 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37177 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37178 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37179 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37180 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37181 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37182 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37183 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37184 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37185 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37186 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37187 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37188 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37189 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37190 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37191 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37192 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37193 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37194 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37195 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37196 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37197 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37198 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37199 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37200 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37201 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37202 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37203 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37204 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37205 \t\t Training Loss: 0.0005807831767015159 \t\n",
      "Epoch 37206 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37207 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37208 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37209 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37210 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37211 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37212 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37213 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37214 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37215 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37216 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37217 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37218 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37219 \t\t Training Loss: 0.0005807832349091768 \t\n",
      "Epoch 37220 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37221 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37222 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37223 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37224 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37225 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37226 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37227 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37228 \t\t Training Loss: 0.000580783118493855 \t\n",
      "Epoch 37229 \t\t Training Loss: 0.0005807830602861941 \t\n",
      "Epoch 37230 \t\t Training Loss: 0.0005807829438708723 \t\n",
      "Epoch 37231 \t\t Training Loss: 0.0005807829438708723 \t\n",
      "Epoch 37232 \t\t Training Loss: 0.0005807829438708723 \t\n",
      "Epoch 37233 \t\t Training Loss: 0.0005807829438708723 \t\n",
      "Epoch 37234 \t\t Training Loss: 0.0005807829438708723 \t\n",
      "Epoch 37235 \t\t Training Loss: 0.0005807829438708723 \t\n",
      "Epoch 37236 \t\t Training Loss: 0.0005807828856632113 \t\n",
      "Epoch 37237 \t\t Training Loss: 0.0005807829438708723 \t\n",
      "Epoch 37238 \t\t Training Loss: 0.0005807828856632113 \t\n",
      "Epoch 37239 \t\t Training Loss: 0.0005807828856632113 \t\n",
      "Epoch 37240 \t\t Training Loss: 0.0005807828856632113 \t\n",
      "Epoch 37241 \t\t Training Loss: 0.0005807828856632113 \t\n",
      "Epoch 37242 \t\t Training Loss: 0.0005807828856632113 \t\n",
      "Epoch 37243 \t\t Training Loss: 0.0005807828856632113 \t\n",
      "Epoch 37244 \t\t Training Loss: 0.0005807828856632113 \t\n",
      "Epoch 37245 \t\t Training Loss: 0.0005807828856632113 \t\n",
      "Epoch 37246 \t\t Training Loss: 0.0005807828856632113 \t\n",
      "Epoch 37247 \t\t Training Loss: 0.0005807828856632113 \t\n",
      "Epoch 37248 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37249 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37250 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37251 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37252 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37253 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37254 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37255 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37256 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37257 \t\t Training Loss: 0.0005807828274555504 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37258 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37259 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37260 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37261 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37262 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37263 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37264 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37265 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37266 \t\t Training Loss: 0.0005807827110402286 \t\n",
      "Epoch 37267 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37268 \t\t Training Loss: 0.0005807827692478895 \t\n",
      "Epoch 37269 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37270 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37271 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37272 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37273 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37274 \t\t Training Loss: 0.0005807827110402286 \t\n",
      "Epoch 37275 \t\t Training Loss: 0.0005807827110402286 \t\n",
      "Epoch 37276 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37277 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37278 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37279 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37280 \t\t Training Loss: 0.0005807827692478895 \t\n",
      "Epoch 37281 \t\t Training Loss: 0.0005807827692478895 \t\n",
      "Epoch 37282 \t\t Training Loss: 0.0005807827692478895 \t\n",
      "Epoch 37283 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37284 \t\t Training Loss: 0.0005807827110402286 \t\n",
      "Epoch 37285 \t\t Training Loss: 0.0005807827692478895 \t\n",
      "Epoch 37286 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37287 \t\t Training Loss: 0.0005807828274555504 \t\n",
      "Epoch 37288 \t\t Training Loss: 0.0005807827692478895 \t\n",
      "Epoch 37289 \t\t Training Loss: 0.0005807827110402286 \t\n",
      "Epoch 37290 \t\t Training Loss: 0.0005807827110402286 \t\n",
      "Epoch 37291 \t\t Training Loss: 0.0005807827110402286 \t\n",
      "Epoch 37292 \t\t Training Loss: 0.0005807827110402286 \t\n",
      "Epoch 37293 \t\t Training Loss: 0.0005807827110402286 \t\n",
      "Epoch 37294 \t\t Training Loss: 0.0005807827110402286 \t\n",
      "Epoch 37295 \t\t Training Loss: 0.0005807827110402286 \t\n",
      "Epoch 37296 \t\t Training Loss: 0.0005807827110402286 \t\n",
      "Epoch 37297 \t\t Training Loss: 0.0005807826528325677 \t\n",
      "Epoch 37298 \t\t Training Loss: 0.0005807827110402286 \t\n",
      "Epoch 37299 \t\t Training Loss: 0.0005807827110402286 \t\n",
      "Epoch 37300 \t\t Training Loss: 0.0005807826528325677 \t\n",
      "Epoch 37301 \t\t Training Loss: 0.0005807826528325677 \t\n",
      "Epoch 37302 \t\t Training Loss: 0.0005807826528325677 \t\n",
      "Epoch 37303 \t\t Training Loss: 0.0005807826528325677 \t\n",
      "Epoch 37304 \t\t Training Loss: 0.0005807826528325677 \t\n",
      "Epoch 37305 \t\t Training Loss: 0.0005807825364172459 \t\n",
      "Epoch 37306 \t\t Training Loss: 0.0005807825364172459 \t\n",
      "Epoch 37307 \t\t Training Loss: 0.0005807825364172459 \t\n",
      "Epoch 37308 \t\t Training Loss: 0.0005807825364172459 \t\n",
      "Epoch 37309 \t\t Training Loss: 0.0005807825364172459 \t\n",
      "Epoch 37310 \t\t Training Loss: 0.000580782478209585 \t\n",
      "Epoch 37311 \t\t Training Loss: 0.000580782478209585 \t\n",
      "Epoch 37312 \t\t Training Loss: 0.000580782478209585 \t\n",
      "Epoch 37313 \t\t Training Loss: 0.000580782478209585 \t\n",
      "Epoch 37314 \t\t Training Loss: 0.0005807825364172459 \t\n",
      "Epoch 37315 \t\t Training Loss: 0.000580782420001924 \t\n",
      "Epoch 37316 \t\t Training Loss: 0.000580782478209585 \t\n",
      "Epoch 37317 \t\t Training Loss: 0.000580782420001924 \t\n",
      "Epoch 37318 \t\t Training Loss: 0.000580782420001924 \t\n",
      "Epoch 37319 \t\t Training Loss: 0.000580782420001924 \t\n",
      "Epoch 37320 \t\t Training Loss: 0.000580782420001924 \t\n",
      "Epoch 37321 \t\t Training Loss: 0.000580782420001924 \t\n",
      "Epoch 37322 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37323 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37324 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37325 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37326 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37327 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37328 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37329 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37330 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37331 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37332 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37333 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37334 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37335 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37336 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37337 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37338 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37339 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37340 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37341 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37342 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37343 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37344 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37345 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37346 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37347 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37348 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37349 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37350 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37351 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37352 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37353 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37354 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37355 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37356 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37357 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37358 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37359 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37360 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37361 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37362 \t\t Training Loss: 0.0005807823035866022 \t\n",
      "Epoch 37363 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37364 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37365 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37366 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37367 \t\t Training Loss: 0.0005807822453789413 \t\n",
      "Epoch 37368 \t\t Training Loss: 0.0005807821871712804 \t\n",
      "Epoch 37369 \t\t Training Loss: 0.0005807821871712804 \t\n",
      "Epoch 37370 \t\t Training Loss: 0.0005807821871712804 \t\n",
      "Epoch 37371 \t\t Training Loss: 0.0005807821871712804 \t\n",
      "Epoch 37372 \t\t Training Loss: 0.0005807821871712804 \t\n",
      "Epoch 37373 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37374 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37375 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37376 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37377 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37378 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37379 \t\t Training Loss: 0.0005807820707559586 \t\n",
      "Epoch 37380 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37381 \t\t Training Loss: 0.0005807820707559586 \t\n",
      "Epoch 37382 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37383 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37384 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37385 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37386 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37387 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37388 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37389 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37390 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37391 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37392 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37393 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37394 \t\t Training Loss: 0.0005807821871712804 \t\n",
      "Epoch 37395 \t\t Training Loss: 0.0005807821871712804 \t\n",
      "Epoch 37396 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37397 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37398 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37399 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37400 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37401 \t\t Training Loss: 0.0005807821289636195 \t\n",
      "Epoch 37402 \t\t Training Loss: 0.0005807820707559586 \t\n",
      "Epoch 37403 \t\t Training Loss: 0.0005807820707559586 \t\n",
      "Epoch 37404 \t\t Training Loss: 0.0005807820707559586 \t\n",
      "Epoch 37405 \t\t Training Loss: 0.0005807820707559586 \t\n",
      "Epoch 37406 \t\t Training Loss: 0.0005807820707559586 \t\n",
      "Epoch 37407 \t\t Training Loss: 0.0005807820707559586 \t\n",
      "Epoch 37408 \t\t Training Loss: 0.0005807820707559586 \t\n",
      "Epoch 37409 \t\t Training Loss: 0.0005807820707559586 \t\n",
      "Epoch 37410 \t\t Training Loss: 0.0005807820707559586 \t\n",
      "Epoch 37411 \t\t Training Loss: 0.0005807820707559586 \t\n",
      "Epoch 37412 \t\t Training Loss: 0.0005807819543406367 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37413 \t\t Training Loss: 0.0005807819543406367 \t\n",
      "Epoch 37414 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37415 \t\t Training Loss: 0.0005807819543406367 \t\n",
      "Epoch 37416 \t\t Training Loss: 0.0005807820707559586 \t\n",
      "Epoch 37417 \t\t Training Loss: 0.0005807819543406367 \t\n",
      "Epoch 37418 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37419 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37420 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37421 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37422 \t\t Training Loss: 0.0005807820125482976 \t\n",
      "Epoch 37423 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37424 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37425 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37426 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37427 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37428 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37429 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37430 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37431 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37432 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37433 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37434 \t\t Training Loss: 0.0005807818379253149 \t\n",
      "Epoch 37435 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37436 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37437 \t\t Training Loss: 0.0005807818379253149 \t\n",
      "Epoch 37438 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37439 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37440 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37441 \t\t Training Loss: 0.0005807818961329758 \t\n",
      "Epoch 37442 \t\t Training Loss: 0.0005807818379253149 \t\n",
      "Epoch 37443 \t\t Training Loss: 0.0005807818379253149 \t\n",
      "Epoch 37444 \t\t Training Loss: 0.0005807818379253149 \t\n",
      "Epoch 37445 \t\t Training Loss: 0.0005807818379253149 \t\n",
      "Epoch 37446 \t\t Training Loss: 0.000580781779717654 \t\n",
      "Epoch 37447 \t\t Training Loss: 0.000580781779717654 \t\n",
      "Epoch 37448 \t\t Training Loss: 0.000580781779717654 \t\n",
      "Epoch 37449 \t\t Training Loss: 0.000580781779717654 \t\n",
      "Epoch 37450 \t\t Training Loss: 0.000580781779717654 \t\n",
      "Epoch 37451 \t\t Training Loss: 0.000580781779717654 \t\n",
      "Epoch 37452 \t\t Training Loss: 0.000580781779717654 \t\n",
      "Epoch 37453 \t\t Training Loss: 0.000580781779717654 \t\n",
      "Epoch 37454 \t\t Training Loss: 0.0005807817215099931 \t\n",
      "Epoch 37455 \t\t Training Loss: 0.0005807817215099931 \t\n",
      "Epoch 37456 \t\t Training Loss: 0.000580781779717654 \t\n",
      "Epoch 37457 \t\t Training Loss: 0.000580781779717654 \t\n",
      "Epoch 37458 \t\t Training Loss: 0.0005807817215099931 \t\n",
      "Epoch 37459 \t\t Training Loss: 0.0005807817215099931 \t\n",
      "Epoch 37460 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37461 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37462 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37463 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37464 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37465 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37466 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37467 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37468 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37469 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37470 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37471 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37472 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37473 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37474 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37475 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37476 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37477 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37478 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37479 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37480 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37481 \t\t Training Loss: 0.0005807815468870103 \t\n",
      "Epoch 37482 \t\t Training Loss: 0.0005807815468870103 \t\n",
      "Epoch 37483 \t\t Training Loss: 0.0005807816633023322 \t\n",
      "Epoch 37484 \t\t Training Loss: 0.0005807815468870103 \t\n",
      "Epoch 37485 \t\t Training Loss: 0.0005807814886793494 \t\n",
      "Epoch 37486 \t\t Training Loss: 0.0005807814886793494 \t\n",
      "Epoch 37487 \t\t Training Loss: 0.0005807815468870103 \t\n",
      "Epoch 37488 \t\t Training Loss: 0.0005807815468870103 \t\n",
      "Epoch 37489 \t\t Training Loss: 0.0005807815468870103 \t\n",
      "Epoch 37490 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37491 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37492 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37493 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37494 \t\t Training Loss: 0.0005807814886793494 \t\n",
      "Epoch 37495 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37496 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37497 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37498 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37499 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37500 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37501 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37502 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37503 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37504 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37505 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37506 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37507 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37508 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37509 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37510 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37511 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37512 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37513 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37514 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37515 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37516 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37517 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37518 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37519 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37520 \t\t Training Loss: 0.0005807813722640276 \t\n",
      "Epoch 37521 \t\t Training Loss: 0.0005807812558487058 \t\n",
      "Epoch 37522 \t\t Training Loss: 0.0005807812558487058 \t\n",
      "Epoch 37523 \t\t Training Loss: 0.0005807812558487058 \t\n",
      "Epoch 37524 \t\t Training Loss: 0.0005807812558487058 \t\n",
      "Epoch 37525 \t\t Training Loss: 0.0005807812558487058 \t\n",
      "Epoch 37526 \t\t Training Loss: 0.0005807812558487058 \t\n",
      "Epoch 37527 \t\t Training Loss: 0.0005807812558487058 \t\n",
      "Epoch 37528 \t\t Training Loss: 0.0005807812558487058 \t\n",
      "Epoch 37529 \t\t Training Loss: 0.0005807812558487058 \t\n",
      "Epoch 37530 \t\t Training Loss: 0.0005807812558487058 \t\n",
      "Epoch 37531 \t\t Training Loss: 0.0005807812558487058 \t\n",
      "Epoch 37532 \t\t Training Loss: 0.0005807811394333839 \t\n",
      "Epoch 37533 \t\t Training Loss: 0.0005807811394333839 \t\n",
      "Epoch 37534 \t\t Training Loss: 0.0005807811394333839 \t\n",
      "Epoch 37535 \t\t Training Loss: 0.0005807811394333839 \t\n",
      "Epoch 37536 \t\t Training Loss: 0.0005807811394333839 \t\n",
      "Epoch 37537 \t\t Training Loss: 0.000580781081225723 \t\n",
      "Epoch 37538 \t\t Training Loss: 0.000580781081225723 \t\n",
      "Epoch 37539 \t\t Training Loss: 0.000580781081225723 \t\n",
      "Epoch 37540 \t\t Training Loss: 0.0005807811394333839 \t\n",
      "Epoch 37541 \t\t Training Loss: 0.0005807811394333839 \t\n",
      "Epoch 37542 \t\t Training Loss: 0.0005807811394333839 \t\n",
      "Epoch 37543 \t\t Training Loss: 0.000580781081225723 \t\n",
      "Epoch 37544 \t\t Training Loss: 0.0005807811394333839 \t\n",
      "Epoch 37545 \t\t Training Loss: 0.000580781081225723 \t\n",
      "Epoch 37546 \t\t Training Loss: 0.0005807809648104012 \t\n",
      "Epoch 37547 \t\t Training Loss: 0.0005807809648104012 \t\n",
      "Epoch 37548 \t\t Training Loss: 0.0005807809648104012 \t\n",
      "Epoch 37549 \t\t Training Loss: 0.0005807809648104012 \t\n",
      "Epoch 37550 \t\t Training Loss: 0.0005807809648104012 \t\n",
      "Epoch 37551 \t\t Training Loss: 0.0005807809648104012 \t\n",
      "Epoch 37552 \t\t Training Loss: 0.0005807809648104012 \t\n",
      "Epoch 37553 \t\t Training Loss: 0.0005807809648104012 \t\n",
      "Epoch 37554 \t\t Training Loss: 0.0005807809648104012 \t\n",
      "Epoch 37555 \t\t Training Loss: 0.0005807808483950794 \t\n",
      "Epoch 37556 \t\t Training Loss: 0.0005807808483950794 \t\n",
      "Epoch 37557 \t\t Training Loss: 0.0005807808483950794 \t\n",
      "Epoch 37558 \t\t Training Loss: 0.0005807808483950794 \t\n",
      "Epoch 37559 \t\t Training Loss: 0.0005807808483950794 \t\n",
      "Epoch 37560 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37561 \t\t Training Loss: 0.0005807808483950794 \t\n",
      "Epoch 37562 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37563 \t\t Training Loss: 0.0005807808483950794 \t\n",
      "Epoch 37564 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37565 \t\t Training Loss: 0.0005807808483950794 \t\n",
      "Epoch 37566 \t\t Training Loss: 0.0005807808483950794 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37567 \t\t Training Loss: 0.0005807808483950794 \t\n",
      "Epoch 37568 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37569 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37570 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37571 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37572 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37573 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37574 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37575 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37576 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37577 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37578 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37579 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37580 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37581 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37582 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37583 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37584 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37585 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37586 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37587 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37588 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37589 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37590 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37591 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37592 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37593 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37594 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37595 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37596 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37597 \t\t Training Loss: 0.0005807807901874185 \t\n",
      "Epoch 37598 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37599 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37600 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37601 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37602 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37603 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37604 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37605 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37606 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37607 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37608 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37609 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37610 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37611 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37612 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37613 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37614 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37615 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37616 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37617 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37618 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37619 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37620 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37621 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37622 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37623 \t\t Training Loss: 0.0005807807319797575 \t\n",
      "Epoch 37624 \t\t Training Loss: 0.0005807806737720966 \t\n",
      "Epoch 37625 \t\t Training Loss: 0.0005807806737720966 \t\n",
      "Epoch 37626 \t\t Training Loss: 0.0005807806737720966 \t\n",
      "Epoch 37627 \t\t Training Loss: 0.0005807806737720966 \t\n",
      "Epoch 37628 \t\t Training Loss: 0.0005807806737720966 \t\n",
      "Epoch 37629 \t\t Training Loss: 0.0005807806737720966 \t\n",
      "Epoch 37630 \t\t Training Loss: 0.0005807806737720966 \t\n",
      "Epoch 37631 \t\t Training Loss: 0.0005807806737720966 \t\n",
      "Epoch 37632 \t\t Training Loss: 0.0005807806737720966 \t\n",
      "Epoch 37633 \t\t Training Loss: 0.0005807806737720966 \t\n",
      "Epoch 37634 \t\t Training Loss: 0.0005807806737720966 \t\n",
      "Epoch 37635 \t\t Training Loss: 0.0005807806737720966 \t\n",
      "Epoch 37636 \t\t Training Loss: 0.0005807806737720966 \t\n",
      "Epoch 37637 \t\t Training Loss: 0.0005807806737720966 \t\n",
      "Epoch 37638 \t\t Training Loss: 0.0005807806737720966 \t\n",
      "Epoch 37639 \t\t Training Loss: 0.0005807805573567748 \t\n",
      "Epoch 37640 \t\t Training Loss: 0.0005807805573567748 \t\n",
      "Epoch 37641 \t\t Training Loss: 0.0005807805573567748 \t\n",
      "Epoch 37642 \t\t Training Loss: 0.0005807805573567748 \t\n",
      "Epoch 37643 \t\t Training Loss: 0.0005807805573567748 \t\n",
      "Epoch 37644 \t\t Training Loss: 0.0005807805573567748 \t\n",
      "Epoch 37645 \t\t Training Loss: 0.0005807805573567748 \t\n",
      "Epoch 37646 \t\t Training Loss: 0.0005807805573567748 \t\n",
      "Epoch 37647 \t\t Training Loss: 0.0005807805573567748 \t\n",
      "Epoch 37648 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37649 \t\t Training Loss: 0.000580780440941453 \t\n",
      "Epoch 37650 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37651 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37652 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37653 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37654 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37655 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37656 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37657 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37658 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37659 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37660 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37661 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37662 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37663 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37664 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37665 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37666 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37667 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37668 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37669 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37670 \t\t Training Loss: 0.0005807802663184702 \t\n",
      "Epoch 37671 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37672 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37673 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37674 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37675 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37676 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37677 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37678 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37679 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37680 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37681 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37682 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37683 \t\t Training Loss: 0.0005807803827337921 \t\n",
      "Epoch 37684 \t\t Training Loss: 0.0005807802663184702 \t\n",
      "Epoch 37685 \t\t Training Loss: 0.0005807802663184702 \t\n",
      "Epoch 37686 \t\t Training Loss: 0.0005807802663184702 \t\n",
      "Epoch 37687 \t\t Training Loss: 0.0005807802663184702 \t\n",
      "Epoch 37688 \t\t Training Loss: 0.0005807802663184702 \t\n",
      "Epoch 37689 \t\t Training Loss: 0.0005807802663184702 \t\n",
      "Epoch 37690 \t\t Training Loss: 0.0005807802663184702 \t\n",
      "Epoch 37691 \t\t Training Loss: 0.0005807802663184702 \t\n",
      "Epoch 37692 \t\t Training Loss: 0.0005807802663184702 \t\n",
      "Epoch 37693 \t\t Training Loss: 0.0005807802663184702 \t\n",
      "Epoch 37694 \t\t Training Loss: 0.0005807802663184702 \t\n",
      "Epoch 37695 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37696 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37697 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37698 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37699 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37700 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37701 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37702 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37703 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37704 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37705 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37706 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37707 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37708 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37709 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37710 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37711 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37712 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37713 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37714 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37715 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37716 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37717 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37718 \t\t Training Loss: 0.0005807800916954875 \t\n",
      "Epoch 37719 \t\t Training Loss: 0.0005807800916954875 \t\n",
      "Epoch 37720 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37721 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37722 \t\t Training Loss: 0.0005807800916954875 \t\n",
      "Epoch 37723 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37724 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37725 \t\t Training Loss: 0.0005807801499031484 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37726 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37727 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37728 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37729 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37730 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37731 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37732 \t\t Training Loss: 0.0005807800916954875 \t\n",
      "Epoch 37733 \t\t Training Loss: 0.0005807801499031484 \t\n",
      "Epoch 37734 \t\t Training Loss: 0.0005807800916954875 \t\n",
      "Epoch 37735 \t\t Training Loss: 0.0005807800916954875 \t\n",
      "Epoch 37736 \t\t Training Loss: 0.0005807800916954875 \t\n",
      "Epoch 37737 \t\t Training Loss: 0.0005807800916954875 \t\n",
      "Epoch 37738 \t\t Training Loss: 0.0005807800916954875 \t\n",
      "Epoch 37739 \t\t Training Loss: 0.0005807800916954875 \t\n",
      "Epoch 37740 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37741 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37742 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37743 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37744 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37745 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37746 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37747 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37748 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37749 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37750 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37751 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37752 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37753 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37754 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37755 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37756 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37757 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37758 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37759 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37760 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37761 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37762 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37763 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37764 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37765 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37766 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37767 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37768 \t\t Training Loss: 0.0005807799752801657 \t\n",
      "Epoch 37769 \t\t Training Loss: 0.0005807798588648438 \t\n",
      "Epoch 37770 \t\t Training Loss: 0.0005807798588648438 \t\n",
      "Epoch 37771 \t\t Training Loss: 0.0005807798588648438 \t\n",
      "Epoch 37772 \t\t Training Loss: 0.0005807798588648438 \t\n",
      "Epoch 37773 \t\t Training Loss: 0.0005807798588648438 \t\n",
      "Epoch 37774 \t\t Training Loss: 0.0005807798588648438 \t\n",
      "Epoch 37775 \t\t Training Loss: 0.0005807798588648438 \t\n",
      "Epoch 37776 \t\t Training Loss: 0.0005807798006571829 \t\n",
      "Epoch 37777 \t\t Training Loss: 0.0005807798006571829 \t\n",
      "Epoch 37778 \t\t Training Loss: 0.0005807798006571829 \t\n",
      "Epoch 37779 \t\t Training Loss: 0.0005807798006571829 \t\n",
      "Epoch 37780 \t\t Training Loss: 0.0005807798006571829 \t\n",
      "Epoch 37781 \t\t Training Loss: 0.0005807798006571829 \t\n",
      "Epoch 37782 \t\t Training Loss: 0.0005807798006571829 \t\n",
      "Epoch 37783 \t\t Training Loss: 0.0005807798006571829 \t\n",
      "Epoch 37784 \t\t Training Loss: 0.0005807798006571829 \t\n",
      "Epoch 37785 \t\t Training Loss: 0.0005807798006571829 \t\n",
      "Epoch 37786 \t\t Training Loss: 0.0005807798006571829 \t\n",
      "Epoch 37787 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37788 \t\t Training Loss: 0.0005807798006571829 \t\n",
      "Epoch 37789 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37790 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37791 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37792 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37793 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37794 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37795 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37796 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37797 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37798 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37799 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37800 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37801 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37802 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37803 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37804 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37805 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37806 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37807 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37808 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37809 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37810 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37811 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37812 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37813 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37814 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37815 \t\t Training Loss: 0.0005807796842418611 \t\n",
      "Epoch 37816 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37817 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37818 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37819 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37820 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37821 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37822 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37823 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37824 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37825 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37826 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37827 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37828 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37829 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37830 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37831 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37832 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37833 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37834 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37835 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37836 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37837 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37838 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37839 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37840 \t\t Training Loss: 0.0005807795678265393 \t\n",
      "Epoch 37841 \t\t Training Loss: 0.0005807795096188784 \t\n",
      "Epoch 37842 \t\t Training Loss: 0.0005807795096188784 \t\n",
      "Epoch 37843 \t\t Training Loss: 0.0005807795096188784 \t\n",
      "Epoch 37844 \t\t Training Loss: 0.0005807794514112175 \t\n",
      "Epoch 37845 \t\t Training Loss: 0.0005807794514112175 \t\n",
      "Epoch 37846 \t\t Training Loss: 0.0005807794514112175 \t\n",
      "Epoch 37847 \t\t Training Loss: 0.0005807794514112175 \t\n",
      "Epoch 37848 \t\t Training Loss: 0.0005807795096188784 \t\n",
      "Epoch 37849 \t\t Training Loss: 0.0005807795096188784 \t\n",
      "Epoch 37850 \t\t Training Loss: 0.0005807794514112175 \t\n",
      "Epoch 37851 \t\t Training Loss: 0.0005807794514112175 \t\n",
      "Epoch 37852 \t\t Training Loss: 0.0005807794514112175 \t\n",
      "Epoch 37853 \t\t Training Loss: 0.0005807793932035565 \t\n",
      "Epoch 37854 \t\t Training Loss: 0.0005807793932035565 \t\n",
      "Epoch 37855 \t\t Training Loss: 0.0005807793932035565 \t\n",
      "Epoch 37856 \t\t Training Loss: 0.0005807794514112175 \t\n",
      "Epoch 37857 \t\t Training Loss: 0.0005807793932035565 \t\n",
      "Epoch 37858 \t\t Training Loss: 0.0005807794514112175 \t\n",
      "Epoch 37859 \t\t Training Loss: 0.0005807793932035565 \t\n",
      "Epoch 37860 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37861 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37862 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37863 \t\t Training Loss: 0.0005807793932035565 \t\n",
      "Epoch 37864 \t\t Training Loss: 0.0005807793932035565 \t\n",
      "Epoch 37865 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37866 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37867 \t\t Training Loss: 0.0005807794514112175 \t\n",
      "Epoch 37868 \t\t Training Loss: 0.0005807794514112175 \t\n",
      "Epoch 37869 \t\t Training Loss: 0.0005807793932035565 \t\n",
      "Epoch 37870 \t\t Training Loss: 0.0005807793932035565 \t\n",
      "Epoch 37871 \t\t Training Loss: 0.0005807794514112175 \t\n",
      "Epoch 37872 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37873 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37874 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37875 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37876 \t\t Training Loss: 0.0005807793932035565 \t\n",
      "Epoch 37877 \t\t Training Loss: 0.0005807793932035565 \t\n",
      "Epoch 37878 \t\t Training Loss: 0.0005807793932035565 \t\n",
      "Epoch 37879 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37880 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37881 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37882 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37883 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37884 \t\t Training Loss: 0.0005807792767882347 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37885 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37886 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37887 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37888 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37889 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37890 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37891 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37892 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37893 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37894 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37895 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37896 \t\t Training Loss: 0.0005807792767882347 \t\n",
      "Epoch 37897 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37898 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37899 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37900 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37901 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37902 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37903 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37904 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37905 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37906 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37907 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37908 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37909 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37910 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37911 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37912 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37913 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37914 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37915 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37916 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37917 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37918 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37919 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37920 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37921 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37922 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37923 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37924 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37925 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37926 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37927 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37928 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37929 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37930 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37931 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37932 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37933 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37934 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37935 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37936 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37937 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37938 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37939 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37940 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37941 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37942 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37943 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37944 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37945 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37946 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37947 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37948 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37949 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37950 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37951 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37952 \t\t Training Loss: 0.0005807791603729129 \t\n",
      "Epoch 37953 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37954 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37955 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37956 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37957 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37958 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37959 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37960 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37961 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37962 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37963 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37964 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37965 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37966 \t\t Training Loss: 0.000580779102165252 \t\n",
      "Epoch 37967 \t\t Training Loss: 0.0005807790439575911 \t\n",
      "Epoch 37968 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37969 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37970 \t\t Training Loss: 0.0005807790439575911 \t\n",
      "Epoch 37971 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37972 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37973 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37974 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37975 \t\t Training Loss: 0.0005807790439575911 \t\n",
      "Epoch 37976 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37977 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37978 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37979 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37980 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37981 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37982 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37983 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37984 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37985 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37986 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37987 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37988 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37989 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37990 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37991 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37992 \t\t Training Loss: 0.0005807789857499301 \t\n",
      "Epoch 37993 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 37994 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 37995 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 37996 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 37997 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 37998 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 37999 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 38000 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 38001 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 38002 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 38003 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 38004 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 38005 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 38006 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 38007 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 38008 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 38009 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 38010 \t\t Training Loss: 0.0005807788693346083 \t\n",
      "Epoch 38011 \t\t Training Loss: 0.0005807788111269474 \t\n",
      "Epoch 38012 \t\t Training Loss: 0.0005807787529192865 \t\n",
      "Epoch 38013 \t\t Training Loss: 0.0005807787529192865 \t\n",
      "Epoch 38014 \t\t Training Loss: 0.0005807787529192865 \t\n",
      "Epoch 38015 \t\t Training Loss: 0.0005807787529192865 \t\n",
      "Epoch 38016 \t\t Training Loss: 0.0005807787529192865 \t\n",
      "Epoch 38017 \t\t Training Loss: 0.0005807787529192865 \t\n",
      "Epoch 38018 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38019 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38020 \t\t Training Loss: 0.0005807785782963037 \t\n",
      "Epoch 38021 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38022 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38023 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38024 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38025 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38026 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38027 \t\t Training Loss: 0.0005807785782963037 \t\n",
      "Epoch 38028 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38029 \t\t Training Loss: 0.0005807785782963037 \t\n",
      "Epoch 38030 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38031 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38032 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38033 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38034 \t\t Training Loss: 0.0005807785782963037 \t\n",
      "Epoch 38035 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38036 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38037 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38038 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38039 \t\t Training Loss: 0.0005807786947116256 \t\n",
      "Epoch 38040 \t\t Training Loss: 0.0005807785782963037 \t\n",
      "Epoch 38041 \t\t Training Loss: 0.0005807785782963037 \t\n",
      "Epoch 38042 \t\t Training Loss: 0.0005807785782963037 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38043 \t\t Training Loss: 0.0005807785782963037 \t\n",
      "Epoch 38044 \t\t Training Loss: 0.0005807785782963037 \t\n",
      "Epoch 38045 \t\t Training Loss: 0.0005807785782963037 \t\n",
      "Epoch 38046 \t\t Training Loss: 0.0005807785200886428 \t\n",
      "Epoch 38047 \t\t Training Loss: 0.0005807785200886428 \t\n",
      "Epoch 38048 \t\t Training Loss: 0.0005807785200886428 \t\n",
      "Epoch 38049 \t\t Training Loss: 0.0005807785782963037 \t\n",
      "Epoch 38050 \t\t Training Loss: 0.0005807785200886428 \t\n",
      "Epoch 38051 \t\t Training Loss: 0.0005807785200886428 \t\n",
      "Epoch 38052 \t\t Training Loss: 0.0005807785200886428 \t\n",
      "Epoch 38053 \t\t Training Loss: 0.0005807784618809819 \t\n",
      "Epoch 38054 \t\t Training Loss: 0.0005807784618809819 \t\n",
      "Epoch 38055 \t\t Training Loss: 0.0005807785200886428 \t\n",
      "Epoch 38056 \t\t Training Loss: 0.0005807784618809819 \t\n",
      "Epoch 38057 \t\t Training Loss: 0.0005807784618809819 \t\n",
      "Epoch 38058 \t\t Training Loss: 0.0005807785200886428 \t\n",
      "Epoch 38059 \t\t Training Loss: 0.0005807785200886428 \t\n",
      "Epoch 38060 \t\t Training Loss: 0.0005807785200886428 \t\n",
      "Epoch 38061 \t\t Training Loss: 0.0005807785200886428 \t\n",
      "Epoch 38062 \t\t Training Loss: 0.0005807784618809819 \t\n",
      "Epoch 38063 \t\t Training Loss: 0.0005807784618809819 \t\n",
      "Epoch 38064 \t\t Training Loss: 0.0005807784618809819 \t\n",
      "Epoch 38065 \t\t Training Loss: 0.0005807784618809819 \t\n",
      "Epoch 38066 \t\t Training Loss: 0.0005807784618809819 \t\n",
      "Epoch 38067 \t\t Training Loss: 0.0005807784618809819 \t\n",
      "Epoch 38068 \t\t Training Loss: 0.000580778403673321 \t\n",
      "Epoch 38069 \t\t Training Loss: 0.000580778403673321 \t\n",
      "Epoch 38070 \t\t Training Loss: 0.000580778403673321 \t\n",
      "Epoch 38071 \t\t Training Loss: 0.000580778403673321 \t\n",
      "Epoch 38072 \t\t Training Loss: 0.000580778403673321 \t\n",
      "Epoch 38073 \t\t Training Loss: 0.000580778403673321 \t\n",
      "Epoch 38074 \t\t Training Loss: 0.0005807783454656601 \t\n",
      "Epoch 38075 \t\t Training Loss: 0.000580778403673321 \t\n",
      "Epoch 38076 \t\t Training Loss: 0.000580778403673321 \t\n",
      "Epoch 38077 \t\t Training Loss: 0.000580778403673321 \t\n",
      "Epoch 38078 \t\t Training Loss: 0.0005807783454656601 \t\n",
      "Epoch 38079 \t\t Training Loss: 0.000580778403673321 \t\n",
      "Epoch 38080 \t\t Training Loss: 0.0005807783454656601 \t\n",
      "Epoch 38081 \t\t Training Loss: 0.000580778403673321 \t\n",
      "Epoch 38082 \t\t Training Loss: 0.0005807783454656601 \t\n",
      "Epoch 38083 \t\t Training Loss: 0.0005807783454656601 \t\n",
      "Epoch 38084 \t\t Training Loss: 0.0005807783454656601 \t\n",
      "Epoch 38085 \t\t Training Loss: 0.0005807783454656601 \t\n",
      "Epoch 38086 \t\t Training Loss: 0.0005807783454656601 \t\n",
      "Epoch 38087 \t\t Training Loss: 0.0005807782872579992 \t\n",
      "Epoch 38088 \t\t Training Loss: 0.0005807783454656601 \t\n",
      "Epoch 38089 \t\t Training Loss: 0.0005807782872579992 \t\n",
      "Epoch 38090 \t\t Training Loss: 0.0005807782872579992 \t\n",
      "Epoch 38091 \t\t Training Loss: 0.0005807782872579992 \t\n",
      "Epoch 38092 \t\t Training Loss: 0.0005807781708426774 \t\n",
      "Epoch 38093 \t\t Training Loss: 0.0005807781708426774 \t\n",
      "Epoch 38094 \t\t Training Loss: 0.0005807781708426774 \t\n",
      "Epoch 38095 \t\t Training Loss: 0.0005807781708426774 \t\n",
      "Epoch 38096 \t\t Training Loss: 0.0005807781708426774 \t\n",
      "Epoch 38097 \t\t Training Loss: 0.0005807781708426774 \t\n",
      "Epoch 38098 \t\t Training Loss: 0.0005807781708426774 \t\n",
      "Epoch 38099 \t\t Training Loss: 0.0005807781708426774 \t\n",
      "Epoch 38100 \t\t Training Loss: 0.0005807781126350164 \t\n",
      "Epoch 38101 \t\t Training Loss: 0.0005807780544273555 \t\n",
      "Epoch 38102 \t\t Training Loss: 0.0005807780544273555 \t\n",
      "Epoch 38103 \t\t Training Loss: 0.0005807780544273555 \t\n",
      "Epoch 38104 \t\t Training Loss: 0.0005807780544273555 \t\n",
      "Epoch 38105 \t\t Training Loss: 0.0005807780544273555 \t\n",
      "Epoch 38106 \t\t Training Loss: 0.0005807779962196946 \t\n",
      "Epoch 38107 \t\t Training Loss: 0.0005807780544273555 \t\n",
      "Epoch 38108 \t\t Training Loss: 0.0005807779962196946 \t\n",
      "Epoch 38109 \t\t Training Loss: 0.0005807779962196946 \t\n",
      "Epoch 38110 \t\t Training Loss: 0.0005807780544273555 \t\n",
      "Epoch 38111 \t\t Training Loss: 0.0005807779962196946 \t\n",
      "Epoch 38112 \t\t Training Loss: 0.0005807779962196946 \t\n",
      "Epoch 38113 \t\t Training Loss: 0.0005807779962196946 \t\n",
      "Epoch 38114 \t\t Training Loss: 0.0005807779962196946 \t\n",
      "Epoch 38115 \t\t Training Loss: 0.0005807779962196946 \t\n",
      "Epoch 38116 \t\t Training Loss: 0.0005807779962196946 \t\n",
      "Epoch 38117 \t\t Training Loss: 0.0005807779380120337 \t\n",
      "Epoch 38118 \t\t Training Loss: 0.0005807778798043728 \t\n",
      "Epoch 38119 \t\t Training Loss: 0.0005807779380120337 \t\n",
      "Epoch 38120 \t\t Training Loss: 0.0005807778798043728 \t\n",
      "Epoch 38121 \t\t Training Loss: 0.0005807779380120337 \t\n",
      "Epoch 38122 \t\t Training Loss: 0.0005807778798043728 \t\n",
      "Epoch 38123 \t\t Training Loss: 0.0005807778798043728 \t\n",
      "Epoch 38124 \t\t Training Loss: 0.0005807778798043728 \t\n",
      "Epoch 38125 \t\t Training Loss: 0.0005807778798043728 \t\n",
      "Epoch 38126 \t\t Training Loss: 0.0005807778798043728 \t\n",
      "Epoch 38127 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38128 \t\t Training Loss: 0.0005807778798043728 \t\n",
      "Epoch 38129 \t\t Training Loss: 0.0005807778798043728 \t\n",
      "Epoch 38130 \t\t Training Loss: 0.0005807778798043728 \t\n",
      "Epoch 38131 \t\t Training Loss: 0.0005807778798043728 \t\n",
      "Epoch 38132 \t\t Training Loss: 0.0005807778798043728 \t\n",
      "Epoch 38133 \t\t Training Loss: 0.0005807778798043728 \t\n",
      "Epoch 38134 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38135 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38136 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38137 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38138 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38139 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38140 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38141 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38142 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38143 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38144 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38145 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38146 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38147 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38148 \t\t Training Loss: 0.00058077770518139 \t\n",
      "Epoch 38149 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38150 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38151 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38152 \t\t Training Loss: 0.0005807778215967119 \t\n",
      "Epoch 38153 \t\t Training Loss: 0.00058077770518139 \t\n",
      "Epoch 38154 \t\t Training Loss: 0.00058077770518139 \t\n",
      "Epoch 38155 \t\t Training Loss: 0.00058077770518139 \t\n",
      "Epoch 38156 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38157 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38158 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38159 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38160 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38161 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38162 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38163 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38164 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38165 \t\t Training Loss: 0.0005807775887660682 \t\n",
      "Epoch 38166 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38167 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38168 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38169 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38170 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38171 \t\t Training Loss: 0.00058077770518139 \t\n",
      "Epoch 38172 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38173 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38174 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38175 \t\t Training Loss: 0.0005807776469737291 \t\n",
      "Epoch 38176 \t\t Training Loss: 0.0005807775887660682 \t\n",
      "Epoch 38177 \t\t Training Loss: 0.0005807775887660682 \t\n",
      "Epoch 38178 \t\t Training Loss: 0.0005807775887660682 \t\n",
      "Epoch 38179 \t\t Training Loss: 0.0005807775887660682 \t\n",
      "Epoch 38180 \t\t Training Loss: 0.0005807775305584073 \t\n",
      "Epoch 38181 \t\t Training Loss: 0.0005807775305584073 \t\n",
      "Epoch 38182 \t\t Training Loss: 0.0005807775305584073 \t\n",
      "Epoch 38183 \t\t Training Loss: 0.0005807775305584073 \t\n",
      "Epoch 38184 \t\t Training Loss: 0.0005807775887660682 \t\n",
      "Epoch 38185 \t\t Training Loss: 0.0005807775305584073 \t\n",
      "Epoch 38186 \t\t Training Loss: 0.0005807775887660682 \t\n",
      "Epoch 38187 \t\t Training Loss: 0.0005807775887660682 \t\n",
      "Epoch 38188 \t\t Training Loss: 0.0005807775887660682 \t\n",
      "Epoch 38189 \t\t Training Loss: 0.0005807775887660682 \t\n",
      "Epoch 38190 \t\t Training Loss: 0.0005807775887660682 \t\n",
      "Epoch 38191 \t\t Training Loss: 0.0005807775305584073 \t\n",
      "Epoch 38192 \t\t Training Loss: 0.0005807775305584073 \t\n",
      "Epoch 38193 \t\t Training Loss: 0.0005807775305584073 \t\n",
      "Epoch 38194 \t\t Training Loss: 0.0005807775305584073 \t\n",
      "Epoch 38195 \t\t Training Loss: 0.0005807775305584073 \t\n",
      "Epoch 38196 \t\t Training Loss: 0.0005807775305584073 \t\n",
      "Epoch 38197 \t\t Training Loss: 0.0005807775305584073 \t\n",
      "Epoch 38198 \t\t Training Loss: 0.0005807775305584073 \t\n",
      "Epoch 38199 \t\t Training Loss: 0.0005807775305584073 \t\n",
      "Epoch 38200 \t\t Training Loss: 0.0005807774723507464 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38201 \t\t Training Loss: 0.0005807774723507464 \t\n",
      "Epoch 38202 \t\t Training Loss: 0.0005807774723507464 \t\n",
      "Epoch 38203 \t\t Training Loss: 0.0005807774723507464 \t\n",
      "Epoch 38204 \t\t Training Loss: 0.0005807774723507464 \t\n",
      "Epoch 38205 \t\t Training Loss: 0.0005807774141430855 \t\n",
      "Epoch 38206 \t\t Training Loss: 0.0005807774141430855 \t\n",
      "Epoch 38207 \t\t Training Loss: 0.0005807773559354246 \t\n",
      "Epoch 38208 \t\t Training Loss: 0.0005807773559354246 \t\n",
      "Epoch 38209 \t\t Training Loss: 0.0005807773559354246 \t\n",
      "Epoch 38210 \t\t Training Loss: 0.0005807773559354246 \t\n",
      "Epoch 38211 \t\t Training Loss: 0.0005807773559354246 \t\n",
      "Epoch 38212 \t\t Training Loss: 0.0005807772977277637 \t\n",
      "Epoch 38213 \t\t Training Loss: 0.0005807772977277637 \t\n",
      "Epoch 38214 \t\t Training Loss: 0.0005807772977277637 \t\n",
      "Epoch 38215 \t\t Training Loss: 0.0005807773559354246 \t\n",
      "Epoch 38216 \t\t Training Loss: 0.0005807772977277637 \t\n",
      "Epoch 38217 \t\t Training Loss: 0.0005807772977277637 \t\n",
      "Epoch 38218 \t\t Training Loss: 0.0005807772977277637 \t\n",
      "Epoch 38219 \t\t Training Loss: 0.0005807772977277637 \t\n",
      "Epoch 38220 \t\t Training Loss: 0.0005807772977277637 \t\n",
      "Epoch 38221 \t\t Training Loss: 0.0005807772977277637 \t\n",
      "Epoch 38222 \t\t Training Loss: 0.0005807772977277637 \t\n",
      "Epoch 38223 \t\t Training Loss: 0.0005807772977277637 \t\n",
      "Epoch 38224 \t\t Training Loss: 0.0005807772977277637 \t\n",
      "Epoch 38225 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38226 \t\t Training Loss: 0.0005807772977277637 \t\n",
      "Epoch 38227 \t\t Training Loss: 0.0005807772977277637 \t\n",
      "Epoch 38228 \t\t Training Loss: 0.0005807772977277637 \t\n",
      "Epoch 38229 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38230 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38231 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38232 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38233 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38234 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38235 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38236 \t\t Training Loss: 0.0005807772977277637 \t\n",
      "Epoch 38237 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38238 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38239 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38240 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38241 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38242 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38243 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38244 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38245 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38246 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38247 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38248 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38249 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38250 \t\t Training Loss: 0.0005807772395201027 \t\n",
      "Epoch 38251 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38252 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38253 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38254 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38255 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38256 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38257 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38258 \t\t Training Loss: 0.00058077706489712 \t\n",
      "Epoch 38259 \t\t Training Loss: 0.0005807770066894591 \t\n",
      "Epoch 38260 \t\t Training Loss: 0.00058077706489712 \t\n",
      "Epoch 38261 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38262 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38263 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38264 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38265 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38266 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38267 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38268 \t\t Training Loss: 0.0005807770066894591 \t\n",
      "Epoch 38269 \t\t Training Loss: 0.0005807771231047809 \t\n",
      "Epoch 38270 \t\t Training Loss: 0.00058077706489712 \t\n",
      "Epoch 38271 \t\t Training Loss: 0.0005807770066894591 \t\n",
      "Epoch 38272 \t\t Training Loss: 0.0005807770066894591 \t\n",
      "Epoch 38273 \t\t Training Loss: 0.0005807770066894591 \t\n",
      "Epoch 38274 \t\t Training Loss: 0.0005807770066894591 \t\n",
      "Epoch 38275 \t\t Training Loss: 0.0005807770066894591 \t\n",
      "Epoch 38276 \t\t Training Loss: 0.0005807770066894591 \t\n",
      "Epoch 38277 \t\t Training Loss: 0.0005807770066894591 \t\n",
      "Epoch 38278 \t\t Training Loss: 0.0005807770066894591 \t\n",
      "Epoch 38279 \t\t Training Loss: 0.0005807770066894591 \t\n",
      "Epoch 38280 \t\t Training Loss: 0.0005807770066894591 \t\n",
      "Epoch 38281 \t\t Training Loss: 0.0005807770066894591 \t\n",
      "Epoch 38282 \t\t Training Loss: 0.0005807770066894591 \t\n",
      "Epoch 38283 \t\t Training Loss: 0.0005807769484817982 \t\n",
      "Epoch 38284 \t\t Training Loss: 0.0005807769484817982 \t\n",
      "Epoch 38285 \t\t Training Loss: 0.0005807769484817982 \t\n",
      "Epoch 38286 \t\t Training Loss: 0.0005807769484817982 \t\n",
      "Epoch 38287 \t\t Training Loss: 0.0005807769484817982 \t\n",
      "Epoch 38288 \t\t Training Loss: 0.0005807768902741373 \t\n",
      "Epoch 38289 \t\t Training Loss: 0.0005807769484817982 \t\n",
      "Epoch 38290 \t\t Training Loss: 0.0005807768902741373 \t\n",
      "Epoch 38291 \t\t Training Loss: 0.0005807768902741373 \t\n",
      "Epoch 38292 \t\t Training Loss: 0.0005807768902741373 \t\n",
      "Epoch 38293 \t\t Training Loss: 0.0005807768902741373 \t\n",
      "Epoch 38294 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38295 \t\t Training Loss: 0.0005807768902741373 \t\n",
      "Epoch 38296 \t\t Training Loss: 0.0005807768902741373 \t\n",
      "Epoch 38297 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38298 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38299 \t\t Training Loss: 0.0005807768902741373 \t\n",
      "Epoch 38300 \t\t Training Loss: 0.0005807768902741373 \t\n",
      "Epoch 38301 \t\t Training Loss: 0.0005807768902741373 \t\n",
      "Epoch 38302 \t\t Training Loss: 0.0005807768902741373 \t\n",
      "Epoch 38303 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38304 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38305 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38306 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38307 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38308 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38309 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38310 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38311 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38312 \t\t Training Loss: 0.0005807767738588154 \t\n",
      "Epoch 38313 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38314 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38315 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38316 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38317 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38318 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38319 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38320 \t\t Training Loss: 0.0005807768320664763 \t\n",
      "Epoch 38321 \t\t Training Loss: 0.0005807767738588154 \t\n",
      "Epoch 38322 \t\t Training Loss: 0.0005807767738588154 \t\n",
      "Epoch 38323 \t\t Training Loss: 0.0005807767738588154 \t\n",
      "Epoch 38324 \t\t Training Loss: 0.0005807767738588154 \t\n",
      "Epoch 38325 \t\t Training Loss: 0.0005807767156511545 \t\n",
      "Epoch 38326 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38327 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38328 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38329 \t\t Training Loss: 0.0005807767156511545 \t\n",
      "Epoch 38330 \t\t Training Loss: 0.0005807767156511545 \t\n",
      "Epoch 38331 \t\t Training Loss: 0.0005807767156511545 \t\n",
      "Epoch 38332 \t\t Training Loss: 0.0005807767156511545 \t\n",
      "Epoch 38333 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38334 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38335 \t\t Training Loss: 0.0005807767156511545 \t\n",
      "Epoch 38336 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38337 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38338 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38339 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38340 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38341 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38342 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38343 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38344 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38345 \t\t Training Loss: 0.0005807765410281718 \t\n",
      "Epoch 38346 \t\t Training Loss: 0.0005807765992358327 \t\n",
      "Epoch 38347 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38348 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38349 \t\t Training Loss: 0.0005807766574434936 \t\n",
      "Epoch 38350 \t\t Training Loss: 0.0005807765992358327 \t\n",
      "Epoch 38351 \t\t Training Loss: 0.0005807765992358327 \t\n",
      "Epoch 38352 \t\t Training Loss: 0.0005807765992358327 \t\n",
      "Epoch 38353 \t\t Training Loss: 0.0005807765410281718 \t\n",
      "Epoch 38354 \t\t Training Loss: 0.0005807765410281718 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38355 \t\t Training Loss: 0.0005807765410281718 \t\n",
      "Epoch 38356 \t\t Training Loss: 0.0005807765410281718 \t\n",
      "Epoch 38357 \t\t Training Loss: 0.0005807765410281718 \t\n",
      "Epoch 38358 \t\t Training Loss: 0.0005807765410281718 \t\n",
      "Epoch 38359 \t\t Training Loss: 0.0005807765410281718 \t\n",
      "Epoch 38360 \t\t Training Loss: 0.0005807765410281718 \t\n",
      "Epoch 38361 \t\t Training Loss: 0.0005807765410281718 \t\n",
      "Epoch 38362 \t\t Training Loss: 0.0005807765410281718 \t\n",
      "Epoch 38363 \t\t Training Loss: 0.0005807765410281718 \t\n",
      "Epoch 38364 \t\t Training Loss: 0.0005807765410281718 \t\n",
      "Epoch 38365 \t\t Training Loss: 0.0005807765410281718 \t\n",
      "Epoch 38366 \t\t Training Loss: 0.0005807765410281718 \t\n",
      "Epoch 38367 \t\t Training Loss: 0.0005807765410281718 \t\n",
      "Epoch 38368 \t\t Training Loss: 0.0005807765410281718 \t\n",
      "Epoch 38369 \t\t Training Loss: 0.00058077642461285 \t\n",
      "Epoch 38370 \t\t Training Loss: 0.00058077642461285 \t\n",
      "Epoch 38371 \t\t Training Loss: 0.00058077642461285 \t\n",
      "Epoch 38372 \t\t Training Loss: 0.00058077642461285 \t\n",
      "Epoch 38373 \t\t Training Loss: 0.00058077642461285 \t\n",
      "Epoch 38374 \t\t Training Loss: 0.0005807764828205109 \t\n",
      "Epoch 38375 \t\t Training Loss: 0.0005807764828205109 \t\n",
      "Epoch 38376 \t\t Training Loss: 0.0005807764828205109 \t\n",
      "Epoch 38377 \t\t Training Loss: 0.0005807764828205109 \t\n",
      "Epoch 38378 \t\t Training Loss: 0.0005807764828205109 \t\n",
      "Epoch 38379 \t\t Training Loss: 0.0005807764828205109 \t\n",
      "Epoch 38380 \t\t Training Loss: 0.0005807764828205109 \t\n",
      "Epoch 38381 \t\t Training Loss: 0.0005807764828205109 \t\n",
      "Epoch 38382 \t\t Training Loss: 0.00058077642461285 \t\n",
      "Epoch 38383 \t\t Training Loss: 0.00058077642461285 \t\n",
      "Epoch 38384 \t\t Training Loss: 0.00058077642461285 \t\n",
      "Epoch 38385 \t\t Training Loss: 0.00058077642461285 \t\n",
      "Epoch 38386 \t\t Training Loss: 0.00058077642461285 \t\n",
      "Epoch 38387 \t\t Training Loss: 0.00058077642461285 \t\n",
      "Epoch 38388 \t\t Training Loss: 0.000580776366405189 \t\n",
      "Epoch 38389 \t\t Training Loss: 0.000580776366405189 \t\n",
      "Epoch 38390 \t\t Training Loss: 0.000580776366405189 \t\n",
      "Epoch 38391 \t\t Training Loss: 0.000580776366405189 \t\n",
      "Epoch 38392 \t\t Training Loss: 0.000580776366405189 \t\n",
      "Epoch 38393 \t\t Training Loss: 0.000580776366405189 \t\n",
      "Epoch 38394 \t\t Training Loss: 0.000580776366405189 \t\n",
      "Epoch 38395 \t\t Training Loss: 0.000580776366405189 \t\n",
      "Epoch 38396 \t\t Training Loss: 0.000580776366405189 \t\n",
      "Epoch 38397 \t\t Training Loss: 0.000580776366405189 \t\n",
      "Epoch 38398 \t\t Training Loss: 0.0005807763081975281 \t\n",
      "Epoch 38399 \t\t Training Loss: 0.0005807762499898672 \t\n",
      "Epoch 38400 \t\t Training Loss: 0.0005807762499898672 \t\n",
      "Epoch 38401 \t\t Training Loss: 0.0005807762499898672 \t\n",
      "Epoch 38402 \t\t Training Loss: 0.0005807762499898672 \t\n",
      "Epoch 38403 \t\t Training Loss: 0.0005807762499898672 \t\n",
      "Epoch 38404 \t\t Training Loss: 0.0005807762499898672 \t\n",
      "Epoch 38405 \t\t Training Loss: 0.0005807762499898672 \t\n",
      "Epoch 38406 \t\t Training Loss: 0.0005807763081975281 \t\n",
      "Epoch 38407 \t\t Training Loss: 0.0005807762499898672 \t\n",
      "Epoch 38408 \t\t Training Loss: 0.0005807762499898672 \t\n",
      "Epoch 38409 \t\t Training Loss: 0.0005807763081975281 \t\n",
      "Epoch 38410 \t\t Training Loss: 0.0005807763081975281 \t\n",
      "Epoch 38411 \t\t Training Loss: 0.0005807763081975281 \t\n",
      "Epoch 38412 \t\t Training Loss: 0.0005807763081975281 \t\n",
      "Epoch 38413 \t\t Training Loss: 0.0005807763081975281 \t\n",
      "Epoch 38414 \t\t Training Loss: 0.0005807763081975281 \t\n",
      "Epoch 38415 \t\t Training Loss: 0.0005807763081975281 \t\n",
      "Epoch 38416 \t\t Training Loss: 0.0005807763081975281 \t\n",
      "Epoch 38417 \t\t Training Loss: 0.0005807763081975281 \t\n",
      "Epoch 38418 \t\t Training Loss: 0.0005807763081975281 \t\n",
      "Epoch 38419 \t\t Training Loss: 0.0005807762499898672 \t\n",
      "Epoch 38420 \t\t Training Loss: 0.0005807762499898672 \t\n",
      "Epoch 38421 \t\t Training Loss: 0.0005807762499898672 \t\n",
      "Epoch 38422 \t\t Training Loss: 0.0005807762499898672 \t\n",
      "Epoch 38423 \t\t Training Loss: 0.0005807761917822063 \t\n",
      "Epoch 38424 \t\t Training Loss: 0.0005807761917822063 \t\n",
      "Epoch 38425 \t\t Training Loss: 0.0005807761917822063 \t\n",
      "Epoch 38426 \t\t Training Loss: 0.0005807761917822063 \t\n",
      "Epoch 38427 \t\t Training Loss: 0.0005807761917822063 \t\n",
      "Epoch 38428 \t\t Training Loss: 0.0005807761917822063 \t\n",
      "Epoch 38429 \t\t Training Loss: 0.0005807761917822063 \t\n",
      "Epoch 38430 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38431 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38432 \t\t Training Loss: 0.0005807761917822063 \t\n",
      "Epoch 38433 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38434 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38435 \t\t Training Loss: 0.0005807761917822063 \t\n",
      "Epoch 38436 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38437 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38438 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38439 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38440 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38441 \t\t Training Loss: 0.0005807761917822063 \t\n",
      "Epoch 38442 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38443 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38444 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38445 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38446 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38447 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38448 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38449 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38450 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38451 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38452 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38453 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38454 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38455 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38456 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38457 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38458 \t\t Training Loss: 0.0005807760171592236 \t\n",
      "Epoch 38459 \t\t Training Loss: 0.0005807760171592236 \t\n",
      "Epoch 38460 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38461 \t\t Training Loss: 0.0005807760753668845 \t\n",
      "Epoch 38462 \t\t Training Loss: 0.0005807760171592236 \t\n",
      "Epoch 38463 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38464 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38465 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38466 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38467 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38468 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38469 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38470 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38471 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38472 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38473 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38474 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38475 \t\t Training Loss: 0.0005807759007439017 \t\n",
      "Epoch 38476 \t\t Training Loss: 0.0005807759007439017 \t\n",
      "Epoch 38477 \t\t Training Loss: 0.0005807759007439017 \t\n",
      "Epoch 38478 \t\t Training Loss: 0.0005807759007439017 \t\n",
      "Epoch 38479 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38480 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38481 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38482 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38483 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38484 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38485 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38486 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38487 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38488 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38489 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38490 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38491 \t\t Training Loss: 0.0005807759589515626 \t\n",
      "Epoch 38492 \t\t Training Loss: 0.0005807759007439017 \t\n",
      "Epoch 38493 \t\t Training Loss: 0.0005807758425362408 \t\n",
      "Epoch 38494 \t\t Training Loss: 0.0005807758425362408 \t\n",
      "Epoch 38495 \t\t Training Loss: 0.0005807758425362408 \t\n",
      "Epoch 38496 \t\t Training Loss: 0.0005807758425362408 \t\n",
      "Epoch 38497 \t\t Training Loss: 0.0005807758425362408 \t\n",
      "Epoch 38498 \t\t Training Loss: 0.0005807758425362408 \t\n",
      "Epoch 38499 \t\t Training Loss: 0.0005807758425362408 \t\n",
      "Epoch 38500 \t\t Training Loss: 0.0005807758425362408 \t\n",
      "Epoch 38501 \t\t Training Loss: 0.0005807758425362408 \t\n",
      "Epoch 38502 \t\t Training Loss: 0.0005807758425362408 \t\n",
      "Epoch 38503 \t\t Training Loss: 0.0005807758425362408 \t\n",
      "Epoch 38504 \t\t Training Loss: 0.0005807757843285799 \t\n",
      "Epoch 38505 \t\t Training Loss: 0.0005807757843285799 \t\n",
      "Epoch 38506 \t\t Training Loss: 0.0005807757843285799 \t\n",
      "Epoch 38507 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38508 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38509 \t\t Training Loss: 0.000580775726120919 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38510 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38511 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38512 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38513 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38514 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38515 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38516 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38517 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38518 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38519 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38520 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38521 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38522 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38523 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38524 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38525 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38526 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38527 \t\t Training Loss: 0.000580775726120919 \t\n",
      "Epoch 38528 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38529 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38530 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38531 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38532 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38533 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38534 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38535 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38536 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38537 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38538 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38539 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38540 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38541 \t\t Training Loss: 0.0005807754932902753 \t\n",
      "Epoch 38542 \t\t Training Loss: 0.0005807754932902753 \t\n",
      "Epoch 38543 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38544 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38545 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38546 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38547 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38548 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38549 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38550 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38551 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38552 \t\t Training Loss: 0.0005807754932902753 \t\n",
      "Epoch 38553 \t\t Training Loss: 0.0005807756097055972 \t\n",
      "Epoch 38554 \t\t Training Loss: 0.0005807754932902753 \t\n",
      "Epoch 38555 \t\t Training Loss: 0.0005807754932902753 \t\n",
      "Epoch 38556 \t\t Training Loss: 0.0005807754932902753 \t\n",
      "Epoch 38557 \t\t Training Loss: 0.0005807754932902753 \t\n",
      "Epoch 38558 \t\t Training Loss: 0.0005807754932902753 \t\n",
      "Epoch 38559 \t\t Training Loss: 0.0005807754932902753 \t\n",
      "Epoch 38560 \t\t Training Loss: 0.0005807754932902753 \t\n",
      "Epoch 38561 \t\t Training Loss: 0.0005807754932902753 \t\n",
      "Epoch 38562 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38563 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38564 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38565 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38566 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38567 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38568 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38569 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38570 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38571 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38572 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38573 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38574 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38575 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38576 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38577 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38578 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38579 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38580 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38581 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38582 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38583 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38584 \t\t Training Loss: 0.0005807753186672926 \t\n",
      "Epoch 38585 \t\t Training Loss: 0.0005807754350826144 \t\n",
      "Epoch 38586 \t\t Training Loss: 0.0005807753186672926 \t\n",
      "Epoch 38587 \t\t Training Loss: 0.0005807753186672926 \t\n",
      "Epoch 38588 \t\t Training Loss: 0.0005807753186672926 \t\n",
      "Epoch 38589 \t\t Training Loss: 0.0005807753186672926 \t\n",
      "Epoch 38590 \t\t Training Loss: 0.0005807753186672926 \t\n",
      "Epoch 38591 \t\t Training Loss: 0.0005807753186672926 \t\n",
      "Epoch 38592 \t\t Training Loss: 0.0005807753186672926 \t\n",
      "Epoch 38593 \t\t Training Loss: 0.0005807753186672926 \t\n",
      "Epoch 38594 \t\t Training Loss: 0.0005807752604596317 \t\n",
      "Epoch 38595 \t\t Training Loss: 0.0005807752604596317 \t\n",
      "Epoch 38596 \t\t Training Loss: 0.0005807752604596317 \t\n",
      "Epoch 38597 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38598 \t\t Training Loss: 0.0005807752604596317 \t\n",
      "Epoch 38599 \t\t Training Loss: 0.0005807752604596317 \t\n",
      "Epoch 38600 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38601 \t\t Training Loss: 0.0005807752604596317 \t\n",
      "Epoch 38602 \t\t Training Loss: 0.0005807752604596317 \t\n",
      "Epoch 38603 \t\t Training Loss: 0.0005807752604596317 \t\n",
      "Epoch 38604 \t\t Training Loss: 0.0005807752604596317 \t\n",
      "Epoch 38605 \t\t Training Loss: 0.0005807752604596317 \t\n",
      "Epoch 38606 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38607 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38608 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38609 \t\t Training Loss: 0.0005807752604596317 \t\n",
      "Epoch 38610 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38611 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38612 \t\t Training Loss: 0.0005807752604596317 \t\n",
      "Epoch 38613 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38614 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38615 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38616 \t\t Training Loss: 0.0005807751440443099 \t\n",
      "Epoch 38617 \t\t Training Loss: 0.0005807751440443099 \t\n",
      "Epoch 38618 \t\t Training Loss: 0.0005807751440443099 \t\n",
      "Epoch 38619 \t\t Training Loss: 0.0005807751440443099 \t\n",
      "Epoch 38620 \t\t Training Loss: 0.0005807751440443099 \t\n",
      "Epoch 38621 \t\t Training Loss: 0.0005807751440443099 \t\n",
      "Epoch 38622 \t\t Training Loss: 0.0005807751440443099 \t\n",
      "Epoch 38623 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38624 \t\t Training Loss: 0.0005807751440443099 \t\n",
      "Epoch 38625 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38626 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38627 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38628 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38629 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38630 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38631 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38632 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38633 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38634 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38635 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38636 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38637 \t\t Training Loss: 0.0005807752022519708 \t\n",
      "Epoch 38638 \t\t Training Loss: 0.0005807751440443099 \t\n",
      "Epoch 38639 \t\t Training Loss: 0.0005807751440443099 \t\n",
      "Epoch 38640 \t\t Training Loss: 0.0005807751440443099 \t\n",
      "Epoch 38641 \t\t Training Loss: 0.0005807751440443099 \t\n",
      "Epoch 38642 \t\t Training Loss: 0.0005807751440443099 \t\n",
      "Epoch 38643 \t\t Training Loss: 0.0005807751440443099 \t\n",
      "Epoch 38644 \t\t Training Loss: 0.0005807751440443099 \t\n",
      "Epoch 38645 \t\t Training Loss: 0.0005807751440443099 \t\n",
      "Epoch 38646 \t\t Training Loss: 0.000580775027628988 \t\n",
      "Epoch 38647 \t\t Training Loss: 0.000580775027628988 \t\n",
      "Epoch 38648 \t\t Training Loss: 0.000580775027628988 \t\n",
      "Epoch 38649 \t\t Training Loss: 0.000580775027628988 \t\n",
      "Epoch 38650 \t\t Training Loss: 0.000580775027628988 \t\n",
      "Epoch 38651 \t\t Training Loss: 0.000580775027628988 \t\n",
      "Epoch 38652 \t\t Training Loss: 0.000580775027628988 \t\n",
      "Epoch 38653 \t\t Training Loss: 0.000580775027628988 \t\n",
      "Epoch 38654 \t\t Training Loss: 0.000580775027628988 \t\n",
      "Epoch 38655 \t\t Training Loss: 0.000580775027628988 \t\n",
      "Epoch 38656 \t\t Training Loss: 0.0005807749112136662 \t\n",
      "Epoch 38657 \t\t Training Loss: 0.000580775027628988 \t\n",
      "Epoch 38658 \t\t Training Loss: 0.000580775027628988 \t\n",
      "Epoch 38659 \t\t Training Loss: 0.0005807749112136662 \t\n",
      "Epoch 38660 \t\t Training Loss: 0.0005807749112136662 \t\n",
      "Epoch 38661 \t\t Training Loss: 0.000580775027628988 \t\n",
      "Epoch 38662 \t\t Training Loss: 0.000580775027628988 \t\n",
      "Epoch 38663 \t\t Training Loss: 0.000580775027628988 \t\n",
      "Epoch 38664 \t\t Training Loss: 0.000580775027628988 \t\n",
      "Epoch 38665 \t\t Training Loss: 0.0005807749112136662 \t\n",
      "Epoch 38666 \t\t Training Loss: 0.0005807749112136662 \t\n",
      "Epoch 38667 \t\t Training Loss: 0.0005807749112136662 \t\n",
      "Epoch 38668 \t\t Training Loss: 0.0005807749112136662 \t\n",
      "Epoch 38669 \t\t Training Loss: 0.0005807749112136662 \t\n",
      "Epoch 38670 \t\t Training Loss: 0.0005807749112136662 \t\n",
      "Epoch 38671 \t\t Training Loss: 0.0005807748530060053 \t\n",
      "Epoch 38672 \t\t Training Loss: 0.0005807749112136662 \t\n",
      "Epoch 38673 \t\t Training Loss: 0.0005807749112136662 \t\n",
      "Epoch 38674 \t\t Training Loss: 0.0005807748530060053 \t\n",
      "Epoch 38675 \t\t Training Loss: 0.0005807748530060053 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38676 \t\t Training Loss: 0.0005807748530060053 \t\n",
      "Epoch 38677 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38678 \t\t Training Loss: 0.0005807748530060053 \t\n",
      "Epoch 38679 \t\t Training Loss: 0.0005807748530060053 \t\n",
      "Epoch 38680 \t\t Training Loss: 0.0005807748530060053 \t\n",
      "Epoch 38681 \t\t Training Loss: 0.0005807748530060053 \t\n",
      "Epoch 38682 \t\t Training Loss: 0.0005807749112136662 \t\n",
      "Epoch 38683 \t\t Training Loss: 0.0005807748530060053 \t\n",
      "Epoch 38684 \t\t Training Loss: 0.0005807748530060053 \t\n",
      "Epoch 38685 \t\t Training Loss: 0.0005807748530060053 \t\n",
      "Epoch 38686 \t\t Training Loss: 0.0005807748530060053 \t\n",
      "Epoch 38687 \t\t Training Loss: 0.0005807748530060053 \t\n",
      "Epoch 38688 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38689 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38690 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38691 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38692 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38693 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38694 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38695 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38696 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38697 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38698 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38699 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38700 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38701 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38702 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38703 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38704 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38705 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38706 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38707 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38708 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38709 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38710 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38711 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38712 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38713 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38714 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38715 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38716 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38717 \t\t Training Loss: 0.0005807747365906835 \t\n",
      "Epoch 38718 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38719 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38720 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38721 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38722 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38723 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38724 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38725 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38726 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38727 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38728 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38729 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38730 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38731 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38732 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38733 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38734 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38735 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38736 \t\t Training Loss: 0.0005807746201753616 \t\n",
      "Epoch 38737 \t\t Training Loss: 0.0005807745037600398 \t\n",
      "Epoch 38738 \t\t Training Loss: 0.0005807745037600398 \t\n",
      "Epoch 38739 \t\t Training Loss: 0.0005807745037600398 \t\n",
      "Epoch 38740 \t\t Training Loss: 0.0005807744455523789 \t\n",
      "Epoch 38741 \t\t Training Loss: 0.0005807744455523789 \t\n",
      "Epoch 38742 \t\t Training Loss: 0.0005807744455523789 \t\n",
      "Epoch 38743 \t\t Training Loss: 0.0005807744455523789 \t\n",
      "Epoch 38744 \t\t Training Loss: 0.0005807744455523789 \t\n",
      "Epoch 38745 \t\t Training Loss: 0.0005807744455523789 \t\n",
      "Epoch 38746 \t\t Training Loss: 0.0005807744455523789 \t\n",
      "Epoch 38747 \t\t Training Loss: 0.0005807744455523789 \t\n",
      "Epoch 38748 \t\t Training Loss: 0.0005807744455523789 \t\n",
      "Epoch 38749 \t\t Training Loss: 0.0005807744455523789 \t\n",
      "Epoch 38750 \t\t Training Loss: 0.0005807744455523789 \t\n",
      "Epoch 38751 \t\t Training Loss: 0.0005807744455523789 \t\n",
      "Epoch 38752 \t\t Training Loss: 0.0005807744455523789 \t\n",
      "Epoch 38753 \t\t Training Loss: 0.0005807744455523789 \t\n",
      "Epoch 38754 \t\t Training Loss: 0.0005807744455523789 \t\n",
      "Epoch 38755 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38756 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38757 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38758 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38759 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38760 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38761 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38762 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38763 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38764 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38765 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38766 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38767 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38768 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38769 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38770 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38771 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38772 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38773 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38774 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38775 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38776 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38777 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38778 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38779 \t\t Training Loss: 0.0005807741545140743 \t\n",
      "Epoch 38780 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38781 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38782 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38783 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38784 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38785 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38786 \t\t Training Loss: 0.0005807741545140743 \t\n",
      "Epoch 38787 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38788 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38789 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38790 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38791 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38792 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38793 \t\t Training Loss: 0.0005807741545140743 \t\n",
      "Epoch 38794 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38795 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38796 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38797 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38798 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38799 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38800 \t\t Training Loss: 0.0005807743291370571 \t\n",
      "Epoch 38801 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38802 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38803 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38804 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38805 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38806 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38807 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38808 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38809 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38810 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38811 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38812 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38813 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38814 \t\t Training Loss: 0.0005807742127217352 \t\n",
      "Epoch 38815 \t\t Training Loss: 0.0005807741545140743 \t\n",
      "Epoch 38816 \t\t Training Loss: 0.0005807741545140743 \t\n",
      "Epoch 38817 \t\t Training Loss: 0.0005807741545140743 \t\n",
      "Epoch 38818 \t\t Training Loss: 0.0005807741545140743 \t\n",
      "Epoch 38819 \t\t Training Loss: 0.0005807741545140743 \t\n",
      "Epoch 38820 \t\t Training Loss: 0.0005807741545140743 \t\n",
      "Epoch 38821 \t\t Training Loss: 0.0005807741545140743 \t\n",
      "Epoch 38822 \t\t Training Loss: 0.0005807741545140743 \t\n",
      "Epoch 38823 \t\t Training Loss: 0.0005807741545140743 \t\n",
      "Epoch 38824 \t\t Training Loss: 0.0005807741545140743 \t\n",
      "Epoch 38825 \t\t Training Loss: 0.0005807741545140743 \t\n",
      "Epoch 38826 \t\t Training Loss: 0.0005807741545140743 \t\n",
      "Epoch 38827 \t\t Training Loss: 0.0005807741545140743 \t\n",
      "Epoch 38828 \t\t Training Loss: 0.0005807740380987525 \t\n",
      "Epoch 38829 \t\t Training Loss: 0.0005807740380987525 \t\n",
      "Epoch 38830 \t\t Training Loss: 0.0005807740380987525 \t\n",
      "Epoch 38831 \t\t Training Loss: 0.0005807740380987525 \t\n",
      "Epoch 38832 \t\t Training Loss: 0.0005807740380987525 \t\n",
      "Epoch 38833 \t\t Training Loss: 0.0005807740380987525 \t\n",
      "Epoch 38834 \t\t Training Loss: 0.0005807740380987525 \t\n",
      "Epoch 38835 \t\t Training Loss: 0.0005807740380987525 \t\n",
      "Epoch 38836 \t\t Training Loss: 0.0005807739216834307 \t\n",
      "Epoch 38837 \t\t Training Loss: 0.0005807739216834307 \t\n",
      "Epoch 38838 \t\t Training Loss: 0.0005807739216834307 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38839 \t\t Training Loss: 0.0005807739216834307 \t\n",
      "Epoch 38840 \t\t Training Loss: 0.0005807739216834307 \t\n",
      "Epoch 38841 \t\t Training Loss: 0.0005807739216834307 \t\n",
      "Epoch 38842 \t\t Training Loss: 0.0005807739216834307 \t\n",
      "Epoch 38843 \t\t Training Loss: 0.0005807739216834307 \t\n",
      "Epoch 38844 \t\t Training Loss: 0.0005807739216834307 \t\n",
      "Epoch 38845 \t\t Training Loss: 0.0005807739216834307 \t\n",
      "Epoch 38846 \t\t Training Loss: 0.0005807739216834307 \t\n",
      "Epoch 38847 \t\t Training Loss: 0.0005807739216834307 \t\n",
      "Epoch 38848 \t\t Training Loss: 0.0005807738634757698 \t\n",
      "Epoch 38849 \t\t Training Loss: 0.0005807738634757698 \t\n",
      "Epoch 38850 \t\t Training Loss: 0.0005807738634757698 \t\n",
      "Epoch 38851 \t\t Training Loss: 0.0005807739216834307 \t\n",
      "Epoch 38852 \t\t Training Loss: 0.0005807739216834307 \t\n",
      "Epoch 38853 \t\t Training Loss: 0.0005807739216834307 \t\n",
      "Epoch 38854 \t\t Training Loss: 0.0005807738634757698 \t\n",
      "Epoch 38855 \t\t Training Loss: 0.0005807738634757698 \t\n",
      "Epoch 38856 \t\t Training Loss: 0.0005807738634757698 \t\n",
      "Epoch 38857 \t\t Training Loss: 0.0005807738634757698 \t\n",
      "Epoch 38858 \t\t Training Loss: 0.0005807738634757698 \t\n",
      "Epoch 38859 \t\t Training Loss: 0.0005807738634757698 \t\n",
      "Epoch 38860 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38861 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38862 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38863 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38864 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38865 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38866 \t\t Training Loss: 0.0005807738634757698 \t\n",
      "Epoch 38867 \t\t Training Loss: 0.0005807738634757698 \t\n",
      "Epoch 38868 \t\t Training Loss: 0.0005807738634757698 \t\n",
      "Epoch 38869 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38870 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38871 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38872 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38873 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38874 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38875 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38876 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38877 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38878 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38879 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38880 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38881 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38882 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38883 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38884 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38885 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38886 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38887 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38888 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38889 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38890 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38891 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38892 \t\t Training Loss: 0.0005807737470604479 \t\n",
      "Epoch 38893 \t\t Training Loss: 0.0005807736306451261 \t\n",
      "Epoch 38894 \t\t Training Loss: 0.0005807736306451261 \t\n",
      "Epoch 38895 \t\t Training Loss: 0.0005807736306451261 \t\n",
      "Epoch 38896 \t\t Training Loss: 0.0005807736306451261 \t\n",
      "Epoch 38897 \t\t Training Loss: 0.0005807736306451261 \t\n",
      "Epoch 38898 \t\t Training Loss: 0.0005807736306451261 \t\n",
      "Epoch 38899 \t\t Training Loss: 0.0005807736306451261 \t\n",
      "Epoch 38900 \t\t Training Loss: 0.0005807736306451261 \t\n",
      "Epoch 38901 \t\t Training Loss: 0.0005807736306451261 \t\n",
      "Epoch 38902 \t\t Training Loss: 0.0005807735724374652 \t\n",
      "Epoch 38903 \t\t Training Loss: 0.0005807735724374652 \t\n",
      "Epoch 38904 \t\t Training Loss: 0.0005807735142298043 \t\n",
      "Epoch 38905 \t\t Training Loss: 0.0005807735142298043 \t\n",
      "Epoch 38906 \t\t Training Loss: 0.0005807735142298043 \t\n",
      "Epoch 38907 \t\t Training Loss: 0.0005807735142298043 \t\n",
      "Epoch 38908 \t\t Training Loss: 0.0005807735142298043 \t\n",
      "Epoch 38909 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38910 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38911 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38912 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38913 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38914 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38915 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38916 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38917 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38918 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38919 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38920 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38921 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38922 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38923 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38924 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38925 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38926 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38927 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38928 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38929 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38930 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38931 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38932 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38933 \t\t Training Loss: 0.0005807735142298043 \t\n",
      "Epoch 38934 \t\t Training Loss: 0.0005807735142298043 \t\n",
      "Epoch 38935 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38936 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38937 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38938 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38939 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38940 \t\t Training Loss: 0.0005807733396068215 \t\n",
      "Epoch 38941 \t\t Training Loss: 0.0005807733396068215 \t\n",
      "Epoch 38942 \t\t Training Loss: 0.0005807734560221434 \t\n",
      "Epoch 38943 \t\t Training Loss: 0.0005807733396068215 \t\n",
      "Epoch 38944 \t\t Training Loss: 0.0005807733396068215 \t\n",
      "Epoch 38945 \t\t Training Loss: 0.0005807733396068215 \t\n",
      "Epoch 38946 \t\t Training Loss: 0.0005807733396068215 \t\n",
      "Epoch 38947 \t\t Training Loss: 0.0005807733396068215 \t\n",
      "Epoch 38948 \t\t Training Loss: 0.0005807732813991606 \t\n",
      "Epoch 38949 \t\t Training Loss: 0.0005807733396068215 \t\n",
      "Epoch 38950 \t\t Training Loss: 0.0005807733396068215 \t\n",
      "Epoch 38951 \t\t Training Loss: 0.0005807732813991606 \t\n",
      "Epoch 38952 \t\t Training Loss: 0.0005807732813991606 \t\n",
      "Epoch 38953 \t\t Training Loss: 0.0005807732813991606 \t\n",
      "Epoch 38954 \t\t Training Loss: 0.0005807732813991606 \t\n",
      "Epoch 38955 \t\t Training Loss: 0.0005807732813991606 \t\n",
      "Epoch 38956 \t\t Training Loss: 0.0005807732813991606 \t\n",
      "Epoch 38957 \t\t Training Loss: 0.0005807732813991606 \t\n",
      "Epoch 38958 \t\t Training Loss: 0.0005807732813991606 \t\n",
      "Epoch 38959 \t\t Training Loss: 0.0005807732813991606 \t\n",
      "Epoch 38960 \t\t Training Loss: 0.0005807731649838388 \t\n",
      "Epoch 38961 \t\t Training Loss: 0.0005807732813991606 \t\n",
      "Epoch 38962 \t\t Training Loss: 0.0005807731649838388 \t\n",
      "Epoch 38963 \t\t Training Loss: 0.0005807731649838388 \t\n",
      "Epoch 38964 \t\t Training Loss: 0.0005807731649838388 \t\n",
      "Epoch 38965 \t\t Training Loss: 0.0005807732813991606 \t\n",
      "Epoch 38966 \t\t Training Loss: 0.0005807731649838388 \t\n",
      "Epoch 38967 \t\t Training Loss: 0.0005807731649838388 \t\n",
      "Epoch 38968 \t\t Training Loss: 0.0005807731649838388 \t\n",
      "Epoch 38969 \t\t Training Loss: 0.0005807731649838388 \t\n",
      "Epoch 38970 \t\t Training Loss: 0.0005807731649838388 \t\n",
      "Epoch 38971 \t\t Training Loss: 0.0005807731649838388 \t\n",
      "Epoch 38972 \t\t Training Loss: 0.0005807731649838388 \t\n",
      "Epoch 38973 \t\t Training Loss: 0.0005807731649838388 \t\n",
      "Epoch 38974 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 38975 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 38976 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 38977 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 38978 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 38979 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 38980 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 38981 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 38982 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 38983 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 38984 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 38985 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 38986 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 38987 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 38988 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 38989 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 38990 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 38991 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 38992 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 38993 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 38994 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 38995 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 38996 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 38997 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 38998 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 38999 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 39000 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 39001 \t\t Training Loss: 0.000580773048568517 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39002 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 39003 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 39004 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 39005 \t\t Training Loss: 0.000580773048568517 \t\n",
      "Epoch 39006 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 39007 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 39008 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 39009 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 39010 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 39011 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 39012 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 39013 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 39014 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 39015 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 39016 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 39017 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 39018 \t\t Training Loss: 0.0005807729321531951 \t\n",
      "Epoch 39019 \t\t Training Loss: 0.0005807728739455342 \t\n",
      "Epoch 39020 \t\t Training Loss: 0.0005807728739455342 \t\n",
      "Epoch 39021 \t\t Training Loss: 0.0005807728739455342 \t\n",
      "Epoch 39022 \t\t Training Loss: 0.0005807728739455342 \t\n",
      "Epoch 39023 \t\t Training Loss: 0.0005807728739455342 \t\n",
      "Epoch 39024 \t\t Training Loss: 0.0005807728739455342 \t\n",
      "Epoch 39025 \t\t Training Loss: 0.0005807728739455342 \t\n",
      "Epoch 39026 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39027 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39028 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39029 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39030 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39031 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39032 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39033 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39034 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39035 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39036 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39037 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39038 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39039 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39040 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39041 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39042 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39043 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39044 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39045 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39046 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39047 \t\t Training Loss: 0.0005807727575302124 \t\n",
      "Epoch 39048 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39049 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39050 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39051 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39052 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39053 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39054 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39055 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39056 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39057 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39058 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39059 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39060 \t\t Training Loss: 0.0005807725829072297 \t\n",
      "Epoch 39061 \t\t Training Loss: 0.0005807725829072297 \t\n",
      "Epoch 39062 \t\t Training Loss: 0.0005807726411148906 \t\n",
      "Epoch 39063 \t\t Training Loss: 0.0005807725829072297 \t\n",
      "Epoch 39064 \t\t Training Loss: 0.0005807725829072297 \t\n",
      "Epoch 39065 \t\t Training Loss: 0.0005807725829072297 \t\n",
      "Epoch 39066 \t\t Training Loss: 0.0005807725829072297 \t\n",
      "Epoch 39067 \t\t Training Loss: 0.0005807725829072297 \t\n",
      "Epoch 39068 \t\t Training Loss: 0.0005807725829072297 \t\n",
      "Epoch 39069 \t\t Training Loss: 0.0005807725829072297 \t\n",
      "Epoch 39070 \t\t Training Loss: 0.0005807725829072297 \t\n",
      "Epoch 39071 \t\t Training Loss: 0.0005807725829072297 \t\n",
      "Epoch 39072 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39073 \t\t Training Loss: 0.0005807725829072297 \t\n",
      "Epoch 39074 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39075 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39076 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39077 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39078 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39079 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39080 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39081 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39082 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39083 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39084 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39085 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39086 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39087 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39088 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39089 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39090 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39091 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39092 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39093 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39094 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39095 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39096 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39097 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39098 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39099 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39100 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39101 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39102 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39103 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39104 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39105 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39106 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39107 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39108 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39109 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39110 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39111 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39112 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39113 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39114 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39115 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39116 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39117 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39118 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39119 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39120 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39121 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39122 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39123 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39124 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39125 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39126 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39127 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39128 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39129 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39130 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39131 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39132 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39133 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39134 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39135 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39136 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39137 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39138 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39139 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39140 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39141 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39142 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39143 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39144 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39145 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39146 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39147 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39148 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39149 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39150 \t\t Training Loss: 0.0005807724082842469 \t\n",
      "Epoch 39151 \t\t Training Loss: 0.0005807724082842469 \t\n",
      "Epoch 39152 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39153 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39154 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39155 \t\t Training Loss: 0.0005807724082842469 \t\n",
      "Epoch 39156 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39157 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39158 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39159 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39160 \t\t Training Loss: 0.0005807724664919078 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39161 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39162 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39163 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39164 \t\t Training Loss: 0.0005807725246995687 \t\n",
      "Epoch 39165 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39166 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39167 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39168 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39169 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39170 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39171 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39172 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39173 \t\t Training Loss: 0.0005807724082842469 \t\n",
      "Epoch 39174 \t\t Training Loss: 0.0005807724664919078 \t\n",
      "Epoch 39175 \t\t Training Loss: 0.0005807724082842469 \t\n",
      "Epoch 39176 \t\t Training Loss: 0.0005807724082842469 \t\n",
      "Epoch 39177 \t\t Training Loss: 0.0005807724082842469 \t\n",
      "Epoch 39178 \t\t Training Loss: 0.0005807724082842469 \t\n",
      "Epoch 39179 \t\t Training Loss: 0.000580772350076586 \t\n",
      "Epoch 39180 \t\t Training Loss: 0.0005807724082842469 \t\n",
      "Epoch 39181 \t\t Training Loss: 0.0005807724082842469 \t\n",
      "Epoch 39182 \t\t Training Loss: 0.000580772350076586 \t\n",
      "Epoch 39183 \t\t Training Loss: 0.0005807722918689251 \t\n",
      "Epoch 39184 \t\t Training Loss: 0.000580772350076586 \t\n",
      "Epoch 39185 \t\t Training Loss: 0.0005807722918689251 \t\n",
      "Epoch 39186 \t\t Training Loss: 0.0005807722918689251 \t\n",
      "Epoch 39187 \t\t Training Loss: 0.000580772350076586 \t\n",
      "Epoch 39188 \t\t Training Loss: 0.0005807722918689251 \t\n",
      "Epoch 39189 \t\t Training Loss: 0.0005807722918689251 \t\n",
      "Epoch 39190 \t\t Training Loss: 0.0005807722918689251 \t\n",
      "Epoch 39191 \t\t Training Loss: 0.0005807722918689251 \t\n",
      "Epoch 39192 \t\t Training Loss: 0.0005807722918689251 \t\n",
      "Epoch 39193 \t\t Training Loss: 0.0005807722918689251 \t\n",
      "Epoch 39194 \t\t Training Loss: 0.0005807722918689251 \t\n",
      "Epoch 39195 \t\t Training Loss: 0.0005807722918689251 \t\n",
      "Epoch 39196 \t\t Training Loss: 0.0005807721754536033 \t\n",
      "Epoch 39197 \t\t Training Loss: 0.0005807721754536033 \t\n",
      "Epoch 39198 \t\t Training Loss: 0.0005807721754536033 \t\n",
      "Epoch 39199 \t\t Training Loss: 0.0005807721172459424 \t\n",
      "Epoch 39200 \t\t Training Loss: 0.0005807721754536033 \t\n",
      "Epoch 39201 \t\t Training Loss: 0.0005807721754536033 \t\n",
      "Epoch 39202 \t\t Training Loss: 0.0005807721172459424 \t\n",
      "Epoch 39203 \t\t Training Loss: 0.0005807721172459424 \t\n",
      "Epoch 39204 \t\t Training Loss: 0.0005807721172459424 \t\n",
      "Epoch 39205 \t\t Training Loss: 0.0005807721172459424 \t\n",
      "Epoch 39206 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39207 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39208 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39209 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39210 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39211 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39212 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39213 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39214 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39215 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39216 \t\t Training Loss: 0.0005807721172459424 \t\n",
      "Epoch 39217 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39218 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39219 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39220 \t\t Training Loss: 0.0005807721172459424 \t\n",
      "Epoch 39221 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39222 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39223 \t\t Training Loss: 0.0005807721172459424 \t\n",
      "Epoch 39224 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39225 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39226 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39227 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39228 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39229 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39230 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39231 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39232 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39233 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39234 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39235 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39236 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39237 \t\t Training Loss: 0.0005807720008306205 \t\n",
      "Epoch 39238 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39239 \t\t Training Loss: 0.0005807720590382814 \t\n",
      "Epoch 39240 \t\t Training Loss: 0.0005807720008306205 \t\n",
      "Epoch 39241 \t\t Training Loss: 0.0005807720008306205 \t\n",
      "Epoch 39242 \t\t Training Loss: 0.0005807720008306205 \t\n",
      "Epoch 39243 \t\t Training Loss: 0.0005807720008306205 \t\n",
      "Epoch 39244 \t\t Training Loss: 0.0005807719426229596 \t\n",
      "Epoch 39245 \t\t Training Loss: 0.0005807719426229596 \t\n",
      "Epoch 39246 \t\t Training Loss: 0.0005807719426229596 \t\n",
      "Epoch 39247 \t\t Training Loss: 0.0005807719426229596 \t\n",
      "Epoch 39248 \t\t Training Loss: 0.0005807719426229596 \t\n",
      "Epoch 39249 \t\t Training Loss: 0.0005807719426229596 \t\n",
      "Epoch 39250 \t\t Training Loss: 0.0005807719426229596 \t\n",
      "Epoch 39251 \t\t Training Loss: 0.0005807719426229596 \t\n",
      "Epoch 39252 \t\t Training Loss: 0.0005807719426229596 \t\n",
      "Epoch 39253 \t\t Training Loss: 0.0005807719426229596 \t\n",
      "Epoch 39254 \t\t Training Loss: 0.0005807719426229596 \t\n",
      "Epoch 39255 \t\t Training Loss: 0.0005807718844152987 \t\n",
      "Epoch 39256 \t\t Training Loss: 0.0005807718844152987 \t\n",
      "Epoch 39257 \t\t Training Loss: 0.0005807718844152987 \t\n",
      "Epoch 39258 \t\t Training Loss: 0.0005807718262076378 \t\n",
      "Epoch 39259 \t\t Training Loss: 0.0005807718844152987 \t\n",
      "Epoch 39260 \t\t Training Loss: 0.0005807718262076378 \t\n",
      "Epoch 39261 \t\t Training Loss: 0.0005807718262076378 \t\n",
      "Epoch 39262 \t\t Training Loss: 0.0005807718262076378 \t\n",
      "Epoch 39263 \t\t Training Loss: 0.0005807718262076378 \t\n",
      "Epoch 39264 \t\t Training Loss: 0.0005807718262076378 \t\n",
      "Epoch 39265 \t\t Training Loss: 0.0005807718262076378 \t\n",
      "Epoch 39266 \t\t Training Loss: 0.0005807718262076378 \t\n",
      "Epoch 39267 \t\t Training Loss: 0.0005807718262076378 \t\n",
      "Epoch 39268 \t\t Training Loss: 0.0005807717679999769 \t\n",
      "Epoch 39269 \t\t Training Loss: 0.0005807717679999769 \t\n",
      "Epoch 39270 \t\t Training Loss: 0.0005807717679999769 \t\n",
      "Epoch 39271 \t\t Training Loss: 0.0005807717679999769 \t\n",
      "Epoch 39272 \t\t Training Loss: 0.0005807717679999769 \t\n",
      "Epoch 39273 \t\t Training Loss: 0.0005807717679999769 \t\n",
      "Epoch 39274 \t\t Training Loss: 0.0005807717679999769 \t\n",
      "Epoch 39275 \t\t Training Loss: 0.0005807717679999769 \t\n",
      "Epoch 39276 \t\t Training Loss: 0.0005807717679999769 \t\n",
      "Epoch 39277 \t\t Training Loss: 0.0005807717679999769 \t\n",
      "Epoch 39278 \t\t Training Loss: 0.0005807717679999769 \t\n",
      "Epoch 39279 \t\t Training Loss: 0.0005807717679999769 \t\n",
      "Epoch 39280 \t\t Training Loss: 0.000580771709792316 \t\n",
      "Epoch 39281 \t\t Training Loss: 0.000580771709792316 \t\n",
      "Epoch 39282 \t\t Training Loss: 0.0005807717679999769 \t\n",
      "Epoch 39283 \t\t Training Loss: 0.0005807717679999769 \t\n",
      "Epoch 39284 \t\t Training Loss: 0.000580771709792316 \t\n",
      "Epoch 39285 \t\t Training Loss: 0.000580771709792316 \t\n",
      "Epoch 39286 \t\t Training Loss: 0.0005807717679999769 \t\n",
      "Epoch 39287 \t\t Training Loss: 0.000580771709792316 \t\n",
      "Epoch 39288 \t\t Training Loss: 0.000580771709792316 \t\n",
      "Epoch 39289 \t\t Training Loss: 0.0005807715933769941 \t\n",
      "Epoch 39290 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39291 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39292 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39293 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39294 \t\t Training Loss: 0.0005807715933769941 \t\n",
      "Epoch 39295 \t\t Training Loss: 0.0005807715933769941 \t\n",
      "Epoch 39296 \t\t Training Loss: 0.0005807715933769941 \t\n",
      "Epoch 39297 \t\t Training Loss: 0.0005807715933769941 \t\n",
      "Epoch 39298 \t\t Training Loss: 0.0005807715933769941 \t\n",
      "Epoch 39299 \t\t Training Loss: 0.0005807715933769941 \t\n",
      "Epoch 39300 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39301 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39302 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39303 \t\t Training Loss: 0.0005807714769616723 \t\n",
      "Epoch 39304 \t\t Training Loss: 0.0005807714769616723 \t\n",
      "Epoch 39305 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39306 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39307 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39308 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39309 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39310 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39311 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39312 \t\t Training Loss: 0.0005807715933769941 \t\n",
      "Epoch 39313 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39314 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39315 \t\t Training Loss: 0.0005807715351693332 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39316 \t\t Training Loss: 0.0005807715351693332 \t\n",
      "Epoch 39317 \t\t Training Loss: 0.0005807714769616723 \t\n",
      "Epoch 39318 \t\t Training Loss: 0.0005807714769616723 \t\n",
      "Epoch 39319 \t\t Training Loss: 0.0005807714769616723 \t\n",
      "Epoch 39320 \t\t Training Loss: 0.0005807714769616723 \t\n",
      "Epoch 39321 \t\t Training Loss: 0.0005807714769616723 \t\n",
      "Epoch 39322 \t\t Training Loss: 0.0005807714769616723 \t\n",
      "Epoch 39323 \t\t Training Loss: 0.0005807714187540114 \t\n",
      "Epoch 39324 \t\t Training Loss: 0.0005807713605463505 \t\n",
      "Epoch 39325 \t\t Training Loss: 0.0005807714187540114 \t\n",
      "Epoch 39326 \t\t Training Loss: 0.0005807713605463505 \t\n",
      "Epoch 39327 \t\t Training Loss: 0.0005807713605463505 \t\n",
      "Epoch 39328 \t\t Training Loss: 0.0005807713605463505 \t\n",
      "Epoch 39329 \t\t Training Loss: 0.0005807713605463505 \t\n",
      "Epoch 39330 \t\t Training Loss: 0.0005807713605463505 \t\n",
      "Epoch 39331 \t\t Training Loss: 0.0005807713023386896 \t\n",
      "Epoch 39332 \t\t Training Loss: 0.0005807713023386896 \t\n",
      "Epoch 39333 \t\t Training Loss: 0.0005807712441310287 \t\n",
      "Epoch 39334 \t\t Training Loss: 0.0005807712441310287 \t\n",
      "Epoch 39335 \t\t Training Loss: 0.0005807712441310287 \t\n",
      "Epoch 39336 \t\t Training Loss: 0.0005807712441310287 \t\n",
      "Epoch 39337 \t\t Training Loss: 0.0005807712441310287 \t\n",
      "Epoch 39338 \t\t Training Loss: 0.0005807712441310287 \t\n",
      "Epoch 39339 \t\t Training Loss: 0.0005807712441310287 \t\n",
      "Epoch 39340 \t\t Training Loss: 0.0005807712441310287 \t\n",
      "Epoch 39341 \t\t Training Loss: 0.0005807712441310287 \t\n",
      "Epoch 39342 \t\t Training Loss: 0.0005807712441310287 \t\n",
      "Epoch 39343 \t\t Training Loss: 0.0005807712441310287 \t\n",
      "Epoch 39344 \t\t Training Loss: 0.0005807712441310287 \t\n",
      "Epoch 39345 \t\t Training Loss: 0.0005807712441310287 \t\n",
      "Epoch 39346 \t\t Training Loss: 0.0005807711859233677 \t\n",
      "Epoch 39347 \t\t Training Loss: 0.0005807711859233677 \t\n",
      "Epoch 39348 \t\t Training Loss: 0.0005807712441310287 \t\n",
      "Epoch 39349 \t\t Training Loss: 0.0005807711859233677 \t\n",
      "Epoch 39350 \t\t Training Loss: 0.0005807711277157068 \t\n",
      "Epoch 39351 \t\t Training Loss: 0.0005807711277157068 \t\n",
      "Epoch 39352 \t\t Training Loss: 0.0005807711277157068 \t\n",
      "Epoch 39353 \t\t Training Loss: 0.0005807711277157068 \t\n",
      "Epoch 39354 \t\t Training Loss: 0.0005807710695080459 \t\n",
      "Epoch 39355 \t\t Training Loss: 0.0005807710695080459 \t\n",
      "Epoch 39356 \t\t Training Loss: 0.0005807710695080459 \t\n",
      "Epoch 39357 \t\t Training Loss: 0.0005807711277157068 \t\n",
      "Epoch 39358 \t\t Training Loss: 0.0005807710695080459 \t\n",
      "Epoch 39359 \t\t Training Loss: 0.000580771011300385 \t\n",
      "Epoch 39360 \t\t Training Loss: 0.000580771011300385 \t\n",
      "Epoch 39361 \t\t Training Loss: 0.000580771011300385 \t\n",
      "Epoch 39362 \t\t Training Loss: 0.000580771011300385 \t\n",
      "Epoch 39363 \t\t Training Loss: 0.000580771011300385 \t\n",
      "Epoch 39364 \t\t Training Loss: 0.000580771011300385 \t\n",
      "Epoch 39365 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39366 \t\t Training Loss: 0.000580771011300385 \t\n",
      "Epoch 39367 \t\t Training Loss: 0.000580771011300385 \t\n",
      "Epoch 39368 \t\t Training Loss: 0.000580771011300385 \t\n",
      "Epoch 39369 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39370 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39371 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39372 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39373 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39374 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39375 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39376 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39377 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39378 \t\t Training Loss: 0.000580771011300385 \t\n",
      "Epoch 39379 \t\t Training Loss: 0.000580771011300385 \t\n",
      "Epoch 39380 \t\t Training Loss: 0.0005807710695080459 \t\n",
      "Epoch 39381 \t\t Training Loss: 0.000580771011300385 \t\n",
      "Epoch 39382 \t\t Training Loss: 0.000580771011300385 \t\n",
      "Epoch 39383 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39384 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39385 \t\t Training Loss: 0.000580771011300385 \t\n",
      "Epoch 39386 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39387 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39388 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39389 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39390 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39391 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39392 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39393 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39394 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39395 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39396 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39397 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39398 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39399 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39400 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39401 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39402 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39403 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39404 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39405 \t\t Training Loss: 0.0005807709530927241 \t\n",
      "Epoch 39406 \t\t Training Loss: 0.0005807708948850632 \t\n",
      "Epoch 39407 \t\t Training Loss: 0.0005807708948850632 \t\n",
      "Epoch 39408 \t\t Training Loss: 0.0005807708948850632 \t\n",
      "Epoch 39409 \t\t Training Loss: 0.0005807708366774023 \t\n",
      "Epoch 39410 \t\t Training Loss: 0.0005807708366774023 \t\n",
      "Epoch 39411 \t\t Training Loss: 0.0005807708366774023 \t\n",
      "Epoch 39412 \t\t Training Loss: 0.0005807708366774023 \t\n",
      "Epoch 39413 \t\t Training Loss: 0.0005807708366774023 \t\n",
      "Epoch 39414 \t\t Training Loss: 0.0005807708366774023 \t\n",
      "Epoch 39415 \t\t Training Loss: 0.0005807708366774023 \t\n",
      "Epoch 39416 \t\t Training Loss: 0.0005807708366774023 \t\n",
      "Epoch 39417 \t\t Training Loss: 0.0005807707784697413 \t\n",
      "Epoch 39418 \t\t Training Loss: 0.0005807708366774023 \t\n",
      "Epoch 39419 \t\t Training Loss: 0.0005807707784697413 \t\n",
      "Epoch 39420 \t\t Training Loss: 0.0005807707784697413 \t\n",
      "Epoch 39421 \t\t Training Loss: 0.0005807707784697413 \t\n",
      "Epoch 39422 \t\t Training Loss: 0.0005807707784697413 \t\n",
      "Epoch 39423 \t\t Training Loss: 0.0005807707784697413 \t\n",
      "Epoch 39424 \t\t Training Loss: 0.0005807707784697413 \t\n",
      "Epoch 39425 \t\t Training Loss: 0.0005807707784697413 \t\n",
      "Epoch 39426 \t\t Training Loss: 0.0005807707784697413 \t\n",
      "Epoch 39427 \t\t Training Loss: 0.0005807707784697413 \t\n",
      "Epoch 39428 \t\t Training Loss: 0.0005807707784697413 \t\n",
      "Epoch 39429 \t\t Training Loss: 0.0005807707202620804 \t\n",
      "Epoch 39430 \t\t Training Loss: 0.0005807706620544195 \t\n",
      "Epoch 39431 \t\t Training Loss: 0.0005807707202620804 \t\n",
      "Epoch 39432 \t\t Training Loss: 0.0005807707202620804 \t\n",
      "Epoch 39433 \t\t Training Loss: 0.0005807707784697413 \t\n",
      "Epoch 39434 \t\t Training Loss: 0.0005807707202620804 \t\n",
      "Epoch 39435 \t\t Training Loss: 0.0005807707202620804 \t\n",
      "Epoch 39436 \t\t Training Loss: 0.0005807707202620804 \t\n",
      "Epoch 39437 \t\t Training Loss: 0.0005807707784697413 \t\n",
      "Epoch 39438 \t\t Training Loss: 0.0005807707202620804 \t\n",
      "Epoch 39439 \t\t Training Loss: 0.0005807707784697413 \t\n",
      "Epoch 39440 \t\t Training Loss: 0.0005807707202620804 \t\n",
      "Epoch 39441 \t\t Training Loss: 0.0005807707202620804 \t\n",
      "Epoch 39442 \t\t Training Loss: 0.0005807707784697413 \t\n",
      "Epoch 39443 \t\t Training Loss: 0.0005807707202620804 \t\n",
      "Epoch 39444 \t\t Training Loss: 0.0005807706620544195 \t\n",
      "Epoch 39445 \t\t Training Loss: 0.0005807706620544195 \t\n",
      "Epoch 39446 \t\t Training Loss: 0.0005807706620544195 \t\n",
      "Epoch 39447 \t\t Training Loss: 0.0005807706620544195 \t\n",
      "Epoch 39448 \t\t Training Loss: 0.0005807706620544195 \t\n",
      "Epoch 39449 \t\t Training Loss: 0.0005807706620544195 \t\n",
      "Epoch 39450 \t\t Training Loss: 0.0005807706620544195 \t\n",
      "Epoch 39451 \t\t Training Loss: 0.0005807706620544195 \t\n",
      "Epoch 39452 \t\t Training Loss: 0.0005807706620544195 \t\n",
      "Epoch 39453 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39454 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39455 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39456 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39457 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39458 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39459 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39460 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39461 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39462 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39463 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39464 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39465 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39466 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39467 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39468 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39469 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39470 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39471 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39472 \t\t Training Loss: 0.0005807704874314368 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39473 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39474 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39475 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39476 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39477 \t\t Training Loss: 0.0005807705456390977 \t\n",
      "Epoch 39478 \t\t Training Loss: 0.0005807704874314368 \t\n",
      "Epoch 39479 \t\t Training Loss: 0.0005807704292237759 \t\n",
      "Epoch 39480 \t\t Training Loss: 0.0005807704292237759 \t\n",
      "Epoch 39481 \t\t Training Loss: 0.0005807704292237759 \t\n",
      "Epoch 39482 \t\t Training Loss: 0.0005807704292237759 \t\n",
      "Epoch 39483 \t\t Training Loss: 0.0005807704292237759 \t\n",
      "Epoch 39484 \t\t Training Loss: 0.0005807704292237759 \t\n",
      "Epoch 39485 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39486 \t\t Training Loss: 0.0005807704292237759 \t\n",
      "Epoch 39487 \t\t Training Loss: 0.0005807704292237759 \t\n",
      "Epoch 39488 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39489 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39490 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39491 \t\t Training Loss: 0.0005807704292237759 \t\n",
      "Epoch 39492 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39493 \t\t Training Loss: 0.0005807704292237759 \t\n",
      "Epoch 39494 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39495 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39496 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39497 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39498 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39499 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39500 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39501 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39502 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39503 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39504 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39505 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39506 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39507 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39508 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39509 \t\t Training Loss: 0.000580770371016115 \t\n",
      "Epoch 39510 \t\t Training Loss: 0.000580770312808454 \t\n",
      "Epoch 39511 \t\t Training Loss: 0.000580770312808454 \t\n",
      "Epoch 39512 \t\t Training Loss: 0.000580770312808454 \t\n",
      "Epoch 39513 \t\t Training Loss: 0.000580770312808454 \t\n",
      "Epoch 39514 \t\t Training Loss: 0.0005807702546007931 \t\n",
      "Epoch 39515 \t\t Training Loss: 0.0005807702546007931 \t\n",
      "Epoch 39516 \t\t Training Loss: 0.000580770312808454 \t\n",
      "Epoch 39517 \t\t Training Loss: 0.0005807702546007931 \t\n",
      "Epoch 39518 \t\t Training Loss: 0.000580770312808454 \t\n",
      "Epoch 39519 \t\t Training Loss: 0.0005807702546007931 \t\n",
      "Epoch 39520 \t\t Training Loss: 0.0005807702546007931 \t\n",
      "Epoch 39521 \t\t Training Loss: 0.0005807702546007931 \t\n",
      "Epoch 39522 \t\t Training Loss: 0.0005807702546007931 \t\n",
      "Epoch 39523 \t\t Training Loss: 0.0005807702546007931 \t\n",
      "Epoch 39524 \t\t Training Loss: 0.0005807701963931322 \t\n",
      "Epoch 39525 \t\t Training Loss: 0.0005807701963931322 \t\n",
      "Epoch 39526 \t\t Training Loss: 0.0005807701963931322 \t\n",
      "Epoch 39527 \t\t Training Loss: 0.0005807701963931322 \t\n",
      "Epoch 39528 \t\t Training Loss: 0.0005807701963931322 \t\n",
      "Epoch 39529 \t\t Training Loss: 0.0005807701963931322 \t\n",
      "Epoch 39530 \t\t Training Loss: 0.0005807701963931322 \t\n",
      "Epoch 39531 \t\t Training Loss: 0.0005807701963931322 \t\n",
      "Epoch 39532 \t\t Training Loss: 0.0005807701963931322 \t\n",
      "Epoch 39533 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39534 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39535 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39536 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39537 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39538 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39539 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39540 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39541 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39542 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39543 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39544 \t\t Training Loss: 0.0005807700217701495 \t\n",
      "Epoch 39545 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39546 \t\t Training Loss: 0.0005807700217701495 \t\n",
      "Epoch 39547 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39548 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39549 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39550 \t\t Training Loss: 0.0005807700217701495 \t\n",
      "Epoch 39551 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39552 \t\t Training Loss: 0.0005807700799778104 \t\n",
      "Epoch 39553 \t\t Training Loss: 0.0005807699635624886 \t\n",
      "Epoch 39554 \t\t Training Loss: 0.0005807699635624886 \t\n",
      "Epoch 39555 \t\t Training Loss: 0.0005807699635624886 \t\n",
      "Epoch 39556 \t\t Training Loss: 0.0005807699635624886 \t\n",
      "Epoch 39557 \t\t Training Loss: 0.0005807699635624886 \t\n",
      "Epoch 39558 \t\t Training Loss: 0.0005807699635624886 \t\n",
      "Epoch 39559 \t\t Training Loss: 0.0005807699053548276 \t\n",
      "Epoch 39560 \t\t Training Loss: 0.0005807699053548276 \t\n",
      "Epoch 39561 \t\t Training Loss: 0.0005807699053548276 \t\n",
      "Epoch 39562 \t\t Training Loss: 0.0005807699053548276 \t\n",
      "Epoch 39563 \t\t Training Loss: 0.0005807699053548276 \t\n",
      "Epoch 39564 \t\t Training Loss: 0.0005807699053548276 \t\n",
      "Epoch 39565 \t\t Training Loss: 0.0005807699053548276 \t\n",
      "Epoch 39566 \t\t Training Loss: 0.0005807699053548276 \t\n",
      "Epoch 39567 \t\t Training Loss: 0.0005807698471471667 \t\n",
      "Epoch 39568 \t\t Training Loss: 0.0005807698471471667 \t\n",
      "Epoch 39569 \t\t Training Loss: 0.0005807698471471667 \t\n",
      "Epoch 39570 \t\t Training Loss: 0.0005807698471471667 \t\n",
      "Epoch 39571 \t\t Training Loss: 0.0005807698471471667 \t\n",
      "Epoch 39572 \t\t Training Loss: 0.0005807698471471667 \t\n",
      "Epoch 39573 \t\t Training Loss: 0.0005807698471471667 \t\n",
      "Epoch 39574 \t\t Training Loss: 0.0005807698471471667 \t\n",
      "Epoch 39575 \t\t Training Loss: 0.0005807698471471667 \t\n",
      "Epoch 39576 \t\t Training Loss: 0.0005807697889395058 \t\n",
      "Epoch 39577 \t\t Training Loss: 0.0005807697889395058 \t\n",
      "Epoch 39578 \t\t Training Loss: 0.0005807697889395058 \t\n",
      "Epoch 39579 \t\t Training Loss: 0.0005807697889395058 \t\n",
      "Epoch 39580 \t\t Training Loss: 0.0005807697889395058 \t\n",
      "Epoch 39581 \t\t Training Loss: 0.0005807697889395058 \t\n",
      "Epoch 39582 \t\t Training Loss: 0.0005807697889395058 \t\n",
      "Epoch 39583 \t\t Training Loss: 0.0005807697889395058 \t\n",
      "Epoch 39584 \t\t Training Loss: 0.0005807697889395058 \t\n",
      "Epoch 39585 \t\t Training Loss: 0.0005807697889395058 \t\n",
      "Epoch 39586 \t\t Training Loss: 0.0005807697889395058 \t\n",
      "Epoch 39587 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39588 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39589 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39590 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39591 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39592 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39593 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39594 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39595 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39596 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39597 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39598 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39599 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39600 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39601 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39602 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39603 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39604 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39605 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39606 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39607 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39608 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39609 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39610 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39611 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39612 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39613 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39614 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39615 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39616 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39617 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39618 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39619 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39620 \t\t Training Loss: 0.000580769672524184 \t\n",
      "Epoch 39621 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39622 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39623 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39624 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39625 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39626 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39627 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39628 \t\t Training Loss: 0.0005807696143165231 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39629 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39630 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39631 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39632 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39633 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39634 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39635 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39636 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39637 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39638 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39639 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39640 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39641 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39642 \t\t Training Loss: 0.0005807696143165231 \t\n",
      "Epoch 39643 \t\t Training Loss: 0.0005807695561088622 \t\n",
      "Epoch 39644 \t\t Training Loss: 0.0005807695561088622 \t\n",
      "Epoch 39645 \t\t Training Loss: 0.0005807695561088622 \t\n",
      "Epoch 39646 \t\t Training Loss: 0.0005807695561088622 \t\n",
      "Epoch 39647 \t\t Training Loss: 0.0005807695561088622 \t\n",
      "Epoch 39648 \t\t Training Loss: 0.0005807695561088622 \t\n",
      "Epoch 39649 \t\t Training Loss: 0.0005807695561088622 \t\n",
      "Epoch 39650 \t\t Training Loss: 0.0005807694979012012 \t\n",
      "Epoch 39651 \t\t Training Loss: 0.0005807694979012012 \t\n",
      "Epoch 39652 \t\t Training Loss: 0.0005807694979012012 \t\n",
      "Epoch 39653 \t\t Training Loss: 0.0005807694979012012 \t\n",
      "Epoch 39654 \t\t Training Loss: 0.0005807693814858794 \t\n",
      "Epoch 39655 \t\t Training Loss: 0.0005807693814858794 \t\n",
      "Epoch 39656 \t\t Training Loss: 0.0005807693814858794 \t\n",
      "Epoch 39657 \t\t Training Loss: 0.0005807693814858794 \t\n",
      "Epoch 39658 \t\t Training Loss: 0.0005807693814858794 \t\n",
      "Epoch 39659 \t\t Training Loss: 0.0005807693814858794 \t\n",
      "Epoch 39660 \t\t Training Loss: 0.0005807694979012012 \t\n",
      "Epoch 39661 \t\t Training Loss: 0.0005807693814858794 \t\n",
      "Epoch 39662 \t\t Training Loss: 0.0005807693814858794 \t\n",
      "Epoch 39663 \t\t Training Loss: 0.0005807693814858794 \t\n",
      "Epoch 39664 \t\t Training Loss: 0.0005807693814858794 \t\n",
      "Epoch 39665 \t\t Training Loss: 0.0005807693814858794 \t\n",
      "Epoch 39666 \t\t Training Loss: 0.0005807693814858794 \t\n",
      "Epoch 39667 \t\t Training Loss: 0.0005807693232782185 \t\n",
      "Epoch 39668 \t\t Training Loss: 0.0005807693814858794 \t\n",
      "Epoch 39669 \t\t Training Loss: 0.0005807693814858794 \t\n",
      "Epoch 39670 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39671 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39672 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39673 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39674 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39675 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39676 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39677 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39678 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39679 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39680 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39681 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39682 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39683 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39684 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39685 \t\t Training Loss: 0.0005807693814858794 \t\n",
      "Epoch 39686 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39687 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39688 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39689 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39690 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39691 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39692 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39693 \t\t Training Loss: 0.0005807692650705576 \t\n",
      "Epoch 39694 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39695 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39696 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39697 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39698 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39699 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39700 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39701 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39702 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39703 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39704 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39705 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39706 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39707 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39708 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39709 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39710 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39711 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39712 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39713 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39714 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39715 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39716 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39717 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39718 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39719 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39720 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39721 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39722 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39723 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39724 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39725 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39726 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39727 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39728 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39729 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39730 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39731 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39732 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39733 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39734 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39735 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39736 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39737 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39738 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39739 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39740 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39741 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39742 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39743 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39744 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39745 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39746 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39747 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39748 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39749 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39750 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39751 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39752 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39753 \t\t Training Loss: 0.0005807692068628967 \t\n",
      "Epoch 39754 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39755 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39756 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39757 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39758 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39759 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39760 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39761 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39762 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39763 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39764 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39765 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39766 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39767 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39768 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39769 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39770 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39771 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39772 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39773 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39774 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39775 \t\t Training Loss: 0.0005807690904475749 \t\n",
      "Epoch 39776 \t\t Training Loss: 0.000580768974032253 \t\n",
      "Epoch 39777 \t\t Training Loss: 0.000580768974032253 \t\n",
      "Epoch 39778 \t\t Training Loss: 0.000580768974032253 \t\n",
      "Epoch 39779 \t\t Training Loss: 0.000580768974032253 \t\n",
      "Epoch 39780 \t\t Training Loss: 0.000580768974032253 \t\n",
      "Epoch 39781 \t\t Training Loss: 0.000580768974032253 \t\n",
      "Epoch 39782 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39783 \t\t Training Loss: 0.000580768974032253 \t\n",
      "Epoch 39784 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39785 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39786 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39787 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39788 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39789 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39790 \t\t Training Loss: 0.0005807689158245921 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39791 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39792 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39793 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39794 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39795 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39796 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39797 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39798 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39799 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39800 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39801 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39802 \t\t Training Loss: 0.0005807687994092703 \t\n",
      "Epoch 39803 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39804 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39805 \t\t Training Loss: 0.0005807687994092703 \t\n",
      "Epoch 39806 \t\t Training Loss: 0.0005807687994092703 \t\n",
      "Epoch 39807 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39808 \t\t Training Loss: 0.0005807687994092703 \t\n",
      "Epoch 39809 \t\t Training Loss: 0.0005807687994092703 \t\n",
      "Epoch 39810 \t\t Training Loss: 0.0005807687994092703 \t\n",
      "Epoch 39811 \t\t Training Loss: 0.0005807687994092703 \t\n",
      "Epoch 39812 \t\t Training Loss: 0.0005807689158245921 \t\n",
      "Epoch 39813 \t\t Training Loss: 0.0005807687994092703 \t\n",
      "Epoch 39814 \t\t Training Loss: 0.0005807687994092703 \t\n",
      "Epoch 39815 \t\t Training Loss: 0.0005807687994092703 \t\n",
      "Epoch 39816 \t\t Training Loss: 0.0005807687994092703 \t\n",
      "Epoch 39817 \t\t Training Loss: 0.0005807687994092703 \t\n",
      "Epoch 39818 \t\t Training Loss: 0.0005807687994092703 \t\n",
      "Epoch 39819 \t\t Training Loss: 0.0005807687994092703 \t\n",
      "Epoch 39820 \t\t Training Loss: 0.0005807687994092703 \t\n",
      "Epoch 39821 \t\t Training Loss: 0.0005807686829939485 \t\n",
      "Epoch 39822 \t\t Training Loss: 0.0005807686829939485 \t\n",
      "Epoch 39823 \t\t Training Loss: 0.0005807686829939485 \t\n",
      "Epoch 39824 \t\t Training Loss: 0.0005807686829939485 \t\n",
      "Epoch 39825 \t\t Training Loss: 0.0005807686829939485 \t\n",
      "Epoch 39826 \t\t Training Loss: 0.0005807686829939485 \t\n",
      "Epoch 39827 \t\t Training Loss: 0.0005807686247862875 \t\n",
      "Epoch 39828 \t\t Training Loss: 0.0005807686247862875 \t\n",
      "Epoch 39829 \t\t Training Loss: 0.0005807686247862875 \t\n",
      "Epoch 39830 \t\t Training Loss: 0.0005807686247862875 \t\n",
      "Epoch 39831 \t\t Training Loss: 0.0005807686247862875 \t\n",
      "Epoch 39832 \t\t Training Loss: 0.0005807686247862875 \t\n",
      "Epoch 39833 \t\t Training Loss: 0.0005807686247862875 \t\n",
      "Epoch 39834 \t\t Training Loss: 0.0005807686247862875 \t\n",
      "Epoch 39835 \t\t Training Loss: 0.0005807686247862875 \t\n",
      "Epoch 39836 \t\t Training Loss: 0.0005807686247862875 \t\n",
      "Epoch 39837 \t\t Training Loss: 0.0005807685083709657 \t\n",
      "Epoch 39838 \t\t Training Loss: 0.0005807686247862875 \t\n",
      "Epoch 39839 \t\t Training Loss: 0.0005807685083709657 \t\n",
      "Epoch 39840 \t\t Training Loss: 0.0005807685083709657 \t\n",
      "Epoch 39841 \t\t Training Loss: 0.0005807685083709657 \t\n",
      "Epoch 39842 \t\t Training Loss: 0.0005807686247862875 \t\n",
      "Epoch 39843 \t\t Training Loss: 0.0005807685083709657 \t\n",
      "Epoch 39844 \t\t Training Loss: 0.0005807685083709657 \t\n",
      "Epoch 39845 \t\t Training Loss: 0.0005807685083709657 \t\n",
      "Epoch 39846 \t\t Training Loss: 0.0005807685083709657 \t\n",
      "Epoch 39847 \t\t Training Loss: 0.0005807685083709657 \t\n",
      "Epoch 39848 \t\t Training Loss: 0.0005807685083709657 \t\n",
      "Epoch 39849 \t\t Training Loss: 0.0005807685083709657 \t\n",
      "Epoch 39850 \t\t Training Loss: 0.0005807685083709657 \t\n",
      "Epoch 39851 \t\t Training Loss: 0.0005807685083709657 \t\n",
      "Epoch 39852 \t\t Training Loss: 0.0005807683919556439 \t\n",
      "Epoch 39853 \t\t Training Loss: 0.0005807683919556439 \t\n",
      "Epoch 39854 \t\t Training Loss: 0.0005807683919556439 \t\n",
      "Epoch 39855 \t\t Training Loss: 0.0005807683919556439 \t\n",
      "Epoch 39856 \t\t Training Loss: 0.0005807682755403221 \t\n",
      "Epoch 39857 \t\t Training Loss: 0.0005807682755403221 \t\n",
      "Epoch 39858 \t\t Training Loss: 0.0005807682755403221 \t\n",
      "Epoch 39859 \t\t Training Loss: 0.0005807682755403221 \t\n",
      "Epoch 39860 \t\t Training Loss: 0.0005807682755403221 \t\n",
      "Epoch 39861 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39862 \t\t Training Loss: 0.0005807682755403221 \t\n",
      "Epoch 39863 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39864 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39865 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39866 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39867 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39868 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39869 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39870 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39871 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39872 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39873 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39874 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39875 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39876 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39877 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39878 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39879 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39880 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39881 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39882 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39883 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39884 \t\t Training Loss: 0.0005807682173326612 \t\n",
      "Epoch 39885 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39886 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39887 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39888 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39889 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39890 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39891 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39892 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39893 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39894 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39895 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39896 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39897 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39898 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39899 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39900 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39901 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39902 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39903 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39904 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39905 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39906 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39907 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39908 \t\t Training Loss: 0.0005807679845020175 \t\n",
      "Epoch 39909 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39910 \t\t Training Loss: 0.0005807681009173393 \t\n",
      "Epoch 39911 \t\t Training Loss: 0.0005807679845020175 \t\n",
      "Epoch 39912 \t\t Training Loss: 0.0005807679845020175 \t\n",
      "Epoch 39913 \t\t Training Loss: 0.0005807679845020175 \t\n",
      "Epoch 39914 \t\t Training Loss: 0.0005807679845020175 \t\n",
      "Epoch 39915 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39916 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39917 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39918 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39919 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39920 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39921 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39922 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39923 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39924 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39925 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39926 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39927 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39928 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39929 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39930 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39931 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39932 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39933 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39934 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39935 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39936 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39937 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39938 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39939 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39940 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39941 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39942 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39943 \t\t Training Loss: 0.0005807679262943566 \t\n",
      "Epoch 39944 \t\t Training Loss: 0.0005807678098790348 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39945 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39946 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39947 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39948 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39949 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39950 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39951 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39952 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39953 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39954 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39955 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39956 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39957 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39958 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39959 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39960 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39961 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39962 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39963 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39964 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39965 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39966 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39967 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39968 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39969 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39970 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39971 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39972 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39973 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39974 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39975 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39976 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39977 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39978 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39979 \t\t Training Loss: 0.0005807678098790348 \t\n",
      "Epoch 39980 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39981 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39982 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39983 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39984 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39985 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39986 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39987 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39988 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39989 \t\t Training Loss: 0.0005807676934637129 \t\n",
      "Epoch 39990 \t\t Training Loss: 0.000580767635256052 \t\n",
      "Epoch 39991 \t\t Training Loss: 0.000580767635256052 \t\n",
      "Epoch 39992 \t\t Training Loss: 0.000580767635256052 \t\n",
      "Epoch 39993 \t\t Training Loss: 0.000580767635256052 \t\n",
      "Epoch 39994 \t\t Training Loss: 0.000580767635256052 \t\n",
      "Epoch 39995 \t\t Training Loss: 0.000580767635256052 \t\n",
      "Epoch 39996 \t\t Training Loss: 0.000580767635256052 \t\n",
      "Epoch 39997 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 39998 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 39999 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40000 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40001 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40002 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40003 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40004 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40005 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40006 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40007 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40008 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40009 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40010 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40011 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40012 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40013 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40014 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40015 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40016 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40017 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40018 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40019 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40020 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40021 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40022 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40023 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40024 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40025 \t\t Training Loss: 0.0005807675188407302 \t\n",
      "Epoch 40026 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40027 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40028 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40029 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40030 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40031 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40032 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40033 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40034 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40035 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40036 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40037 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40038 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40039 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40040 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40041 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40042 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40043 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40044 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40045 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40046 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40047 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40048 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40049 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40050 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40051 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40052 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40053 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40054 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40055 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40056 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40057 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40058 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40059 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40060 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40061 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40062 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40063 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40064 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40065 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40066 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40067 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40068 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40069 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40070 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40071 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40072 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40073 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40074 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40075 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40076 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40077 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40078 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40079 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40080 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40081 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40082 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40083 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40084 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40085 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40086 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40087 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40088 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40089 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40090 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40091 \t\t Training Loss: 0.0005807674024254084 \t\n",
      "Epoch 40092 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40093 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40094 \t\t Training Loss: 0.0005807672278024256 \t\n",
      "Epoch 40095 \t\t Training Loss: 0.0005807673442177474 \t\n",
      "Epoch 40096 \t\t Training Loss: 0.0005807672278024256 \t\n",
      "Epoch 40097 \t\t Training Loss: 0.0005807672278024256 \t\n",
      "Epoch 40098 \t\t Training Loss: 0.0005807672278024256 \t\n",
      "Epoch 40099 \t\t Training Loss: 0.0005807672278024256 \t\n",
      "Epoch 40100 \t\t Training Loss: 0.0005807672278024256 \t\n",
      "Epoch 40101 \t\t Training Loss: 0.0005807672278024256 \t\n",
      "Epoch 40102 \t\t Training Loss: 0.0005807672278024256 \t\n",
      "Epoch 40103 \t\t Training Loss: 0.0005807672278024256 \t\n",
      "Epoch 40104 \t\t Training Loss: 0.0005807672278024256 \t\n",
      "Epoch 40105 \t\t Training Loss: 0.0005807672278024256 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40106 \t\t Training Loss: 0.0005807672278024256 \t\n",
      "Epoch 40107 \t\t Training Loss: 0.0005807672278024256 \t\n",
      "Epoch 40108 \t\t Training Loss: 0.0005807671113871038 \t\n",
      "Epoch 40109 \t\t Training Loss: 0.0005807671113871038 \t\n",
      "Epoch 40110 \t\t Training Loss: 0.0005807671113871038 \t\n",
      "Epoch 40111 \t\t Training Loss: 0.0005807670531794429 \t\n",
      "Epoch 40112 \t\t Training Loss: 0.0005807670531794429 \t\n",
      "Epoch 40113 \t\t Training Loss: 0.0005807670531794429 \t\n",
      "Epoch 40114 \t\t Training Loss: 0.0005807670531794429 \t\n",
      "Epoch 40115 \t\t Training Loss: 0.0005807670531794429 \t\n",
      "Epoch 40116 \t\t Training Loss: 0.0005807670531794429 \t\n",
      "Epoch 40117 \t\t Training Loss: 0.0005807670531794429 \t\n",
      "Epoch 40118 \t\t Training Loss: 0.0005807670531794429 \t\n",
      "Epoch 40119 \t\t Training Loss: 0.0005807670531794429 \t\n",
      "Epoch 40120 \t\t Training Loss: 0.0005807671113871038 \t\n",
      "Epoch 40121 \t\t Training Loss: 0.0005807671113871038 \t\n",
      "Epoch 40122 \t\t Training Loss: 0.0005807670531794429 \t\n",
      "Epoch 40123 \t\t Training Loss: 0.0005807670531794429 \t\n",
      "Epoch 40124 \t\t Training Loss: 0.0005807670531794429 \t\n",
      "Epoch 40125 \t\t Training Loss: 0.0005807670531794429 \t\n",
      "Epoch 40126 \t\t Training Loss: 0.0005807670531794429 \t\n",
      "Epoch 40127 \t\t Training Loss: 0.0005807670531794429 \t\n",
      "Epoch 40128 \t\t Training Loss: 0.0005807670531794429 \t\n",
      "Epoch 40129 \t\t Training Loss: 0.000580766994971782 \t\n",
      "Epoch 40130 \t\t Training Loss: 0.000580766994971782 \t\n",
      "Epoch 40131 \t\t Training Loss: 0.000580766994971782 \t\n",
      "Epoch 40132 \t\t Training Loss: 0.000580766994971782 \t\n",
      "Epoch 40133 \t\t Training Loss: 0.000580766994971782 \t\n",
      "Epoch 40134 \t\t Training Loss: 0.000580766994971782 \t\n",
      "Epoch 40135 \t\t Training Loss: 0.000580766994971782 \t\n",
      "Epoch 40136 \t\t Training Loss: 0.000580766994971782 \t\n",
      "Epoch 40137 \t\t Training Loss: 0.0005807669367641211 \t\n",
      "Epoch 40138 \t\t Training Loss: 0.0005807669367641211 \t\n",
      "Epoch 40139 \t\t Training Loss: 0.0005807669367641211 \t\n",
      "Epoch 40140 \t\t Training Loss: 0.0005807668785564601 \t\n",
      "Epoch 40141 \t\t Training Loss: 0.0005807668785564601 \t\n",
      "Epoch 40142 \t\t Training Loss: 0.0005807668785564601 \t\n",
      "Epoch 40143 \t\t Training Loss: 0.0005807668785564601 \t\n",
      "Epoch 40144 \t\t Training Loss: 0.0005807668785564601 \t\n",
      "Epoch 40145 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40146 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40147 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40148 \t\t Training Loss: 0.0005807668203487992 \t\n",
      "Epoch 40149 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40150 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40151 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40152 \t\t Training Loss: 0.0005807668203487992 \t\n",
      "Epoch 40153 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40154 \t\t Training Loss: 0.0005807668203487992 \t\n",
      "Epoch 40155 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40156 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40157 \t\t Training Loss: 0.0005807668203487992 \t\n",
      "Epoch 40158 \t\t Training Loss: 0.0005807668203487992 \t\n",
      "Epoch 40159 \t\t Training Loss: 0.0005807668203487992 \t\n",
      "Epoch 40160 \t\t Training Loss: 0.0005807668203487992 \t\n",
      "Epoch 40161 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40162 \t\t Training Loss: 0.0005807668203487992 \t\n",
      "Epoch 40163 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40164 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40165 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40166 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40167 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40168 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40169 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40170 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40171 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40172 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40173 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40174 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40175 \t\t Training Loss: 0.0005807667039334774 \t\n",
      "Epoch 40176 \t\t Training Loss: 0.0005807665875181556 \t\n",
      "Epoch 40177 \t\t Training Loss: 0.0005807666457258165 \t\n",
      "Epoch 40178 \t\t Training Loss: 0.0005807665875181556 \t\n",
      "Epoch 40179 \t\t Training Loss: 0.0005807666457258165 \t\n",
      "Epoch 40180 \t\t Training Loss: 0.0005807666457258165 \t\n",
      "Epoch 40181 \t\t Training Loss: 0.0005807665875181556 \t\n",
      "Epoch 40182 \t\t Training Loss: 0.0005807665875181556 \t\n",
      "Epoch 40183 \t\t Training Loss: 0.0005807665875181556 \t\n",
      "Epoch 40184 \t\t Training Loss: 0.0005807665875181556 \t\n",
      "Epoch 40185 \t\t Training Loss: 0.0005807666457258165 \t\n",
      "Epoch 40186 \t\t Training Loss: 0.0005807666457258165 \t\n",
      "Epoch 40187 \t\t Training Loss: 0.0005807666457258165 \t\n",
      "Epoch 40188 \t\t Training Loss: 0.0005807666457258165 \t\n",
      "Epoch 40189 \t\t Training Loss: 0.0005807666457258165 \t\n",
      "Epoch 40190 \t\t Training Loss: 0.0005807666457258165 \t\n",
      "Epoch 40191 \t\t Training Loss: 0.0005807665875181556 \t\n",
      "Epoch 40192 \t\t Training Loss: 0.0005807665875181556 \t\n",
      "Epoch 40193 \t\t Training Loss: 0.0005807665875181556 \t\n",
      "Epoch 40194 \t\t Training Loss: 0.0005807665875181556 \t\n",
      "Epoch 40195 \t\t Training Loss: 0.0005807665875181556 \t\n",
      "Epoch 40196 \t\t Training Loss: 0.0005807665293104947 \t\n",
      "Epoch 40197 \t\t Training Loss: 0.0005807665293104947 \t\n",
      "Epoch 40198 \t\t Training Loss: 0.0005807665293104947 \t\n",
      "Epoch 40199 \t\t Training Loss: 0.0005807665293104947 \t\n",
      "Epoch 40200 \t\t Training Loss: 0.0005807665293104947 \t\n",
      "Epoch 40201 \t\t Training Loss: 0.0005807665293104947 \t\n",
      "Epoch 40202 \t\t Training Loss: 0.0005807665293104947 \t\n",
      "Epoch 40203 \t\t Training Loss: 0.0005807665293104947 \t\n",
      "Epoch 40204 \t\t Training Loss: 0.0005807665293104947 \t\n",
      "Epoch 40205 \t\t Training Loss: 0.0005807665293104947 \t\n",
      "Epoch 40206 \t\t Training Loss: 0.0005807665293104947 \t\n",
      "Epoch 40207 \t\t Training Loss: 0.0005807665293104947 \t\n",
      "Epoch 40208 \t\t Training Loss: 0.0005807665293104947 \t\n",
      "Epoch 40209 \t\t Training Loss: 0.0005807664128951728 \t\n",
      "Epoch 40210 \t\t Training Loss: 0.0005807665293104947 \t\n",
      "Epoch 40211 \t\t Training Loss: 0.0005807665293104947 \t\n",
      "Epoch 40212 \t\t Training Loss: 0.0005807664128951728 \t\n",
      "Epoch 40213 \t\t Training Loss: 0.0005807664128951728 \t\n",
      "Epoch 40214 \t\t Training Loss: 0.0005807664128951728 \t\n",
      "Epoch 40215 \t\t Training Loss: 0.0005807664128951728 \t\n",
      "Epoch 40216 \t\t Training Loss: 0.0005807664128951728 \t\n",
      "Epoch 40217 \t\t Training Loss: 0.0005807664128951728 \t\n",
      "Epoch 40218 \t\t Training Loss: 0.0005807664128951728 \t\n",
      "Epoch 40219 \t\t Training Loss: 0.0005807664128951728 \t\n",
      "Epoch 40220 \t\t Training Loss: 0.0005807663546875119 \t\n",
      "Epoch 40221 \t\t Training Loss: 0.0005807663546875119 \t\n",
      "Epoch 40222 \t\t Training Loss: 0.0005807663546875119 \t\n",
      "Epoch 40223 \t\t Training Loss: 0.0005807663546875119 \t\n",
      "Epoch 40224 \t\t Training Loss: 0.0005807663546875119 \t\n",
      "Epoch 40225 \t\t Training Loss: 0.0005807662382721901 \t\n",
      "Epoch 40226 \t\t Training Loss: 0.0005807662382721901 \t\n",
      "Epoch 40227 \t\t Training Loss: 0.0005807662382721901 \t\n",
      "Epoch 40228 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40229 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40230 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40231 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40232 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40233 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40234 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40235 \t\t Training Loss: 0.000580766296479851 \t\n",
      "Epoch 40236 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40237 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40238 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40239 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40240 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40241 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40242 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40243 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40244 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40245 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40246 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40247 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40248 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40249 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40250 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40251 \t\t Training Loss: 0.0005807661800645292 \t\n",
      "Epoch 40252 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40253 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40254 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40255 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40256 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40257 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40258 \t\t Training Loss: 0.0005807661218568683 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40259 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40260 \t\t Training Loss: 0.0005807660636492074 \t\n",
      "Epoch 40261 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40262 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40263 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40264 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40265 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40266 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40267 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40268 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40269 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40270 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40271 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40272 \t\t Training Loss: 0.0005807660636492074 \t\n",
      "Epoch 40273 \t\t Training Loss: 0.0005807660636492074 \t\n",
      "Epoch 40274 \t\t Training Loss: 0.0005807660636492074 \t\n",
      "Epoch 40275 \t\t Training Loss: 0.0005807661218568683 \t\n",
      "Epoch 40276 \t\t Training Loss: 0.0005807660636492074 \t\n",
      "Epoch 40277 \t\t Training Loss: 0.0005807660636492074 \t\n",
      "Epoch 40278 \t\t Training Loss: 0.0005807660636492074 \t\n",
      "Epoch 40279 \t\t Training Loss: 0.0005807660636492074 \t\n",
      "Epoch 40280 \t\t Training Loss: 0.0005807660636492074 \t\n",
      "Epoch 40281 \t\t Training Loss: 0.0005807660636492074 \t\n",
      "Epoch 40282 \t\t Training Loss: 0.0005807660636492074 \t\n",
      "Epoch 40283 \t\t Training Loss: 0.0005807660636492074 \t\n",
      "Epoch 40284 \t\t Training Loss: 0.0005807660636492074 \t\n",
      "Epoch 40285 \t\t Training Loss: 0.0005807660636492074 \t\n",
      "Epoch 40286 \t\t Training Loss: 0.0005807660636492074 \t\n",
      "Epoch 40287 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40288 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40289 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40290 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40291 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40292 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40293 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40294 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40295 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40296 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40297 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40298 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40299 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40300 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40301 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40302 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40303 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40304 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40305 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40306 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40307 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40308 \t\t Training Loss: 0.0005807660054415464 \t\n",
      "Epoch 40309 \t\t Training Loss: 0.0005807660054415464 \t\n",
      "Epoch 40310 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40311 \t\t Training Loss: 0.0005807660054415464 \t\n",
      "Epoch 40312 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40313 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40314 \t\t Training Loss: 0.0005807660054415464 \t\n",
      "Epoch 40315 \t\t Training Loss: 0.0005807660054415464 \t\n",
      "Epoch 40316 \t\t Training Loss: 0.0005807660054415464 \t\n",
      "Epoch 40317 \t\t Training Loss: 0.0005807660054415464 \t\n",
      "Epoch 40318 \t\t Training Loss: 0.0005807660054415464 \t\n",
      "Epoch 40319 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40320 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40321 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40322 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40323 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40324 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40325 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40326 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40327 \t\t Training Loss: 0.0005807659472338855 \t\n",
      "Epoch 40328 \t\t Training Loss: 0.0005807658890262246 \t\n",
      "Epoch 40329 \t\t Training Loss: 0.0005807658308185637 \t\n",
      "Epoch 40330 \t\t Training Loss: 0.0005807658308185637 \t\n",
      "Epoch 40331 \t\t Training Loss: 0.0005807658308185637 \t\n",
      "Epoch 40332 \t\t Training Loss: 0.0005807658308185637 \t\n",
      "Epoch 40333 \t\t Training Loss: 0.0005807658308185637 \t\n",
      "Epoch 40334 \t\t Training Loss: 0.0005807658308185637 \t\n",
      "Epoch 40335 \t\t Training Loss: 0.0005807658308185637 \t\n",
      "Epoch 40336 \t\t Training Loss: 0.0005807658308185637 \t\n",
      "Epoch 40337 \t\t Training Loss: 0.0005807658308185637 \t\n",
      "Epoch 40338 \t\t Training Loss: 0.0005807658308185637 \t\n",
      "Epoch 40339 \t\t Training Loss: 0.0005807658308185637 \t\n",
      "Epoch 40340 \t\t Training Loss: 0.0005807658308185637 \t\n",
      "Epoch 40341 \t\t Training Loss: 0.0005807658308185637 \t\n",
      "Epoch 40342 \t\t Training Loss: 0.0005807657144032419 \t\n",
      "Epoch 40343 \t\t Training Loss: 0.0005807658308185637 \t\n",
      "Epoch 40344 \t\t Training Loss: 0.0005807657144032419 \t\n",
      "Epoch 40345 \t\t Training Loss: 0.0005807657144032419 \t\n",
      "Epoch 40346 \t\t Training Loss: 0.0005807657144032419 \t\n",
      "Epoch 40347 \t\t Training Loss: 0.0005807657144032419 \t\n",
      "Epoch 40348 \t\t Training Loss: 0.0005807657144032419 \t\n",
      "Epoch 40349 \t\t Training Loss: 0.0005807657144032419 \t\n",
      "Epoch 40350 \t\t Training Loss: 0.0005807657144032419 \t\n",
      "Epoch 40351 \t\t Training Loss: 0.0005807657144032419 \t\n",
      "Epoch 40352 \t\t Training Loss: 0.0005807657144032419 \t\n",
      "Epoch 40353 \t\t Training Loss: 0.0005807657144032419 \t\n",
      "Epoch 40354 \t\t Training Loss: 0.0005807657144032419 \t\n",
      "Epoch 40355 \t\t Training Loss: 0.000580765656195581 \t\n",
      "Epoch 40356 \t\t Training Loss: 0.000580765656195581 \t\n",
      "Epoch 40357 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40358 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40359 \t\t Training Loss: 0.000580765656195581 \t\n",
      "Epoch 40360 \t\t Training Loss: 0.0005807657144032419 \t\n",
      "Epoch 40361 \t\t Training Loss: 0.0005807657144032419 \t\n",
      "Epoch 40362 \t\t Training Loss: 0.0005807657144032419 \t\n",
      "Epoch 40363 \t\t Training Loss: 0.0005807657144032419 \t\n",
      "Epoch 40364 \t\t Training Loss: 0.000580765656195581 \t\n",
      "Epoch 40365 \t\t Training Loss: 0.000580765656195581 \t\n",
      "Epoch 40366 \t\t Training Loss: 0.000580765656195581 \t\n",
      "Epoch 40367 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40368 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40369 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40370 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40371 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40372 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40373 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40374 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40375 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40376 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40377 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40378 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40379 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40380 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40381 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40382 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40383 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40384 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40385 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40386 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40387 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40388 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40389 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40390 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40391 \t\t Training Loss: 0.00058076559798792 \t\n",
      "Epoch 40392 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40393 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40394 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40395 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40396 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40397 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40398 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40399 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40400 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40401 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40402 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40403 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40404 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40405 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40406 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40407 \t\t Training Loss: 0.0005807654815725982 \t\n",
      "Epoch 40408 \t\t Training Loss: 0.0005807654815725982 \t\n",
      "Epoch 40409 \t\t Training Loss: 0.0005807654815725982 \t\n",
      "Epoch 40410 \t\t Training Loss: 0.0005807654815725982 \t\n",
      "Epoch 40411 \t\t Training Loss: 0.0005807654815725982 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40412 \t\t Training Loss: 0.0005807654815725982 \t\n",
      "Epoch 40413 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40414 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40415 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40416 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40417 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40418 \t\t Training Loss: 0.0005807654815725982 \t\n",
      "Epoch 40419 \t\t Training Loss: 0.0005807654815725982 \t\n",
      "Epoch 40420 \t\t Training Loss: 0.0005807654815725982 \t\n",
      "Epoch 40421 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40422 \t\t Training Loss: 0.0005807655397802591 \t\n",
      "Epoch 40423 \t\t Training Loss: 0.0005807654815725982 \t\n",
      "Epoch 40424 \t\t Training Loss: 0.0005807654815725982 \t\n",
      "Epoch 40425 \t\t Training Loss: 0.0005807654815725982 \t\n",
      "Epoch 40426 \t\t Training Loss: 0.0005807654815725982 \t\n",
      "Epoch 40427 \t\t Training Loss: 0.0005807654233649373 \t\n",
      "Epoch 40428 \t\t Training Loss: 0.0005807654233649373 \t\n",
      "Epoch 40429 \t\t Training Loss: 0.0005807654233649373 \t\n",
      "Epoch 40430 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40431 \t\t Training Loss: 0.0005807654233649373 \t\n",
      "Epoch 40432 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40433 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40434 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40435 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40436 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40437 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40438 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40439 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40440 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40441 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40442 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40443 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40444 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40445 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40446 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40447 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40448 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40449 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40450 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40451 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40452 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40453 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40454 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40455 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40456 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40457 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40458 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40459 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40460 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40461 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40462 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40463 \t\t Training Loss: 0.0005807653069496155 \t\n",
      "Epoch 40464 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40465 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40466 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40467 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40468 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40469 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40470 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40471 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40472 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40473 \t\t Training Loss: 0.0005807651323266327 \t\n",
      "Epoch 40474 \t\t Training Loss: 0.0005807651323266327 \t\n",
      "Epoch 40475 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40476 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40477 \t\t Training Loss: 0.0005807651323266327 \t\n",
      "Epoch 40478 \t\t Training Loss: 0.0005807651323266327 \t\n",
      "Epoch 40479 \t\t Training Loss: 0.0005807651323266327 \t\n",
      "Epoch 40480 \t\t Training Loss: 0.0005807651323266327 \t\n",
      "Epoch 40481 \t\t Training Loss: 0.0005807651323266327 \t\n",
      "Epoch 40482 \t\t Training Loss: 0.0005807651323266327 \t\n",
      "Epoch 40483 \t\t Training Loss: 0.0005807651323266327 \t\n",
      "Epoch 40484 \t\t Training Loss: 0.0005807651323266327 \t\n",
      "Epoch 40485 \t\t Training Loss: 0.0005807651323266327 \t\n",
      "Epoch 40486 \t\t Training Loss: 0.0005807652487419546 \t\n",
      "Epoch 40487 \t\t Training Loss: 0.0005807651323266327 \t\n",
      "Epoch 40488 \t\t Training Loss: 0.0005807651323266327 \t\n",
      "Epoch 40489 \t\t Training Loss: 0.0005807650741189718 \t\n",
      "Epoch 40490 \t\t Training Loss: 0.0005807651323266327 \t\n",
      "Epoch 40491 \t\t Training Loss: 0.0005807651323266327 \t\n",
      "Epoch 40492 \t\t Training Loss: 0.0005807650741189718 \t\n",
      "Epoch 40493 \t\t Training Loss: 0.0005807650741189718 \t\n",
      "Epoch 40494 \t\t Training Loss: 0.0005807650741189718 \t\n",
      "Epoch 40495 \t\t Training Loss: 0.0005807650741189718 \t\n",
      "Epoch 40496 \t\t Training Loss: 0.0005807650159113109 \t\n",
      "Epoch 40497 \t\t Training Loss: 0.0005807650159113109 \t\n",
      "Epoch 40498 \t\t Training Loss: 0.0005807650159113109 \t\n",
      "Epoch 40499 \t\t Training Loss: 0.0005807650159113109 \t\n",
      "Epoch 40500 \t\t Training Loss: 0.0005807650159113109 \t\n",
      "Epoch 40501 \t\t Training Loss: 0.0005807650159113109 \t\n",
      "Epoch 40502 \t\t Training Loss: 0.0005807650741189718 \t\n",
      "Epoch 40503 \t\t Training Loss: 0.0005807650741189718 \t\n",
      "Epoch 40504 \t\t Training Loss: 0.0005807651323266327 \t\n",
      "Epoch 40505 \t\t Training Loss: 0.0005807650741189718 \t\n",
      "Epoch 40506 \t\t Training Loss: 0.0005807650741189718 \t\n",
      "Epoch 40507 \t\t Training Loss: 0.0005807650741189718 \t\n",
      "Epoch 40508 \t\t Training Loss: 0.0005807650741189718 \t\n",
      "Epoch 40509 \t\t Training Loss: 0.0005807650741189718 \t\n",
      "Epoch 40510 \t\t Training Loss: 0.0005807650741189718 \t\n",
      "Epoch 40511 \t\t Training Loss: 0.0005807650741189718 \t\n",
      "Epoch 40512 \t\t Training Loss: 0.0005807650159113109 \t\n",
      "Epoch 40513 \t\t Training Loss: 0.0005807650159113109 \t\n",
      "Epoch 40514 \t\t Training Loss: 0.0005807650159113109 \t\n",
      "Epoch 40515 \t\t Training Loss: 0.0005807650159113109 \t\n",
      "Epoch 40516 \t\t Training Loss: 0.0005807650159113109 \t\n",
      "Epoch 40517 \t\t Training Loss: 0.0005807650159113109 \t\n",
      "Epoch 40518 \t\t Training Loss: 0.0005807650159113109 \t\n",
      "Epoch 40519 \t\t Training Loss: 0.00058076495770365 \t\n",
      "Epoch 40520 \t\t Training Loss: 0.00058076495770365 \t\n",
      "Epoch 40521 \t\t Training Loss: 0.00058076495770365 \t\n",
      "Epoch 40522 \t\t Training Loss: 0.00058076495770365 \t\n",
      "Epoch 40523 \t\t Training Loss: 0.00058076495770365 \t\n",
      "Epoch 40524 \t\t Training Loss: 0.0005807648994959891 \t\n",
      "Epoch 40525 \t\t Training Loss: 0.00058076495770365 \t\n",
      "Epoch 40526 \t\t Training Loss: 0.0005807648994959891 \t\n",
      "Epoch 40527 \t\t Training Loss: 0.0005807648412883282 \t\n",
      "Epoch 40528 \t\t Training Loss: 0.0005807648412883282 \t\n",
      "Epoch 40529 \t\t Training Loss: 0.0005807648412883282 \t\n",
      "Epoch 40530 \t\t Training Loss: 0.0005807648412883282 \t\n",
      "Epoch 40531 \t\t Training Loss: 0.0005807648412883282 \t\n",
      "Epoch 40532 \t\t Training Loss: 0.0005807648412883282 \t\n",
      "Epoch 40533 \t\t Training Loss: 0.0005807648412883282 \t\n",
      "Epoch 40534 \t\t Training Loss: 0.0005807648412883282 \t\n",
      "Epoch 40535 \t\t Training Loss: 0.0005807648412883282 \t\n",
      "Epoch 40536 \t\t Training Loss: 0.0005807648412883282 \t\n",
      "Epoch 40537 \t\t Training Loss: 0.0005807648412883282 \t\n",
      "Epoch 40538 \t\t Training Loss: 0.0005807648412883282 \t\n",
      "Epoch 40539 \t\t Training Loss: 0.0005807648412883282 \t\n",
      "Epoch 40540 \t\t Training Loss: 0.0005807647830806673 \t\n",
      "Epoch 40541 \t\t Training Loss: 0.0005807648412883282 \t\n",
      "Epoch 40542 \t\t Training Loss: 0.0005807647830806673 \t\n",
      "Epoch 40543 \t\t Training Loss: 0.0005807647830806673 \t\n",
      "Epoch 40544 \t\t Training Loss: 0.0005807648412883282 \t\n",
      "Epoch 40545 \t\t Training Loss: 0.0005807647830806673 \t\n",
      "Epoch 40546 \t\t Training Loss: 0.0005807647830806673 \t\n",
      "Epoch 40547 \t\t Training Loss: 0.0005807647830806673 \t\n",
      "Epoch 40548 \t\t Training Loss: 0.0005807647830806673 \t\n",
      "Epoch 40549 \t\t Training Loss: 0.0005807647830806673 \t\n",
      "Epoch 40550 \t\t Training Loss: 0.0005807647830806673 \t\n",
      "Epoch 40551 \t\t Training Loss: 0.0005807647830806673 \t\n",
      "Epoch 40552 \t\t Training Loss: 0.0005807647830806673 \t\n",
      "Epoch 40553 \t\t Training Loss: 0.0005807647248730063 \t\n",
      "Epoch 40554 \t\t Training Loss: 0.0005807647830806673 \t\n",
      "Epoch 40555 \t\t Training Loss: 0.0005807647248730063 \t\n",
      "Epoch 40556 \t\t Training Loss: 0.0005807647248730063 \t\n",
      "Epoch 40557 \t\t Training Loss: 0.0005807647248730063 \t\n",
      "Epoch 40558 \t\t Training Loss: 0.0005807647248730063 \t\n",
      "Epoch 40559 \t\t Training Loss: 0.0005807647248730063 \t\n",
      "Epoch 40560 \t\t Training Loss: 0.0005807647248730063 \t\n",
      "Epoch 40561 \t\t Training Loss: 0.0005807646666653454 \t\n",
      "Epoch 40562 \t\t Training Loss: 0.0005807646666653454 \t\n",
      "Epoch 40563 \t\t Training Loss: 0.0005807646666653454 \t\n",
      "Epoch 40564 \t\t Training Loss: 0.0005807646666653454 \t\n",
      "Epoch 40565 \t\t Training Loss: 0.0005807646666653454 \t\n",
      "Epoch 40566 \t\t Training Loss: 0.0005807646666653454 \t\n",
      "Epoch 40567 \t\t Training Loss: 0.0005807646666653454 \t\n",
      "Epoch 40568 \t\t Training Loss: 0.0005807646666653454 \t\n",
      "Epoch 40569 \t\t Training Loss: 0.0005807646666653454 \t\n",
      "Epoch 40570 \t\t Training Loss: 0.0005807646666653454 \t\n",
      "Epoch 40571 \t\t Training Loss: 0.0005807645502500236 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40572 \t\t Training Loss: 0.0005807646666653454 \t\n",
      "Epoch 40573 \t\t Training Loss: 0.0005807645502500236 \t\n",
      "Epoch 40574 \t\t Training Loss: 0.0005807645502500236 \t\n",
      "Epoch 40575 \t\t Training Loss: 0.0005807645502500236 \t\n",
      "Epoch 40576 \t\t Training Loss: 0.0005807645502500236 \t\n",
      "Epoch 40577 \t\t Training Loss: 0.0005807645502500236 \t\n",
      "Epoch 40578 \t\t Training Loss: 0.0005807645502500236 \t\n",
      "Epoch 40579 \t\t Training Loss: 0.0005807646084576845 \t\n",
      "Epoch 40580 \t\t Training Loss: 0.0005807645502500236 \t\n",
      "Epoch 40581 \t\t Training Loss: 0.0005807645502500236 \t\n",
      "Epoch 40582 \t\t Training Loss: 0.0005807645502500236 \t\n",
      "Epoch 40583 \t\t Training Loss: 0.0005807645502500236 \t\n",
      "Epoch 40584 \t\t Training Loss: 0.0005807645502500236 \t\n",
      "Epoch 40585 \t\t Training Loss: 0.0005807644920423627 \t\n",
      "Epoch 40586 \t\t Training Loss: 0.0005807644920423627 \t\n",
      "Epoch 40587 \t\t Training Loss: 0.0005807644920423627 \t\n",
      "Epoch 40588 \t\t Training Loss: 0.0005807644920423627 \t\n",
      "Epoch 40589 \t\t Training Loss: 0.0005807644920423627 \t\n",
      "Epoch 40590 \t\t Training Loss: 0.0005807644338347018 \t\n",
      "Epoch 40591 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40592 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40593 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40594 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40595 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40596 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40597 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40598 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40599 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40600 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40601 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40602 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40603 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40604 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40605 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40606 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40607 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40608 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40609 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40610 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40611 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40612 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40613 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40614 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40615 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40616 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40617 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40618 \t\t Training Loss: 0.00058076431741938 \t\n",
      "Epoch 40619 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40620 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40621 \t\t Training Loss: 0.00058076431741938 \t\n",
      "Epoch 40622 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40623 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40624 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40625 \t\t Training Loss: 0.00058076431741938 \t\n",
      "Epoch 40626 \t\t Training Loss: 0.000580764259211719 \t\n",
      "Epoch 40627 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40628 \t\t Training Loss: 0.0005807643756270409 \t\n",
      "Epoch 40629 \t\t Training Loss: 0.000580764259211719 \t\n",
      "Epoch 40630 \t\t Training Loss: 0.00058076431741938 \t\n",
      "Epoch 40631 \t\t Training Loss: 0.00058076431741938 \t\n",
      "Epoch 40632 \t\t Training Loss: 0.000580764259211719 \t\n",
      "Epoch 40633 \t\t Training Loss: 0.000580764259211719 \t\n",
      "Epoch 40634 \t\t Training Loss: 0.000580764259211719 \t\n",
      "Epoch 40635 \t\t Training Loss: 0.000580764259211719 \t\n",
      "Epoch 40636 \t\t Training Loss: 0.000580764259211719 \t\n",
      "Epoch 40637 \t\t Training Loss: 0.000580764259211719 \t\n",
      "Epoch 40638 \t\t Training Loss: 0.000580764259211719 \t\n",
      "Epoch 40639 \t\t Training Loss: 0.000580764259211719 \t\n",
      "Epoch 40640 \t\t Training Loss: 0.0005807642010040581 \t\n",
      "Epoch 40641 \t\t Training Loss: 0.0005807642010040581 \t\n",
      "Epoch 40642 \t\t Training Loss: 0.0005807642010040581 \t\n",
      "Epoch 40643 \t\t Training Loss: 0.0005807642010040581 \t\n",
      "Epoch 40644 \t\t Training Loss: 0.0005807642010040581 \t\n",
      "Epoch 40645 \t\t Training Loss: 0.000580764259211719 \t\n",
      "Epoch 40646 \t\t Training Loss: 0.0005807642010040581 \t\n",
      "Epoch 40647 \t\t Training Loss: 0.0005807642010040581 \t\n",
      "Epoch 40648 \t\t Training Loss: 0.0005807642010040581 \t\n",
      "Epoch 40649 \t\t Training Loss: 0.0005807642010040581 \t\n",
      "Epoch 40650 \t\t Training Loss: 0.0005807642010040581 \t\n",
      "Epoch 40651 \t\t Training Loss: 0.0005807640845887363 \t\n",
      "Epoch 40652 \t\t Training Loss: 0.0005807640845887363 \t\n",
      "Epoch 40653 \t\t Training Loss: 0.0005807642010040581 \t\n",
      "Epoch 40654 \t\t Training Loss: 0.0005807640845887363 \t\n",
      "Epoch 40655 \t\t Training Loss: 0.0005807640845887363 \t\n",
      "Epoch 40656 \t\t Training Loss: 0.0005807640845887363 \t\n",
      "Epoch 40657 \t\t Training Loss: 0.0005807640845887363 \t\n",
      "Epoch 40658 \t\t Training Loss: 0.0005807640845887363 \t\n",
      "Epoch 40659 \t\t Training Loss: 0.0005807640845887363 \t\n",
      "Epoch 40660 \t\t Training Loss: 0.0005807640845887363 \t\n",
      "Epoch 40661 \t\t Training Loss: 0.0005807640845887363 \t\n",
      "Epoch 40662 \t\t Training Loss: 0.0005807640845887363 \t\n",
      "Epoch 40663 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40664 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40665 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40666 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40667 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40668 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40669 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40670 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40671 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40672 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40673 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40674 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40675 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40676 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40677 \t\t Training Loss: 0.0005807640263810754 \t\n",
      "Epoch 40678 \t\t Training Loss: 0.0005807640263810754 \t\n",
      "Epoch 40679 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40680 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40681 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40682 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40683 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40684 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40685 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40686 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40687 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40688 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40689 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40690 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40691 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40692 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40693 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40694 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40695 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40696 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40697 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40698 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40699 \t\t Training Loss: 0.0005807639681734145 \t\n",
      "Epoch 40700 \t\t Training Loss: 0.0005807638517580926 \t\n",
      "Epoch 40701 \t\t Training Loss: 0.0005807638517580926 \t\n",
      "Epoch 40702 \t\t Training Loss: 0.0005807638517580926 \t\n",
      "Epoch 40703 \t\t Training Loss: 0.0005807637935504317 \t\n",
      "Epoch 40704 \t\t Training Loss: 0.0005807637935504317 \t\n",
      "Epoch 40705 \t\t Training Loss: 0.0005807637935504317 \t\n",
      "Epoch 40706 \t\t Training Loss: 0.0005807637935504317 \t\n",
      "Epoch 40707 \t\t Training Loss: 0.0005807637935504317 \t\n",
      "Epoch 40708 \t\t Training Loss: 0.0005807637935504317 \t\n",
      "Epoch 40709 \t\t Training Loss: 0.0005807637935504317 \t\n",
      "Epoch 40710 \t\t Training Loss: 0.0005807637935504317 \t\n",
      "Epoch 40711 \t\t Training Loss: 0.0005807637935504317 \t\n",
      "Epoch 40712 \t\t Training Loss: 0.0005807637935504317 \t\n",
      "Epoch 40713 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40714 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40715 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40716 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40717 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40718 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40719 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40720 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40721 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40722 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40723 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40724 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40725 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40726 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40727 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40728 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40729 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40730 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40731 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40732 \t\t Training Loss: 0.0005807637353427708 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40733 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40734 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40735 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40736 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40737 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40738 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40739 \t\t Training Loss: 0.0005807637353427708 \t\n",
      "Epoch 40740 \t\t Training Loss: 0.000580763618927449 \t\n",
      "Epoch 40741 \t\t Training Loss: 0.000580763618927449 \t\n",
      "Epoch 40742 \t\t Training Loss: 0.000580763618927449 \t\n",
      "Epoch 40743 \t\t Training Loss: 0.000580763618927449 \t\n",
      "Epoch 40744 \t\t Training Loss: 0.000580763618927449 \t\n",
      "Epoch 40745 \t\t Training Loss: 0.000580763618927449 \t\n",
      "Epoch 40746 \t\t Training Loss: 0.0005807635607197881 \t\n",
      "Epoch 40747 \t\t Training Loss: 0.0005807635607197881 \t\n",
      "Epoch 40748 \t\t Training Loss: 0.0005807635607197881 \t\n",
      "Epoch 40749 \t\t Training Loss: 0.0005807635607197881 \t\n",
      "Epoch 40750 \t\t Training Loss: 0.0005807635607197881 \t\n",
      "Epoch 40751 \t\t Training Loss: 0.0005807635607197881 \t\n",
      "Epoch 40752 \t\t Training Loss: 0.0005807635607197881 \t\n",
      "Epoch 40753 \t\t Training Loss: 0.0005807634443044662 \t\n",
      "Epoch 40754 \t\t Training Loss: 0.0005807633860968053 \t\n",
      "Epoch 40755 \t\t Training Loss: 0.0005807633860968053 \t\n",
      "Epoch 40756 \t\t Training Loss: 0.0005807633860968053 \t\n",
      "Epoch 40757 \t\t Training Loss: 0.0005807633860968053 \t\n",
      "Epoch 40758 \t\t Training Loss: 0.0005807633860968053 \t\n",
      "Epoch 40759 \t\t Training Loss: 0.0005807633860968053 \t\n",
      "Epoch 40760 \t\t Training Loss: 0.0005807633278891444 \t\n",
      "Epoch 40761 \t\t Training Loss: 0.0005807633278891444 \t\n",
      "Epoch 40762 \t\t Training Loss: 0.0005807633278891444 \t\n",
      "Epoch 40763 \t\t Training Loss: 0.0005807633278891444 \t\n",
      "Epoch 40764 \t\t Training Loss: 0.0005807633278891444 \t\n",
      "Epoch 40765 \t\t Training Loss: 0.0005807633278891444 \t\n",
      "Epoch 40766 \t\t Training Loss: 0.0005807633278891444 \t\n",
      "Epoch 40767 \t\t Training Loss: 0.0005807633278891444 \t\n",
      "Epoch 40768 \t\t Training Loss: 0.0005807633278891444 \t\n",
      "Epoch 40769 \t\t Training Loss: 0.0005807633278891444 \t\n",
      "Epoch 40770 \t\t Training Loss: 0.0005807633278891444 \t\n",
      "Epoch 40771 \t\t Training Loss: 0.0005807633278891444 \t\n",
      "Epoch 40772 \t\t Training Loss: 0.0005807633278891444 \t\n",
      "Epoch 40773 \t\t Training Loss: 0.0005807633278891444 \t\n",
      "Epoch 40774 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40775 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40776 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40777 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40778 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40779 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40780 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40781 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40782 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40783 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40784 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40785 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40786 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40787 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40788 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40789 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40790 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40791 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40792 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40793 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40794 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40795 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40796 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40797 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40798 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40799 \t\t Training Loss: 0.0005807632696814835 \t\n",
      "Epoch 40800 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40801 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40802 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40803 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40804 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40805 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40806 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40807 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40808 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40809 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40810 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40811 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40812 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40813 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40814 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40815 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40816 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40817 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40818 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40819 \t\t Training Loss: 0.0005807630368508399 \t\n",
      "Epoch 40820 \t\t Training Loss: 0.0005807630368508399 \t\n",
      "Epoch 40821 \t\t Training Loss: 0.0005807630368508399 \t\n",
      "Epoch 40822 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40823 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40824 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40825 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40826 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40827 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40828 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40829 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40830 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40831 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40832 \t\t Training Loss: 0.0005807630368508399 \t\n",
      "Epoch 40833 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40834 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40835 \t\t Training Loss: 0.0005807630368508399 \t\n",
      "Epoch 40836 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40837 \t\t Training Loss: 0.0005807630368508399 \t\n",
      "Epoch 40838 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40839 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40840 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40841 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40842 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40843 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40844 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40845 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40846 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40847 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40848 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40849 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40850 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40851 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40852 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40853 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40854 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40855 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40856 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40857 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40858 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40859 \t\t Training Loss: 0.0005807631532661617 \t\n",
      "Epoch 40860 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40861 \t\t Training Loss: 0.0005807630950585008 \t\n",
      "Epoch 40862 \t\t Training Loss: 0.0005807630368508399 \t\n",
      "Epoch 40863 \t\t Training Loss: 0.0005807630368508399 \t\n",
      "Epoch 40864 \t\t Training Loss: 0.0005807630368508399 \t\n",
      "Epoch 40865 \t\t Training Loss: 0.0005807630368508399 \t\n",
      "Epoch 40866 \t\t Training Loss: 0.0005807630368508399 \t\n",
      "Epoch 40867 \t\t Training Loss: 0.0005807630368508399 \t\n",
      "Epoch 40868 \t\t Training Loss: 0.0005807630368508399 \t\n",
      "Epoch 40869 \t\t Training Loss: 0.0005807630368508399 \t\n",
      "Epoch 40870 \t\t Training Loss: 0.0005807629786431789 \t\n",
      "Epoch 40871 \t\t Training Loss: 0.0005807629786431789 \t\n",
      "Epoch 40872 \t\t Training Loss: 0.0005807629786431789 \t\n",
      "Epoch 40873 \t\t Training Loss: 0.0005807629786431789 \t\n",
      "Epoch 40874 \t\t Training Loss: 0.0005807629786431789 \t\n",
      "Epoch 40875 \t\t Training Loss: 0.0005807629786431789 \t\n",
      "Epoch 40876 \t\t Training Loss: 0.0005807629786431789 \t\n",
      "Epoch 40877 \t\t Training Loss: 0.0005807629786431789 \t\n",
      "Epoch 40878 \t\t Training Loss: 0.0005807629786431789 \t\n",
      "Epoch 40879 \t\t Training Loss: 0.0005807629786431789 \t\n",
      "Epoch 40880 \t\t Training Loss: 0.0005807629786431789 \t\n",
      "Epoch 40881 \t\t Training Loss: 0.0005807629786431789 \t\n",
      "Epoch 40882 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40883 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40884 \t\t Training Loss: 0.0005807629786431789 \t\n",
      "Epoch 40885 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40886 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40887 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40888 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40889 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40890 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40891 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40892 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40893 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40894 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40895 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40896 \t\t Training Loss: 0.0005807628622278571 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40897 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40898 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40899 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40900 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40901 \t\t Training Loss: 0.0005807627458125353 \t\n",
      "Epoch 40902 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40903 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40904 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40905 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40906 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40907 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40908 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40909 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40910 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40911 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40912 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40913 \t\t Training Loss: 0.0005807628622278571 \t\n",
      "Epoch 40914 \t\t Training Loss: 0.0005807627458125353 \t\n",
      "Epoch 40915 \t\t Training Loss: 0.0005807627458125353 \t\n",
      "Epoch 40916 \t\t Training Loss: 0.0005807627458125353 \t\n",
      "Epoch 40917 \t\t Training Loss: 0.0005807626876048744 \t\n",
      "Epoch 40918 \t\t Training Loss: 0.0005807626876048744 \t\n",
      "Epoch 40919 \t\t Training Loss: 0.0005807626876048744 \t\n",
      "Epoch 40920 \t\t Training Loss: 0.0005807626876048744 \t\n",
      "Epoch 40921 \t\t Training Loss: 0.0005807626876048744 \t\n",
      "Epoch 40922 \t\t Training Loss: 0.0005807626876048744 \t\n",
      "Epoch 40923 \t\t Training Loss: 0.0005807626876048744 \t\n",
      "Epoch 40924 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40925 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40926 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40927 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40928 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40929 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40930 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40931 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40932 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40933 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40934 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40935 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40936 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40937 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40938 \t\t Training Loss: 0.0005807626876048744 \t\n",
      "Epoch 40939 \t\t Training Loss: 0.0005807626876048744 \t\n",
      "Epoch 40940 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40941 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40942 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40943 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40944 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40945 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40946 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40947 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40948 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40949 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40950 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40951 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40952 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40953 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40954 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40955 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40956 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40957 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40958 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40959 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40960 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40961 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40962 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40963 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40964 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40965 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40966 \t\t Training Loss: 0.0005807625711895525 \t\n",
      "Epoch 40967 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40968 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40969 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40970 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40971 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40972 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40973 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40974 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40975 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40976 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40977 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40978 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40979 \t\t Training Loss: 0.0005807624547742307 \t\n",
      "Epoch 40980 \t\t Training Loss: 0.0005807623965665698 \t\n",
      "Epoch 40981 \t\t Training Loss: 0.0005807623965665698 \t\n",
      "Epoch 40982 \t\t Training Loss: 0.000580762280151248 \t\n",
      "Epoch 40983 \t\t Training Loss: 0.000580762280151248 \t\n",
      "Epoch 40984 \t\t Training Loss: 0.000580762280151248 \t\n",
      "Epoch 40985 \t\t Training Loss: 0.000580762280151248 \t\n",
      "Epoch 40986 \t\t Training Loss: 0.000580762280151248 \t\n",
      "Epoch 40987 \t\t Training Loss: 0.000580762280151248 \t\n",
      "Epoch 40988 \t\t Training Loss: 0.000580762280151248 \t\n",
      "Epoch 40989 \t\t Training Loss: 0.000580762280151248 \t\n",
      "Epoch 40990 \t\t Training Loss: 0.000580762280151248 \t\n",
      "Epoch 40991 \t\t Training Loss: 0.000580762280151248 \t\n",
      "Epoch 40992 \t\t Training Loss: 0.000580762280151248 \t\n",
      "Epoch 40993 \t\t Training Loss: 0.000580762280151248 \t\n",
      "Epoch 40994 \t\t Training Loss: 0.0005807621637359262 \t\n",
      "Epoch 40995 \t\t Training Loss: 0.0005807621637359262 \t\n",
      "Epoch 40996 \t\t Training Loss: 0.0005807621637359262 \t\n",
      "Epoch 40997 \t\t Training Loss: 0.0005807621637359262 \t\n",
      "Epoch 40998 \t\t Training Loss: 0.0005807621637359262 \t\n",
      "Epoch 40999 \t\t Training Loss: 0.0005807621637359262 \t\n",
      "Epoch 41000 \t\t Training Loss: 0.0005807621637359262 \t\n",
      "Epoch 41001 \t\t Training Loss: 0.0005807621637359262 \t\n",
      "Epoch 41002 \t\t Training Loss: 0.0005807621637359262 \t\n",
      "Epoch 41003 \t\t Training Loss: 0.0005807621637359262 \t\n",
      "Epoch 41004 \t\t Training Loss: 0.0005807621637359262 \t\n",
      "Epoch 41005 \t\t Training Loss: 0.0005807621637359262 \t\n",
      "Epoch 41006 \t\t Training Loss: 0.0005807621637359262 \t\n",
      "Epoch 41007 \t\t Training Loss: 0.0005807620473206043 \t\n",
      "Epoch 41008 \t\t Training Loss: 0.0005807620473206043 \t\n",
      "Epoch 41009 \t\t Training Loss: 0.0005807620473206043 \t\n",
      "Epoch 41010 \t\t Training Loss: 0.0005807620473206043 \t\n",
      "Epoch 41011 \t\t Training Loss: 0.0005807620473206043 \t\n",
      "Epoch 41012 \t\t Training Loss: 0.0005807620473206043 \t\n",
      "Epoch 41013 \t\t Training Loss: 0.0005807621637359262 \t\n",
      "Epoch 41014 \t\t Training Loss: 0.0005807621637359262 \t\n",
      "Epoch 41015 \t\t Training Loss: 0.0005807621637359262 \t\n",
      "Epoch 41016 \t\t Training Loss: 0.0005807620473206043 \t\n",
      "Epoch 41017 \t\t Training Loss: 0.0005807620473206043 \t\n",
      "Epoch 41018 \t\t Training Loss: 0.0005807619891129434 \t\n",
      "Epoch 41019 \t\t Training Loss: 0.0005807619891129434 \t\n",
      "Epoch 41020 \t\t Training Loss: 0.0005807619891129434 \t\n",
      "Epoch 41021 \t\t Training Loss: 0.0005807619891129434 \t\n",
      "Epoch 41022 \t\t Training Loss: 0.0005807619891129434 \t\n",
      "Epoch 41023 \t\t Training Loss: 0.0005807619891129434 \t\n",
      "Epoch 41024 \t\t Training Loss: 0.0005807619891129434 \t\n",
      "Epoch 41025 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41026 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41027 \t\t Training Loss: 0.0005807619891129434 \t\n",
      "Epoch 41028 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41029 \t\t Training Loss: 0.0005807619891129434 \t\n",
      "Epoch 41030 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41031 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41032 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41033 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41034 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41035 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41036 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41037 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41038 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41039 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41040 \t\t Training Loss: 0.0005807619891129434 \t\n",
      "Epoch 41041 \t\t Training Loss: 0.0005807619891129434 \t\n",
      "Epoch 41042 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41043 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41044 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41045 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41046 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41047 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41048 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41049 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41050 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41051 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41052 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41053 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41054 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41055 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41056 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41057 \t\t Training Loss: 0.0005807618726976216 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41058 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41059 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41060 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41061 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41062 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41063 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41064 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41065 \t\t Training Loss: 0.0005807618726976216 \t\n",
      "Epoch 41066 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41067 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41068 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41069 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41070 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41071 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41072 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41073 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41074 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41075 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41076 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41077 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41078 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41079 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41080 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41081 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41082 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41083 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41084 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41085 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41086 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41087 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41088 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41089 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41090 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41091 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41092 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41093 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41094 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41095 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41096 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41097 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41098 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41099 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41100 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41101 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41102 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41103 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41104 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41105 \t\t Training Loss: 0.0005807617562822998 \t\n",
      "Epoch 41106 \t\t Training Loss: 0.0005807616980746388 \t\n",
      "Epoch 41107 \t\t Training Loss: 0.0005807616980746388 \t\n",
      "Epoch 41108 \t\t Training Loss: 0.0005807616980746388 \t\n",
      "Epoch 41109 \t\t Training Loss: 0.0005807616980746388 \t\n",
      "Epoch 41110 \t\t Training Loss: 0.0005807616980746388 \t\n",
      "Epoch 41111 \t\t Training Loss: 0.0005807616980746388 \t\n",
      "Epoch 41112 \t\t Training Loss: 0.0005807616980746388 \t\n",
      "Epoch 41113 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41114 \t\t Training Loss: 0.0005807616980746388 \t\n",
      "Epoch 41115 \t\t Training Loss: 0.0005807616980746388 \t\n",
      "Epoch 41116 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41117 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41118 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41119 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41120 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41121 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41122 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41123 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41124 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41125 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41126 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41127 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41128 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41129 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41130 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41131 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41132 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41133 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41134 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41135 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41136 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41137 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41138 \t\t Training Loss: 0.000580761581659317 \t\n",
      "Epoch 41139 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41140 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41141 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41142 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41143 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41144 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41145 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41146 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41147 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41148 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41149 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41150 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41151 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41152 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41153 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41154 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41155 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41156 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41157 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41158 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41159 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41160 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41161 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41162 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41163 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41164 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41165 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41166 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41167 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41168 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41169 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41170 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41171 \t\t Training Loss: 0.0005807614070363343 \t\n",
      "Epoch 41172 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41173 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41174 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41175 \t\t Training Loss: 0.0005807614070363343 \t\n",
      "Epoch 41176 \t\t Training Loss: 0.0005807614070363343 \t\n",
      "Epoch 41177 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41178 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41179 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41180 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41181 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41182 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41183 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41184 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41185 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41186 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41187 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41188 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41189 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41190 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41191 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41192 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41193 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41194 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41195 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41196 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41197 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41198 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41199 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41200 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41201 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41202 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41203 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41204 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41205 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41206 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41207 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41208 \t\t Training Loss: 0.0005807613488286734 \t\n",
      "Epoch 41209 \t\t Training Loss: 0.0005807614070363343 \t\n",
      "Epoch 41210 \t\t Training Loss: 0.0005807614652439952 \t\n",
      "Epoch 41211 \t\t Training Loss: 0.0005807614070363343 \t\n",
      "Epoch 41212 \t\t Training Loss: 0.0005807612906210124 \t\n",
      "Epoch 41213 \t\t Training Loss: 0.0005807612906210124 \t\n",
      "Epoch 41214 \t\t Training Loss: 0.0005807612906210124 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41215 \t\t Training Loss: 0.0005807612906210124 \t\n",
      "Epoch 41216 \t\t Training Loss: 0.0005807612906210124 \t\n",
      "Epoch 41217 \t\t Training Loss: 0.0005807612906210124 \t\n",
      "Epoch 41218 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41219 \t\t Training Loss: 0.0005807612906210124 \t\n",
      "Epoch 41220 \t\t Training Loss: 0.0005807612906210124 \t\n",
      "Epoch 41221 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41222 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41223 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41224 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41225 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41226 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41227 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41228 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41229 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41230 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41231 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41232 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41233 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41234 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41235 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41236 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41237 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41238 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41239 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41240 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41241 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41242 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41243 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41244 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41245 \t\t Training Loss: 0.0005807611159980297 \t\n",
      "Epoch 41246 \t\t Training Loss: 0.0005807610577903688 \t\n",
      "Epoch 41247 \t\t Training Loss: 0.0005807611159980297 \t\n",
      "Epoch 41248 \t\t Training Loss: 0.0005807611159980297 \t\n",
      "Epoch 41249 \t\t Training Loss: 0.0005807611159980297 \t\n",
      "Epoch 41250 \t\t Training Loss: 0.0005807610577903688 \t\n",
      "Epoch 41251 \t\t Training Loss: 0.0005807611742056906 \t\n",
      "Epoch 41252 \t\t Training Loss: 0.0005807611159980297 \t\n",
      "Epoch 41253 \t\t Training Loss: 0.0005807611159980297 \t\n",
      "Epoch 41254 \t\t Training Loss: 0.0005807610577903688 \t\n",
      "Epoch 41255 \t\t Training Loss: 0.0005807610577903688 \t\n",
      "Epoch 41256 \t\t Training Loss: 0.0005807610577903688 \t\n",
      "Epoch 41257 \t\t Training Loss: 0.0005807610577903688 \t\n",
      "Epoch 41258 \t\t Training Loss: 0.0005807610577903688 \t\n",
      "Epoch 41259 \t\t Training Loss: 0.0005807610577903688 \t\n",
      "Epoch 41260 \t\t Training Loss: 0.0005807610577903688 \t\n",
      "Epoch 41261 \t\t Training Loss: 0.0005807610577903688 \t\n",
      "Epoch 41262 \t\t Training Loss: 0.0005807610577903688 \t\n",
      "Epoch 41263 \t\t Training Loss: 0.0005807610577903688 \t\n",
      "Epoch 41264 \t\t Training Loss: 0.0005807610577903688 \t\n",
      "Epoch 41265 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41266 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41267 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41268 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41269 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41270 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41271 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41272 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41273 \t\t Training Loss: 0.0005807610577903688 \t\n",
      "Epoch 41274 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41275 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41276 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41277 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41278 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41279 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41280 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41281 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41282 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41283 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41284 \t\t Training Loss: 0.0005807609995827079 \t\n",
      "Epoch 41285 \t\t Training Loss: 0.0005807608831673861 \t\n",
      "Epoch 41286 \t\t Training Loss: 0.0005807608831673861 \t\n",
      "Epoch 41287 \t\t Training Loss: 0.0005807608831673861 \t\n",
      "Epoch 41288 \t\t Training Loss: 0.0005807608831673861 \t\n",
      "Epoch 41289 \t\t Training Loss: 0.0005807608831673861 \t\n",
      "Epoch 41290 \t\t Training Loss: 0.0005807608831673861 \t\n",
      "Epoch 41291 \t\t Training Loss: 0.0005807608831673861 \t\n",
      "Epoch 41292 \t\t Training Loss: 0.0005807608831673861 \t\n",
      "Epoch 41293 \t\t Training Loss: 0.0005807607667520642 \t\n",
      "Epoch 41294 \t\t Training Loss: 0.0005807607667520642 \t\n",
      "Epoch 41295 \t\t Training Loss: 0.0005807607667520642 \t\n",
      "Epoch 41296 \t\t Training Loss: 0.0005807607667520642 \t\n",
      "Epoch 41297 \t\t Training Loss: 0.0005807607667520642 \t\n",
      "Epoch 41298 \t\t Training Loss: 0.0005807607667520642 \t\n",
      "Epoch 41299 \t\t Training Loss: 0.0005807607085444033 \t\n",
      "Epoch 41300 \t\t Training Loss: 0.0005807607085444033 \t\n",
      "Epoch 41301 \t\t Training Loss: 0.0005807607085444033 \t\n",
      "Epoch 41302 \t\t Training Loss: 0.0005807607085444033 \t\n",
      "Epoch 41303 \t\t Training Loss: 0.0005807607085444033 \t\n",
      "Epoch 41304 \t\t Training Loss: 0.0005807607085444033 \t\n",
      "Epoch 41305 \t\t Training Loss: 0.0005807607085444033 \t\n",
      "Epoch 41306 \t\t Training Loss: 0.0005807606503367424 \t\n",
      "Epoch 41307 \t\t Training Loss: 0.0005807606503367424 \t\n",
      "Epoch 41308 \t\t Training Loss: 0.0005807606503367424 \t\n",
      "Epoch 41309 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41310 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41311 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41312 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41313 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41314 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41315 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41316 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41317 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41318 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41319 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41320 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41321 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41322 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41323 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41324 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41325 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41326 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41327 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41328 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41329 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41330 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41331 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41332 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41333 \t\t Training Loss: 0.0005807605921290815 \t\n",
      "Epoch 41334 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41335 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41336 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41337 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41338 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41339 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41340 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41341 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41342 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41343 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41344 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41345 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41346 \t\t Training Loss: 0.0005807604175060987 \t\n",
      "Epoch 41347 \t\t Training Loss: 0.0005807604757137597 \t\n",
      "Epoch 41348 \t\t Training Loss: 0.0005807604175060987 \t\n",
      "Epoch 41349 \t\t Training Loss: 0.0005807604175060987 \t\n",
      "Epoch 41350 \t\t Training Loss: 0.0005807604175060987 \t\n",
      "Epoch 41351 \t\t Training Loss: 0.0005807604175060987 \t\n",
      "Epoch 41352 \t\t Training Loss: 0.0005807604175060987 \t\n",
      "Epoch 41353 \t\t Training Loss: 0.0005807604175060987 \t\n",
      "Epoch 41354 \t\t Training Loss: 0.0005807604175060987 \t\n",
      "Epoch 41355 \t\t Training Loss: 0.0005807604175060987 \t\n",
      "Epoch 41356 \t\t Training Loss: 0.0005807603010907769 \t\n",
      "Epoch 41357 \t\t Training Loss: 0.0005807603010907769 \t\n",
      "Epoch 41358 \t\t Training Loss: 0.0005807603010907769 \t\n",
      "Epoch 41359 \t\t Training Loss: 0.0005807603010907769 \t\n",
      "Epoch 41360 \t\t Training Loss: 0.0005807603010907769 \t\n",
      "Epoch 41361 \t\t Training Loss: 0.0005807603010907769 \t\n",
      "Epoch 41362 \t\t Training Loss: 0.0005807603010907769 \t\n",
      "Epoch 41363 \t\t Training Loss: 0.0005807603010907769 \t\n",
      "Epoch 41364 \t\t Training Loss: 0.0005807603010907769 \t\n",
      "Epoch 41365 \t\t Training Loss: 0.000580760242883116 \t\n",
      "Epoch 41366 \t\t Training Loss: 0.000580760242883116 \t\n",
      "Epoch 41367 \t\t Training Loss: 0.000580760242883116 \t\n",
      "Epoch 41368 \t\t Training Loss: 0.000580760242883116 \t\n",
      "Epoch 41369 \t\t Training Loss: 0.000580760242883116 \t\n",
      "Epoch 41370 \t\t Training Loss: 0.000580760242883116 \t\n",
      "Epoch 41371 \t\t Training Loss: 0.000580760242883116 \t\n",
      "Epoch 41372 \t\t Training Loss: 0.000580760242883116 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41373 \t\t Training Loss: 0.000580760242883116 \t\n",
      "Epoch 41374 \t\t Training Loss: 0.000580760242883116 \t\n",
      "Epoch 41375 \t\t Training Loss: 0.0005807601846754551 \t\n",
      "Epoch 41376 \t\t Training Loss: 0.0005807601846754551 \t\n",
      "Epoch 41377 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41378 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41379 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41380 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41381 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41382 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41383 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41384 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41385 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41386 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41387 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41388 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41389 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41390 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41391 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41392 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41393 \t\t Training Loss: 0.0005807601846754551 \t\n",
      "Epoch 41394 \t\t Training Loss: 0.0005807601846754551 \t\n",
      "Epoch 41395 \t\t Training Loss: 0.0005807601846754551 \t\n",
      "Epoch 41396 \t\t Training Loss: 0.0005807601846754551 \t\n",
      "Epoch 41397 \t\t Training Loss: 0.0005807601846754551 \t\n",
      "Epoch 41398 \t\t Training Loss: 0.0005807601846754551 \t\n",
      "Epoch 41399 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41400 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41401 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41402 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41403 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41404 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41405 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41406 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41407 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41408 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41409 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41410 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41411 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41412 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41413 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41414 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41415 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41416 \t\t Training Loss: 0.0005807601264677942 \t\n",
      "Epoch 41417 \t\t Training Loss: 0.0005807600100524724 \t\n",
      "Epoch 41418 \t\t Training Loss: 0.0005807600100524724 \t\n",
      "Epoch 41419 \t\t Training Loss: 0.0005807600100524724 \t\n",
      "Epoch 41420 \t\t Training Loss: 0.0005807600100524724 \t\n",
      "Epoch 41421 \t\t Training Loss: 0.0005807600100524724 \t\n",
      "Epoch 41422 \t\t Training Loss: 0.0005807600100524724 \t\n",
      "Epoch 41423 \t\t Training Loss: 0.0005807600100524724 \t\n",
      "Epoch 41424 \t\t Training Loss: 0.0005807599518448114 \t\n",
      "Epoch 41425 \t\t Training Loss: 0.0005807599518448114 \t\n",
      "Epoch 41426 \t\t Training Loss: 0.0005807599518448114 \t\n",
      "Epoch 41427 \t\t Training Loss: 0.0005807599518448114 \t\n",
      "Epoch 41428 \t\t Training Loss: 0.0005807598936371505 \t\n",
      "Epoch 41429 \t\t Training Loss: 0.0005807598936371505 \t\n",
      "Epoch 41430 \t\t Training Loss: 0.0005807598354294896 \t\n",
      "Epoch 41431 \t\t Training Loss: 0.0005807598354294896 \t\n",
      "Epoch 41432 \t\t Training Loss: 0.0005807598354294896 \t\n",
      "Epoch 41433 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41434 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41435 \t\t Training Loss: 0.0005807597772218287 \t\n",
      "Epoch 41436 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41437 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41438 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41439 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41440 \t\t Training Loss: 0.0005807597772218287 \t\n",
      "Epoch 41441 \t\t Training Loss: 0.0005807598354294896 \t\n",
      "Epoch 41442 \t\t Training Loss: 0.0005807598354294896 \t\n",
      "Epoch 41443 \t\t Training Loss: 0.0005807598354294896 \t\n",
      "Epoch 41444 \t\t Training Loss: 0.0005807598354294896 \t\n",
      "Epoch 41445 \t\t Training Loss: 0.0005807598354294896 \t\n",
      "Epoch 41446 \t\t Training Loss: 0.0005807598354294896 \t\n",
      "Epoch 41447 \t\t Training Loss: 0.0005807598354294896 \t\n",
      "Epoch 41448 \t\t Training Loss: 0.0005807598354294896 \t\n",
      "Epoch 41449 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41450 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41451 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41452 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41453 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41454 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41455 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41456 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41457 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41458 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41459 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41460 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41461 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41462 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41463 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41464 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41465 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41466 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41467 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41468 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41469 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41470 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41471 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41472 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41473 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41474 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41475 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41476 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41477 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41478 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41479 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41480 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41481 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41482 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41483 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41484 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41485 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41486 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41487 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41488 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41489 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41490 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41491 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41492 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41493 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41494 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41495 \t\t Training Loss: 0.0005807597190141678 \t\n",
      "Epoch 41496 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41497 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41498 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41499 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41500 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41501 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41502 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41503 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41504 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41505 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41506 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41507 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41508 \t\t Training Loss: 0.0005807596608065069 \t\n",
      "Epoch 41509 \t\t Training Loss: 0.000580759602598846 \t\n",
      "Epoch 41510 \t\t Training Loss: 0.000580759602598846 \t\n",
      "Epoch 41511 \t\t Training Loss: 0.000580759602598846 \t\n",
      "Epoch 41512 \t\t Training Loss: 0.000580759602598846 \t\n",
      "Epoch 41513 \t\t Training Loss: 0.000580759602598846 \t\n",
      "Epoch 41514 \t\t Training Loss: 0.000580759602598846 \t\n",
      "Epoch 41515 \t\t Training Loss: 0.000580759602598846 \t\n",
      "Epoch 41516 \t\t Training Loss: 0.000580759602598846 \t\n",
      "Epoch 41517 \t\t Training Loss: 0.000580759602598846 \t\n",
      "Epoch 41518 \t\t Training Loss: 0.000580759544391185 \t\n",
      "Epoch 41519 \t\t Training Loss: 0.000580759544391185 \t\n",
      "Epoch 41520 \t\t Training Loss: 0.000580759544391185 \t\n",
      "Epoch 41521 \t\t Training Loss: 0.000580759544391185 \t\n",
      "Epoch 41522 \t\t Training Loss: 0.000580759602598846 \t\n",
      "Epoch 41523 \t\t Training Loss: 0.000580759544391185 \t\n",
      "Epoch 41524 \t\t Training Loss: 0.000580759544391185 \t\n",
      "Epoch 41525 \t\t Training Loss: 0.000580759544391185 \t\n",
      "Epoch 41526 \t\t Training Loss: 0.000580759544391185 \t\n",
      "Epoch 41527 \t\t Training Loss: 0.000580759544391185 \t\n",
      "Epoch 41528 \t\t Training Loss: 0.000580759544391185 \t\n",
      "Epoch 41529 \t\t Training Loss: 0.000580759544391185 \t\n",
      "Epoch 41530 \t\t Training Loss: 0.000580759544391185 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41531 \t\t Training Loss: 0.0005807594861835241 \t\n",
      "Epoch 41532 \t\t Training Loss: 0.0005807594861835241 \t\n",
      "Epoch 41533 \t\t Training Loss: 0.0005807594861835241 \t\n",
      "Epoch 41534 \t\t Training Loss: 0.0005807594861835241 \t\n",
      "Epoch 41535 \t\t Training Loss: 0.0005807594279758632 \t\n",
      "Epoch 41536 \t\t Training Loss: 0.0005807594861835241 \t\n",
      "Epoch 41537 \t\t Training Loss: 0.0005807594861835241 \t\n",
      "Epoch 41538 \t\t Training Loss: 0.000580759544391185 \t\n",
      "Epoch 41539 \t\t Training Loss: 0.0005807594861835241 \t\n",
      "Epoch 41540 \t\t Training Loss: 0.0005807594861835241 \t\n",
      "Epoch 41541 \t\t Training Loss: 0.0005807594861835241 \t\n",
      "Epoch 41542 \t\t Training Loss: 0.0005807594861835241 \t\n",
      "Epoch 41543 \t\t Training Loss: 0.0005807594861835241 \t\n",
      "Epoch 41544 \t\t Training Loss: 0.000580759544391185 \t\n",
      "Epoch 41545 \t\t Training Loss: 0.000580759544391185 \t\n",
      "Epoch 41546 \t\t Training Loss: 0.0005807594861835241 \t\n",
      "Epoch 41547 \t\t Training Loss: 0.0005807594861835241 \t\n",
      "Epoch 41548 \t\t Training Loss: 0.0005807594861835241 \t\n",
      "Epoch 41549 \t\t Training Loss: 0.0005807594279758632 \t\n",
      "Epoch 41550 \t\t Training Loss: 0.0005807594279758632 \t\n",
      "Epoch 41551 \t\t Training Loss: 0.0005807594279758632 \t\n",
      "Epoch 41552 \t\t Training Loss: 0.0005807594279758632 \t\n",
      "Epoch 41553 \t\t Training Loss: 0.0005807594279758632 \t\n",
      "Epoch 41554 \t\t Training Loss: 0.0005807594279758632 \t\n",
      "Epoch 41555 \t\t Training Loss: 0.0005807593697682023 \t\n",
      "Epoch 41556 \t\t Training Loss: 0.0005807594279758632 \t\n",
      "Epoch 41557 \t\t Training Loss: 0.0005807593697682023 \t\n",
      "Epoch 41558 \t\t Training Loss: 0.0005807593697682023 \t\n",
      "Epoch 41559 \t\t Training Loss: 0.0005807593697682023 \t\n",
      "Epoch 41560 \t\t Training Loss: 0.0005807593697682023 \t\n",
      "Epoch 41561 \t\t Training Loss: 0.0005807593697682023 \t\n",
      "Epoch 41562 \t\t Training Loss: 0.0005807593697682023 \t\n",
      "Epoch 41563 \t\t Training Loss: 0.0005807593697682023 \t\n",
      "Epoch 41564 \t\t Training Loss: 0.0005807593697682023 \t\n",
      "Epoch 41565 \t\t Training Loss: 0.0005807593697682023 \t\n",
      "Epoch 41566 \t\t Training Loss: 0.0005807593697682023 \t\n",
      "Epoch 41567 \t\t Training Loss: 0.0005807592533528805 \t\n",
      "Epoch 41568 \t\t Training Loss: 0.0005807592533528805 \t\n",
      "Epoch 41569 \t\t Training Loss: 0.0005807592533528805 \t\n",
      "Epoch 41570 \t\t Training Loss: 0.0005807592533528805 \t\n",
      "Epoch 41571 \t\t Training Loss: 0.0005807592533528805 \t\n",
      "Epoch 41572 \t\t Training Loss: 0.0005807592533528805 \t\n",
      "Epoch 41573 \t\t Training Loss: 0.0005807592533528805 \t\n",
      "Epoch 41574 \t\t Training Loss: 0.0005807592533528805 \t\n",
      "Epoch 41575 \t\t Training Loss: 0.0005807591951452196 \t\n",
      "Epoch 41576 \t\t Training Loss: 0.0005807591951452196 \t\n",
      "Epoch 41577 \t\t Training Loss: 0.0005807591951452196 \t\n",
      "Epoch 41578 \t\t Training Loss: 0.0005807591951452196 \t\n",
      "Epoch 41579 \t\t Training Loss: 0.0005807591951452196 \t\n",
      "Epoch 41580 \t\t Training Loss: 0.0005807591951452196 \t\n",
      "Epoch 41581 \t\t Training Loss: 0.0005807591951452196 \t\n",
      "Epoch 41582 \t\t Training Loss: 0.0005807591951452196 \t\n",
      "Epoch 41583 \t\t Training Loss: 0.0005807591369375587 \t\n",
      "Epoch 41584 \t\t Training Loss: 0.0005807591951452196 \t\n",
      "Epoch 41585 \t\t Training Loss: 0.0005807591369375587 \t\n",
      "Epoch 41586 \t\t Training Loss: 0.0005807591369375587 \t\n",
      "Epoch 41587 \t\t Training Loss: 0.0005807591369375587 \t\n",
      "Epoch 41588 \t\t Training Loss: 0.0005807591369375587 \t\n",
      "Epoch 41589 \t\t Training Loss: 0.0005807591369375587 \t\n",
      "Epoch 41590 \t\t Training Loss: 0.0005807591369375587 \t\n",
      "Epoch 41591 \t\t Training Loss: 0.0005807591369375587 \t\n",
      "Epoch 41592 \t\t Training Loss: 0.0005807591369375587 \t\n",
      "Epoch 41593 \t\t Training Loss: 0.0005807590787298977 \t\n",
      "Epoch 41594 \t\t Training Loss: 0.0005807590787298977 \t\n",
      "Epoch 41595 \t\t Training Loss: 0.0005807590787298977 \t\n",
      "Epoch 41596 \t\t Training Loss: 0.0005807590787298977 \t\n",
      "Epoch 41597 \t\t Training Loss: 0.0005807590787298977 \t\n",
      "Epoch 41598 \t\t Training Loss: 0.0005807591369375587 \t\n",
      "Epoch 41599 \t\t Training Loss: 0.0005807591369375587 \t\n",
      "Epoch 41600 \t\t Training Loss: 0.0005807590787298977 \t\n",
      "Epoch 41601 \t\t Training Loss: 0.0005807590787298977 \t\n",
      "Epoch 41602 \t\t Training Loss: 0.0005807590787298977 \t\n",
      "Epoch 41603 \t\t Training Loss: 0.0005807591369375587 \t\n",
      "Epoch 41604 \t\t Training Loss: 0.0005807590787298977 \t\n",
      "Epoch 41605 \t\t Training Loss: 0.0005807590787298977 \t\n",
      "Epoch 41606 \t\t Training Loss: 0.0005807590787298977 \t\n",
      "Epoch 41607 \t\t Training Loss: 0.0005807590787298977 \t\n",
      "Epoch 41608 \t\t Training Loss: 0.0005807590787298977 \t\n",
      "Epoch 41609 \t\t Training Loss: 0.0005807590205222368 \t\n",
      "Epoch 41610 \t\t Training Loss: 0.0005807590205222368 \t\n",
      "Epoch 41611 \t\t Training Loss: 0.0005807590205222368 \t\n",
      "Epoch 41612 \t\t Training Loss: 0.0005807589623145759 \t\n",
      "Epoch 41613 \t\t Training Loss: 0.0005807589623145759 \t\n",
      "Epoch 41614 \t\t Training Loss: 0.0005807589623145759 \t\n",
      "Epoch 41615 \t\t Training Loss: 0.0005807589623145759 \t\n",
      "Epoch 41616 \t\t Training Loss: 0.0005807589623145759 \t\n",
      "Epoch 41617 \t\t Training Loss: 0.0005807590205222368 \t\n",
      "Epoch 41618 \t\t Training Loss: 0.0005807590205222368 \t\n",
      "Epoch 41619 \t\t Training Loss: 0.0005807589623145759 \t\n",
      "Epoch 41620 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41621 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41622 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41623 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41624 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41625 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41626 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41627 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41628 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41629 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41630 \t\t Training Loss: 0.0005807589623145759 \t\n",
      "Epoch 41631 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41632 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41633 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41634 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41635 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41636 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41637 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41638 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41639 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41640 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41641 \t\t Training Loss: 0.0005807588458992541 \t\n",
      "Epoch 41642 \t\t Training Loss: 0.000580758904106915 \t\n",
      "Epoch 41643 \t\t Training Loss: 0.0005807588458992541 \t\n",
      "Epoch 41644 \t\t Training Loss: 0.0005807588458992541 \t\n",
      "Epoch 41645 \t\t Training Loss: 0.0005807587876915932 \t\n",
      "Epoch 41646 \t\t Training Loss: 0.0005807587876915932 \t\n",
      "Epoch 41647 \t\t Training Loss: 0.0005807587876915932 \t\n",
      "Epoch 41648 \t\t Training Loss: 0.0005807587876915932 \t\n",
      "Epoch 41649 \t\t Training Loss: 0.0005807587876915932 \t\n",
      "Epoch 41650 \t\t Training Loss: 0.0005807587876915932 \t\n",
      "Epoch 41651 \t\t Training Loss: 0.0005807587876915932 \t\n",
      "Epoch 41652 \t\t Training Loss: 0.0005807586712762713 \t\n",
      "Epoch 41653 \t\t Training Loss: 0.0005807587876915932 \t\n",
      "Epoch 41654 \t\t Training Loss: 0.0005807586712762713 \t\n",
      "Epoch 41655 \t\t Training Loss: 0.0005807586712762713 \t\n",
      "Epoch 41656 \t\t Training Loss: 0.0005807586712762713 \t\n",
      "Epoch 41657 \t\t Training Loss: 0.0005807586130686104 \t\n",
      "Epoch 41658 \t\t Training Loss: 0.0005807585548609495 \t\n",
      "Epoch 41659 \t\t Training Loss: 0.0005807586130686104 \t\n",
      "Epoch 41660 \t\t Training Loss: 0.0005807585548609495 \t\n",
      "Epoch 41661 \t\t Training Loss: 0.0005807586130686104 \t\n",
      "Epoch 41662 \t\t Training Loss: 0.0005807586130686104 \t\n",
      "Epoch 41663 \t\t Training Loss: 0.0005807585548609495 \t\n",
      "Epoch 41664 \t\t Training Loss: 0.0005807585548609495 \t\n",
      "Epoch 41665 \t\t Training Loss: 0.0005807585548609495 \t\n",
      "Epoch 41666 \t\t Training Loss: 0.0005807585548609495 \t\n",
      "Epoch 41667 \t\t Training Loss: 0.0005807585548609495 \t\n",
      "Epoch 41668 \t\t Training Loss: 0.0005807585548609495 \t\n",
      "Epoch 41669 \t\t Training Loss: 0.0005807585548609495 \t\n",
      "Epoch 41670 \t\t Training Loss: 0.0005807585548609495 \t\n",
      "Epoch 41671 \t\t Training Loss: 0.0005807585548609495 \t\n",
      "Epoch 41672 \t\t Training Loss: 0.0005807585548609495 \t\n",
      "Epoch 41673 \t\t Training Loss: 0.0005807585548609495 \t\n",
      "Epoch 41674 \t\t Training Loss: 0.0005807585548609495 \t\n",
      "Epoch 41675 \t\t Training Loss: 0.0005807585548609495 \t\n",
      "Epoch 41676 \t\t Training Loss: 0.0005807585548609495 \t\n",
      "Epoch 41677 \t\t Training Loss: 0.0005807584966532886 \t\n",
      "Epoch 41678 \t\t Training Loss: 0.0005807584966532886 \t\n",
      "Epoch 41679 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41680 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41681 \t\t Training Loss: 0.0005807584966532886 \t\n",
      "Epoch 41682 \t\t Training Loss: 0.0005807584966532886 \t\n",
      "Epoch 41683 \t\t Training Loss: 0.0005807584966532886 \t\n",
      "Epoch 41684 \t\t Training Loss: 0.0005807584966532886 \t\n",
      "Epoch 41685 \t\t Training Loss: 0.0005807584966532886 \t\n",
      "Epoch 41686 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41687 \t\t Training Loss: 0.0005807585548609495 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41688 \t\t Training Loss: 0.0005807584966532886 \t\n",
      "Epoch 41689 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41690 \t\t Training Loss: 0.0005807584966532886 \t\n",
      "Epoch 41691 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41692 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41693 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41694 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41695 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41696 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41697 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41698 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41699 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41700 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41701 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41702 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41703 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41704 \t\t Training Loss: 0.0005807584384456277 \t\n",
      "Epoch 41705 \t\t Training Loss: 0.0005807583802379668 \t\n",
      "Epoch 41706 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41707 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41708 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41709 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41710 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41711 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41712 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41713 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41714 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41715 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41716 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41717 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41718 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41719 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41720 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41721 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41722 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41723 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41724 \t\t Training Loss: 0.0005807583220303059 \t\n",
      "Epoch 41725 \t\t Training Loss: 0.000580758205614984 \t\n",
      "Epoch 41726 \t\t Training Loss: 0.000580758205614984 \t\n",
      "Epoch 41727 \t\t Training Loss: 0.000580758205614984 \t\n",
      "Epoch 41728 \t\t Training Loss: 0.000580758205614984 \t\n",
      "Epoch 41729 \t\t Training Loss: 0.000580758205614984 \t\n",
      "Epoch 41730 \t\t Training Loss: 0.000580758205614984 \t\n",
      "Epoch 41731 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41732 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41733 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41734 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41735 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41736 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41737 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41738 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41739 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41740 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41741 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41742 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41743 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41744 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41745 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41746 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41747 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41748 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41749 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41750 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41751 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41752 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41753 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41754 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41755 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41756 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41757 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41758 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41759 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41760 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41761 \t\t Training Loss: 0.0005807580891996622 \t\n",
      "Epoch 41762 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41763 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41764 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41765 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41766 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41767 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41768 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41769 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41770 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41771 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41772 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41773 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41774 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41775 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41776 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41777 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41778 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41779 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41780 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41781 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41782 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41783 \t\t Training Loss: 0.0005807580309920013 \t\n",
      "Epoch 41784 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41785 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41786 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41787 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41788 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41789 \t\t Training Loss: 0.0005807578563690186 \t\n",
      "Epoch 41790 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41791 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41792 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41793 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41794 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41795 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41796 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41797 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41798 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41799 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41800 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41801 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41802 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41803 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41804 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41805 \t\t Training Loss: 0.0005807578563690186 \t\n",
      "Epoch 41806 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41807 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41808 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41809 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41810 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41811 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41812 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41813 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41814 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41815 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41816 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41817 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41818 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41819 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41820 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41821 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41822 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41823 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41824 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41825 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41826 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41827 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41828 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41829 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41830 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41831 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41832 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41833 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41834 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41835 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41836 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41837 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41838 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41839 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41840 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41841 \t\t Training Loss: 0.0005807578563690186 \t\n",
      "Epoch 41842 \t\t Training Loss: 0.0005807579145766795 \t\n",
      "Epoch 41843 \t\t Training Loss: 0.0005807578563690186 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41844 \t\t Training Loss: 0.0005807578563690186 \t\n",
      "Epoch 41845 \t\t Training Loss: 0.0005807578563690186 \t\n",
      "Epoch 41846 \t\t Training Loss: 0.0005807577981613576 \t\n",
      "Epoch 41847 \t\t Training Loss: 0.0005807578563690186 \t\n",
      "Epoch 41848 \t\t Training Loss: 0.0005807577981613576 \t\n",
      "Epoch 41849 \t\t Training Loss: 0.0005807578563690186 \t\n",
      "Epoch 41850 \t\t Training Loss: 0.0005807577981613576 \t\n",
      "Epoch 41851 \t\t Training Loss: 0.0005807577981613576 \t\n",
      "Epoch 41852 \t\t Training Loss: 0.0005807577981613576 \t\n",
      "Epoch 41853 \t\t Training Loss: 0.0005807577981613576 \t\n",
      "Epoch 41854 \t\t Training Loss: 0.0005807577981613576 \t\n",
      "Epoch 41855 \t\t Training Loss: 0.0005807577399536967 \t\n",
      "Epoch 41856 \t\t Training Loss: 0.0005807577399536967 \t\n",
      "Epoch 41857 \t\t Training Loss: 0.0005807577399536967 \t\n",
      "Epoch 41858 \t\t Training Loss: 0.0005807577399536967 \t\n",
      "Epoch 41859 \t\t Training Loss: 0.0005807577399536967 \t\n",
      "Epoch 41860 \t\t Training Loss: 0.0005807577399536967 \t\n",
      "Epoch 41861 \t\t Training Loss: 0.0005807577399536967 \t\n",
      "Epoch 41862 \t\t Training Loss: 0.0005807577399536967 \t\n",
      "Epoch 41863 \t\t Training Loss: 0.0005807577399536967 \t\n",
      "Epoch 41864 \t\t Training Loss: 0.0005807577399536967 \t\n",
      "Epoch 41865 \t\t Training Loss: 0.0005807577399536967 \t\n",
      "Epoch 41866 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41867 \t\t Training Loss: 0.0005807577399536967 \t\n",
      "Epoch 41868 \t\t Training Loss: 0.0005807577399536967 \t\n",
      "Epoch 41869 \t\t Training Loss: 0.0005807577399536967 \t\n",
      "Epoch 41870 \t\t Training Loss: 0.0005807577399536967 \t\n",
      "Epoch 41871 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41872 \t\t Training Loss: 0.0005807577399536967 \t\n",
      "Epoch 41873 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41874 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41875 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41876 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41877 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41878 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41879 \t\t Training Loss: 0.000580757565330714 \t\n",
      "Epoch 41880 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41881 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41882 \t\t Training Loss: 0.000580757565330714 \t\n",
      "Epoch 41883 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41884 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41885 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41886 \t\t Training Loss: 0.000580757565330714 \t\n",
      "Epoch 41887 \t\t Training Loss: 0.000580757565330714 \t\n",
      "Epoch 41888 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41889 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41890 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41891 \t\t Training Loss: 0.000580757565330714 \t\n",
      "Epoch 41892 \t\t Training Loss: 0.0005807576235383749 \t\n",
      "Epoch 41893 \t\t Training Loss: 0.000580757565330714 \t\n",
      "Epoch 41894 \t\t Training Loss: 0.000580757565330714 \t\n",
      "Epoch 41895 \t\t Training Loss: 0.000580757565330714 \t\n",
      "Epoch 41896 \t\t Training Loss: 0.000580757565330714 \t\n",
      "Epoch 41897 \t\t Training Loss: 0.0005807575071230531 \t\n",
      "Epoch 41898 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41899 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41900 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41901 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41902 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41903 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41904 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41905 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41906 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41907 \t\t Training Loss: 0.0005807575071230531 \t\n",
      "Epoch 41908 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41909 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41910 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41911 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41912 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41913 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41914 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41915 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41916 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41917 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41918 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41919 \t\t Training Loss: 0.0005807573907077312 \t\n",
      "Epoch 41920 \t\t Training Loss: 0.0005807573907077312 \t\n",
      "Epoch 41921 \t\t Training Loss: 0.0005807573907077312 \t\n",
      "Epoch 41922 \t\t Training Loss: 0.0005807573907077312 \t\n",
      "Epoch 41923 \t\t Training Loss: 0.0005807573907077312 \t\n",
      "Epoch 41924 \t\t Training Loss: 0.0005807573907077312 \t\n",
      "Epoch 41925 \t\t Training Loss: 0.0005807573907077312 \t\n",
      "Epoch 41926 \t\t Training Loss: 0.0005807574489153922 \t\n",
      "Epoch 41927 \t\t Training Loss: 0.0005807573907077312 \t\n",
      "Epoch 41928 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41929 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41930 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41931 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41932 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41933 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41934 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41935 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41936 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41937 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41938 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41939 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41940 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41941 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41942 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41943 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41944 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41945 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41946 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41947 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41948 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41949 \t\t Training Loss: 0.0005807573325000703 \t\n",
      "Epoch 41950 \t\t Training Loss: 0.0005807572160847485 \t\n",
      "Epoch 41951 \t\t Training Loss: 0.0005807572160847485 \t\n",
      "Epoch 41952 \t\t Training Loss: 0.0005807572160847485 \t\n",
      "Epoch 41953 \t\t Training Loss: 0.0005807572160847485 \t\n",
      "Epoch 41954 \t\t Training Loss: 0.0005807571578770876 \t\n",
      "Epoch 41955 \t\t Training Loss: 0.0005807571578770876 \t\n",
      "Epoch 41956 \t\t Training Loss: 0.0005807571578770876 \t\n",
      "Epoch 41957 \t\t Training Loss: 0.0005807571578770876 \t\n",
      "Epoch 41958 \t\t Training Loss: 0.0005807571578770876 \t\n",
      "Epoch 41959 \t\t Training Loss: 0.0005807571578770876 \t\n",
      "Epoch 41960 \t\t Training Loss: 0.0005807570414617658 \t\n",
      "Epoch 41961 \t\t Training Loss: 0.0005807570414617658 \t\n",
      "Epoch 41962 \t\t Training Loss: 0.0005807570414617658 \t\n",
      "Epoch 41963 \t\t Training Loss: 0.0005807570414617658 \t\n",
      "Epoch 41964 \t\t Training Loss: 0.0005807570414617658 \t\n",
      "Epoch 41965 \t\t Training Loss: 0.0005807570414617658 \t\n",
      "Epoch 41966 \t\t Training Loss: 0.0005807570414617658 \t\n",
      "Epoch 41967 \t\t Training Loss: 0.0005807570414617658 \t\n",
      "Epoch 41968 \t\t Training Loss: 0.0005807570414617658 \t\n",
      "Epoch 41969 \t\t Training Loss: 0.0005807570414617658 \t\n",
      "Epoch 41970 \t\t Training Loss: 0.0005807570414617658 \t\n",
      "Epoch 41971 \t\t Training Loss: 0.0005807570414617658 \t\n",
      "Epoch 41972 \t\t Training Loss: 0.0005807570414617658 \t\n",
      "Epoch 41973 \t\t Training Loss: 0.0005807570414617658 \t\n",
      "Epoch 41974 \t\t Training Loss: 0.0005807570414617658 \t\n",
      "Epoch 41975 \t\t Training Loss: 0.0005807569250464439 \t\n",
      "Epoch 41976 \t\t Training Loss: 0.0005807569250464439 \t\n",
      "Epoch 41977 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 41978 \t\t Training Loss: 0.0005807569250464439 \t\n",
      "Epoch 41979 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 41980 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 41981 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 41982 \t\t Training Loss: 0.0005807569250464439 \t\n",
      "Epoch 41983 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 41984 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 41985 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 41986 \t\t Training Loss: 0.0005807569250464439 \t\n",
      "Epoch 41987 \t\t Training Loss: 0.0005807569250464439 \t\n",
      "Epoch 41988 \t\t Training Loss: 0.0005807569250464439 \t\n",
      "Epoch 41989 \t\t Training Loss: 0.0005807569250464439 \t\n",
      "Epoch 41990 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 41991 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 41992 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 41993 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 41994 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 41995 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 41996 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 41997 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 41998 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 41999 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42000 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42001 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42002 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42003 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42004 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42005 \t\t Training Loss: 0.0005807568086311221 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42006 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42007 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42008 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42009 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42010 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42011 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42012 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42013 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42014 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42015 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42016 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42017 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42018 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42019 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42020 \t\t Training Loss: 0.0005807568086311221 \t\n",
      "Epoch 42021 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42022 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42023 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42024 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42025 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42026 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42027 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42028 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42029 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42030 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42031 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42032 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42033 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42034 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42035 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42036 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42037 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42038 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42039 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42040 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42041 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42042 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42043 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42044 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42045 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42046 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42047 \t\t Training Loss: 0.0005807567504234612 \t\n",
      "Epoch 42048 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42049 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42050 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42051 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42052 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42053 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42054 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42055 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42056 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42057 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42058 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42059 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42060 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42061 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42062 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42063 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42064 \t\t Training Loss: 0.0005807566340081394 \t\n",
      "Epoch 42065 \t\t Training Loss: 0.0005807565175928175 \t\n",
      "Epoch 42066 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42067 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42068 \t\t Training Loss: 0.0005807565175928175 \t\n",
      "Epoch 42069 \t\t Training Loss: 0.0005807565175928175 \t\n",
      "Epoch 42070 \t\t Training Loss: 0.0005807565175928175 \t\n",
      "Epoch 42071 \t\t Training Loss: 0.0005807565175928175 \t\n",
      "Epoch 42072 \t\t Training Loss: 0.0005807565175928175 \t\n",
      "Epoch 42073 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42074 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42075 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42076 \t\t Training Loss: 0.0005807565175928175 \t\n",
      "Epoch 42077 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42078 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42079 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42080 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42081 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42082 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42083 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42084 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42085 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42086 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42087 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42088 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42089 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42090 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42091 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42092 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42093 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42094 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42095 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42096 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42097 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42098 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42099 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42100 \t\t Training Loss: 0.0005807564593851566 \t\n",
      "Epoch 42101 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42102 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42103 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42104 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42105 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42106 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42107 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42108 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42109 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42110 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42111 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42112 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42113 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42114 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42115 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42116 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42117 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42118 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42119 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42120 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42121 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42122 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42123 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42124 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42125 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42126 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42127 \t\t Training Loss: 0.0005807563429698348 \t\n",
      "Epoch 42128 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42129 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42130 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42131 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42132 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42133 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42134 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42135 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42136 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42137 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42138 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42139 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42140 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42141 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42142 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42143 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42144 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42145 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42146 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42147 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42148 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42149 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42150 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42151 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42152 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42153 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42154 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42155 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42156 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42157 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42158 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42159 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42160 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42161 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42162 \t\t Training Loss: 0.000580756226554513 \t\n",
      "Epoch 42163 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42164 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42165 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42166 \t\t Training Loss: 0.0005807561683468521 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42167 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42168 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42169 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42170 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42171 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42172 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42173 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42174 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42175 \t\t Training Loss: 0.0005807560519315302 \t\n",
      "Epoch 42176 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42177 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42178 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42179 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42180 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42181 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42182 \t\t Training Loss: 0.0005807560519315302 \t\n",
      "Epoch 42183 \t\t Training Loss: 0.0005807560519315302 \t\n",
      "Epoch 42184 \t\t Training Loss: 0.0005807560519315302 \t\n",
      "Epoch 42185 \t\t Training Loss: 0.0005807560519315302 \t\n",
      "Epoch 42186 \t\t Training Loss: 0.0005807560519315302 \t\n",
      "Epoch 42187 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42188 \t\t Training Loss: 0.0005807561683468521 \t\n",
      "Epoch 42189 \t\t Training Loss: 0.0005807560519315302 \t\n",
      "Epoch 42190 \t\t Training Loss: 0.0005807560519315302 \t\n",
      "Epoch 42191 \t\t Training Loss: 0.0005807560519315302 \t\n",
      "Epoch 42192 \t\t Training Loss: 0.0005807560519315302 \t\n",
      "Epoch 42193 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42194 \t\t Training Loss: 0.0005807560519315302 \t\n",
      "Epoch 42195 \t\t Training Loss: 0.0005807560519315302 \t\n",
      "Epoch 42196 \t\t Training Loss: 0.0005807560519315302 \t\n",
      "Epoch 42197 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42198 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42199 \t\t Training Loss: 0.0005807560519315302 \t\n",
      "Epoch 42200 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42201 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42202 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42203 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42204 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42205 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42206 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42207 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42208 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42209 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42210 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42211 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42212 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42213 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42214 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42215 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42216 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42217 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42218 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42219 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42220 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42221 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42222 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42223 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42224 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42225 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42226 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42227 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42228 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42229 \t\t Training Loss: 0.0005807559355162084 \t\n",
      "Epoch 42230 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42231 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42232 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42233 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42234 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42235 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42236 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42237 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42238 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42239 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42240 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42241 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42242 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42243 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42244 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42245 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42246 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42247 \t\t Training Loss: 0.0005807558191008866 \t\n",
      "Epoch 42248 \t\t Training Loss: 0.0005807557608932257 \t\n",
      "Epoch 42249 \t\t Training Loss: 0.0005807557608932257 \t\n",
      "Epoch 42250 \t\t Training Loss: 0.0005807557608932257 \t\n",
      "Epoch 42251 \t\t Training Loss: 0.0005807557608932257 \t\n",
      "Epoch 42252 \t\t Training Loss: 0.0005807557608932257 \t\n",
      "Epoch 42253 \t\t Training Loss: 0.0005807557608932257 \t\n",
      "Epoch 42254 \t\t Training Loss: 0.0005807557608932257 \t\n",
      "Epoch 42255 \t\t Training Loss: 0.0005807557608932257 \t\n",
      "Epoch 42256 \t\t Training Loss: 0.0005807557608932257 \t\n",
      "Epoch 42257 \t\t Training Loss: 0.0005807557608932257 \t\n",
      "Epoch 42258 \t\t Training Loss: 0.0005807556444779038 \t\n",
      "Epoch 42259 \t\t Training Loss: 0.0005807556444779038 \t\n",
      "Epoch 42260 \t\t Training Loss: 0.0005807556444779038 \t\n",
      "Epoch 42261 \t\t Training Loss: 0.0005807556444779038 \t\n",
      "Epoch 42262 \t\t Training Loss: 0.0005807556444779038 \t\n",
      "Epoch 42263 \t\t Training Loss: 0.0005807556444779038 \t\n",
      "Epoch 42264 \t\t Training Loss: 0.0005807556444779038 \t\n",
      "Epoch 42265 \t\t Training Loss: 0.0005807556444779038 \t\n",
      "Epoch 42266 \t\t Training Loss: 0.0005807556444779038 \t\n",
      "Epoch 42267 \t\t Training Loss: 0.0005807556444779038 \t\n",
      "Epoch 42268 \t\t Training Loss: 0.0005807556444779038 \t\n",
      "Epoch 42269 \t\t Training Loss: 0.0005807556444779038 \t\n",
      "Epoch 42270 \t\t Training Loss: 0.0005807556444779038 \t\n",
      "Epoch 42271 \t\t Training Loss: 0.0005807556444779038 \t\n",
      "Epoch 42272 \t\t Training Loss: 0.0005807556444779038 \t\n",
      "Epoch 42273 \t\t Training Loss: 0.0005807556444779038 \t\n",
      "Epoch 42274 \t\t Training Loss: 0.000580755528062582 \t\n",
      "Epoch 42275 \t\t Training Loss: 0.000580755528062582 \t\n",
      "Epoch 42276 \t\t Training Loss: 0.000580755528062582 \t\n",
      "Epoch 42277 \t\t Training Loss: 0.000580755528062582 \t\n",
      "Epoch 42278 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42279 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42280 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42281 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42282 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42283 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42284 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42285 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42286 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42287 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42288 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42289 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42290 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42291 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42292 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42293 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42294 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42295 \t\t Training Loss: 0.0005807554698549211 \t\n",
      "Epoch 42296 \t\t Training Loss: 0.0005807553534395993 \t\n",
      "Epoch 42297 \t\t Training Loss: 0.0005807553534395993 \t\n",
      "Epoch 42298 \t\t Training Loss: 0.0005807553534395993 \t\n",
      "Epoch 42299 \t\t Training Loss: 0.0005807553534395993 \t\n",
      "Epoch 42300 \t\t Training Loss: 0.0005807553534395993 \t\n",
      "Epoch 42301 \t\t Training Loss: 0.0005807553534395993 \t\n",
      "Epoch 42302 \t\t Training Loss: 0.0005807553534395993 \t\n",
      "Epoch 42303 \t\t Training Loss: 0.0005807553534395993 \t\n",
      "Epoch 42304 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42305 \t\t Training Loss: 0.0005807553534395993 \t\n",
      "Epoch 42306 \t\t Training Loss: 0.0005807553534395993 \t\n",
      "Epoch 42307 \t\t Training Loss: 0.0005807553534395993 \t\n",
      "Epoch 42308 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42309 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42310 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42311 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42312 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42313 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42314 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42315 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42316 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42317 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42318 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42319 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42320 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42321 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42322 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42323 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42324 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42325 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42326 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42327 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42328 \t\t Training Loss: 0.0005807552370242774 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42329 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42330 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42331 \t\t Training Loss: 0.0005807552370242774 \t\n",
      "Epoch 42332 \t\t Training Loss: 0.0005807551788166165 \t\n",
      "Epoch 42333 \t\t Training Loss: 0.0005807551206089556 \t\n",
      "Epoch 42334 \t\t Training Loss: 0.0005807551206089556 \t\n",
      "Epoch 42335 \t\t Training Loss: 0.0005807551788166165 \t\n",
      "Epoch 42336 \t\t Training Loss: 0.0005807551206089556 \t\n",
      "Epoch 42337 \t\t Training Loss: 0.0005807551788166165 \t\n",
      "Epoch 42338 \t\t Training Loss: 0.0005807551788166165 \t\n",
      "Epoch 42339 \t\t Training Loss: 0.0005807551206089556 \t\n",
      "Epoch 42340 \t\t Training Loss: 0.0005807551206089556 \t\n",
      "Epoch 42341 \t\t Training Loss: 0.0005807551206089556 \t\n",
      "Epoch 42342 \t\t Training Loss: 0.0005807550624012947 \t\n",
      "Epoch 42343 \t\t Training Loss: 0.0005807549459859729 \t\n",
      "Epoch 42344 \t\t Training Loss: 0.0005807549459859729 \t\n",
      "Epoch 42345 \t\t Training Loss: 0.0005807549459859729 \t\n",
      "Epoch 42346 \t\t Training Loss: 0.0005807549459859729 \t\n",
      "Epoch 42347 \t\t Training Loss: 0.0005807549459859729 \t\n",
      "Epoch 42348 \t\t Training Loss: 0.0005807549459859729 \t\n",
      "Epoch 42349 \t\t Training Loss: 0.0005807549459859729 \t\n",
      "Epoch 42350 \t\t Training Loss: 0.0005807549459859729 \t\n",
      "Epoch 42351 \t\t Training Loss: 0.0005807549459859729 \t\n",
      "Epoch 42352 \t\t Training Loss: 0.000580754887778312 \t\n",
      "Epoch 42353 \t\t Training Loss: 0.0005807549459859729 \t\n",
      "Epoch 42354 \t\t Training Loss: 0.000580754887778312 \t\n",
      "Epoch 42355 \t\t Training Loss: 0.000580754887778312 \t\n",
      "Epoch 42356 \t\t Training Loss: 0.000580754887778312 \t\n",
      "Epoch 42357 \t\t Training Loss: 0.000580754887778312 \t\n",
      "Epoch 42358 \t\t Training Loss: 0.000580754887778312 \t\n",
      "Epoch 42359 \t\t Training Loss: 0.000580754887778312 \t\n",
      "Epoch 42360 \t\t Training Loss: 0.000580754887778312 \t\n",
      "Epoch 42361 \t\t Training Loss: 0.000580754887778312 \t\n",
      "Epoch 42362 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42363 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42364 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42365 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42366 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42367 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42368 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42369 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42370 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42371 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42372 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42373 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42374 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42375 \t\t Training Loss: 0.000580754887778312 \t\n",
      "Epoch 42376 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42377 \t\t Training Loss: 0.000580754887778312 \t\n",
      "Epoch 42378 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42379 \t\t Training Loss: 0.000580754887778312 \t\n",
      "Epoch 42380 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42381 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42382 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42383 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42384 \t\t Training Loss: 0.000580754887778312 \t\n",
      "Epoch 42385 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42386 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42387 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42388 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42389 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42390 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42391 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42392 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42393 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42394 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42395 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42396 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42397 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42398 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42399 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42400 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42401 \t\t Training Loss: 0.0005807548295706511 \t\n",
      "Epoch 42402 \t\t Training Loss: 0.0005807547131553292 \t\n",
      "Epoch 42403 \t\t Training Loss: 0.0005807547131553292 \t\n",
      "Epoch 42404 \t\t Training Loss: 0.0005807547131553292 \t\n",
      "Epoch 42405 \t\t Training Loss: 0.0005807547131553292 \t\n",
      "Epoch 42406 \t\t Training Loss: 0.0005807547131553292 \t\n",
      "Epoch 42407 \t\t Training Loss: 0.0005807547131553292 \t\n",
      "Epoch 42408 \t\t Training Loss: 0.0005807547131553292 \t\n",
      "Epoch 42409 \t\t Training Loss: 0.0005807547131553292 \t\n",
      "Epoch 42410 \t\t Training Loss: 0.0005807547131553292 \t\n",
      "Epoch 42411 \t\t Training Loss: 0.0005807546549476683 \t\n",
      "Epoch 42412 \t\t Training Loss: 0.0005807546549476683 \t\n",
      "Epoch 42413 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42414 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42415 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42416 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42417 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42418 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42419 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42420 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42421 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42422 \t\t Training Loss: 0.0005807546549476683 \t\n",
      "Epoch 42423 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42424 \t\t Training Loss: 0.0005807546549476683 \t\n",
      "Epoch 42425 \t\t Training Loss: 0.0005807546549476683 \t\n",
      "Epoch 42426 \t\t Training Loss: 0.0005807546549476683 \t\n",
      "Epoch 42427 \t\t Training Loss: 0.0005807546549476683 \t\n",
      "Epoch 42428 \t\t Training Loss: 0.0005807546549476683 \t\n",
      "Epoch 42429 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42430 \t\t Training Loss: 0.0005807547131553292 \t\n",
      "Epoch 42431 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42432 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42433 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42434 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42435 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42436 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42437 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42438 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42439 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42440 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42441 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42442 \t\t Training Loss: 0.0005807545967400074 \t\n",
      "Epoch 42443 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42444 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42445 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42446 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42447 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42448 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42449 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42450 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42451 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42452 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42453 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42454 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42455 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42456 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42457 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42458 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42459 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42460 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42461 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42462 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42463 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42464 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42465 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42466 \t\t Training Loss: 0.0005807545385323465 \t\n",
      "Epoch 42467 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42468 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42469 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42470 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42471 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42472 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42473 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42474 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42475 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42476 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42477 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42478 \t\t Training Loss: 0.0005807544221170247 \t\n",
      "Epoch 42479 \t\t Training Loss: 0.0005807543639093637 \t\n",
      "Epoch 42480 \t\t Training Loss: 0.0005807543639093637 \t\n",
      "Epoch 42481 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42482 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42483 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42484 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42485 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42486 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42487 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42488 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42489 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42490 \t\t Training Loss: 0.0005807542474940419 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42491 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42492 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42493 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42494 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42495 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42496 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42497 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42498 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42499 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42500 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42501 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42502 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42503 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42504 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42505 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42506 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42507 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42508 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42509 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42510 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42511 \t\t Training Loss: 0.000580754189286381 \t\n",
      "Epoch 42512 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42513 \t\t Training Loss: 0.0005807542474940419 \t\n",
      "Epoch 42514 \t\t Training Loss: 0.000580754189286381 \t\n",
      "Epoch 42515 \t\t Training Loss: 0.000580754189286381 \t\n",
      "Epoch 42516 \t\t Training Loss: 0.000580754189286381 \t\n",
      "Epoch 42517 \t\t Training Loss: 0.0005807541310787201 \t\n",
      "Epoch 42518 \t\t Training Loss: 0.0005807541310787201 \t\n",
      "Epoch 42519 \t\t Training Loss: 0.0005807541310787201 \t\n",
      "Epoch 42520 \t\t Training Loss: 0.0005807541310787201 \t\n",
      "Epoch 42521 \t\t Training Loss: 0.0005807541310787201 \t\n",
      "Epoch 42522 \t\t Training Loss: 0.0005807541310787201 \t\n",
      "Epoch 42523 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42524 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42525 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42526 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42527 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42528 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42529 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42530 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42531 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42532 \t\t Training Loss: 0.0005807540146633983 \t\n",
      "Epoch 42533 \t\t Training Loss: 0.0005807540146633983 \t\n",
      "Epoch 42534 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42535 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42536 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42537 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42538 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42539 \t\t Training Loss: 0.0005807539564557374 \t\n",
      "Epoch 42540 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42541 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42542 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42543 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42544 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42545 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42546 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42547 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42548 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42549 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42550 \t\t Training Loss: 0.0005807540146633983 \t\n",
      "Epoch 42551 \t\t Training Loss: 0.0005807540728710592 \t\n",
      "Epoch 42552 \t\t Training Loss: 0.0005807540146633983 \t\n",
      "Epoch 42553 \t\t Training Loss: 0.0005807540146633983 \t\n",
      "Epoch 42554 \t\t Training Loss: 0.0005807539564557374 \t\n",
      "Epoch 42555 \t\t Training Loss: 0.0005807539564557374 \t\n",
      "Epoch 42556 \t\t Training Loss: 0.0005807540146633983 \t\n",
      "Epoch 42557 \t\t Training Loss: 0.0005807540146633983 \t\n",
      "Epoch 42558 \t\t Training Loss: 0.0005807539564557374 \t\n",
      "Epoch 42559 \t\t Training Loss: 0.0005807539564557374 \t\n",
      "Epoch 42560 \t\t Training Loss: 0.0005807539564557374 \t\n",
      "Epoch 42561 \t\t Training Loss: 0.0005807539564557374 \t\n",
      "Epoch 42562 \t\t Training Loss: 0.0005807539564557374 \t\n",
      "Epoch 42563 \t\t Training Loss: 0.0005807539564557374 \t\n",
      "Epoch 42564 \t\t Training Loss: 0.0005807539564557374 \t\n",
      "Epoch 42565 \t\t Training Loss: 0.0005807538400404155 \t\n",
      "Epoch 42566 \t\t Training Loss: 0.0005807539564557374 \t\n",
      "Epoch 42567 \t\t Training Loss: 0.0005807538400404155 \t\n",
      "Epoch 42568 \t\t Training Loss: 0.0005807538400404155 \t\n",
      "Epoch 42569 \t\t Training Loss: 0.0005807538400404155 \t\n",
      "Epoch 42570 \t\t Training Loss: 0.0005807538400404155 \t\n",
      "Epoch 42571 \t\t Training Loss: 0.0005807538400404155 \t\n",
      "Epoch 42572 \t\t Training Loss: 0.0005807538400404155 \t\n",
      "Epoch 42573 \t\t Training Loss: 0.0005807538400404155 \t\n",
      "Epoch 42574 \t\t Training Loss: 0.0005807538400404155 \t\n",
      "Epoch 42575 \t\t Training Loss: 0.0005807538400404155 \t\n",
      "Epoch 42576 \t\t Training Loss: 0.0005807537818327546 \t\n",
      "Epoch 42577 \t\t Training Loss: 0.0005807537818327546 \t\n",
      "Epoch 42578 \t\t Training Loss: 0.0005807537818327546 \t\n",
      "Epoch 42579 \t\t Training Loss: 0.0005807537818327546 \t\n",
      "Epoch 42580 \t\t Training Loss: 0.0005807537236250937 \t\n",
      "Epoch 42581 \t\t Training Loss: 0.0005807537236250937 \t\n",
      "Epoch 42582 \t\t Training Loss: 0.0005807537236250937 \t\n",
      "Epoch 42583 \t\t Training Loss: 0.0005807537236250937 \t\n",
      "Epoch 42584 \t\t Training Loss: 0.0005807537236250937 \t\n",
      "Epoch 42585 \t\t Training Loss: 0.0005807537236250937 \t\n",
      "Epoch 42586 \t\t Training Loss: 0.0005807537236250937 \t\n",
      "Epoch 42587 \t\t Training Loss: 0.0005807537236250937 \t\n",
      "Epoch 42588 \t\t Training Loss: 0.0005807537818327546 \t\n",
      "Epoch 42589 \t\t Training Loss: 0.0005807537818327546 \t\n",
      "Epoch 42590 \t\t Training Loss: 0.0005807537236250937 \t\n",
      "Epoch 42591 \t\t Training Loss: 0.0005807537236250937 \t\n",
      "Epoch 42592 \t\t Training Loss: 0.0005807537818327546 \t\n",
      "Epoch 42593 \t\t Training Loss: 0.0005807537818327546 \t\n",
      "Epoch 42594 \t\t Training Loss: 0.0005807537236250937 \t\n",
      "Epoch 42595 \t\t Training Loss: 0.0005807537818327546 \t\n",
      "Epoch 42596 \t\t Training Loss: 0.0005807537236250937 \t\n",
      "Epoch 42597 \t\t Training Loss: 0.0005807537236250937 \t\n",
      "Epoch 42598 \t\t Training Loss: 0.0005807537236250937 \t\n",
      "Epoch 42599 \t\t Training Loss: 0.0005807537236250937 \t\n",
      "Epoch 42600 \t\t Training Loss: 0.0005807536654174328 \t\n",
      "Epoch 42601 \t\t Training Loss: 0.0005807536654174328 \t\n",
      "Epoch 42602 \t\t Training Loss: 0.0005807536654174328 \t\n",
      "Epoch 42603 \t\t Training Loss: 0.0005807536072097719 \t\n",
      "Epoch 42604 \t\t Training Loss: 0.0005807536654174328 \t\n",
      "Epoch 42605 \t\t Training Loss: 0.0005807536072097719 \t\n",
      "Epoch 42606 \t\t Training Loss: 0.0005807536072097719 \t\n",
      "Epoch 42607 \t\t Training Loss: 0.0005807536654174328 \t\n",
      "Epoch 42608 \t\t Training Loss: 0.0005807536072097719 \t\n",
      "Epoch 42609 \t\t Training Loss: 0.0005807536072097719 \t\n",
      "Epoch 42610 \t\t Training Loss: 0.0005807536072097719 \t\n",
      "Epoch 42611 \t\t Training Loss: 0.0005807536654174328 \t\n",
      "Epoch 42612 \t\t Training Loss: 0.0005807536654174328 \t\n",
      "Epoch 42613 \t\t Training Loss: 0.0005807536654174328 \t\n",
      "Epoch 42614 \t\t Training Loss: 0.0005807536654174328 \t\n",
      "Epoch 42615 \t\t Training Loss: 0.0005807536654174328 \t\n",
      "Epoch 42616 \t\t Training Loss: 0.0005807536654174328 \t\n",
      "Epoch 42617 \t\t Training Loss: 0.0005807536654174328 \t\n",
      "Epoch 42618 \t\t Training Loss: 0.0005807536654174328 \t\n",
      "Epoch 42619 \t\t Training Loss: 0.0005807536654174328 \t\n",
      "Epoch 42620 \t\t Training Loss: 0.0005807536654174328 \t\n",
      "Epoch 42621 \t\t Training Loss: 0.0005807536654174328 \t\n",
      "Epoch 42622 \t\t Training Loss: 0.0005807536072097719 \t\n",
      "Epoch 42623 \t\t Training Loss: 0.0005807536072097719 \t\n",
      "Epoch 42624 \t\t Training Loss: 0.0005807536072097719 \t\n",
      "Epoch 42625 \t\t Training Loss: 0.0005807536072097719 \t\n",
      "Epoch 42626 \t\t Training Loss: 0.0005807536072097719 \t\n",
      "Epoch 42627 \t\t Training Loss: 0.0005807536072097719 \t\n",
      "Epoch 42628 \t\t Training Loss: 0.000580753549002111 \t\n",
      "Epoch 42629 \t\t Training Loss: 0.000580753549002111 \t\n",
      "Epoch 42630 \t\t Training Loss: 0.000580753549002111 \t\n",
      "Epoch 42631 \t\t Training Loss: 0.000580753549002111 \t\n",
      "Epoch 42632 \t\t Training Loss: 0.000580753549002111 \t\n",
      "Epoch 42633 \t\t Training Loss: 0.000580753549002111 \t\n",
      "Epoch 42634 \t\t Training Loss: 0.000580753549002111 \t\n",
      "Epoch 42635 \t\t Training Loss: 0.000580753549002111 \t\n",
      "Epoch 42636 \t\t Training Loss: 0.00058075349079445 \t\n",
      "Epoch 42637 \t\t Training Loss: 0.00058075349079445 \t\n",
      "Epoch 42638 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42639 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42640 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42641 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42642 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42643 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42644 \t\t Training Loss: 0.0005807533743791282 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42645 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42646 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42647 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42648 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42649 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42650 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42651 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42652 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42653 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42654 \t\t Training Loss: 0.0005807533161714673 \t\n",
      "Epoch 42655 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42656 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42657 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42658 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42659 \t\t Training Loss: 0.0005807533161714673 \t\n",
      "Epoch 42660 \t\t Training Loss: 0.0005807533743791282 \t\n",
      "Epoch 42661 \t\t Training Loss: 0.0005807533161714673 \t\n",
      "Epoch 42662 \t\t Training Loss: 0.0005807533161714673 \t\n",
      "Epoch 42663 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42664 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42665 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42666 \t\t Training Loss: 0.0005807533161714673 \t\n",
      "Epoch 42667 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42668 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42669 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42670 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42671 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42672 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42673 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42674 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42675 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42676 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42677 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42678 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42679 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42680 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42681 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42682 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42683 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42684 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42685 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42686 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42687 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42688 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42689 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42690 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42691 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42692 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42693 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42694 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42695 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42696 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42697 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42698 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42699 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42700 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42701 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42702 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42703 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42704 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42705 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42706 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42707 \t\t Training Loss: 0.0005807531997561455 \t\n",
      "Epoch 42708 \t\t Training Loss: 0.0005807531997561455 \t\n",
      "Epoch 42709 \t\t Training Loss: 0.0005807531997561455 \t\n",
      "Epoch 42710 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42711 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42712 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42713 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42714 \t\t Training Loss: 0.0005807531997561455 \t\n",
      "Epoch 42715 \t\t Training Loss: 0.0005807532579638064 \t\n",
      "Epoch 42716 \t\t Training Loss: 0.0005807531997561455 \t\n",
      "Epoch 42717 \t\t Training Loss: 0.0005807531997561455 \t\n",
      "Epoch 42718 \t\t Training Loss: 0.0005807531997561455 \t\n",
      "Epoch 42719 \t\t Training Loss: 0.0005807531997561455 \t\n",
      "Epoch 42720 \t\t Training Loss: 0.0005807531997561455 \t\n",
      "Epoch 42721 \t\t Training Loss: 0.0005807531997561455 \t\n",
      "Epoch 42722 \t\t Training Loss: 0.0005807531997561455 \t\n",
      "Epoch 42723 \t\t Training Loss: 0.0005807531997561455 \t\n",
      "Epoch 42724 \t\t Training Loss: 0.0005807531997561455 \t\n",
      "Epoch 42725 \t\t Training Loss: 0.0005807531997561455 \t\n",
      "Epoch 42726 \t\t Training Loss: 0.0005807531415484846 \t\n",
      "Epoch 42727 \t\t Training Loss: 0.0005807531415484846 \t\n",
      "Epoch 42728 \t\t Training Loss: 0.0005807531997561455 \t\n",
      "Epoch 42729 \t\t Training Loss: 0.0005807531415484846 \t\n",
      "Epoch 42730 \t\t Training Loss: 0.0005807531415484846 \t\n",
      "Epoch 42731 \t\t Training Loss: 0.0005807530833408237 \t\n",
      "Epoch 42732 \t\t Training Loss: 0.0005807531415484846 \t\n",
      "Epoch 42733 \t\t Training Loss: 0.0005807530833408237 \t\n",
      "Epoch 42734 \t\t Training Loss: 0.0005807530833408237 \t\n",
      "Epoch 42735 \t\t Training Loss: 0.0005807530833408237 \t\n",
      "Epoch 42736 \t\t Training Loss: 0.0005807530251331627 \t\n",
      "Epoch 42737 \t\t Training Loss: 0.0005807530833408237 \t\n",
      "Epoch 42738 \t\t Training Loss: 0.0005807530251331627 \t\n",
      "Epoch 42739 \t\t Training Loss: 0.0005807530251331627 \t\n",
      "Epoch 42740 \t\t Training Loss: 0.0005807530251331627 \t\n",
      "Epoch 42741 \t\t Training Loss: 0.0005807530251331627 \t\n",
      "Epoch 42742 \t\t Training Loss: 0.0005807530251331627 \t\n",
      "Epoch 42743 \t\t Training Loss: 0.0005807530251331627 \t\n",
      "Epoch 42744 \t\t Training Loss: 0.0005807530251331627 \t\n",
      "Epoch 42745 \t\t Training Loss: 0.0005807530251331627 \t\n",
      "Epoch 42746 \t\t Training Loss: 0.0005807530251331627 \t\n",
      "Epoch 42747 \t\t Training Loss: 0.0005807530251331627 \t\n",
      "Epoch 42748 \t\t Training Loss: 0.0005807529669255018 \t\n",
      "Epoch 42749 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42750 \t\t Training Loss: 0.0005807529669255018 \t\n",
      "Epoch 42751 \t\t Training Loss: 0.0005807529669255018 \t\n",
      "Epoch 42752 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42753 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42754 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42755 \t\t Training Loss: 0.0005807529669255018 \t\n",
      "Epoch 42756 \t\t Training Loss: 0.0005807529669255018 \t\n",
      "Epoch 42757 \t\t Training Loss: 0.0005807529669255018 \t\n",
      "Epoch 42758 \t\t Training Loss: 0.0005807529669255018 \t\n",
      "Epoch 42759 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42760 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42761 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42762 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42763 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42764 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42765 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42766 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42767 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42768 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42769 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42770 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42771 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42772 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42773 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42774 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42775 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42776 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42777 \t\t Training Loss: 0.0005807529087178409 \t\n",
      "Epoch 42778 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42779 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42780 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42781 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42782 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42783 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42784 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42785 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42786 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42787 \t\t Training Loss: 0.0005807527340948582 \t\n",
      "Epoch 42788 \t\t Training Loss: 0.0005807527340948582 \t\n",
      "Epoch 42789 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42790 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42791 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42792 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42793 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42794 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42795 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42796 \t\t Training Loss: 0.0005807527340948582 \t\n",
      "Epoch 42797 \t\t Training Loss: 0.0005807527340948582 \t\n",
      "Epoch 42798 \t\t Training Loss: 0.0005807527340948582 \t\n",
      "Epoch 42799 \t\t Training Loss: 0.0005807526758871973 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42800 \t\t Training Loss: 0.0005807526758871973 \t\n",
      "Epoch 42801 \t\t Training Loss: 0.0005807526758871973 \t\n",
      "Epoch 42802 \t\t Training Loss: 0.0005807526758871973 \t\n",
      "Epoch 42803 \t\t Training Loss: 0.0005807527340948582 \t\n",
      "Epoch 42804 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42805 \t\t Training Loss: 0.0005807526758871973 \t\n",
      "Epoch 42806 \t\t Training Loss: 0.0005807526758871973 \t\n",
      "Epoch 42807 \t\t Training Loss: 0.0005807526758871973 \t\n",
      "Epoch 42808 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42809 \t\t Training Loss: 0.0005807527340948582 \t\n",
      "Epoch 42810 \t\t Training Loss: 0.0005807527340948582 \t\n",
      "Epoch 42811 \t\t Training Loss: 0.0005807527923025191 \t\n",
      "Epoch 42812 \t\t Training Loss: 0.0005807527340948582 \t\n",
      "Epoch 42813 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42814 \t\t Training Loss: 0.0005807527340948582 \t\n",
      "Epoch 42815 \t\t Training Loss: 0.0005807527340948582 \t\n",
      "Epoch 42816 \t\t Training Loss: 0.0005807527340948582 \t\n",
      "Epoch 42817 \t\t Training Loss: 0.0005807526758871973 \t\n",
      "Epoch 42818 \t\t Training Loss: 0.0005807526758871973 \t\n",
      "Epoch 42819 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42820 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42821 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42822 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42823 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42824 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42825 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42826 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42827 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42828 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42829 \t\t Training Loss: 0.0005807526758871973 \t\n",
      "Epoch 42830 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42831 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42832 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42833 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42834 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42835 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42836 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42837 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42838 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42839 \t\t Training Loss: 0.0005807526176795363 \t\n",
      "Epoch 42840 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42841 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42842 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42843 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42844 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42845 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42846 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42847 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42848 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42849 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42850 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42851 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42852 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42853 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42854 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42855 \t\t Training Loss: 0.0005807525012642145 \t\n",
      "Epoch 42856 \t\t Training Loss: 0.0005807525012642145 \t\n",
      "Epoch 42857 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42858 \t\t Training Loss: 0.0005807525594718754 \t\n",
      "Epoch 42859 \t\t Training Loss: 0.0005807525012642145 \t\n",
      "Epoch 42860 \t\t Training Loss: 0.0005807525012642145 \t\n",
      "Epoch 42861 \t\t Training Loss: 0.0005807525012642145 \t\n",
      "Epoch 42862 \t\t Training Loss: 0.0005807525012642145 \t\n",
      "Epoch 42863 \t\t Training Loss: 0.0005807525012642145 \t\n",
      "Epoch 42864 \t\t Training Loss: 0.0005807525012642145 \t\n",
      "Epoch 42865 \t\t Training Loss: 0.0005807525012642145 \t\n",
      "Epoch 42866 \t\t Training Loss: 0.0005807525012642145 \t\n",
      "Epoch 42867 \t\t Training Loss: 0.0005807524430565536 \t\n",
      "Epoch 42868 \t\t Training Loss: 0.0005807524430565536 \t\n",
      "Epoch 42869 \t\t Training Loss: 0.0005807523848488927 \t\n",
      "Epoch 42870 \t\t Training Loss: 0.0005807523848488927 \t\n",
      "Epoch 42871 \t\t Training Loss: 0.0005807523848488927 \t\n",
      "Epoch 42872 \t\t Training Loss: 0.0005807523848488927 \t\n",
      "Epoch 42873 \t\t Training Loss: 0.0005807523848488927 \t\n",
      "Epoch 42874 \t\t Training Loss: 0.0005807523848488927 \t\n",
      "Epoch 42875 \t\t Training Loss: 0.0005807523848488927 \t\n",
      "Epoch 42876 \t\t Training Loss: 0.0005807523848488927 \t\n",
      "Epoch 42877 \t\t Training Loss: 0.0005807523848488927 \t\n",
      "Epoch 42878 \t\t Training Loss: 0.0005807523848488927 \t\n",
      "Epoch 42879 \t\t Training Loss: 0.0005807523848488927 \t\n",
      "Epoch 42880 \t\t Training Loss: 0.0005807523848488927 \t\n",
      "Epoch 42881 \t\t Training Loss: 0.0005807523848488927 \t\n",
      "Epoch 42882 \t\t Training Loss: 0.0005807523848488927 \t\n",
      "Epoch 42883 \t\t Training Loss: 0.0005807523266412318 \t\n",
      "Epoch 42884 \t\t Training Loss: 0.0005807523266412318 \t\n",
      "Epoch 42885 \t\t Training Loss: 0.0005807523266412318 \t\n",
      "Epoch 42886 \t\t Training Loss: 0.0005807523266412318 \t\n",
      "Epoch 42887 \t\t Training Loss: 0.0005807523266412318 \t\n",
      "Epoch 42888 \t\t Training Loss: 0.0005807523266412318 \t\n",
      "Epoch 42889 \t\t Training Loss: 0.0005807523266412318 \t\n",
      "Epoch 42890 \t\t Training Loss: 0.0005807523266412318 \t\n",
      "Epoch 42891 \t\t Training Loss: 0.0005807523266412318 \t\n",
      "Epoch 42892 \t\t Training Loss: 0.0005807523266412318 \t\n",
      "Epoch 42893 \t\t Training Loss: 0.0005807523266412318 \t\n",
      "Epoch 42894 \t\t Training Loss: 0.0005807523266412318 \t\n",
      "Epoch 42895 \t\t Training Loss: 0.0005807523266412318 \t\n",
      "Epoch 42896 \t\t Training Loss: 0.0005807523266412318 \t\n",
      "Epoch 42897 \t\t Training Loss: 0.0005807523266412318 \t\n",
      "Epoch 42898 \t\t Training Loss: 0.00058075221022591 \t\n",
      "Epoch 42899 \t\t Training Loss: 0.00058075221022591 \t\n",
      "Epoch 42900 \t\t Training Loss: 0.00058075221022591 \t\n",
      "Epoch 42901 \t\t Training Loss: 0.00058075221022591 \t\n",
      "Epoch 42902 \t\t Training Loss: 0.00058075221022591 \t\n",
      "Epoch 42903 \t\t Training Loss: 0.00058075221022591 \t\n",
      "Epoch 42904 \t\t Training Loss: 0.00058075221022591 \t\n",
      "Epoch 42905 \t\t Training Loss: 0.00058075221022591 \t\n",
      "Epoch 42906 \t\t Training Loss: 0.00058075221022591 \t\n",
      "Epoch 42907 \t\t Training Loss: 0.00058075221022591 \t\n",
      "Epoch 42908 \t\t Training Loss: 0.00058075221022591 \t\n",
      "Epoch 42909 \t\t Training Loss: 0.00058075221022591 \t\n",
      "Epoch 42910 \t\t Training Loss: 0.0005807522684335709 \t\n",
      "Epoch 42911 \t\t Training Loss: 0.00058075221022591 \t\n",
      "Epoch 42912 \t\t Training Loss: 0.00058075221022591 \t\n",
      "Epoch 42913 \t\t Training Loss: 0.000580752152018249 \t\n",
      "Epoch 42914 \t\t Training Loss: 0.0005807520938105881 \t\n",
      "Epoch 42915 \t\t Training Loss: 0.0005807520938105881 \t\n",
      "Epoch 42916 \t\t Training Loss: 0.0005807520938105881 \t\n",
      "Epoch 42917 \t\t Training Loss: 0.0005807520938105881 \t\n",
      "Epoch 42918 \t\t Training Loss: 0.0005807520938105881 \t\n",
      "Epoch 42919 \t\t Training Loss: 0.0005807520938105881 \t\n",
      "Epoch 42920 \t\t Training Loss: 0.0005807520356029272 \t\n",
      "Epoch 42921 \t\t Training Loss: 0.0005807520356029272 \t\n",
      "Epoch 42922 \t\t Training Loss: 0.0005807520938105881 \t\n",
      "Epoch 42923 \t\t Training Loss: 0.0005807520356029272 \t\n",
      "Epoch 42924 \t\t Training Loss: 0.0005807520356029272 \t\n",
      "Epoch 42925 \t\t Training Loss: 0.0005807520356029272 \t\n",
      "Epoch 42926 \t\t Training Loss: 0.0005807520356029272 \t\n",
      "Epoch 42927 \t\t Training Loss: 0.0005807520356029272 \t\n",
      "Epoch 42928 \t\t Training Loss: 0.0005807519773952663 \t\n",
      "Epoch 42929 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42930 \t\t Training Loss: 0.0005807519773952663 \t\n",
      "Epoch 42931 \t\t Training Loss: 0.0005807519773952663 \t\n",
      "Epoch 42932 \t\t Training Loss: 0.0005807519773952663 \t\n",
      "Epoch 42933 \t\t Training Loss: 0.0005807519773952663 \t\n",
      "Epoch 42934 \t\t Training Loss: 0.0005807519773952663 \t\n",
      "Epoch 42935 \t\t Training Loss: 0.0005807519773952663 \t\n",
      "Epoch 42936 \t\t Training Loss: 0.0005807519773952663 \t\n",
      "Epoch 42937 \t\t Training Loss: 0.0005807519773952663 \t\n",
      "Epoch 42938 \t\t Training Loss: 0.0005807519773952663 \t\n",
      "Epoch 42939 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42940 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42941 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42942 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42943 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42944 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42945 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42946 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42947 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42948 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42949 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42950 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42951 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42952 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42953 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42954 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42955 \t\t Training Loss: 0.0005807519191876054 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42956 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42957 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42958 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42959 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42960 \t\t Training Loss: 0.0005807518609799445 \t\n",
      "Epoch 42961 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42962 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42963 \t\t Training Loss: 0.0005807518609799445 \t\n",
      "Epoch 42964 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42965 \t\t Training Loss: 0.0005807518609799445 \t\n",
      "Epoch 42966 \t\t Training Loss: 0.0005807518609799445 \t\n",
      "Epoch 42967 \t\t Training Loss: 0.0005807518609799445 \t\n",
      "Epoch 42968 \t\t Training Loss: 0.0005807518609799445 \t\n",
      "Epoch 42969 \t\t Training Loss: 0.0005807518609799445 \t\n",
      "Epoch 42970 \t\t Training Loss: 0.0005807518609799445 \t\n",
      "Epoch 42971 \t\t Training Loss: 0.0005807519191876054 \t\n",
      "Epoch 42972 \t\t Training Loss: 0.0005807518609799445 \t\n",
      "Epoch 42973 \t\t Training Loss: 0.0005807518027722836 \t\n",
      "Epoch 42974 \t\t Training Loss: 0.0005807518027722836 \t\n",
      "Epoch 42975 \t\t Training Loss: 0.0005807518609799445 \t\n",
      "Epoch 42976 \t\t Training Loss: 0.0005807518609799445 \t\n",
      "Epoch 42977 \t\t Training Loss: 0.0005807518027722836 \t\n",
      "Epoch 42978 \t\t Training Loss: 0.0005807518027722836 \t\n",
      "Epoch 42979 \t\t Training Loss: 0.0005807518027722836 \t\n",
      "Epoch 42980 \t\t Training Loss: 0.0005807518027722836 \t\n",
      "Epoch 42981 \t\t Training Loss: 0.0005807518027722836 \t\n",
      "Epoch 42982 \t\t Training Loss: 0.0005807518027722836 \t\n",
      "Epoch 42983 \t\t Training Loss: 0.0005807518027722836 \t\n",
      "Epoch 42984 \t\t Training Loss: 0.0005807518027722836 \t\n",
      "Epoch 42985 \t\t Training Loss: 0.0005807518027722836 \t\n",
      "Epoch 42986 \t\t Training Loss: 0.0005807518027722836 \t\n",
      "Epoch 42987 \t\t Training Loss: 0.0005807518027722836 \t\n",
      "Epoch 42988 \t\t Training Loss: 0.0005807518027722836 \t\n",
      "Epoch 42989 \t\t Training Loss: 0.0005807518027722836 \t\n",
      "Epoch 42990 \t\t Training Loss: 0.0005807516863569617 \t\n",
      "Epoch 42991 \t\t Training Loss: 0.0005807516863569617 \t\n",
      "Epoch 42992 \t\t Training Loss: 0.0005807518027722836 \t\n",
      "Epoch 42993 \t\t Training Loss: 0.0005807516863569617 \t\n",
      "Epoch 42994 \t\t Training Loss: 0.0005807516281493008 \t\n",
      "Epoch 42995 \t\t Training Loss: 0.0005807516281493008 \t\n",
      "Epoch 42996 \t\t Training Loss: 0.0005807516281493008 \t\n",
      "Epoch 42997 \t\t Training Loss: 0.0005807516281493008 \t\n",
      "Epoch 42998 \t\t Training Loss: 0.0005807516281493008 \t\n",
      "Epoch 42999 \t\t Training Loss: 0.0005807516281493008 \t\n",
      "Epoch 43000 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43001 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43002 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43003 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43004 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43005 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43006 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43007 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43008 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43009 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43010 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43011 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43012 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43013 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43014 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43015 \t\t Training Loss: 0.000580751511733979 \t\n",
      "Epoch 43016 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43017 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43018 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43019 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43020 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43021 \t\t Training Loss: 0.000580751511733979 \t\n",
      "Epoch 43022 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43023 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43024 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43025 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43026 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43027 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43028 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43029 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43030 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43031 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43032 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43033 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43034 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43035 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43036 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43037 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43038 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43039 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43040 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43041 \t\t Training Loss: 0.000580751511733979 \t\n",
      "Epoch 43042 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43043 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43044 \t\t Training Loss: 0.0005807515699416399 \t\n",
      "Epoch 43045 \t\t Training Loss: 0.000580751511733979 \t\n",
      "Epoch 43046 \t\t Training Loss: 0.000580751511733979 \t\n",
      "Epoch 43047 \t\t Training Loss: 0.000580751511733979 \t\n",
      "Epoch 43048 \t\t Training Loss: 0.000580751511733979 \t\n",
      "Epoch 43049 \t\t Training Loss: 0.000580751511733979 \t\n",
      "Epoch 43050 \t\t Training Loss: 0.000580751511733979 \t\n",
      "Epoch 43051 \t\t Training Loss: 0.000580751511733979 \t\n",
      "Epoch 43052 \t\t Training Loss: 0.0005807514535263181 \t\n",
      "Epoch 43053 \t\t Training Loss: 0.0005807514535263181 \t\n",
      "Epoch 43054 \t\t Training Loss: 0.0005807514535263181 \t\n",
      "Epoch 43055 \t\t Training Loss: 0.0005807513953186572 \t\n",
      "Epoch 43056 \t\t Training Loss: 0.0005807514535263181 \t\n",
      "Epoch 43057 \t\t Training Loss: 0.0005807514535263181 \t\n",
      "Epoch 43058 \t\t Training Loss: 0.0005807513953186572 \t\n",
      "Epoch 43059 \t\t Training Loss: 0.0005807513953186572 \t\n",
      "Epoch 43060 \t\t Training Loss: 0.0005807513953186572 \t\n",
      "Epoch 43061 \t\t Training Loss: 0.0005807513953186572 \t\n",
      "Epoch 43062 \t\t Training Loss: 0.0005807513953186572 \t\n",
      "Epoch 43063 \t\t Training Loss: 0.0005807513953186572 \t\n",
      "Epoch 43064 \t\t Training Loss: 0.0005807513953186572 \t\n",
      "Epoch 43065 \t\t Training Loss: 0.0005807513953186572 \t\n",
      "Epoch 43066 \t\t Training Loss: 0.0005807513953186572 \t\n",
      "Epoch 43067 \t\t Training Loss: 0.0005807513953186572 \t\n",
      "Epoch 43068 \t\t Training Loss: 0.0005807513953186572 \t\n",
      "Epoch 43069 \t\t Training Loss: 0.0005807513953186572 \t\n",
      "Epoch 43070 \t\t Training Loss: 0.0005807513953186572 \t\n",
      "Epoch 43071 \t\t Training Loss: 0.0005807512789033353 \t\n",
      "Epoch 43072 \t\t Training Loss: 0.0005807512789033353 \t\n",
      "Epoch 43073 \t\t Training Loss: 0.0005807512789033353 \t\n",
      "Epoch 43074 \t\t Training Loss: 0.0005807512789033353 \t\n",
      "Epoch 43075 \t\t Training Loss: 0.0005807512789033353 \t\n",
      "Epoch 43076 \t\t Training Loss: 0.0005807512789033353 \t\n",
      "Epoch 43077 \t\t Training Loss: 0.0005807511624880135 \t\n",
      "Epoch 43078 \t\t Training Loss: 0.0005807511624880135 \t\n",
      "Epoch 43079 \t\t Training Loss: 0.0005807511624880135 \t\n",
      "Epoch 43080 \t\t Training Loss: 0.0005807511624880135 \t\n",
      "Epoch 43081 \t\t Training Loss: 0.0005807511624880135 \t\n",
      "Epoch 43082 \t\t Training Loss: 0.0005807511624880135 \t\n",
      "Epoch 43083 \t\t Training Loss: 0.0005807511624880135 \t\n",
      "Epoch 43084 \t\t Training Loss: 0.0005807511624880135 \t\n",
      "Epoch 43085 \t\t Training Loss: 0.0005807511624880135 \t\n",
      "Epoch 43086 \t\t Training Loss: 0.0005807511624880135 \t\n",
      "Epoch 43087 \t\t Training Loss: 0.0005807511624880135 \t\n",
      "Epoch 43088 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43089 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43090 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43091 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43092 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43093 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43094 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43095 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43096 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43097 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43098 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43099 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43100 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43101 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43102 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43103 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43104 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43105 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43106 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43107 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43108 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43109 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43110 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43111 \t\t Training Loss: 0.0005807509878650308 \t\n",
      "Epoch 43112 \t\t Training Loss: 0.0005807509878650308 \t\n",
      "Epoch 43113 \t\t Training Loss: 0.0005807511042803526 \t\n",
      "Epoch 43114 \t\t Training Loss: 0.0005807509878650308 \t\n",
      "Epoch 43115 \t\t Training Loss: 0.0005807509878650308 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43116 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43117 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43118 \t\t Training Loss: 0.0005807509878650308 \t\n",
      "Epoch 43119 \t\t Training Loss: 0.0005807509878650308 \t\n",
      "Epoch 43120 \t\t Training Loss: 0.0005807509878650308 \t\n",
      "Epoch 43121 \t\t Training Loss: 0.0005807509878650308 \t\n",
      "Epoch 43122 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43123 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43124 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43125 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43126 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43127 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43128 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43129 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43130 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43131 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43132 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43133 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43134 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43135 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43136 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43137 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43138 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43139 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43140 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43141 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43142 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43143 \t\t Training Loss: 0.0005807509296573699 \t\n",
      "Epoch 43144 \t\t Training Loss: 0.0005807508714497089 \t\n",
      "Epoch 43145 \t\t Training Loss: 0.0005807508714497089 \t\n",
      "Epoch 43146 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43147 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43148 \t\t Training Loss: 0.0005807508714497089 \t\n",
      "Epoch 43149 \t\t Training Loss: 0.0005807508714497089 \t\n",
      "Epoch 43150 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43151 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43152 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43153 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43154 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43155 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43156 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43157 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43158 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43159 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43160 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43161 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43162 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43163 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43164 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43165 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43166 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43167 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43168 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43169 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43170 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43171 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43172 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43173 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43174 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43175 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43176 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43177 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43178 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43179 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43180 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43181 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43182 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43183 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43184 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43185 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43186 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43187 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43188 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43189 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43190 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43191 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43192 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43193 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43194 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43195 \t\t Training Loss: 0.000580750813242048 \t\n",
      "Epoch 43196 \t\t Training Loss: 0.0005807506968267262 \t\n",
      "Epoch 43197 \t\t Training Loss: 0.0005807506968267262 \t\n",
      "Epoch 43198 \t\t Training Loss: 0.0005807506968267262 \t\n",
      "Epoch 43199 \t\t Training Loss: 0.0005807506968267262 \t\n",
      "Epoch 43200 \t\t Training Loss: 0.0005807506968267262 \t\n",
      "Epoch 43201 \t\t Training Loss: 0.0005807506968267262 \t\n",
      "Epoch 43202 \t\t Training Loss: 0.0005807506968267262 \t\n",
      "Epoch 43203 \t\t Training Loss: 0.0005807505804114044 \t\n",
      "Epoch 43204 \t\t Training Loss: 0.0005807505804114044 \t\n",
      "Epoch 43205 \t\t Training Loss: 0.0005807506968267262 \t\n",
      "Epoch 43206 \t\t Training Loss: 0.0005807506968267262 \t\n",
      "Epoch 43207 \t\t Training Loss: 0.0005807506968267262 \t\n",
      "Epoch 43208 \t\t Training Loss: 0.0005807505804114044 \t\n",
      "Epoch 43209 \t\t Training Loss: 0.0005807505804114044 \t\n",
      "Epoch 43210 \t\t Training Loss: 0.0005807505804114044 \t\n",
      "Epoch 43211 \t\t Training Loss: 0.0005807505804114044 \t\n",
      "Epoch 43212 \t\t Training Loss: 0.0005807505804114044 \t\n",
      "Epoch 43213 \t\t Training Loss: 0.0005807505804114044 \t\n",
      "Epoch 43214 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43215 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43216 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43217 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43218 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43219 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43220 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43221 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43222 \t\t Training Loss: 0.0005807505804114044 \t\n",
      "Epoch 43223 \t\t Training Loss: 0.0005807505804114044 \t\n",
      "Epoch 43224 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43225 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43226 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43227 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43228 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43229 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43230 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43231 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43232 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43233 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43234 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43235 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43236 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43237 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43238 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43239 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43240 \t\t Training Loss: 0.0005807505222037435 \t\n",
      "Epoch 43241 \t\t Training Loss: 0.0005807504057884216 \t\n",
      "Epoch 43242 \t\t Training Loss: 0.0005807504057884216 \t\n",
      "Epoch 43243 \t\t Training Loss: 0.0005807504057884216 \t\n",
      "Epoch 43244 \t\t Training Loss: 0.0005807504057884216 \t\n",
      "Epoch 43245 \t\t Training Loss: 0.0005807504057884216 \t\n",
      "Epoch 43246 \t\t Training Loss: 0.0005807504057884216 \t\n",
      "Epoch 43247 \t\t Training Loss: 0.0005807504057884216 \t\n",
      "Epoch 43248 \t\t Training Loss: 0.0005807504057884216 \t\n",
      "Epoch 43249 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43250 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43251 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43252 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43253 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43254 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43255 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43256 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43257 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43258 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43259 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43260 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43261 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43262 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43263 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43264 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43265 \t\t Training Loss: 0.0005807502311654389 \t\n",
      "Epoch 43266 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43267 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43268 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43269 \t\t Training Loss: 0.0005807502311654389 \t\n",
      "Epoch 43270 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43271 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43272 \t\t Training Loss: 0.0005807502311654389 \t\n",
      "Epoch 43273 \t\t Training Loss: 0.0005807502311654389 \t\n",
      "Epoch 43274 \t\t Training Loss: 0.0005807501147501171 \t\n",
      "Epoch 43275 \t\t Training Loss: 0.0005807501147501171 \t\n",
      "Epoch 43276 \t\t Training Loss: 0.0005807502893730998 \t\n",
      "Epoch 43277 \t\t Training Loss: 0.0005807502311654389 \t\n",
      "Epoch 43278 \t\t Training Loss: 0.0005807501147501171 \t\n",
      "Epoch 43279 \t\t Training Loss: 0.0005807501147501171 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43280 \t\t Training Loss: 0.0005807501147501171 \t\n",
      "Epoch 43281 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43282 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43283 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43284 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43285 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43286 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43287 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43288 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43289 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43290 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43291 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43292 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43293 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43294 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43295 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43296 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43297 \t\t Training Loss: 0.0005807498819194734 \t\n",
      "Epoch 43298 \t\t Training Loss: 0.0005807499983347952 \t\n",
      "Epoch 43299 \t\t Training Loss: 0.0005807498819194734 \t\n",
      "Epoch 43300 \t\t Training Loss: 0.0005807498819194734 \t\n",
      "Epoch 43301 \t\t Training Loss: 0.0005807498819194734 \t\n",
      "Epoch 43302 \t\t Training Loss: 0.0005807498819194734 \t\n",
      "Epoch 43303 \t\t Training Loss: 0.0005807498819194734 \t\n",
      "Epoch 43304 \t\t Training Loss: 0.0005807498819194734 \t\n",
      "Epoch 43305 \t\t Training Loss: 0.0005807498819194734 \t\n",
      "Epoch 43306 \t\t Training Loss: 0.0005807498819194734 \t\n",
      "Epoch 43307 \t\t Training Loss: 0.0005807498819194734 \t\n",
      "Epoch 43308 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43309 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43310 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43311 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43312 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43313 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43314 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43315 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43316 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43317 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43318 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43319 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43320 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43321 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43322 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43323 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43324 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43325 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43326 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43327 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43328 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43329 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43330 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43331 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43332 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43333 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43334 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43335 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43336 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43337 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43338 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43339 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43340 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43341 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43342 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43343 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43344 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43345 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43346 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43347 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43348 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43349 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43350 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43351 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43352 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43353 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43354 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43355 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43356 \t\t Training Loss: 0.0005807498237118125 \t\n",
      "Epoch 43357 \t\t Training Loss: 0.0005807497072964907 \t\n",
      "Epoch 43358 \t\t Training Loss: 0.0005807497072964907 \t\n",
      "Epoch 43359 \t\t Training Loss: 0.0005807497072964907 \t\n",
      "Epoch 43360 \t\t Training Loss: 0.0005807497072964907 \t\n",
      "Epoch 43361 \t\t Training Loss: 0.0005807497072964907 \t\n",
      "Epoch 43362 \t\t Training Loss: 0.0005807497072964907 \t\n",
      "Epoch 43363 \t\t Training Loss: 0.0005807497072964907 \t\n",
      "Epoch 43364 \t\t Training Loss: 0.0005807497072964907 \t\n",
      "Epoch 43365 \t\t Training Loss: 0.0005807497072964907 \t\n",
      "Epoch 43366 \t\t Training Loss: 0.0005807497072964907 \t\n",
      "Epoch 43367 \t\t Training Loss: 0.0005807495908811688 \t\n",
      "Epoch 43368 \t\t Training Loss: 0.0005807495908811688 \t\n",
      "Epoch 43369 \t\t Training Loss: 0.0005807495908811688 \t\n",
      "Epoch 43370 \t\t Training Loss: 0.0005807495908811688 \t\n",
      "Epoch 43371 \t\t Training Loss: 0.0005807495326735079 \t\n",
      "Epoch 43372 \t\t Training Loss: 0.0005807495326735079 \t\n",
      "Epoch 43373 \t\t Training Loss: 0.0005807495326735079 \t\n",
      "Epoch 43374 \t\t Training Loss: 0.0005807495326735079 \t\n",
      "Epoch 43375 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43376 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43377 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43378 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43379 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43380 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43381 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43382 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43383 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43384 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43385 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43386 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43387 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43388 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43389 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43390 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43391 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43392 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43393 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43394 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43395 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43396 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43397 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43398 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43399 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43400 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43401 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43402 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43403 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43404 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43405 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43406 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43407 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43408 \t\t Training Loss: 0.0005807494162581861 \t\n",
      "Epoch 43409 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43410 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43411 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43412 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43413 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43414 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43415 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43416 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43417 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43418 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43419 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43420 \t\t Training Loss: 0.0005807492998428643 \t\n",
      "Epoch 43421 \t\t Training Loss: 0.0005807492416352034 \t\n",
      "Epoch 43422 \t\t Training Loss: 0.0005807492416352034 \t\n",
      "Epoch 43423 \t\t Training Loss: 0.0005807492416352034 \t\n",
      "Epoch 43424 \t\t Training Loss: 0.0005807492416352034 \t\n",
      "Epoch 43425 \t\t Training Loss: 0.0005807492416352034 \t\n",
      "Epoch 43426 \t\t Training Loss: 0.0005807492416352034 \t\n",
      "Epoch 43427 \t\t Training Loss: 0.0005807492416352034 \t\n",
      "Epoch 43428 \t\t Training Loss: 0.0005807492416352034 \t\n",
      "Epoch 43429 \t\t Training Loss: 0.0005807492416352034 \t\n",
      "Epoch 43430 \t\t Training Loss: 0.0005807492416352034 \t\n",
      "Epoch 43431 \t\t Training Loss: 0.0005807492416352034 \t\n",
      "Epoch 43432 \t\t Training Loss: 0.0005807492416352034 \t\n",
      "Epoch 43433 \t\t Training Loss: 0.0005807491834275424 \t\n",
      "Epoch 43434 \t\t Training Loss: 0.0005807491834275424 \t\n",
      "Epoch 43435 \t\t Training Loss: 0.0005807492416352034 \t\n",
      "Epoch 43436 \t\t Training Loss: 0.0005807492416352034 \t\n",
      "Epoch 43437 \t\t Training Loss: 0.0005807491834275424 \t\n",
      "Epoch 43438 \t\t Training Loss: 0.0005807491834275424 \t\n",
      "Epoch 43439 \t\t Training Loss: 0.0005807492416352034 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43440 \t\t Training Loss: 0.0005807492416352034 \t\n",
      "Epoch 43441 \t\t Training Loss: 0.0005807491834275424 \t\n",
      "Epoch 43442 \t\t Training Loss: 0.0005807491834275424 \t\n",
      "Epoch 43443 \t\t Training Loss: 0.0005807491834275424 \t\n",
      "Epoch 43444 \t\t Training Loss: 0.0005807491834275424 \t\n",
      "Epoch 43445 \t\t Training Loss: 0.0005807491834275424 \t\n",
      "Epoch 43446 \t\t Training Loss: 0.0005807491834275424 \t\n",
      "Epoch 43447 \t\t Training Loss: 0.0005807491834275424 \t\n",
      "Epoch 43448 \t\t Training Loss: 0.0005807491834275424 \t\n",
      "Epoch 43449 \t\t Training Loss: 0.0005807491834275424 \t\n",
      "Epoch 43450 \t\t Training Loss: 0.0005807491834275424 \t\n",
      "Epoch 43451 \t\t Training Loss: 0.0005807491252198815 \t\n",
      "Epoch 43452 \t\t Training Loss: 0.0005807491252198815 \t\n",
      "Epoch 43453 \t\t Training Loss: 0.0005807491252198815 \t\n",
      "Epoch 43454 \t\t Training Loss: 0.0005807491252198815 \t\n",
      "Epoch 43455 \t\t Training Loss: 0.0005807491252198815 \t\n",
      "Epoch 43456 \t\t Training Loss: 0.0005807491252198815 \t\n",
      "Epoch 43457 \t\t Training Loss: 0.0005807491834275424 \t\n",
      "Epoch 43458 \t\t Training Loss: 0.0005807491252198815 \t\n",
      "Epoch 43459 \t\t Training Loss: 0.0005807491834275424 \t\n",
      "Epoch 43460 \t\t Training Loss: 0.0005807490088045597 \t\n",
      "Epoch 43461 \t\t Training Loss: 0.0005807490088045597 \t\n",
      "Epoch 43462 \t\t Training Loss: 0.0005807491252198815 \t\n",
      "Epoch 43463 \t\t Training Loss: 0.0005807490088045597 \t\n",
      "Epoch 43464 \t\t Training Loss: 0.0005807490088045597 \t\n",
      "Epoch 43465 \t\t Training Loss: 0.0005807490088045597 \t\n",
      "Epoch 43466 \t\t Training Loss: 0.0005807490088045597 \t\n",
      "Epoch 43467 \t\t Training Loss: 0.0005807491252198815 \t\n",
      "Epoch 43468 \t\t Training Loss: 0.0005807490088045597 \t\n",
      "Epoch 43469 \t\t Training Loss: 0.0005807490088045597 \t\n",
      "Epoch 43470 \t\t Training Loss: 0.0005807490088045597 \t\n",
      "Epoch 43471 \t\t Training Loss: 0.0005807490088045597 \t\n",
      "Epoch 43472 \t\t Training Loss: 0.0005807490088045597 \t\n",
      "Epoch 43473 \t\t Training Loss: 0.0005807490088045597 \t\n",
      "Epoch 43474 \t\t Training Loss: 0.0005807488923892379 \t\n",
      "Epoch 43475 \t\t Training Loss: 0.0005807489505968988 \t\n",
      "Epoch 43476 \t\t Training Loss: 0.0005807488923892379 \t\n",
      "Epoch 43477 \t\t Training Loss: 0.0005807489505968988 \t\n",
      "Epoch 43478 \t\t Training Loss: 0.0005807489505968988 \t\n",
      "Epoch 43479 \t\t Training Loss: 0.0005807490088045597 \t\n",
      "Epoch 43480 \t\t Training Loss: 0.0005807490088045597 \t\n",
      "Epoch 43481 \t\t Training Loss: 0.0005807490088045597 \t\n",
      "Epoch 43482 \t\t Training Loss: 0.0005807489505968988 \t\n",
      "Epoch 43483 \t\t Training Loss: 0.0005807490088045597 \t\n",
      "Epoch 43484 \t\t Training Loss: 0.0005807489505968988 \t\n",
      "Epoch 43485 \t\t Training Loss: 0.0005807489505968988 \t\n",
      "Epoch 43486 \t\t Training Loss: 0.0005807489505968988 \t\n",
      "Epoch 43487 \t\t Training Loss: 0.0005807489505968988 \t\n",
      "Epoch 43488 \t\t Training Loss: 0.0005807489505968988 \t\n",
      "Epoch 43489 \t\t Training Loss: 0.0005807489505968988 \t\n",
      "Epoch 43490 \t\t Training Loss: 0.0005807489505968988 \t\n",
      "Epoch 43491 \t\t Training Loss: 0.0005807489505968988 \t\n",
      "Epoch 43492 \t\t Training Loss: 0.0005807488923892379 \t\n",
      "Epoch 43493 \t\t Training Loss: 0.0005807488923892379 \t\n",
      "Epoch 43494 \t\t Training Loss: 0.0005807488923892379 \t\n",
      "Epoch 43495 \t\t Training Loss: 0.0005807488923892379 \t\n",
      "Epoch 43496 \t\t Training Loss: 0.0005807489505968988 \t\n",
      "Epoch 43497 \t\t Training Loss: 0.0005807488923892379 \t\n",
      "Epoch 43498 \t\t Training Loss: 0.0005807488923892379 \t\n",
      "Epoch 43499 \t\t Training Loss: 0.0005807488923892379 \t\n",
      "Epoch 43500 \t\t Training Loss: 0.0005807488923892379 \t\n",
      "Epoch 43501 \t\t Training Loss: 0.0005807488923892379 \t\n",
      "Epoch 43502 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43503 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43504 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43505 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43506 \t\t Training Loss: 0.0005807487177662551 \t\n",
      "Epoch 43507 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43508 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43509 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43510 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43511 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43512 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43513 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43514 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43515 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43516 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43517 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43518 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43519 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43520 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43521 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43522 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43523 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43524 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43525 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43526 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43527 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43528 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43529 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43530 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43531 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43532 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43533 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43534 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43535 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43536 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43537 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43538 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43539 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43540 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43541 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43542 \t\t Training Loss: 0.000580748834181577 \t\n",
      "Epoch 43543 \t\t Training Loss: 0.0005807487177662551 \t\n",
      "Epoch 43544 \t\t Training Loss: 0.0005807487177662551 \t\n",
      "Epoch 43545 \t\t Training Loss: 0.0005807487177662551 \t\n",
      "Epoch 43546 \t\t Training Loss: 0.0005807487177662551 \t\n",
      "Epoch 43547 \t\t Training Loss: 0.0005807487177662551 \t\n",
      "Epoch 43548 \t\t Training Loss: 0.0005807487177662551 \t\n",
      "Epoch 43549 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43550 \t\t Training Loss: 0.0005807487177662551 \t\n",
      "Epoch 43551 \t\t Training Loss: 0.0005807487177662551 \t\n",
      "Epoch 43552 \t\t Training Loss: 0.0005807487177662551 \t\n",
      "Epoch 43553 \t\t Training Loss: 0.0005807487177662551 \t\n",
      "Epoch 43554 \t\t Training Loss: 0.0005807487177662551 \t\n",
      "Epoch 43555 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43556 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43557 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43558 \t\t Training Loss: 0.0005807487177662551 \t\n",
      "Epoch 43559 \t\t Training Loss: 0.0005807487177662551 \t\n",
      "Epoch 43560 \t\t Training Loss: 0.0005807487177662551 \t\n",
      "Epoch 43561 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43562 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43563 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43564 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43565 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43566 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43567 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43568 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43569 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43570 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43571 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43572 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43573 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43574 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43575 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43576 \t\t Training Loss: 0.0005807486595585942 \t\n",
      "Epoch 43577 \t\t Training Loss: 0.0005807486013509333 \t\n",
      "Epoch 43578 \t\t Training Loss: 0.0005807486013509333 \t\n",
      "Epoch 43579 \t\t Training Loss: 0.0005807486013509333 \t\n",
      "Epoch 43580 \t\t Training Loss: 0.0005807486013509333 \t\n",
      "Epoch 43581 \t\t Training Loss: 0.0005807486013509333 \t\n",
      "Epoch 43582 \t\t Training Loss: 0.0005807486013509333 \t\n",
      "Epoch 43583 \t\t Training Loss: 0.0005807486013509333 \t\n",
      "Epoch 43584 \t\t Training Loss: 0.0005807486013509333 \t\n",
      "Epoch 43585 \t\t Training Loss: 0.0005807486013509333 \t\n",
      "Epoch 43586 \t\t Training Loss: 0.0005807486013509333 \t\n",
      "Epoch 43587 \t\t Training Loss: 0.0005807486013509333 \t\n",
      "Epoch 43588 \t\t Training Loss: 0.0005807486013509333 \t\n",
      "Epoch 43589 \t\t Training Loss: 0.0005807486013509333 \t\n",
      "Epoch 43590 \t\t Training Loss: 0.0005807486013509333 \t\n",
      "Epoch 43591 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43592 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43593 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43594 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43595 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43596 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43597 \t\t Training Loss: 0.0005807486013509333 \t\n",
      "Epoch 43598 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43599 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43600 \t\t Training Loss: 0.0005807486013509333 \t\n",
      "Epoch 43601 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43602 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43603 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43604 \t\t Training Loss: 0.0005807485431432724 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43605 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43606 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43607 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43608 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43609 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43610 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43611 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43612 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43613 \t\t Training Loss: 0.0005807484267279506 \t\n",
      "Epoch 43614 \t\t Training Loss: 0.0005807484267279506 \t\n",
      "Epoch 43615 \t\t Training Loss: 0.0005807484267279506 \t\n",
      "Epoch 43616 \t\t Training Loss: 0.0005807484267279506 \t\n",
      "Epoch 43617 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43618 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43619 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43620 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43621 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43622 \t\t Training Loss: 0.0005807485431432724 \t\n",
      "Epoch 43623 \t\t Training Loss: 0.0005807484267279506 \t\n",
      "Epoch 43624 \t\t Training Loss: 0.0005807484267279506 \t\n",
      "Epoch 43625 \t\t Training Loss: 0.0005807484267279506 \t\n",
      "Epoch 43626 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43627 \t\t Training Loss: 0.0005807484267279506 \t\n",
      "Epoch 43628 \t\t Training Loss: 0.0005807484267279506 \t\n",
      "Epoch 43629 \t\t Training Loss: 0.0005807484267279506 \t\n",
      "Epoch 43630 \t\t Training Loss: 0.0005807484267279506 \t\n",
      "Epoch 43631 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43632 \t\t Training Loss: 0.0005807484267279506 \t\n",
      "Epoch 43633 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43634 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43635 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43636 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43637 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43638 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43639 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43640 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43641 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43642 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43643 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43644 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43645 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43646 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43647 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43648 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43649 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43650 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43651 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43652 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43653 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43654 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43655 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43656 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43657 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43658 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43659 \t\t Training Loss: 0.0005807482521049678 \t\n",
      "Epoch 43660 \t\t Training Loss: 0.0005807482521049678 \t\n",
      "Epoch 43661 \t\t Training Loss: 0.0005807482521049678 \t\n",
      "Epoch 43662 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43663 \t\t Training Loss: 0.0005807482521049678 \t\n",
      "Epoch 43664 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43665 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43666 \t\t Training Loss: 0.0005807483103126287 \t\n",
      "Epoch 43667 \t\t Training Loss: 0.0005807482521049678 \t\n",
      "Epoch 43668 \t\t Training Loss: 0.0005807482521049678 \t\n",
      "Epoch 43669 \t\t Training Loss: 0.0005807482521049678 \t\n",
      "Epoch 43670 \t\t Training Loss: 0.0005807482521049678 \t\n",
      "Epoch 43671 \t\t Training Loss: 0.0005807482521049678 \t\n",
      "Epoch 43672 \t\t Training Loss: 0.0005807482521049678 \t\n",
      "Epoch 43673 \t\t Training Loss: 0.0005807481938973069 \t\n",
      "Epoch 43674 \t\t Training Loss: 0.0005807481938973069 \t\n",
      "Epoch 43675 \t\t Training Loss: 0.0005807481938973069 \t\n",
      "Epoch 43676 \t\t Training Loss: 0.0005807481938973069 \t\n",
      "Epoch 43677 \t\t Training Loss: 0.0005807481938973069 \t\n",
      "Epoch 43678 \t\t Training Loss: 0.0005807481938973069 \t\n",
      "Epoch 43679 \t\t Training Loss: 0.0005807481938973069 \t\n",
      "Epoch 43680 \t\t Training Loss: 0.000580748135689646 \t\n",
      "Epoch 43681 \t\t Training Loss: 0.0005807481938973069 \t\n",
      "Epoch 43682 \t\t Training Loss: 0.0005807480774819851 \t\n",
      "Epoch 43683 \t\t Training Loss: 0.0005807480774819851 \t\n",
      "Epoch 43684 \t\t Training Loss: 0.0005807480774819851 \t\n",
      "Epoch 43685 \t\t Training Loss: 0.0005807480774819851 \t\n",
      "Epoch 43686 \t\t Training Loss: 0.0005807480774819851 \t\n",
      "Epoch 43687 \t\t Training Loss: 0.0005807480192743242 \t\n",
      "Epoch 43688 \t\t Training Loss: 0.0005807480192743242 \t\n",
      "Epoch 43689 \t\t Training Loss: 0.0005807480192743242 \t\n",
      "Epoch 43690 \t\t Training Loss: 0.0005807480774819851 \t\n",
      "Epoch 43691 \t\t Training Loss: 0.0005807480774819851 \t\n",
      "Epoch 43692 \t\t Training Loss: 0.0005807480774819851 \t\n",
      "Epoch 43693 \t\t Training Loss: 0.0005807480192743242 \t\n",
      "Epoch 43694 \t\t Training Loss: 0.0005807480774819851 \t\n",
      "Epoch 43695 \t\t Training Loss: 0.0005807480774819851 \t\n",
      "Epoch 43696 \t\t Training Loss: 0.0005807480774819851 \t\n",
      "Epoch 43697 \t\t Training Loss: 0.0005807480774819851 \t\n",
      "Epoch 43698 \t\t Training Loss: 0.0005807480192743242 \t\n",
      "Epoch 43699 \t\t Training Loss: 0.0005807480192743242 \t\n",
      "Epoch 43700 \t\t Training Loss: 0.0005807480192743242 \t\n",
      "Epoch 43701 \t\t Training Loss: 0.0005807480192743242 \t\n",
      "Epoch 43702 \t\t Training Loss: 0.0005807480192743242 \t\n",
      "Epoch 43703 \t\t Training Loss: 0.0005807480192743242 \t\n",
      "Epoch 43704 \t\t Training Loss: 0.0005807479610666633 \t\n",
      "Epoch 43705 \t\t Training Loss: 0.0005807479610666633 \t\n",
      "Epoch 43706 \t\t Training Loss: 0.0005807480192743242 \t\n",
      "Epoch 43707 \t\t Training Loss: 0.0005807480192743242 \t\n",
      "Epoch 43708 \t\t Training Loss: 0.0005807479610666633 \t\n",
      "Epoch 43709 \t\t Training Loss: 0.0005807479610666633 \t\n",
      "Epoch 43710 \t\t Training Loss: 0.0005807479610666633 \t\n",
      "Epoch 43711 \t\t Training Loss: 0.0005807479610666633 \t\n",
      "Epoch 43712 \t\t Training Loss: 0.0005807479610666633 \t\n",
      "Epoch 43713 \t\t Training Loss: 0.0005807479028590024 \t\n",
      "Epoch 43714 \t\t Training Loss: 0.0005807479028590024 \t\n",
      "Epoch 43715 \t\t Training Loss: 0.0005807478446513414 \t\n",
      "Epoch 43716 \t\t Training Loss: 0.0005807478446513414 \t\n",
      "Epoch 43717 \t\t Training Loss: 0.0005807478446513414 \t\n",
      "Epoch 43718 \t\t Training Loss: 0.0005807478446513414 \t\n",
      "Epoch 43719 \t\t Training Loss: 0.0005807478446513414 \t\n",
      "Epoch 43720 \t\t Training Loss: 0.0005807477864436805 \t\n",
      "Epoch 43721 \t\t Training Loss: 0.0005807477864436805 \t\n",
      "Epoch 43722 \t\t Training Loss: 0.0005807477864436805 \t\n",
      "Epoch 43723 \t\t Training Loss: 0.0005807477864436805 \t\n",
      "Epoch 43724 \t\t Training Loss: 0.0005807478446513414 \t\n",
      "Epoch 43725 \t\t Training Loss: 0.0005807478446513414 \t\n",
      "Epoch 43726 \t\t Training Loss: 0.0005807477864436805 \t\n",
      "Epoch 43727 \t\t Training Loss: 0.0005807477864436805 \t\n",
      "Epoch 43728 \t\t Training Loss: 0.0005807477864436805 \t\n",
      "Epoch 43729 \t\t Training Loss: 0.0005807477864436805 \t\n",
      "Epoch 43730 \t\t Training Loss: 0.0005807478446513414 \t\n",
      "Epoch 43731 \t\t Training Loss: 0.0005807478446513414 \t\n",
      "Epoch 43732 \t\t Training Loss: 0.0005807477864436805 \t\n",
      "Epoch 43733 \t\t Training Loss: 0.0005807477864436805 \t\n",
      "Epoch 43734 \t\t Training Loss: 0.0005807478446513414 \t\n",
      "Epoch 43735 \t\t Training Loss: 0.0005807477864436805 \t\n",
      "Epoch 43736 \t\t Training Loss: 0.0005807477864436805 \t\n",
      "Epoch 43737 \t\t Training Loss: 0.0005807477282360196 \t\n",
      "Epoch 43738 \t\t Training Loss: 0.0005807477864436805 \t\n",
      "Epoch 43739 \t\t Training Loss: 0.0005807477282360196 \t\n",
      "Epoch 43740 \t\t Training Loss: 0.0005807477864436805 \t\n",
      "Epoch 43741 \t\t Training Loss: 0.0005807477864436805 \t\n",
      "Epoch 43742 \t\t Training Loss: 0.0005807477282360196 \t\n",
      "Epoch 43743 \t\t Training Loss: 0.0005807477282360196 \t\n",
      "Epoch 43744 \t\t Training Loss: 0.0005807477282360196 \t\n",
      "Epoch 43745 \t\t Training Loss: 0.0005807477282360196 \t\n",
      "Epoch 43746 \t\t Training Loss: 0.0005807477282360196 \t\n",
      "Epoch 43747 \t\t Training Loss: 0.0005807477282360196 \t\n",
      "Epoch 43748 \t\t Training Loss: 0.0005807476700283587 \t\n",
      "Epoch 43749 \t\t Training Loss: 0.0005807476700283587 \t\n",
      "Epoch 43750 \t\t Training Loss: 0.0005807476700283587 \t\n",
      "Epoch 43751 \t\t Training Loss: 0.0005807476700283587 \t\n",
      "Epoch 43752 \t\t Training Loss: 0.0005807476700283587 \t\n",
      "Epoch 43753 \t\t Training Loss: 0.0005807476700283587 \t\n",
      "Epoch 43754 \t\t Training Loss: 0.0005807476700283587 \t\n",
      "Epoch 43755 \t\t Training Loss: 0.0005807476118206978 \t\n",
      "Epoch 43756 \t\t Training Loss: 0.0005807476118206978 \t\n",
      "Epoch 43757 \t\t Training Loss: 0.0005807476118206978 \t\n",
      "Epoch 43758 \t\t Training Loss: 0.0005807476118206978 \t\n",
      "Epoch 43759 \t\t Training Loss: 0.0005807476118206978 \t\n",
      "Epoch 43760 \t\t Training Loss: 0.0005807476118206978 \t\n",
      "Epoch 43761 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43762 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43763 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43764 \t\t Training Loss: 0.0005807476118206978 \t\n",
      "Epoch 43765 \t\t Training Loss: 0.0005807476118206978 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43766 \t\t Training Loss: 0.0005807476118206978 \t\n",
      "Epoch 43767 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43768 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43769 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43770 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43771 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43772 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43773 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43774 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43775 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43776 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43777 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43778 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43779 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43780 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43781 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43782 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43783 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43784 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43785 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43786 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43787 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43788 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43789 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43790 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43791 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43792 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43793 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43794 \t\t Training Loss: 0.000580747495405376 \t\n",
      "Epoch 43795 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43796 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43797 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43798 \t\t Training Loss: 0.0005807475536130369 \t\n",
      "Epoch 43799 \t\t Training Loss: 0.000580747495405376 \t\n",
      "Epoch 43800 \t\t Training Loss: 0.000580747495405376 \t\n",
      "Epoch 43801 \t\t Training Loss: 0.000580747495405376 \t\n",
      "Epoch 43802 \t\t Training Loss: 0.0005807473789900541 \t\n",
      "Epoch 43803 \t\t Training Loss: 0.0005807473789900541 \t\n",
      "Epoch 43804 \t\t Training Loss: 0.0005807473789900541 \t\n",
      "Epoch 43805 \t\t Training Loss: 0.0005807473207823932 \t\n",
      "Epoch 43806 \t\t Training Loss: 0.0005807473207823932 \t\n",
      "Epoch 43807 \t\t Training Loss: 0.0005807473207823932 \t\n",
      "Epoch 43808 \t\t Training Loss: 0.0005807473207823932 \t\n",
      "Epoch 43809 \t\t Training Loss: 0.0005807473207823932 \t\n",
      "Epoch 43810 \t\t Training Loss: 0.0005807473789900541 \t\n",
      "Epoch 43811 \t\t Training Loss: 0.0005807473789900541 \t\n",
      "Epoch 43812 \t\t Training Loss: 0.0005807473789900541 \t\n",
      "Epoch 43813 \t\t Training Loss: 0.0005807473207823932 \t\n",
      "Epoch 43814 \t\t Training Loss: 0.0005807473207823932 \t\n",
      "Epoch 43815 \t\t Training Loss: 0.0005807473207823932 \t\n",
      "Epoch 43816 \t\t Training Loss: 0.0005807473207823932 \t\n",
      "Epoch 43817 \t\t Training Loss: 0.0005807473207823932 \t\n",
      "Epoch 43818 \t\t Training Loss: 0.0005807473207823932 \t\n",
      "Epoch 43819 \t\t Training Loss: 0.0005807472625747323 \t\n",
      "Epoch 43820 \t\t Training Loss: 0.0005807472625747323 \t\n",
      "Epoch 43821 \t\t Training Loss: 0.0005807472043670714 \t\n",
      "Epoch 43822 \t\t Training Loss: 0.0005807472043670714 \t\n",
      "Epoch 43823 \t\t Training Loss: 0.0005807472043670714 \t\n",
      "Epoch 43824 \t\t Training Loss: 0.0005807472043670714 \t\n",
      "Epoch 43825 \t\t Training Loss: 0.0005807472043670714 \t\n",
      "Epoch 43826 \t\t Training Loss: 0.0005807472043670714 \t\n",
      "Epoch 43827 \t\t Training Loss: 0.0005807472043670714 \t\n",
      "Epoch 43828 \t\t Training Loss: 0.0005807472043670714 \t\n",
      "Epoch 43829 \t\t Training Loss: 0.0005807472043670714 \t\n",
      "Epoch 43830 \t\t Training Loss: 0.0005807472043670714 \t\n",
      "Epoch 43831 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43832 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43833 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43834 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43835 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43836 \t\t Training Loss: 0.0005807472043670714 \t\n",
      "Epoch 43837 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43838 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43839 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43840 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43841 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43842 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43843 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43844 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43845 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43846 \t\t Training Loss: 0.0005807470879517496 \t\n",
      "Epoch 43847 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43848 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43849 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43850 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43851 \t\t Training Loss: 0.0005807470879517496 \t\n",
      "Epoch 43852 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43853 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43854 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43855 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43856 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43857 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43858 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43859 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43860 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43861 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43862 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43863 \t\t Training Loss: 0.0005807471461594105 \t\n",
      "Epoch 43864 \t\t Training Loss: 0.0005807470879517496 \t\n",
      "Epoch 43865 \t\t Training Loss: 0.0005807470879517496 \t\n",
      "Epoch 43866 \t\t Training Loss: 0.0005807470879517496 \t\n",
      "Epoch 43867 \t\t Training Loss: 0.0005807470879517496 \t\n",
      "Epoch 43868 \t\t Training Loss: 0.0005807470879517496 \t\n",
      "Epoch 43869 \t\t Training Loss: 0.0005807470879517496 \t\n",
      "Epoch 43870 \t\t Training Loss: 0.0005807470879517496 \t\n",
      "Epoch 43871 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43872 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43873 \t\t Training Loss: 0.0005807470297440886 \t\n",
      "Epoch 43874 \t\t Training Loss: 0.0005807470879517496 \t\n",
      "Epoch 43875 \t\t Training Loss: 0.0005807470879517496 \t\n",
      "Epoch 43876 \t\t Training Loss: 0.0005807470879517496 \t\n",
      "Epoch 43877 \t\t Training Loss: 0.0005807470879517496 \t\n",
      "Epoch 43878 \t\t Training Loss: 0.0005807470879517496 \t\n",
      "Epoch 43879 \t\t Training Loss: 0.0005807470879517496 \t\n",
      "Epoch 43880 \t\t Training Loss: 0.0005807470297440886 \t\n",
      "Epoch 43881 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43882 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43883 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43884 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43885 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43886 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43887 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43888 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43889 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43890 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43891 \t\t Training Loss: 0.0005807469133287668 \t\n",
      "Epoch 43892 \t\t Training Loss: 0.0005807469133287668 \t\n",
      "Epoch 43893 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43894 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43895 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43896 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43897 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43898 \t\t Training Loss: 0.0005807469133287668 \t\n",
      "Epoch 43899 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43900 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43901 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43902 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43903 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43904 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43905 \t\t Training Loss: 0.0005807469715364277 \t\n",
      "Epoch 43906 \t\t Training Loss: 0.0005807469133287668 \t\n",
      "Epoch 43907 \t\t Training Loss: 0.000580746796913445 \t\n",
      "Epoch 43908 \t\t Training Loss: 0.0005807469133287668 \t\n",
      "Epoch 43909 \t\t Training Loss: 0.000580746796913445 \t\n",
      "Epoch 43910 \t\t Training Loss: 0.0005807467387057841 \t\n",
      "Epoch 43911 \t\t Training Loss: 0.0005807467387057841 \t\n",
      "Epoch 43912 \t\t Training Loss: 0.0005807466804981232 \t\n",
      "Epoch 43913 \t\t Training Loss: 0.0005807466804981232 \t\n",
      "Epoch 43914 \t\t Training Loss: 0.0005807466804981232 \t\n",
      "Epoch 43915 \t\t Training Loss: 0.0005807466804981232 \t\n",
      "Epoch 43916 \t\t Training Loss: 0.0005807466804981232 \t\n",
      "Epoch 43917 \t\t Training Loss: 0.0005807466804981232 \t\n",
      "Epoch 43918 \t\t Training Loss: 0.0005807466804981232 \t\n",
      "Epoch 43919 \t\t Training Loss: 0.0005807466804981232 \t\n",
      "Epoch 43920 \t\t Training Loss: 0.0005807466804981232 \t\n",
      "Epoch 43921 \t\t Training Loss: 0.0005807466804981232 \t\n",
      "Epoch 43922 \t\t Training Loss: 0.0005807466804981232 \t\n",
      "Epoch 43923 \t\t Training Loss: 0.0005807466804981232 \t\n",
      "Epoch 43924 \t\t Training Loss: 0.0005807466804981232 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43925 \t\t Training Loss: 0.0005807466804981232 \t\n",
      "Epoch 43926 \t\t Training Loss: 0.0005807466222904623 \t\n",
      "Epoch 43927 \t\t Training Loss: 0.0005807466222904623 \t\n",
      "Epoch 43928 \t\t Training Loss: 0.0005807465640828013 \t\n",
      "Epoch 43929 \t\t Training Loss: 0.0005807466222904623 \t\n",
      "Epoch 43930 \t\t Training Loss: 0.0005807465640828013 \t\n",
      "Epoch 43931 \t\t Training Loss: 0.0005807465640828013 \t\n",
      "Epoch 43932 \t\t Training Loss: 0.0005807465640828013 \t\n",
      "Epoch 43933 \t\t Training Loss: 0.0005807465640828013 \t\n",
      "Epoch 43934 \t\t Training Loss: 0.0005807465058751404 \t\n",
      "Epoch 43935 \t\t Training Loss: 0.0005807465058751404 \t\n",
      "Epoch 43936 \t\t Training Loss: 0.0005807465058751404 \t\n",
      "Epoch 43937 \t\t Training Loss: 0.0005807465058751404 \t\n",
      "Epoch 43938 \t\t Training Loss: 0.0005807465058751404 \t\n",
      "Epoch 43939 \t\t Training Loss: 0.0005807465058751404 \t\n",
      "Epoch 43940 \t\t Training Loss: 0.0005807465058751404 \t\n",
      "Epoch 43941 \t\t Training Loss: 0.0005807465058751404 \t\n",
      "Epoch 43942 \t\t Training Loss: 0.0005807465058751404 \t\n",
      "Epoch 43943 \t\t Training Loss: 0.0005807465058751404 \t\n",
      "Epoch 43944 \t\t Training Loss: 0.0005807465058751404 \t\n",
      "Epoch 43945 \t\t Training Loss: 0.0005807464476674795 \t\n",
      "Epoch 43946 \t\t Training Loss: 0.0005807465058751404 \t\n",
      "Epoch 43947 \t\t Training Loss: 0.0005807464476674795 \t\n",
      "Epoch 43948 \t\t Training Loss: 0.0005807464476674795 \t\n",
      "Epoch 43949 \t\t Training Loss: 0.0005807464476674795 \t\n",
      "Epoch 43950 \t\t Training Loss: 0.0005807464476674795 \t\n",
      "Epoch 43951 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43952 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43953 \t\t Training Loss: 0.0005807463894598186 \t\n",
      "Epoch 43954 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43955 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43956 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43957 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43958 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43959 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43960 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43961 \t\t Training Loss: 0.0005807464476674795 \t\n",
      "Epoch 43962 \t\t Training Loss: 0.0005807464476674795 \t\n",
      "Epoch 43963 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43964 \t\t Training Loss: 0.0005807464476674795 \t\n",
      "Epoch 43965 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43966 \t\t Training Loss: 0.0005807463894598186 \t\n",
      "Epoch 43967 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43968 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43969 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43970 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43971 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43972 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43973 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43974 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43975 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43976 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43977 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43978 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43979 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 43980 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 43981 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 43982 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 43983 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 43984 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 43985 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43986 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43987 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43988 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43989 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 43990 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 43991 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 43992 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 43993 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 43994 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 43995 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 43996 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 43997 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 43998 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 43999 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 44000 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 44001 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44002 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 44003 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 44004 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 44005 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 44006 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44007 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 44008 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44009 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44010 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44011 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44012 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44013 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44014 \t\t Training Loss: 0.000580746156629175 \t\n",
      "Epoch 44015 \t\t Training Loss: 0.000580746156629175 \t\n",
      "Epoch 44016 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44017 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44018 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44019 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 44020 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 44021 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 44022 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 44023 \t\t Training Loss: 0.0005807462730444968 \t\n",
      "Epoch 44024 \t\t Training Loss: 0.0005807463312521577 \t\n",
      "Epoch 44025 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44026 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44027 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44028 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44029 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44030 \t\t Training Loss: 0.000580746156629175 \t\n",
      "Epoch 44031 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44032 \t\t Training Loss: 0.000580746156629175 \t\n",
      "Epoch 44033 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44034 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44035 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44036 \t\t Training Loss: 0.000580746156629175 \t\n",
      "Epoch 44037 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44038 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44039 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44040 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44041 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44042 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44043 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44044 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44045 \t\t Training Loss: 0.0005807462148368359 \t\n",
      "Epoch 44046 \t\t Training Loss: 0.000580746156629175 \t\n",
      "Epoch 44047 \t\t Training Loss: 0.000580746156629175 \t\n",
      "Epoch 44048 \t\t Training Loss: 0.000580746156629175 \t\n",
      "Epoch 44049 \t\t Training Loss: 0.000580746156629175 \t\n",
      "Epoch 44050 \t\t Training Loss: 0.000580746098421514 \t\n",
      "Epoch 44051 \t\t Training Loss: 0.000580746098421514 \t\n",
      "Epoch 44052 \t\t Training Loss: 0.000580746098421514 \t\n",
      "Epoch 44053 \t\t Training Loss: 0.000580746098421514 \t\n",
      "Epoch 44054 \t\t Training Loss: 0.000580746098421514 \t\n",
      "Epoch 44055 \t\t Training Loss: 0.000580746098421514 \t\n",
      "Epoch 44056 \t\t Training Loss: 0.000580746098421514 \t\n",
      "Epoch 44057 \t\t Training Loss: 0.000580746098421514 \t\n",
      "Epoch 44058 \t\t Training Loss: 0.000580746098421514 \t\n",
      "Epoch 44059 \t\t Training Loss: 0.0005807460402138531 \t\n",
      "Epoch 44060 \t\t Training Loss: 0.000580746098421514 \t\n",
      "Epoch 44061 \t\t Training Loss: 0.000580746098421514 \t\n",
      "Epoch 44062 \t\t Training Loss: 0.000580746098421514 \t\n",
      "Epoch 44063 \t\t Training Loss: 0.000580746098421514 \t\n",
      "Epoch 44064 \t\t Training Loss: 0.000580746098421514 \t\n",
      "Epoch 44065 \t\t Training Loss: 0.000580746098421514 \t\n",
      "Epoch 44066 \t\t Training Loss: 0.000580746098421514 \t\n",
      "Epoch 44067 \t\t Training Loss: 0.0005807460402138531 \t\n",
      "Epoch 44068 \t\t Training Loss: 0.0005807460402138531 \t\n",
      "Epoch 44069 \t\t Training Loss: 0.0005807460402138531 \t\n",
      "Epoch 44070 \t\t Training Loss: 0.0005807460402138531 \t\n",
      "Epoch 44071 \t\t Training Loss: 0.0005807460402138531 \t\n",
      "Epoch 44072 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44073 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44074 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44075 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44076 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44077 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44078 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44079 \t\t Training Loss: 0.0005807459820061922 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44080 \t\t Training Loss: 0.0005807460402138531 \t\n",
      "Epoch 44081 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44082 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44083 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44084 \t\t Training Loss: 0.0005807460402138531 \t\n",
      "Epoch 44085 \t\t Training Loss: 0.0005807460402138531 \t\n",
      "Epoch 44086 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44087 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44088 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44089 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44090 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44091 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44092 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44093 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44094 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44095 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44096 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44097 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44098 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44099 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44100 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44101 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44102 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44103 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44104 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44105 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44106 \t\t Training Loss: 0.0005807459820061922 \t\n",
      "Epoch 44107 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44108 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44109 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44110 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44111 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44112 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44113 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44114 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44115 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44116 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44117 \t\t Training Loss: 0.0005807458655908704 \t\n",
      "Epoch 44118 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44119 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44120 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44121 \t\t Training Loss: 0.0005807459237985313 \t\n",
      "Epoch 44122 \t\t Training Loss: 0.0005807458655908704 \t\n",
      "Epoch 44123 \t\t Training Loss: 0.0005807458655908704 \t\n",
      "Epoch 44124 \t\t Training Loss: 0.0005807458655908704 \t\n",
      "Epoch 44125 \t\t Training Loss: 0.0005807458655908704 \t\n",
      "Epoch 44126 \t\t Training Loss: 0.0005807457491755486 \t\n",
      "Epoch 44127 \t\t Training Loss: 0.0005807457491755486 \t\n",
      "Epoch 44128 \t\t Training Loss: 0.0005807457491755486 \t\n",
      "Epoch 44129 \t\t Training Loss: 0.0005807457491755486 \t\n",
      "Epoch 44130 \t\t Training Loss: 0.0005807457491755486 \t\n",
      "Epoch 44131 \t\t Training Loss: 0.0005807457491755486 \t\n",
      "Epoch 44132 \t\t Training Loss: 0.0005807456909678876 \t\n",
      "Epoch 44133 \t\t Training Loss: 0.0005807456909678876 \t\n",
      "Epoch 44134 \t\t Training Loss: 0.0005807456327602267 \t\n",
      "Epoch 44135 \t\t Training Loss: 0.0005807456909678876 \t\n",
      "Epoch 44136 \t\t Training Loss: 0.0005807457491755486 \t\n",
      "Epoch 44137 \t\t Training Loss: 0.0005807456909678876 \t\n",
      "Epoch 44138 \t\t Training Loss: 0.0005807456327602267 \t\n",
      "Epoch 44139 \t\t Training Loss: 0.0005807456327602267 \t\n",
      "Epoch 44140 \t\t Training Loss: 0.0005807456327602267 \t\n",
      "Epoch 44141 \t\t Training Loss: 0.0005807456327602267 \t\n",
      "Epoch 44142 \t\t Training Loss: 0.0005807456327602267 \t\n",
      "Epoch 44143 \t\t Training Loss: 0.0005807456327602267 \t\n",
      "Epoch 44144 \t\t Training Loss: 0.0005807456327602267 \t\n",
      "Epoch 44145 \t\t Training Loss: 0.0005807456909678876 \t\n",
      "Epoch 44146 \t\t Training Loss: 0.0005807456327602267 \t\n",
      "Epoch 44147 \t\t Training Loss: 0.0005807456327602267 \t\n",
      "Epoch 44148 \t\t Training Loss: 0.0005807456327602267 \t\n",
      "Epoch 44149 \t\t Training Loss: 0.0005807456327602267 \t\n",
      "Epoch 44150 \t\t Training Loss: 0.0005807456327602267 \t\n",
      "Epoch 44151 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44152 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44153 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44154 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44155 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44156 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44157 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44158 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44159 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44160 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44161 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44162 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44163 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44164 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44165 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44166 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44167 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44168 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44169 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44170 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44171 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44172 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44173 \t\t Training Loss: 0.000580745458137244 \t\n",
      "Epoch 44174 \t\t Training Loss: 0.000580745458137244 \t\n",
      "Epoch 44175 \t\t Training Loss: 0.000580745458137244 \t\n",
      "Epoch 44176 \t\t Training Loss: 0.000580745458137244 \t\n",
      "Epoch 44177 \t\t Training Loss: 0.000580745458137244 \t\n",
      "Epoch 44178 \t\t Training Loss: 0.000580745458137244 \t\n",
      "Epoch 44179 \t\t Training Loss: 0.000580745458137244 \t\n",
      "Epoch 44180 \t\t Training Loss: 0.000580745458137244 \t\n",
      "Epoch 44181 \t\t Training Loss: 0.000580745458137244 \t\n",
      "Epoch 44182 \t\t Training Loss: 0.0005807455745525658 \t\n",
      "Epoch 44183 \t\t Training Loss: 0.000580745458137244 \t\n",
      "Epoch 44184 \t\t Training Loss: 0.000580745458137244 \t\n",
      "Epoch 44185 \t\t Training Loss: 0.0005807453417219222 \t\n",
      "Epoch 44186 \t\t Training Loss: 0.0005807453417219222 \t\n",
      "Epoch 44187 \t\t Training Loss: 0.0005807453417219222 \t\n",
      "Epoch 44188 \t\t Training Loss: 0.0005807453417219222 \t\n",
      "Epoch 44189 \t\t Training Loss: 0.0005807452835142612 \t\n",
      "Epoch 44190 \t\t Training Loss: 0.0005807452835142612 \t\n",
      "Epoch 44191 \t\t Training Loss: 0.0005807452835142612 \t\n",
      "Epoch 44192 \t\t Training Loss: 0.0005807452835142612 \t\n",
      "Epoch 44193 \t\t Training Loss: 0.0005807452835142612 \t\n",
      "Epoch 44194 \t\t Training Loss: 0.0005807452835142612 \t\n",
      "Epoch 44195 \t\t Training Loss: 0.0005807453417219222 \t\n",
      "Epoch 44196 \t\t Training Loss: 0.0005807453417219222 \t\n",
      "Epoch 44197 \t\t Training Loss: 0.0005807453417219222 \t\n",
      "Epoch 44198 \t\t Training Loss: 0.0005807453417219222 \t\n",
      "Epoch 44199 \t\t Training Loss: 0.0005807453417219222 \t\n",
      "Epoch 44200 \t\t Training Loss: 0.0005807452835142612 \t\n",
      "Epoch 44201 \t\t Training Loss: 0.0005807452835142612 \t\n",
      "Epoch 44202 \t\t Training Loss: 0.0005807452835142612 \t\n",
      "Epoch 44203 \t\t Training Loss: 0.0005807452835142612 \t\n",
      "Epoch 44204 \t\t Training Loss: 0.0005807451670989394 \t\n",
      "Epoch 44205 \t\t Training Loss: 0.0005807451670989394 \t\n",
      "Epoch 44206 \t\t Training Loss: 0.0005807451670989394 \t\n",
      "Epoch 44207 \t\t Training Loss: 0.0005807452835142612 \t\n",
      "Epoch 44208 \t\t Training Loss: 0.0005807451670989394 \t\n",
      "Epoch 44209 \t\t Training Loss: 0.0005807451670989394 \t\n",
      "Epoch 44210 \t\t Training Loss: 0.0005807451670989394 \t\n",
      "Epoch 44211 \t\t Training Loss: 0.0005807452835142612 \t\n",
      "Epoch 44212 \t\t Training Loss: 0.0005807452835142612 \t\n",
      "Epoch 44213 \t\t Training Loss: 0.0005807452835142612 \t\n",
      "Epoch 44214 \t\t Training Loss: 0.0005807452835142612 \t\n",
      "Epoch 44215 \t\t Training Loss: 0.0005807452835142612 \t\n",
      "Epoch 44216 \t\t Training Loss: 0.0005807451670989394 \t\n",
      "Epoch 44217 \t\t Training Loss: 0.0005807451670989394 \t\n",
      "Epoch 44218 \t\t Training Loss: 0.0005807451670989394 \t\n",
      "Epoch 44219 \t\t Training Loss: 0.0005807451670989394 \t\n",
      "Epoch 44220 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44221 \t\t Training Loss: 0.0005807451670989394 \t\n",
      "Epoch 44222 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44223 \t\t Training Loss: 0.0005807451670989394 \t\n",
      "Epoch 44224 \t\t Training Loss: 0.0005807451670989394 \t\n",
      "Epoch 44225 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44226 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44227 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44228 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44229 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44230 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44231 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44232 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44233 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44234 \t\t Training Loss: 0.0005807451670989394 \t\n",
      "Epoch 44235 \t\t Training Loss: 0.0005807451670989394 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44236 \t\t Training Loss: 0.0005807451670989394 \t\n",
      "Epoch 44237 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44238 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44239 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44240 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44241 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44242 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44243 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44244 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44245 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44246 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44247 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44248 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44249 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44250 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44251 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44252 \t\t Training Loss: 0.0005807449924759567 \t\n",
      "Epoch 44253 \t\t Training Loss: 0.0005807449924759567 \t\n",
      "Epoch 44254 \t\t Training Loss: 0.0005807449924759567 \t\n",
      "Epoch 44255 \t\t Training Loss: 0.0005807449924759567 \t\n",
      "Epoch 44256 \t\t Training Loss: 0.0005807450506836176 \t\n",
      "Epoch 44257 \t\t Training Loss: 0.0005807449342682958 \t\n",
      "Epoch 44258 \t\t Training Loss: 0.0005807449924759567 \t\n",
      "Epoch 44259 \t\t Training Loss: 0.0005807449342682958 \t\n",
      "Epoch 44260 \t\t Training Loss: 0.0005807449342682958 \t\n",
      "Epoch 44261 \t\t Training Loss: 0.0005807449342682958 \t\n",
      "Epoch 44262 \t\t Training Loss: 0.0005807449924759567 \t\n",
      "Epoch 44263 \t\t Training Loss: 0.0005807449342682958 \t\n",
      "Epoch 44264 \t\t Training Loss: 0.0005807449342682958 \t\n",
      "Epoch 44265 \t\t Training Loss: 0.0005807449342682958 \t\n",
      "Epoch 44266 \t\t Training Loss: 0.0005807449342682958 \t\n",
      "Epoch 44267 \t\t Training Loss: 0.0005807449342682958 \t\n",
      "Epoch 44268 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44269 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44270 \t\t Training Loss: 0.0005807449342682958 \t\n",
      "Epoch 44271 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44272 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44273 \t\t Training Loss: 0.0005807449342682958 \t\n",
      "Epoch 44274 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44275 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44276 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44277 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44278 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44279 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44280 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44281 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44282 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44283 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44284 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44285 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44286 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44287 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44288 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44289 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44290 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44291 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44292 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44293 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44294 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44295 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44296 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44297 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44298 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44299 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44300 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44301 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44302 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44303 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44304 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44305 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44306 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44307 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44308 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44309 \t\t Training Loss: 0.0005807448760606349 \t\n",
      "Epoch 44310 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44311 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44312 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44313 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44314 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44315 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44316 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44317 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44318 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44319 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44320 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44321 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44322 \t\t Training Loss: 0.0005807446432299912 \t\n",
      "Epoch 44323 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44324 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44325 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44326 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44327 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44328 \t\t Training Loss: 0.000580744759645313 \t\n",
      "Epoch 44329 \t\t Training Loss: 0.0005807446432299912 \t\n",
      "Epoch 44330 \t\t Training Loss: 0.0005807446432299912 \t\n",
      "Epoch 44331 \t\t Training Loss: 0.0005807445850223303 \t\n",
      "Epoch 44332 \t\t Training Loss: 0.0005807445850223303 \t\n",
      "Epoch 44333 \t\t Training Loss: 0.0005807445850223303 \t\n",
      "Epoch 44334 \t\t Training Loss: 0.0005807445850223303 \t\n",
      "Epoch 44335 \t\t Training Loss: 0.0005807445850223303 \t\n",
      "Epoch 44336 \t\t Training Loss: 0.0005807445850223303 \t\n",
      "Epoch 44337 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44338 \t\t Training Loss: 0.0005807445850223303 \t\n",
      "Epoch 44339 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44340 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44341 \t\t Training Loss: 0.0005807445850223303 \t\n",
      "Epoch 44342 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44343 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44344 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44345 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44346 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44347 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44348 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44349 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44350 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44351 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44352 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44353 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44354 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44355 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44356 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44357 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44358 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44359 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44360 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44361 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44362 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44363 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44364 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44365 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44366 \t\t Training Loss: 0.0005807444686070085 \t\n",
      "Epoch 44367 \t\t Training Loss: 0.0005807443521916866 \t\n",
      "Epoch 44368 \t\t Training Loss: 0.0005807443521916866 \t\n",
      "Epoch 44369 \t\t Training Loss: 0.0005807443521916866 \t\n",
      "Epoch 44370 \t\t Training Loss: 0.0005807443521916866 \t\n",
      "Epoch 44371 \t\t Training Loss: 0.0005807443521916866 \t\n",
      "Epoch 44372 \t\t Training Loss: 0.0005807443521916866 \t\n",
      "Epoch 44373 \t\t Training Loss: 0.0005807443521916866 \t\n",
      "Epoch 44374 \t\t Training Loss: 0.0005807443521916866 \t\n",
      "Epoch 44375 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44376 \t\t Training Loss: 0.0005807443521916866 \t\n",
      "Epoch 44377 \t\t Training Loss: 0.0005807443521916866 \t\n",
      "Epoch 44378 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44379 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44380 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44381 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44382 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44383 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44384 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44385 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44386 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44387 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44388 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44389 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44390 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44391 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44392 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44393 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44394 \t\t Training Loss: 0.0005807441775687039 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44395 \t\t Training Loss: 0.0005807441775687039 \t\n",
      "Epoch 44396 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44397 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44398 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44399 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44400 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44401 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44402 \t\t Training Loss: 0.0005807441775687039 \t\n",
      "Epoch 44403 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44404 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44405 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44406 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44407 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44408 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44409 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44410 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44411 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44412 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44413 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44414 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44415 \t\t Training Loss: 0.0005807442939840257 \t\n",
      "Epoch 44416 \t\t Training Loss: 0.0005807441775687039 \t\n",
      "Epoch 44417 \t\t Training Loss: 0.0005807441775687039 \t\n",
      "Epoch 44418 \t\t Training Loss: 0.0005807441775687039 \t\n",
      "Epoch 44419 \t\t Training Loss: 0.0005807441775687039 \t\n",
      "Epoch 44420 \t\t Training Loss: 0.0005807441775687039 \t\n",
      "Epoch 44421 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44422 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44423 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44424 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44425 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44426 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44427 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44428 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44429 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44430 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44431 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44432 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44433 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44434 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44435 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44436 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44437 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44438 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44439 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44440 \t\t Training Loss: 0.0005807441775687039 \t\n",
      "Epoch 44441 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44442 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44443 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44444 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44445 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44446 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44447 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44448 \t\t Training Loss: 0.0005807440029457211 \t\n",
      "Epoch 44449 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44450 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44451 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44452 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44453 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44454 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44455 \t\t Training Loss: 0.0005807440611533821 \t\n",
      "Epoch 44456 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44457 \t\t Training Loss: 0.0005807440029457211 \t\n",
      "Epoch 44458 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44459 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44460 \t\t Training Loss: 0.0005807440029457211 \t\n",
      "Epoch 44461 \t\t Training Loss: 0.0005807440029457211 \t\n",
      "Epoch 44462 \t\t Training Loss: 0.0005807440029457211 \t\n",
      "Epoch 44463 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44464 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44465 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44466 \t\t Training Loss: 0.0005807440029457211 \t\n",
      "Epoch 44467 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44468 \t\t Training Loss: 0.0005807440029457211 \t\n",
      "Epoch 44469 \t\t Training Loss: 0.0005807440029457211 \t\n",
      "Epoch 44470 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44471 \t\t Training Loss: 0.0005807440029457211 \t\n",
      "Epoch 44472 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44473 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44474 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44475 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44476 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44477 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44478 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44479 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44480 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44481 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44482 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44483 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44484 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44485 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44486 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44487 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44488 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44489 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44490 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44491 \t\t Training Loss: 0.0005807438865303993 \t\n",
      "Epoch 44492 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44493 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44494 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44495 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44496 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44497 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44498 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44499 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44500 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44501 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44502 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44503 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44504 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44505 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44506 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44507 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44508 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44509 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44510 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44511 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44512 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44513 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44514 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44515 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44516 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44517 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44518 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44519 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44520 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44521 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44522 \t\t Training Loss: 0.0005807436536997557 \t\n",
      "Epoch 44523 \t\t Training Loss: 0.0005807436536997557 \t\n",
      "Epoch 44524 \t\t Training Loss: 0.0005807437701150775 \t\n",
      "Epoch 44525 \t\t Training Loss: 0.0005807436536997557 \t\n",
      "Epoch 44526 \t\t Training Loss: 0.0005807436536997557 \t\n",
      "Epoch 44527 \t\t Training Loss: 0.0005807436536997557 \t\n",
      "Epoch 44528 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44529 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44530 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44531 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44532 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44533 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44534 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44535 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44536 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44537 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44538 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44539 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44540 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44541 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44542 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44543 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44544 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44545 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44546 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44547 \t\t Training Loss: 0.0005807434790767729 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44548 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44549 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44550 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44551 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44552 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44553 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44554 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44555 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44556 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44557 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44558 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44559 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44560 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44561 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44562 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44563 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44564 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44565 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44566 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44567 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44568 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44569 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44570 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44571 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44572 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44573 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44574 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44575 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44576 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44577 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44578 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44579 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44580 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44581 \t\t Training Loss: 0.0005807435954920948 \t\n",
      "Epoch 44582 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44583 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44584 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44585 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44586 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44587 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44588 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44589 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44590 \t\t Training Loss: 0.0005807434790767729 \t\n",
      "Epoch 44591 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44592 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44593 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44594 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44595 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44596 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44597 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44598 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44599 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44600 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44601 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44602 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44603 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44604 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44605 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44606 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44607 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44608 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44609 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44610 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44611 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44612 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44613 \t\t Training Loss: 0.0005807433626614511 \t\n",
      "Epoch 44614 \t\t Training Loss: 0.0005807433044537902 \t\n",
      "Epoch 44615 \t\t Training Loss: 0.0005807433044537902 \t\n",
      "Epoch 44616 \t\t Training Loss: 0.0005807433044537902 \t\n",
      "Epoch 44617 \t\t Training Loss: 0.0005807431880384684 \t\n",
      "Epoch 44618 \t\t Training Loss: 0.0005807431880384684 \t\n",
      "Epoch 44619 \t\t Training Loss: 0.0005807431880384684 \t\n",
      "Epoch 44620 \t\t Training Loss: 0.0005807431880384684 \t\n",
      "Epoch 44621 \t\t Training Loss: 0.0005807431880384684 \t\n",
      "Epoch 44622 \t\t Training Loss: 0.0005807431880384684 \t\n",
      "Epoch 44623 \t\t Training Loss: 0.0005807431880384684 \t\n",
      "Epoch 44624 \t\t Training Loss: 0.0005807431880384684 \t\n",
      "Epoch 44625 \t\t Training Loss: 0.0005807431880384684 \t\n",
      "Epoch 44626 \t\t Training Loss: 0.0005807431880384684 \t\n",
      "Epoch 44627 \t\t Training Loss: 0.0005807431880384684 \t\n",
      "Epoch 44628 \t\t Training Loss: 0.0005807431880384684 \t\n",
      "Epoch 44629 \t\t Training Loss: 0.0005807431880384684 \t\n",
      "Epoch 44630 \t\t Training Loss: 0.0005807430716231465 \t\n",
      "Epoch 44631 \t\t Training Loss: 0.0005807430716231465 \t\n",
      "Epoch 44632 \t\t Training Loss: 0.0005807430716231465 \t\n",
      "Epoch 44633 \t\t Training Loss: 0.0005807430716231465 \t\n",
      "Epoch 44634 \t\t Training Loss: 0.0005807430716231465 \t\n",
      "Epoch 44635 \t\t Training Loss: 0.0005807430716231465 \t\n",
      "Epoch 44636 \t\t Training Loss: 0.0005807430134154856 \t\n",
      "Epoch 44637 \t\t Training Loss: 0.0005807430716231465 \t\n",
      "Epoch 44638 \t\t Training Loss: 0.0005807430716231465 \t\n",
      "Epoch 44639 \t\t Training Loss: 0.0005807430716231465 \t\n",
      "Epoch 44640 \t\t Training Loss: 0.0005807430134154856 \t\n",
      "Epoch 44641 \t\t Training Loss: 0.0005807430134154856 \t\n",
      "Epoch 44642 \t\t Training Loss: 0.0005807429552078247 \t\n",
      "Epoch 44643 \t\t Training Loss: 0.0005807429552078247 \t\n",
      "Epoch 44644 \t\t Training Loss: 0.0005807430134154856 \t\n",
      "Epoch 44645 \t\t Training Loss: 0.0005807430134154856 \t\n",
      "Epoch 44646 \t\t Training Loss: 0.0005807430134154856 \t\n",
      "Epoch 44647 \t\t Training Loss: 0.0005807429552078247 \t\n",
      "Epoch 44648 \t\t Training Loss: 0.0005807430134154856 \t\n",
      "Epoch 44649 \t\t Training Loss: 0.0005807429552078247 \t\n",
      "Epoch 44650 \t\t Training Loss: 0.0005807430134154856 \t\n",
      "Epoch 44651 \t\t Training Loss: 0.0005807430134154856 \t\n",
      "Epoch 44652 \t\t Training Loss: 0.0005807429552078247 \t\n",
      "Epoch 44653 \t\t Training Loss: 0.0005807429552078247 \t\n",
      "Epoch 44654 \t\t Training Loss: 0.0005807429552078247 \t\n",
      "Epoch 44655 \t\t Training Loss: 0.0005807430134154856 \t\n",
      "Epoch 44656 \t\t Training Loss: 0.0005807429552078247 \t\n",
      "Epoch 44657 \t\t Training Loss: 0.0005807429552078247 \t\n",
      "Epoch 44658 \t\t Training Loss: 0.0005807429552078247 \t\n",
      "Epoch 44659 \t\t Training Loss: 0.0005807428970001638 \t\n",
      "Epoch 44660 \t\t Training Loss: 0.0005807428970001638 \t\n",
      "Epoch 44661 \t\t Training Loss: 0.0005807429552078247 \t\n",
      "Epoch 44662 \t\t Training Loss: 0.0005807429552078247 \t\n",
      "Epoch 44663 \t\t Training Loss: 0.0005807429552078247 \t\n",
      "Epoch 44664 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44665 \t\t Training Loss: 0.0005807428970001638 \t\n",
      "Epoch 44666 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44667 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44668 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44669 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44670 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44671 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44672 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44673 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44674 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44675 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44676 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44677 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44678 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44679 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44680 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44681 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44682 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44683 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44684 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44685 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44686 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44687 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44688 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44689 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44690 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44691 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44692 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44693 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44694 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44695 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44696 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44697 \t\t Training Loss: 0.000580742780584842 \t\n",
      "Epoch 44698 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44699 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44700 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44701 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44702 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44703 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44704 \t\t Training Loss: 0.0005807426059618592 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44705 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44706 \t\t Training Loss: 0.0005807426641695201 \t\n",
      "Epoch 44707 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44708 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44709 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44710 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44711 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44712 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44713 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44714 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44715 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44716 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44717 \t\t Training Loss: 0.000580742722377181 \t\n",
      "Epoch 44718 \t\t Training Loss: 0.0005807426641695201 \t\n",
      "Epoch 44719 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44720 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44721 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44722 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44723 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44724 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44725 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44726 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44727 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44728 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44729 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44730 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44731 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44732 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44733 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44734 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44735 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44736 \t\t Training Loss: 0.0005807426059618592 \t\n",
      "Epoch 44737 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44738 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44739 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44740 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44741 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44742 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44743 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44744 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44745 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44746 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44747 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44748 \t\t Training Loss: 0.0005807424895465374 \t\n",
      "Epoch 44749 \t\t Training Loss: 0.0005807424895465374 \t\n",
      "Epoch 44750 \t\t Training Loss: 0.0005807424895465374 \t\n",
      "Epoch 44751 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44752 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44753 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44754 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44755 \t\t Training Loss: 0.0005807425477541983 \t\n",
      "Epoch 44756 \t\t Training Loss: 0.0005807424895465374 \t\n",
      "Epoch 44757 \t\t Training Loss: 0.0005807424895465374 \t\n",
      "Epoch 44758 \t\t Training Loss: 0.0005807424313388765 \t\n",
      "Epoch 44759 \t\t Training Loss: 0.0005807424313388765 \t\n",
      "Epoch 44760 \t\t Training Loss: 0.0005807424313388765 \t\n",
      "Epoch 44761 \t\t Training Loss: 0.0005807424313388765 \t\n",
      "Epoch 44762 \t\t Training Loss: 0.0005807423731312156 \t\n",
      "Epoch 44763 \t\t Training Loss: 0.0005807423731312156 \t\n",
      "Epoch 44764 \t\t Training Loss: 0.0005807423731312156 \t\n",
      "Epoch 44765 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44766 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44767 \t\t Training Loss: 0.0005807422567158937 \t\n",
      "Epoch 44768 \t\t Training Loss: 0.0005807422567158937 \t\n",
      "Epoch 44769 \t\t Training Loss: 0.0005807422567158937 \t\n",
      "Epoch 44770 \t\t Training Loss: 0.0005807422567158937 \t\n",
      "Epoch 44771 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44772 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44773 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44774 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44775 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44776 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44777 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44778 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44779 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44780 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44781 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44782 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44783 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44784 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44785 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44786 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44787 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44788 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44789 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44790 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44791 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44792 \t\t Training Loss: 0.0005807422567158937 \t\n",
      "Epoch 44793 \t\t Training Loss: 0.0005807423149235547 \t\n",
      "Epoch 44794 \t\t Training Loss: 0.0005807422567158937 \t\n",
      "Epoch 44795 \t\t Training Loss: 0.0005807421985082328 \t\n",
      "Epoch 44796 \t\t Training Loss: 0.0005807421985082328 \t\n",
      "Epoch 44797 \t\t Training Loss: 0.0005807421985082328 \t\n",
      "Epoch 44798 \t\t Training Loss: 0.0005807421985082328 \t\n",
      "Epoch 44799 \t\t Training Loss: 0.0005807421985082328 \t\n",
      "Epoch 44800 \t\t Training Loss: 0.0005807421985082328 \t\n",
      "Epoch 44801 \t\t Training Loss: 0.0005807421985082328 \t\n",
      "Epoch 44802 \t\t Training Loss: 0.0005807421985082328 \t\n",
      "Epoch 44803 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44804 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44805 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44806 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44807 \t\t Training Loss: 0.0005807421985082328 \t\n",
      "Epoch 44808 \t\t Training Loss: 0.0005807421985082328 \t\n",
      "Epoch 44809 \t\t Training Loss: 0.0005807421985082328 \t\n",
      "Epoch 44810 \t\t Training Loss: 0.0005807421985082328 \t\n",
      "Epoch 44811 \t\t Training Loss: 0.0005807421985082328 \t\n",
      "Epoch 44812 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44813 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44814 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44815 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44816 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44817 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44818 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44819 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44820 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44821 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44822 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44823 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44824 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44825 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44826 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44827 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44828 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44829 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44830 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44831 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44832 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44833 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44834 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44835 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44836 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44837 \t\t Training Loss: 0.000580742082092911 \t\n",
      "Epoch 44838 \t\t Training Loss: 0.0005807419656775892 \t\n",
      "Epoch 44839 \t\t Training Loss: 0.0005807419656775892 \t\n",
      "Epoch 44840 \t\t Training Loss: 0.0005807419656775892 \t\n",
      "Epoch 44841 \t\t Training Loss: 0.0005807418492622674 \t\n",
      "Epoch 44842 \t\t Training Loss: 0.0005807418492622674 \t\n",
      "Epoch 44843 \t\t Training Loss: 0.0005807418492622674 \t\n",
      "Epoch 44844 \t\t Training Loss: 0.0005807418492622674 \t\n",
      "Epoch 44845 \t\t Training Loss: 0.0005807418492622674 \t\n",
      "Epoch 44846 \t\t Training Loss: 0.0005807418492622674 \t\n",
      "Epoch 44847 \t\t Training Loss: 0.0005807418492622674 \t\n",
      "Epoch 44848 \t\t Training Loss: 0.0005807418492622674 \t\n",
      "Epoch 44849 \t\t Training Loss: 0.0005807418492622674 \t\n",
      "Epoch 44850 \t\t Training Loss: 0.0005807417910546064 \t\n",
      "Epoch 44851 \t\t Training Loss: 0.0005807417910546064 \t\n",
      "Epoch 44852 \t\t Training Loss: 0.0005807417910546064 \t\n",
      "Epoch 44853 \t\t Training Loss: 0.0005807417910546064 \t\n",
      "Epoch 44854 \t\t Training Loss: 0.0005807418492622674 \t\n",
      "Epoch 44855 \t\t Training Loss: 0.0005807417910546064 \t\n",
      "Epoch 44856 \t\t Training Loss: 0.0005807417910546064 \t\n",
      "Epoch 44857 \t\t Training Loss: 0.0005807417910546064 \t\n",
      "Epoch 44858 \t\t Training Loss: 0.0005807417328469455 \t\n",
      "Epoch 44859 \t\t Training Loss: 0.0005807417328469455 \t\n",
      "Epoch 44860 \t\t Training Loss: 0.0005807417328469455 \t\n",
      "Epoch 44861 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44862 \t\t Training Loss: 0.0005807417328469455 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44863 \t\t Training Loss: 0.0005807417910546064 \t\n",
      "Epoch 44864 \t\t Training Loss: 0.0005807417328469455 \t\n",
      "Epoch 44865 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44866 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44867 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44868 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44869 \t\t Training Loss: 0.0005807417328469455 \t\n",
      "Epoch 44870 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44871 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44872 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44873 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44874 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44875 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44876 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44877 \t\t Training Loss: 0.0005807417328469455 \t\n",
      "Epoch 44878 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44879 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44880 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44881 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44882 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44883 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44884 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44885 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44886 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44887 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44888 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44889 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44890 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44891 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44892 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44893 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44894 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44895 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44896 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44897 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44898 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44899 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44900 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44901 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44902 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44903 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44904 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44905 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44906 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44907 \t\t Training Loss: 0.0005807415582239628 \t\n",
      "Epoch 44908 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44909 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44910 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44911 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44912 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44913 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44914 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44915 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44916 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44917 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44918 \t\t Training Loss: 0.0005807416746392846 \t\n",
      "Epoch 44919 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44920 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44921 \t\t Training Loss: 0.0005807415582239628 \t\n",
      "Epoch 44922 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44923 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44924 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44925 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44926 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44927 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44928 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44929 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44930 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44931 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44932 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44933 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44934 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44935 \t\t Training Loss: 0.0005807415582239628 \t\n",
      "Epoch 44936 \t\t Training Loss: 0.0005807415582239628 \t\n",
      "Epoch 44937 \t\t Training Loss: 0.0005807415582239628 \t\n",
      "Epoch 44938 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44939 \t\t Training Loss: 0.0005807416164316237 \t\n",
      "Epoch 44940 \t\t Training Loss: 0.0005807415582239628 \t\n",
      "Epoch 44941 \t\t Training Loss: 0.0005807415582239628 \t\n",
      "Epoch 44942 \t\t Training Loss: 0.0005807415582239628 \t\n",
      "Epoch 44943 \t\t Training Loss: 0.0005807415582239628 \t\n",
      "Epoch 44944 \t\t Training Loss: 0.0005807415582239628 \t\n",
      "Epoch 44945 \t\t Training Loss: 0.0005807415582239628 \t\n",
      "Epoch 44946 \t\t Training Loss: 0.0005807415582239628 \t\n",
      "Epoch 44947 \t\t Training Loss: 0.0005807415582239628 \t\n",
      "Epoch 44948 \t\t Training Loss: 0.0005807415582239628 \t\n",
      "Epoch 44949 \t\t Training Loss: 0.0005807415582239628 \t\n",
      "Epoch 44950 \t\t Training Loss: 0.00058074138360098 \t\n",
      "Epoch 44951 \t\t Training Loss: 0.00058074138360098 \t\n",
      "Epoch 44952 \t\t Training Loss: 0.00058074138360098 \t\n",
      "Epoch 44953 \t\t Training Loss: 0.00058074138360098 \t\n",
      "Epoch 44954 \t\t Training Loss: 0.00058074138360098 \t\n",
      "Epoch 44955 \t\t Training Loss: 0.00058074138360098 \t\n",
      "Epoch 44956 \t\t Training Loss: 0.00058074138360098 \t\n",
      "Epoch 44957 \t\t Training Loss: 0.00058074138360098 \t\n",
      "Epoch 44958 \t\t Training Loss: 0.00058074138360098 \t\n",
      "Epoch 44959 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44960 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44961 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44962 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44963 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44964 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44965 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44966 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44967 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44968 \t\t Training Loss: 0.0005807413253933191 \t\n",
      "Epoch 44969 \t\t Training Loss: 0.0005807413253933191 \t\n",
      "Epoch 44970 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44971 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44972 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44973 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44974 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44975 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44976 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44977 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44978 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44979 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44980 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44981 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44982 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44983 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44984 \t\t Training Loss: 0.0005807412671856582 \t\n",
      "Epoch 44985 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 44986 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 44987 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 44988 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 44989 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 44990 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 44991 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 44992 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 44993 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 44994 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 44995 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 44996 \t\t Training Loss: 0.0005807412089779973 \t\n",
      "Epoch 44997 \t\t Training Loss: 0.0005807412089779973 \t\n",
      "Epoch 44998 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 44999 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 45000 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 45001 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 45002 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 45003 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 45004 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 45005 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 45006 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 45007 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45008 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 45009 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45010 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 45011 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 45012 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 45013 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 45014 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 45015 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45016 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 45017 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45018 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45019 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45020 \t\t Training Loss: 0.0005807410925626755 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45021 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45022 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45023 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45024 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45025 \t\t Training Loss: 0.0005807411507703364 \t\n",
      "Epoch 45026 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45027 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45028 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45029 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45030 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45031 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45032 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45033 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45034 \t\t Training Loss: 0.0005807410343550146 \t\n",
      "Epoch 45035 \t\t Training Loss: 0.0005807410343550146 \t\n",
      "Epoch 45036 \t\t Training Loss: 0.0005807409761473536 \t\n",
      "Epoch 45037 \t\t Training Loss: 0.0005807410343550146 \t\n",
      "Epoch 45038 \t\t Training Loss: 0.0005807409761473536 \t\n",
      "Epoch 45039 \t\t Training Loss: 0.0005807410343550146 \t\n",
      "Epoch 45040 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45041 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45042 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45043 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45044 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45045 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45046 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45047 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45048 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45049 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45050 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45051 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45052 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45053 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45054 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45055 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45056 \t\t Training Loss: 0.0005807410343550146 \t\n",
      "Epoch 45057 \t\t Training Loss: 0.0005807410343550146 \t\n",
      "Epoch 45058 \t\t Training Loss: 0.0005807410343550146 \t\n",
      "Epoch 45059 \t\t Training Loss: 0.0005807409179396927 \t\n",
      "Epoch 45060 \t\t Training Loss: 0.0005807410343550146 \t\n",
      "Epoch 45061 \t\t Training Loss: 0.0005807410343550146 \t\n",
      "Epoch 45062 \t\t Training Loss: 0.0005807409761473536 \t\n",
      "Epoch 45063 \t\t Training Loss: 0.0005807409761473536 \t\n",
      "Epoch 45064 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45065 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45066 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45067 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45068 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45069 \t\t Training Loss: 0.0005807410925626755 \t\n",
      "Epoch 45070 \t\t Training Loss: 0.0005807410343550146 \t\n",
      "Epoch 45071 \t\t Training Loss: 0.0005807409761473536 \t\n",
      "Epoch 45072 \t\t Training Loss: 0.0005807409179396927 \t\n",
      "Epoch 45073 \t\t Training Loss: 0.0005807410343550146 \t\n",
      "Epoch 45074 \t\t Training Loss: 0.0005807409179396927 \t\n",
      "Epoch 45075 \t\t Training Loss: 0.0005807409179396927 \t\n",
      "Epoch 45076 \t\t Training Loss: 0.0005807409179396927 \t\n",
      "Epoch 45077 \t\t Training Loss: 0.0005807409179396927 \t\n",
      "Epoch 45078 \t\t Training Loss: 0.0005807409179396927 \t\n",
      "Epoch 45079 \t\t Training Loss: 0.0005807409179396927 \t\n",
      "Epoch 45080 \t\t Training Loss: 0.0005807409179396927 \t\n",
      "Epoch 45081 \t\t Training Loss: 0.0005807408015243709 \t\n",
      "Epoch 45082 \t\t Training Loss: 0.0005807408015243709 \t\n",
      "Epoch 45083 \t\t Training Loss: 0.0005807408015243709 \t\n",
      "Epoch 45084 \t\t Training Loss: 0.0005807408015243709 \t\n",
      "Epoch 45085 \t\t Training Loss: 0.0005807408015243709 \t\n",
      "Epoch 45086 \t\t Training Loss: 0.00058074074331671 \t\n",
      "Epoch 45087 \t\t Training Loss: 0.00058074074331671 \t\n",
      "Epoch 45088 \t\t Training Loss: 0.0005807406851090491 \t\n",
      "Epoch 45089 \t\t Training Loss: 0.0005807406851090491 \t\n",
      "Epoch 45090 \t\t Training Loss: 0.0005807406851090491 \t\n",
      "Epoch 45091 \t\t Training Loss: 0.0005807406851090491 \t\n",
      "Epoch 45092 \t\t Training Loss: 0.0005807406851090491 \t\n",
      "Epoch 45093 \t\t Training Loss: 0.0005807406851090491 \t\n",
      "Epoch 45094 \t\t Training Loss: 0.0005807406851090491 \t\n",
      "Epoch 45095 \t\t Training Loss: 0.0005807406851090491 \t\n",
      "Epoch 45096 \t\t Training Loss: 0.0005807406851090491 \t\n",
      "Epoch 45097 \t\t Training Loss: 0.0005807406851090491 \t\n",
      "Epoch 45098 \t\t Training Loss: 0.0005807406851090491 \t\n",
      "Epoch 45099 \t\t Training Loss: 0.0005807406851090491 \t\n",
      "Epoch 45100 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45101 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45102 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45103 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45104 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45105 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45106 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45107 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45108 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45109 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45110 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45111 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45112 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45113 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45114 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45115 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45116 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45117 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45118 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45119 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45120 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45121 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45122 \t\t Training Loss: 0.0005807406269013882 \t\n",
      "Epoch 45123 \t\t Training Loss: 0.0005807405104860663 \t\n",
      "Epoch 45124 \t\t Training Loss: 0.0005807405104860663 \t\n",
      "Epoch 45125 \t\t Training Loss: 0.0005807405104860663 \t\n",
      "Epoch 45126 \t\t Training Loss: 0.0005807405104860663 \t\n",
      "Epoch 45127 \t\t Training Loss: 0.0005807405104860663 \t\n",
      "Epoch 45128 \t\t Training Loss: 0.0005807405104860663 \t\n",
      "Epoch 45129 \t\t Training Loss: 0.0005807405104860663 \t\n",
      "Epoch 45130 \t\t Training Loss: 0.0005807405104860663 \t\n",
      "Epoch 45131 \t\t Training Loss: 0.0005807405104860663 \t\n",
      "Epoch 45132 \t\t Training Loss: 0.0005807405104860663 \t\n",
      "Epoch 45133 \t\t Training Loss: 0.0005807405104860663 \t\n",
      "Epoch 45134 \t\t Training Loss: 0.0005807404522784054 \t\n",
      "Epoch 45135 \t\t Training Loss: 0.0005807404522784054 \t\n",
      "Epoch 45136 \t\t Training Loss: 0.0005807404522784054 \t\n",
      "Epoch 45137 \t\t Training Loss: 0.0005807404522784054 \t\n",
      "Epoch 45138 \t\t Training Loss: 0.0005807404522784054 \t\n",
      "Epoch 45139 \t\t Training Loss: 0.0005807404522784054 \t\n",
      "Epoch 45140 \t\t Training Loss: 0.0005807404522784054 \t\n",
      "Epoch 45141 \t\t Training Loss: 0.0005807404522784054 \t\n",
      "Epoch 45142 \t\t Training Loss: 0.0005807404522784054 \t\n",
      "Epoch 45143 \t\t Training Loss: 0.0005807404522784054 \t\n",
      "Epoch 45144 \t\t Training Loss: 0.0005807404522784054 \t\n",
      "Epoch 45145 \t\t Training Loss: 0.0005807404522784054 \t\n",
      "Epoch 45146 \t\t Training Loss: 0.0005807403940707445 \t\n",
      "Epoch 45147 \t\t Training Loss: 0.0005807404522784054 \t\n",
      "Epoch 45148 \t\t Training Loss: 0.0005807403940707445 \t\n",
      "Epoch 45149 \t\t Training Loss: 0.0005807403940707445 \t\n",
      "Epoch 45150 \t\t Training Loss: 0.0005807403358630836 \t\n",
      "Epoch 45151 \t\t Training Loss: 0.0005807404522784054 \t\n",
      "Epoch 45152 \t\t Training Loss: 0.0005807403940707445 \t\n",
      "Epoch 45153 \t\t Training Loss: 0.0005807403940707445 \t\n",
      "Epoch 45154 \t\t Training Loss: 0.0005807403940707445 \t\n",
      "Epoch 45155 \t\t Training Loss: 0.0005807403940707445 \t\n",
      "Epoch 45156 \t\t Training Loss: 0.0005807403940707445 \t\n",
      "Epoch 45157 \t\t Training Loss: 0.0005807403940707445 \t\n",
      "Epoch 45158 \t\t Training Loss: 0.0005807403940707445 \t\n",
      "Epoch 45159 \t\t Training Loss: 0.0005807403358630836 \t\n",
      "Epoch 45160 \t\t Training Loss: 0.0005807403358630836 \t\n",
      "Epoch 45161 \t\t Training Loss: 0.0005807403358630836 \t\n",
      "Epoch 45162 \t\t Training Loss: 0.0005807403940707445 \t\n",
      "Epoch 45163 \t\t Training Loss: 0.0005807404522784054 \t\n",
      "Epoch 45164 \t\t Training Loss: 0.0005807403358630836 \t\n",
      "Epoch 45165 \t\t Training Loss: 0.0005807403358630836 \t\n",
      "Epoch 45166 \t\t Training Loss: 0.0005807403358630836 \t\n",
      "Epoch 45167 \t\t Training Loss: 0.0005807403940707445 \t\n",
      "Epoch 45168 \t\t Training Loss: 0.0005807403940707445 \t\n",
      "Epoch 45169 \t\t Training Loss: 0.0005807403940707445 \t\n",
      "Epoch 45170 \t\t Training Loss: 0.0005807403358630836 \t\n",
      "Epoch 45171 \t\t Training Loss: 0.0005807403358630836 \t\n",
      "Epoch 45172 \t\t Training Loss: 0.0005807403358630836 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45173 \t\t Training Loss: 0.0005807403358630836 \t\n",
      "Epoch 45174 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45175 \t\t Training Loss: 0.0005807403358630836 \t\n",
      "Epoch 45176 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45177 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45178 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45179 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45180 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45181 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45182 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45183 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45184 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45185 \t\t Training Loss: 0.0005807402776554227 \t\n",
      "Epoch 45186 \t\t Training Loss: 0.0005807402776554227 \t\n",
      "Epoch 45187 \t\t Training Loss: 0.0005807403358630836 \t\n",
      "Epoch 45188 \t\t Training Loss: 0.0005807403358630836 \t\n",
      "Epoch 45189 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45190 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45191 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45192 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45193 \t\t Training Loss: 0.0005807401612401009 \t\n",
      "Epoch 45194 \t\t Training Loss: 0.0005807401612401009 \t\n",
      "Epoch 45195 \t\t Training Loss: 0.0005807401612401009 \t\n",
      "Epoch 45196 \t\t Training Loss: 0.0005807401612401009 \t\n",
      "Epoch 45197 \t\t Training Loss: 0.0005807401612401009 \t\n",
      "Epoch 45198 \t\t Training Loss: 0.0005807401612401009 \t\n",
      "Epoch 45199 \t\t Training Loss: 0.0005807401612401009 \t\n",
      "Epoch 45200 \t\t Training Loss: 0.0005807401612401009 \t\n",
      "Epoch 45201 \t\t Training Loss: 0.0005807401612401009 \t\n",
      "Epoch 45202 \t\t Training Loss: 0.0005807401612401009 \t\n",
      "Epoch 45203 \t\t Training Loss: 0.0005807401612401009 \t\n",
      "Epoch 45204 \t\t Training Loss: 0.0005807401612401009 \t\n",
      "Epoch 45205 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45206 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45207 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45208 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45209 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45210 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45211 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45212 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45213 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45214 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45215 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45216 \t\t Training Loss: 0.0005807402194477618 \t\n",
      "Epoch 45217 \t\t Training Loss: 0.00058074010303244 \t\n",
      "Epoch 45218 \t\t Training Loss: 0.00058074010303244 \t\n",
      "Epoch 45219 \t\t Training Loss: 0.00058074010303244 \t\n",
      "Epoch 45220 \t\t Training Loss: 0.00058074010303244 \t\n",
      "Epoch 45221 \t\t Training Loss: 0.000580740044824779 \t\n",
      "Epoch 45222 \t\t Training Loss: 0.00058074010303244 \t\n",
      "Epoch 45223 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45224 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45225 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45226 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45227 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45228 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45229 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45230 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45231 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45232 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45233 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45234 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45235 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45236 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45237 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45238 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45239 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45240 \t\t Training Loss: 0.000580740044824779 \t\n",
      "Epoch 45241 \t\t Training Loss: 0.0005807399866171181 \t\n",
      "Epoch 45242 \t\t Training Loss: 0.0005807399284094572 \t\n",
      "Epoch 45243 \t\t Training Loss: 0.0005807399284094572 \t\n",
      "Epoch 45244 \t\t Training Loss: 0.0005807399284094572 \t\n",
      "Epoch 45245 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45246 \t\t Training Loss: 0.0005807399284094572 \t\n",
      "Epoch 45247 \t\t Training Loss: 0.0005807399284094572 \t\n",
      "Epoch 45248 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45249 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45250 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45251 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45252 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45253 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45254 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45255 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45256 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45257 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45258 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45259 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45260 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45261 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45262 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45263 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45264 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45265 \t\t Training Loss: 0.0005807398119941354 \t\n",
      "Epoch 45266 \t\t Training Loss: 0.0005807397537864745 \t\n",
      "Epoch 45267 \t\t Training Loss: 0.0005807398119941354 \t\n",
      "Epoch 45268 \t\t Training Loss: 0.0005807397537864745 \t\n",
      "Epoch 45269 \t\t Training Loss: 0.0005807398702017963 \t\n",
      "Epoch 45270 \t\t Training Loss: 0.0005807397537864745 \t\n",
      "Epoch 45271 \t\t Training Loss: 0.0005807397537864745 \t\n",
      "Epoch 45272 \t\t Training Loss: 0.0005807397537864745 \t\n",
      "Epoch 45273 \t\t Training Loss: 0.0005807397537864745 \t\n",
      "Epoch 45274 \t\t Training Loss: 0.0005807397537864745 \t\n",
      "Epoch 45275 \t\t Training Loss: 0.0005807397537864745 \t\n",
      "Epoch 45276 \t\t Training Loss: 0.0005807397537864745 \t\n",
      "Epoch 45277 \t\t Training Loss: 0.0005807397537864745 \t\n",
      "Epoch 45278 \t\t Training Loss: 0.0005807396955788136 \t\n",
      "Epoch 45279 \t\t Training Loss: 0.0005807397537864745 \t\n",
      "Epoch 45280 \t\t Training Loss: 0.0005807397537864745 \t\n",
      "Epoch 45281 \t\t Training Loss: 0.0005807396955788136 \t\n",
      "Epoch 45282 \t\t Training Loss: 0.0005807396373711526 \t\n",
      "Epoch 45283 \t\t Training Loss: 0.0005807396955788136 \t\n",
      "Epoch 45284 \t\t Training Loss: 0.0005807396955788136 \t\n",
      "Epoch 45285 \t\t Training Loss: 0.0005807396955788136 \t\n",
      "Epoch 45286 \t\t Training Loss: 0.0005807396955788136 \t\n",
      "Epoch 45287 \t\t Training Loss: 0.0005807396373711526 \t\n",
      "Epoch 45288 \t\t Training Loss: 0.0005807396955788136 \t\n",
      "Epoch 45289 \t\t Training Loss: 0.0005807396955788136 \t\n",
      "Epoch 45290 \t\t Training Loss: 0.0005807396373711526 \t\n",
      "Epoch 45291 \t\t Training Loss: 0.0005807396373711526 \t\n",
      "Epoch 45292 \t\t Training Loss: 0.0005807396373711526 \t\n",
      "Epoch 45293 \t\t Training Loss: 0.0005807396373711526 \t\n",
      "Epoch 45294 \t\t Training Loss: 0.0005807396373711526 \t\n",
      "Epoch 45295 \t\t Training Loss: 0.0005807396373711526 \t\n",
      "Epoch 45296 \t\t Training Loss: 0.0005807396373711526 \t\n",
      "Epoch 45297 \t\t Training Loss: 0.0005807396373711526 \t\n",
      "Epoch 45298 \t\t Training Loss: 0.0005807396373711526 \t\n",
      "Epoch 45299 \t\t Training Loss: 0.0005807396373711526 \t\n",
      "Epoch 45300 \t\t Training Loss: 0.0005807396373711526 \t\n",
      "Epoch 45301 \t\t Training Loss: 0.0005807396373711526 \t\n",
      "Epoch 45302 \t\t Training Loss: 0.0005807396373711526 \t\n",
      "Epoch 45303 \t\t Training Loss: 0.0005807396373711526 \t\n",
      "Epoch 45304 \t\t Training Loss: 0.0005807395209558308 \t\n",
      "Epoch 45305 \t\t Training Loss: 0.0005807395209558308 \t\n",
      "Epoch 45306 \t\t Training Loss: 0.0005807395209558308 \t\n",
      "Epoch 45307 \t\t Training Loss: 0.0005807395209558308 \t\n",
      "Epoch 45308 \t\t Training Loss: 0.0005807395209558308 \t\n",
      "Epoch 45309 \t\t Training Loss: 0.0005807395209558308 \t\n",
      "Epoch 45310 \t\t Training Loss: 0.0005807395209558308 \t\n",
      "Epoch 45311 \t\t Training Loss: 0.0005807395209558308 \t\n",
      "Epoch 45312 \t\t Training Loss: 0.0005807395209558308 \t\n",
      "Epoch 45313 \t\t Training Loss: 0.0005807395209558308 \t\n",
      "Epoch 45314 \t\t Training Loss: 0.0005807394627481699 \t\n",
      "Epoch 45315 \t\t Training Loss: 0.0005807395209558308 \t\n",
      "Epoch 45316 \t\t Training Loss: 0.0005807394627481699 \t\n",
      "Epoch 45317 \t\t Training Loss: 0.0005807394627481699 \t\n",
      "Epoch 45318 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45319 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45320 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45321 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45322 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45323 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45324 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45325 \t\t Training Loss: 0.0005807394627481699 \t\n",
      "Epoch 45326 \t\t Training Loss: 0.0005807394627481699 \t\n",
      "Epoch 45327 \t\t Training Loss: 0.0005807394627481699 \t\n",
      "Epoch 45328 \t\t Training Loss: 0.0005807394627481699 \t\n",
      "Epoch 45329 \t\t Training Loss: 0.0005807394627481699 \t\n",
      "Epoch 45330 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45331 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45332 \t\t Training Loss: 0.0005807394627481699 \t\n",
      "Epoch 45333 \t\t Training Loss: 0.0005807394627481699 \t\n",
      "Epoch 45334 \t\t Training Loss: 0.0005807394627481699 \t\n",
      "Epoch 45335 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45336 \t\t Training Loss: 0.000580739404540509 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45337 \t\t Training Loss: 0.0005807394627481699 \t\n",
      "Epoch 45338 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45339 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45340 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45341 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45342 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45343 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45344 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45345 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45346 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45347 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45348 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45349 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45350 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45351 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45352 \t\t Training Loss: 0.000580739404540509 \t\n",
      "Epoch 45353 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45354 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45355 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45356 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45357 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45358 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45359 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45360 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45361 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45362 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45363 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45364 \t\t Training Loss: 0.0005807391717098653 \t\n",
      "Epoch 45365 \t\t Training Loss: 0.0005807392299175262 \t\n",
      "Epoch 45366 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45367 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45368 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45369 \t\t Training Loss: 0.0005807391717098653 \t\n",
      "Epoch 45370 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45371 \t\t Training Loss: 0.0005807391717098653 \t\n",
      "Epoch 45372 \t\t Training Loss: 0.0005807391717098653 \t\n",
      "Epoch 45373 \t\t Training Loss: 0.0005807391717098653 \t\n",
      "Epoch 45374 \t\t Training Loss: 0.0005807393463328481 \t\n",
      "Epoch 45375 \t\t Training Loss: 0.0005807391717098653 \t\n",
      "Epoch 45376 \t\t Training Loss: 0.0005807390552945435 \t\n",
      "Epoch 45377 \t\t Training Loss: 0.0005807391135022044 \t\n",
      "Epoch 45378 \t\t Training Loss: 0.0005807391135022044 \t\n",
      "Epoch 45379 \t\t Training Loss: 0.0005807391135022044 \t\n",
      "Epoch 45380 \t\t Training Loss: 0.0005807390552945435 \t\n",
      "Epoch 45381 \t\t Training Loss: 0.0005807390552945435 \t\n",
      "Epoch 45382 \t\t Training Loss: 0.0005807390552945435 \t\n",
      "Epoch 45383 \t\t Training Loss: 0.0005807390552945435 \t\n",
      "Epoch 45384 \t\t Training Loss: 0.0005807390552945435 \t\n",
      "Epoch 45385 \t\t Training Loss: 0.0005807389970868826 \t\n",
      "Epoch 45386 \t\t Training Loss: 0.0005807389970868826 \t\n",
      "Epoch 45387 \t\t Training Loss: 0.0005807389970868826 \t\n",
      "Epoch 45388 \t\t Training Loss: 0.0005807389970868826 \t\n",
      "Epoch 45389 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45390 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45391 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45392 \t\t Training Loss: 0.0005807390552945435 \t\n",
      "Epoch 45393 \t\t Training Loss: 0.0005807389970868826 \t\n",
      "Epoch 45394 \t\t Training Loss: 0.0005807389970868826 \t\n",
      "Epoch 45395 \t\t Training Loss: 0.0005807389970868826 \t\n",
      "Epoch 45396 \t\t Training Loss: 0.0005807390552945435 \t\n",
      "Epoch 45397 \t\t Training Loss: 0.0005807390552945435 \t\n",
      "Epoch 45398 \t\t Training Loss: 0.0005807390552945435 \t\n",
      "Epoch 45399 \t\t Training Loss: 0.0005807390552945435 \t\n",
      "Epoch 45400 \t\t Training Loss: 0.0005807390552945435 \t\n",
      "Epoch 45401 \t\t Training Loss: 0.0005807390552945435 \t\n",
      "Epoch 45402 \t\t Training Loss: 0.0005807389970868826 \t\n",
      "Epoch 45403 \t\t Training Loss: 0.0005807389970868826 \t\n",
      "Epoch 45404 \t\t Training Loss: 0.0005807389970868826 \t\n",
      "Epoch 45405 \t\t Training Loss: 0.0005807390552945435 \t\n",
      "Epoch 45406 \t\t Training Loss: 0.0005807389970868826 \t\n",
      "Epoch 45407 \t\t Training Loss: 0.0005807389970868826 \t\n",
      "Epoch 45408 \t\t Training Loss: 0.0005807389970868826 \t\n",
      "Epoch 45409 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45410 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45411 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45412 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45413 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45414 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45415 \t\t Training Loss: 0.0005807389970868826 \t\n",
      "Epoch 45416 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45417 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45418 \t\t Training Loss: 0.0005807389970868826 \t\n",
      "Epoch 45419 \t\t Training Loss: 0.0005807389970868826 \t\n",
      "Epoch 45420 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45421 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45422 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45423 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45424 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45425 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45426 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45427 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45428 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45429 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45430 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45431 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45432 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45433 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45434 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45435 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45436 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45437 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45438 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45439 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45440 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45441 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45442 \t\t Training Loss: 0.0005807388224638999 \t\n",
      "Epoch 45443 \t\t Training Loss: 0.0005807389388792217 \t\n",
      "Epoch 45444 \t\t Training Loss: 0.0005807388224638999 \t\n",
      "Epoch 45445 \t\t Training Loss: 0.0005807388224638999 \t\n",
      "Epoch 45446 \t\t Training Loss: 0.0005807388224638999 \t\n",
      "Epoch 45447 \t\t Training Loss: 0.0005807388224638999 \t\n",
      "Epoch 45448 \t\t Training Loss: 0.0005807388224638999 \t\n",
      "Epoch 45449 \t\t Training Loss: 0.0005807388224638999 \t\n",
      "Epoch 45450 \t\t Training Loss: 0.0005807388224638999 \t\n",
      "Epoch 45451 \t\t Training Loss: 0.0005807388224638999 \t\n",
      "Epoch 45452 \t\t Training Loss: 0.0005807387642562389 \t\n",
      "Epoch 45453 \t\t Training Loss: 0.0005807388224638999 \t\n",
      "Epoch 45454 \t\t Training Loss: 0.0005807387642562389 \t\n",
      "Epoch 45455 \t\t Training Loss: 0.0005807387642562389 \t\n",
      "Epoch 45456 \t\t Training Loss: 0.0005807388224638999 \t\n",
      "Epoch 45457 \t\t Training Loss: 0.000580738706048578 \t\n",
      "Epoch 45458 \t\t Training Loss: 0.000580738706048578 \t\n",
      "Epoch 45459 \t\t Training Loss: 0.000580738706048578 \t\n",
      "Epoch 45460 \t\t Training Loss: 0.000580738706048578 \t\n",
      "Epoch 45461 \t\t Training Loss: 0.000580738706048578 \t\n",
      "Epoch 45462 \t\t Training Loss: 0.000580738706048578 \t\n",
      "Epoch 45463 \t\t Training Loss: 0.000580738706048578 \t\n",
      "Epoch 45464 \t\t Training Loss: 0.000580738706048578 \t\n",
      "Epoch 45465 \t\t Training Loss: 0.0005807386478409171 \t\n",
      "Epoch 45466 \t\t Training Loss: 0.000580738706048578 \t\n",
      "Epoch 45467 \t\t Training Loss: 0.000580738706048578 \t\n",
      "Epoch 45468 \t\t Training Loss: 0.000580738706048578 \t\n",
      "Epoch 45469 \t\t Training Loss: 0.000580738706048578 \t\n",
      "Epoch 45470 \t\t Training Loss: 0.0005807386478409171 \t\n",
      "Epoch 45471 \t\t Training Loss: 0.0005807386478409171 \t\n",
      "Epoch 45472 \t\t Training Loss: 0.0005807386478409171 \t\n",
      "Epoch 45473 \t\t Training Loss: 0.000580738706048578 \t\n",
      "Epoch 45474 \t\t Training Loss: 0.000580738706048578 \t\n",
      "Epoch 45475 \t\t Training Loss: 0.000580738706048578 \t\n",
      "Epoch 45476 \t\t Training Loss: 0.0005807386478409171 \t\n",
      "Epoch 45477 \t\t Training Loss: 0.000580738706048578 \t\n",
      "Epoch 45478 \t\t Training Loss: 0.0005807386478409171 \t\n",
      "Epoch 45479 \t\t Training Loss: 0.0005807386478409171 \t\n",
      "Epoch 45480 \t\t Training Loss: 0.0005807386478409171 \t\n",
      "Epoch 45481 \t\t Training Loss: 0.0005807386478409171 \t\n",
      "Epoch 45482 \t\t Training Loss: 0.0005807386478409171 \t\n",
      "Epoch 45483 \t\t Training Loss: 0.0005807386478409171 \t\n",
      "Epoch 45484 \t\t Training Loss: 0.0005807386478409171 \t\n",
      "Epoch 45485 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45486 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45487 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45488 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45489 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45490 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45491 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45492 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45493 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45494 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45495 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45496 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45497 \t\t Training Loss: 0.0005807385314255953 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45498 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45499 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45500 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45501 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45502 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45503 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45504 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45505 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45506 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45507 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45508 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45509 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45510 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45511 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45512 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45513 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45514 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45515 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45516 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45517 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45518 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45519 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45520 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45521 \t\t Training Loss: 0.0005807385314255953 \t\n",
      "Epoch 45522 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45523 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45524 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45525 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45526 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45527 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45528 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45529 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45530 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45531 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45532 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45533 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45534 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45535 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45536 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45537 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45538 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45539 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45540 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45541 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45542 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45543 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45544 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45545 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45546 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45547 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45548 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45549 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45550 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45551 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45552 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45553 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45554 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45555 \t\t Training Loss: 0.0005807384150102735 \t\n",
      "Epoch 45556 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45557 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45558 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45559 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45560 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45561 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45562 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45563 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45564 \t\t Training Loss: 0.0005807383568026125 \t\n",
      "Epoch 45565 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45566 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45567 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45568 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45569 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45570 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45571 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45572 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45573 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45574 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45575 \t\t Training Loss: 0.0005807381239719689 \t\n",
      "Epoch 45576 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45577 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45578 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45579 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45580 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45581 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45582 \t\t Training Loss: 0.0005807381239719689 \t\n",
      "Epoch 45583 \t\t Training Loss: 0.0005807381239719689 \t\n",
      "Epoch 45584 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45585 \t\t Training Loss: 0.0005807382403872907 \t\n",
      "Epoch 45586 \t\t Training Loss: 0.0005807381239719689 \t\n",
      "Epoch 45587 \t\t Training Loss: 0.0005807381239719689 \t\n",
      "Epoch 45588 \t\t Training Loss: 0.0005807381239719689 \t\n",
      "Epoch 45589 \t\t Training Loss: 0.0005807381239719689 \t\n",
      "Epoch 45590 \t\t Training Loss: 0.0005807381239719689 \t\n",
      "Epoch 45591 \t\t Training Loss: 0.0005807381239719689 \t\n",
      "Epoch 45592 \t\t Training Loss: 0.0005807381239719689 \t\n",
      "Epoch 45593 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45594 \t\t Training Loss: 0.0005807381239719689 \t\n",
      "Epoch 45595 \t\t Training Loss: 0.0005807381239719689 \t\n",
      "Epoch 45596 \t\t Training Loss: 0.0005807381239719689 \t\n",
      "Epoch 45597 \t\t Training Loss: 0.0005807381239719689 \t\n",
      "Epoch 45598 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45599 \t\t Training Loss: 0.0005807381239719689 \t\n",
      "Epoch 45600 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45601 \t\t Training Loss: 0.0005807381239719689 \t\n",
      "Epoch 45602 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45603 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45604 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45605 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45606 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45607 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45608 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45609 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45610 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45611 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45612 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45613 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45614 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45615 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45616 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45617 \t\t Training Loss: 0.0005807379493489861 \t\n",
      "Epoch 45618 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45619 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45620 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45621 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45622 \t\t Training Loss: 0.0005807379493489861 \t\n",
      "Epoch 45623 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45624 \t\t Training Loss: 0.000580738065764308 \t\n",
      "Epoch 45625 \t\t Training Loss: 0.0005807379493489861 \t\n",
      "Epoch 45626 \t\t Training Loss: 0.0005807379493489861 \t\n",
      "Epoch 45627 \t\t Training Loss: 0.0005807379493489861 \t\n",
      "Epoch 45628 \t\t Training Loss: 0.0005807379493489861 \t\n",
      "Epoch 45629 \t\t Training Loss: 0.0005807379493489861 \t\n",
      "Epoch 45630 \t\t Training Loss: 0.0005807379493489861 \t\n",
      "Epoch 45631 \t\t Training Loss: 0.0005807379493489861 \t\n",
      "Epoch 45632 \t\t Training Loss: 0.0005807379493489861 \t\n",
      "Epoch 45633 \t\t Training Loss: 0.0005807379493489861 \t\n",
      "Epoch 45634 \t\t Training Loss: 0.0005807379493489861 \t\n",
      "Epoch 45635 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45636 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45637 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45638 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45639 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45640 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45641 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45642 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45643 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45644 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45645 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45646 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45647 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45648 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45649 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45650 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45651 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45652 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45653 \t\t Training Loss: 0.0005807378329336643 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45654 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45655 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45656 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45657 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45658 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45659 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45660 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45661 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45662 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45663 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45664 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45665 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45666 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45667 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45668 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45669 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45670 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45671 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45672 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45673 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45674 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45675 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45676 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45677 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45678 \t\t Training Loss: 0.0005807377747260034 \t\n",
      "Epoch 45679 \t\t Training Loss: 0.0005807377747260034 \t\n",
      "Epoch 45680 \t\t Training Loss: 0.0005807377747260034 \t\n",
      "Epoch 45681 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45682 \t\t Training Loss: 0.0005807378329336643 \t\n",
      "Epoch 45683 \t\t Training Loss: 0.0005807377747260034 \t\n",
      "Epoch 45684 \t\t Training Loss: 0.0005807377747260034 \t\n",
      "Epoch 45685 \t\t Training Loss: 0.0005807377747260034 \t\n",
      "Epoch 45686 \t\t Training Loss: 0.0005807377747260034 \t\n",
      "Epoch 45687 \t\t Training Loss: 0.0005807377747260034 \t\n",
      "Epoch 45688 \t\t Training Loss: 0.0005807375418953598 \t\n",
      "Epoch 45689 \t\t Training Loss: 0.0005807377747260034 \t\n",
      "Epoch 45690 \t\t Training Loss: 0.0005807376583106816 \t\n",
      "Epoch 45691 \t\t Training Loss: 0.0005807375418953598 \t\n",
      "Epoch 45692 \t\t Training Loss: 0.0005807376583106816 \t\n",
      "Epoch 45693 \t\t Training Loss: 0.0005807375418953598 \t\n",
      "Epoch 45694 \t\t Training Loss: 0.0005807375418953598 \t\n",
      "Epoch 45695 \t\t Training Loss: 0.0005807375418953598 \t\n",
      "Epoch 45696 \t\t Training Loss: 0.0005807375418953598 \t\n",
      "Epoch 45697 \t\t Training Loss: 0.0005807375418953598 \t\n",
      "Epoch 45698 \t\t Training Loss: 0.0005807375418953598 \t\n",
      "Epoch 45699 \t\t Training Loss: 0.0005807375418953598 \t\n",
      "Epoch 45700 \t\t Training Loss: 0.0005807375418953598 \t\n",
      "Epoch 45701 \t\t Training Loss: 0.0005807375418953598 \t\n",
      "Epoch 45702 \t\t Training Loss: 0.0005807375418953598 \t\n",
      "Epoch 45703 \t\t Training Loss: 0.0005807375418953598 \t\n",
      "Epoch 45704 \t\t Training Loss: 0.0005807375418953598 \t\n",
      "Epoch 45705 \t\t Training Loss: 0.0005807375418953598 \t\n",
      "Epoch 45706 \t\t Training Loss: 0.0005807375418953598 \t\n",
      "Epoch 45707 \t\t Training Loss: 0.0005807374254800379 \t\n",
      "Epoch 45708 \t\t Training Loss: 0.0005807374254800379 \t\n",
      "Epoch 45709 \t\t Training Loss: 0.0005807374254800379 \t\n",
      "Epoch 45710 \t\t Training Loss: 0.0005807374254800379 \t\n",
      "Epoch 45711 \t\t Training Loss: 0.0005807374254800379 \t\n",
      "Epoch 45712 \t\t Training Loss: 0.0005807374254800379 \t\n",
      "Epoch 45713 \t\t Training Loss: 0.0005807374254800379 \t\n",
      "Epoch 45714 \t\t Training Loss: 0.0005807374254800379 \t\n",
      "Epoch 45715 \t\t Training Loss: 0.0005807374254800379 \t\n",
      "Epoch 45716 \t\t Training Loss: 0.0005807374254800379 \t\n",
      "Epoch 45717 \t\t Training Loss: 0.0005807374254800379 \t\n",
      "Epoch 45718 \t\t Training Loss: 0.0005807374254800379 \t\n",
      "Epoch 45719 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45720 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45721 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45722 \t\t Training Loss: 0.0005807374254800379 \t\n",
      "Epoch 45723 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45724 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45725 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45726 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45727 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45728 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45729 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45730 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45731 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45732 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45733 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45734 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45735 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45736 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45737 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45738 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45739 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45740 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45741 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45742 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45743 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45744 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45745 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45746 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45747 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45748 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45749 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45750 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45751 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45752 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45753 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45754 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45755 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45756 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45757 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45758 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45759 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45760 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45761 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45762 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45763 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45764 \t\t Training Loss: 0.000580737367272377 \t\n",
      "Epoch 45765 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45766 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45767 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45768 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45769 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45770 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45771 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45772 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45773 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45774 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45775 \t\t Training Loss: 0.0005807372508570552 \t\n",
      "Epoch 45776 \t\t Training Loss: 0.0005807371344417334 \t\n",
      "Epoch 45777 \t\t Training Loss: 0.0005807371344417334 \t\n",
      "Epoch 45778 \t\t Training Loss: 0.0005807371344417334 \t\n",
      "Epoch 45779 \t\t Training Loss: 0.0005807371344417334 \t\n",
      "Epoch 45780 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45781 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45782 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45783 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45784 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45785 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45786 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45787 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45788 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45789 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45790 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45791 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45792 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45793 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45794 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45795 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45796 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45797 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45798 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45799 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45800 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45801 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45802 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45803 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45804 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45805 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45806 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45807 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45808 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45809 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45810 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45811 \t\t Training Loss: 0.0005807369598187506 \t\n",
      "Epoch 45812 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45813 \t\t Training Loss: 0.0005807368434034288 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45814 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45815 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45816 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45817 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45818 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45819 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45820 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45821 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45822 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45823 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45824 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45825 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45826 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45827 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45828 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45829 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45830 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45831 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45832 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45833 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45834 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45835 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45836 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45837 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45838 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45839 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45840 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45841 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45842 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45843 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45844 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45845 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45846 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45847 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45848 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45849 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45850 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45851 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45852 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45853 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45854 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45855 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45856 \t\t Training Loss: 0.0005807368434034288 \t\n",
      "Epoch 45857 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45858 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45859 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45860 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45861 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45862 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45863 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45864 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45865 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45866 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45867 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45868 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45869 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45870 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45871 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45872 \t\t Training Loss: 0.0005807367851957679 \t\n",
      "Epoch 45873 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45874 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45875 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45876 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45877 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45878 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45879 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45880 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45881 \t\t Training Loss: 0.000580736726988107 \t\n",
      "Epoch 45882 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45883 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45884 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45885 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45886 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45887 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45888 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45889 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45890 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45891 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45892 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45893 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45894 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45895 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45896 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45897 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45898 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45899 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45900 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45901 \t\t Training Loss: 0.000580736668780446 \t\n",
      "Epoch 45902 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45903 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45904 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45905 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45906 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45907 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45908 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45909 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45910 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45911 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45912 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45913 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45914 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45915 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45916 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45917 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45918 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45919 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45920 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45921 \t\t Training Loss: 0.0005807365523651242 \t\n",
      "Epoch 45922 \t\t Training Loss: 0.0005807364359498024 \t\n",
      "Epoch 45923 \t\t Training Loss: 0.0005807364359498024 \t\n",
      "Epoch 45924 \t\t Training Loss: 0.0005807364359498024 \t\n",
      "Epoch 45925 \t\t Training Loss: 0.0005807364359498024 \t\n",
      "Epoch 45926 \t\t Training Loss: 0.0005807364359498024 \t\n",
      "Epoch 45927 \t\t Training Loss: 0.0005807364359498024 \t\n",
      "Epoch 45928 \t\t Training Loss: 0.0005807364359498024 \t\n",
      "Epoch 45929 \t\t Training Loss: 0.0005807364359498024 \t\n",
      "Epoch 45930 \t\t Training Loss: 0.0005807364359498024 \t\n",
      "Epoch 45931 \t\t Training Loss: 0.0005807364359498024 \t\n",
      "Epoch 45932 \t\t Training Loss: 0.0005807363777421415 \t\n",
      "Epoch 45933 \t\t Training Loss: 0.0005807363777421415 \t\n",
      "Epoch 45934 \t\t Training Loss: 0.0005807364359498024 \t\n",
      "Epoch 45935 \t\t Training Loss: 0.0005807363777421415 \t\n",
      "Epoch 45936 \t\t Training Loss: 0.0005807363777421415 \t\n",
      "Epoch 45937 \t\t Training Loss: 0.0005807363777421415 \t\n",
      "Epoch 45938 \t\t Training Loss: 0.0005807363777421415 \t\n",
      "Epoch 45939 \t\t Training Loss: 0.0005807363777421415 \t\n",
      "Epoch 45940 \t\t Training Loss: 0.0005807363777421415 \t\n",
      "Epoch 45941 \t\t Training Loss: 0.0005807363777421415 \t\n",
      "Epoch 45942 \t\t Training Loss: 0.0005807363777421415 \t\n",
      "Epoch 45943 \t\t Training Loss: 0.0005807363195344806 \t\n",
      "Epoch 45944 \t\t Training Loss: 0.0005807363195344806 \t\n",
      "Epoch 45945 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45946 \t\t Training Loss: 0.0005807363195344806 \t\n",
      "Epoch 45947 \t\t Training Loss: 0.0005807363195344806 \t\n",
      "Epoch 45948 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45949 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45950 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45951 \t\t Training Loss: 0.0005807363195344806 \t\n",
      "Epoch 45952 \t\t Training Loss: 0.0005807363195344806 \t\n",
      "Epoch 45953 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45954 \t\t Training Loss: 0.0005807363195344806 \t\n",
      "Epoch 45955 \t\t Training Loss: 0.0005807363195344806 \t\n",
      "Epoch 45956 \t\t Training Loss: 0.0005807363195344806 \t\n",
      "Epoch 45957 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45958 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45959 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45960 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45961 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45962 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45963 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45964 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45965 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45966 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45967 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45968 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45969 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45970 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45971 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45972 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45973 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45974 \t\t Training Loss: 0.0005807362613268197 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45975 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45976 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45977 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45978 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45979 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45980 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45981 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45982 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45983 \t\t Training Loss: 0.0005807362031191587 \t\n",
      "Epoch 45984 \t\t Training Loss: 0.0005807362031191587 \t\n",
      "Epoch 45985 \t\t Training Loss: 0.0005807362613268197 \t\n",
      "Epoch 45986 \t\t Training Loss: 0.0005807362031191587 \t\n",
      "Epoch 45987 \t\t Training Loss: 0.0005807362031191587 \t\n",
      "Epoch 45988 \t\t Training Loss: 0.0005807361449114978 \t\n",
      "Epoch 45989 \t\t Training Loss: 0.0005807362031191587 \t\n",
      "Epoch 45990 \t\t Training Loss: 0.0005807361449114978 \t\n",
      "Epoch 45991 \t\t Training Loss: 0.0005807361449114978 \t\n",
      "Epoch 45992 \t\t Training Loss: 0.0005807361449114978 \t\n",
      "Epoch 45993 \t\t Training Loss: 0.0005807361449114978 \t\n",
      "Epoch 45994 \t\t Training Loss: 0.0005807361449114978 \t\n",
      "Epoch 45995 \t\t Training Loss: 0.0005807361449114978 \t\n",
      "Epoch 45996 \t\t Training Loss: 0.0005807361449114978 \t\n",
      "Epoch 45997 \t\t Training Loss: 0.0005807361449114978 \t\n",
      "Epoch 45998 \t\t Training Loss: 0.0005807360867038369 \t\n",
      "Epoch 45999 \t\t Training Loss: 0.000580736028496176 \t\n",
      "Epoch 46000 \t\t Training Loss: 0.0005807359702885151 \t\n",
      "Epoch 46001 \t\t Training Loss: 0.0005807359702885151 \t\n",
      "Epoch 46002 \t\t Training Loss: 0.0005807358538731933 \t\n",
      "Epoch 46003 \t\t Training Loss: 0.0005807359702885151 \t\n",
      "Epoch 46004 \t\t Training Loss: 0.0005807359702885151 \t\n",
      "Epoch 46005 \t\t Training Loss: 0.0005807359702885151 \t\n",
      "Epoch 46006 \t\t Training Loss: 0.0005807359702885151 \t\n",
      "Epoch 46007 \t\t Training Loss: 0.0005807359702885151 \t\n",
      "Epoch 46008 \t\t Training Loss: 0.0005807359702885151 \t\n",
      "Epoch 46009 \t\t Training Loss: 0.0005807359702885151 \t\n",
      "Epoch 46010 \t\t Training Loss: 0.0005807359702885151 \t\n",
      "Epoch 46011 \t\t Training Loss: 0.0005807359702885151 \t\n",
      "Epoch 46012 \t\t Training Loss: 0.0005807358538731933 \t\n",
      "Epoch 46013 \t\t Training Loss: 0.0005807358538731933 \t\n",
      "Epoch 46014 \t\t Training Loss: 0.0005807359702885151 \t\n",
      "Epoch 46015 \t\t Training Loss: 0.0005807359702885151 \t\n",
      "Epoch 46016 \t\t Training Loss: 0.0005807358538731933 \t\n",
      "Epoch 46017 \t\t Training Loss: 0.0005807358538731933 \t\n",
      "Epoch 46018 \t\t Training Loss: 0.0005807358538731933 \t\n",
      "Epoch 46019 \t\t Training Loss: 0.0005807358538731933 \t\n",
      "Epoch 46020 \t\t Training Loss: 0.0005807358538731933 \t\n",
      "Epoch 46021 \t\t Training Loss: 0.0005807358538731933 \t\n",
      "Epoch 46022 \t\t Training Loss: 0.0005807358538731933 \t\n",
      "Epoch 46023 \t\t Training Loss: 0.0005807358538731933 \t\n",
      "Epoch 46024 \t\t Training Loss: 0.0005807357956655324 \t\n",
      "Epoch 46025 \t\t Training Loss: 0.0005807357956655324 \t\n",
      "Epoch 46026 \t\t Training Loss: 0.0005807357956655324 \t\n",
      "Epoch 46027 \t\t Training Loss: 0.0005807357374578714 \t\n",
      "Epoch 46028 \t\t Training Loss: 0.0005807357374578714 \t\n",
      "Epoch 46029 \t\t Training Loss: 0.0005807357374578714 \t\n",
      "Epoch 46030 \t\t Training Loss: 0.0005807357374578714 \t\n",
      "Epoch 46031 \t\t Training Loss: 0.0005807357374578714 \t\n",
      "Epoch 46032 \t\t Training Loss: 0.0005807357374578714 \t\n",
      "Epoch 46033 \t\t Training Loss: 0.0005807357374578714 \t\n",
      "Epoch 46034 \t\t Training Loss: 0.0005807357374578714 \t\n",
      "Epoch 46035 \t\t Training Loss: 0.0005807357374578714 \t\n",
      "Epoch 46036 \t\t Training Loss: 0.0005807357374578714 \t\n",
      "Epoch 46037 \t\t Training Loss: 0.0005807357374578714 \t\n",
      "Epoch 46038 \t\t Training Loss: 0.0005807356792502105 \t\n",
      "Epoch 46039 \t\t Training Loss: 0.0005807357374578714 \t\n",
      "Epoch 46040 \t\t Training Loss: 0.0005807356792502105 \t\n",
      "Epoch 46041 \t\t Training Loss: 0.0005807357374578714 \t\n",
      "Epoch 46042 \t\t Training Loss: 0.0005807356792502105 \t\n",
      "Epoch 46043 \t\t Training Loss: 0.0005807357374578714 \t\n",
      "Epoch 46044 \t\t Training Loss: 0.0005807357374578714 \t\n",
      "Epoch 46045 \t\t Training Loss: 0.0005807357374578714 \t\n",
      "Epoch 46046 \t\t Training Loss: 0.0005807356792502105 \t\n",
      "Epoch 46047 \t\t Training Loss: 0.0005807356792502105 \t\n",
      "Epoch 46048 \t\t Training Loss: 0.0005807356210425496 \t\n",
      "Epoch 46049 \t\t Training Loss: 0.0005807356210425496 \t\n",
      "Epoch 46050 \t\t Training Loss: 0.0005807356210425496 \t\n",
      "Epoch 46051 \t\t Training Loss: 0.0005807356210425496 \t\n",
      "Epoch 46052 \t\t Training Loss: 0.0005807356210425496 \t\n",
      "Epoch 46053 \t\t Training Loss: 0.0005807356210425496 \t\n",
      "Epoch 46054 \t\t Training Loss: 0.0005807356210425496 \t\n",
      "Epoch 46055 \t\t Training Loss: 0.0005807356210425496 \t\n",
      "Epoch 46056 \t\t Training Loss: 0.0005807356210425496 \t\n",
      "Epoch 46057 \t\t Training Loss: 0.0005807356210425496 \t\n",
      "Epoch 46058 \t\t Training Loss: 0.0005807356210425496 \t\n",
      "Epoch 46059 \t\t Training Loss: 0.0005807356210425496 \t\n",
      "Epoch 46060 \t\t Training Loss: 0.0005807355628348887 \t\n",
      "Epoch 46061 \t\t Training Loss: 0.0005807355628348887 \t\n",
      "Epoch 46062 \t\t Training Loss: 0.0005807355628348887 \t\n",
      "Epoch 46063 \t\t Training Loss: 0.0005807355628348887 \t\n",
      "Epoch 46064 \t\t Training Loss: 0.0005807355628348887 \t\n",
      "Epoch 46065 \t\t Training Loss: 0.0005807355628348887 \t\n",
      "Epoch 46066 \t\t Training Loss: 0.0005807354464195669 \t\n",
      "Epoch 46067 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46068 \t\t Training Loss: 0.0005807355046272278 \t\n",
      "Epoch 46069 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46070 \t\t Training Loss: 0.0005807354464195669 \t\n",
      "Epoch 46071 \t\t Training Loss: 0.0005807355046272278 \t\n",
      "Epoch 46072 \t\t Training Loss: 0.0005807355046272278 \t\n",
      "Epoch 46073 \t\t Training Loss: 0.0005807355046272278 \t\n",
      "Epoch 46074 \t\t Training Loss: 0.0005807355046272278 \t\n",
      "Epoch 46075 \t\t Training Loss: 0.0005807355046272278 \t\n",
      "Epoch 46076 \t\t Training Loss: 0.0005807355046272278 \t\n",
      "Epoch 46077 \t\t Training Loss: 0.0005807355046272278 \t\n",
      "Epoch 46078 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46079 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46080 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46081 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46082 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46083 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46084 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46085 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46086 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46087 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46088 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46089 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46090 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46091 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46092 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46093 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46094 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46095 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46096 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46097 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46098 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46099 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46100 \t\t Training Loss: 0.0005807355046272278 \t\n",
      "Epoch 46101 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46102 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46103 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46104 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46105 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46106 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46107 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46108 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46109 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46110 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46111 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46112 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46113 \t\t Training Loss: 0.000580735388211906 \t\n",
      "Epoch 46114 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46115 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46116 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46117 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46118 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46119 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46120 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46121 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46122 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46123 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46124 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46125 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46126 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46127 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46128 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46129 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46130 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46131 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46132 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46133 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46134 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46135 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46136 \t\t Training Loss: 0.000580735330004245 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46137 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46138 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46139 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46140 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46141 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46142 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46143 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46144 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46145 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46146 \t\t Training Loss: 0.000580735330004245 \t\n",
      "Epoch 46147 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46148 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46149 \t\t Training Loss: 0.0005807352717965841 \t\n",
      "Epoch 46150 \t\t Training Loss: 0.0005807352135889232 \t\n",
      "Epoch 46151 \t\t Training Loss: 0.0005807352135889232 \t\n",
      "Epoch 46152 \t\t Training Loss: 0.0005807352135889232 \t\n",
      "Epoch 46153 \t\t Training Loss: 0.0005807352135889232 \t\n",
      "Epoch 46154 \t\t Training Loss: 0.0005807352135889232 \t\n",
      "Epoch 46155 \t\t Training Loss: 0.0005807352135889232 \t\n",
      "Epoch 46156 \t\t Training Loss: 0.0005807352135889232 \t\n",
      "Epoch 46157 \t\t Training Loss: 0.0005807352135889232 \t\n",
      "Epoch 46158 \t\t Training Loss: 0.0005807350971736014 \t\n",
      "Epoch 46159 \t\t Training Loss: 0.0005807350971736014 \t\n",
      "Epoch 46160 \t\t Training Loss: 0.0005807350971736014 \t\n",
      "Epoch 46161 \t\t Training Loss: 0.0005807350971736014 \t\n",
      "Epoch 46162 \t\t Training Loss: 0.0005807350971736014 \t\n",
      "Epoch 46163 \t\t Training Loss: 0.0005807350971736014 \t\n",
      "Epoch 46164 \t\t Training Loss: 0.0005807350971736014 \t\n",
      "Epoch 46165 \t\t Training Loss: 0.0005807350971736014 \t\n",
      "Epoch 46166 \t\t Training Loss: 0.0005807350971736014 \t\n",
      "Epoch 46167 \t\t Training Loss: 0.0005807350971736014 \t\n",
      "Epoch 46168 \t\t Training Loss: 0.0005807350389659405 \t\n",
      "Epoch 46169 \t\t Training Loss: 0.0005807350389659405 \t\n",
      "Epoch 46170 \t\t Training Loss: 0.0005807349807582796 \t\n",
      "Epoch 46171 \t\t Training Loss: 0.0005807349807582796 \t\n",
      "Epoch 46172 \t\t Training Loss: 0.0005807349807582796 \t\n",
      "Epoch 46173 \t\t Training Loss: 0.0005807349807582796 \t\n",
      "Epoch 46174 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46175 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46176 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46177 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46178 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46179 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46180 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46181 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46182 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46183 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46184 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46185 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46186 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46187 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46188 \t\t Training Loss: 0.0005807348061352968 \t\n",
      "Epoch 46189 \t\t Training Loss: 0.0005807348061352968 \t\n",
      "Epoch 46190 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46191 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46192 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46193 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46194 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46195 \t\t Training Loss: 0.0005807349807582796 \t\n",
      "Epoch 46196 \t\t Training Loss: 0.0005807349807582796 \t\n",
      "Epoch 46197 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46198 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46199 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46200 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46201 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46202 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46203 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46204 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46205 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46206 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46207 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46208 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46209 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46210 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46211 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46212 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46213 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46214 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46215 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46216 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46217 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46218 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46219 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46220 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46221 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46222 \t\t Training Loss: 0.0005807348061352968 \t\n",
      "Epoch 46223 \t\t Training Loss: 0.0005807348061352968 \t\n",
      "Epoch 46224 \t\t Training Loss: 0.0005807348061352968 \t\n",
      "Epoch 46225 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46226 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46227 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46228 \t\t Training Loss: 0.0005807349225506186 \t\n",
      "Epoch 46229 \t\t Training Loss: 0.0005807348061352968 \t\n",
      "Epoch 46230 \t\t Training Loss: 0.0005807348061352968 \t\n",
      "Epoch 46231 \t\t Training Loss: 0.0005807348061352968 \t\n",
      "Epoch 46232 \t\t Training Loss: 0.0005807348061352968 \t\n",
      "Epoch 46233 \t\t Training Loss: 0.0005807348061352968 \t\n",
      "Epoch 46234 \t\t Training Loss: 0.0005807348061352968 \t\n",
      "Epoch 46235 \t\t Training Loss: 0.0005807348061352968 \t\n",
      "Epoch 46236 \t\t Training Loss: 0.0005807348061352968 \t\n",
      "Epoch 46237 \t\t Training Loss: 0.0005807348061352968 \t\n",
      "Epoch 46238 \t\t Training Loss: 0.000580734689719975 \t\n",
      "Epoch 46239 \t\t Training Loss: 0.000580734689719975 \t\n",
      "Epoch 46240 \t\t Training Loss: 0.000580734689719975 \t\n",
      "Epoch 46241 \t\t Training Loss: 0.000580734689719975 \t\n",
      "Epoch 46242 \t\t Training Loss: 0.000580734689719975 \t\n",
      "Epoch 46243 \t\t Training Loss: 0.0005807348061352968 \t\n",
      "Epoch 46244 \t\t Training Loss: 0.0005807347479276359 \t\n",
      "Epoch 46245 \t\t Training Loss: 0.0005807347479276359 \t\n",
      "Epoch 46246 \t\t Training Loss: 0.000580734689719975 \t\n",
      "Epoch 46247 \t\t Training Loss: 0.0005807347479276359 \t\n",
      "Epoch 46248 \t\t Training Loss: 0.0005807347479276359 \t\n",
      "Epoch 46249 \t\t Training Loss: 0.0005807347479276359 \t\n",
      "Epoch 46250 \t\t Training Loss: 0.000580734689719975 \t\n",
      "Epoch 46251 \t\t Training Loss: 0.000580734689719975 \t\n",
      "Epoch 46252 \t\t Training Loss: 0.000580734689719975 \t\n",
      "Epoch 46253 \t\t Training Loss: 0.000580734689719975 \t\n",
      "Epoch 46254 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46255 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46256 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46257 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46258 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46259 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46260 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46261 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46262 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46263 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46264 \t\t Training Loss: 0.000580734689719975 \t\n",
      "Epoch 46265 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46266 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46267 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46268 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46269 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46270 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46271 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46272 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46273 \t\t Training Loss: 0.0005807346315123141 \t\n",
      "Epoch 46274 \t\t Training Loss: 0.0005807345733046532 \t\n",
      "Epoch 46275 \t\t Training Loss: 0.0005807345733046532 \t\n",
      "Epoch 46276 \t\t Training Loss: 0.0005807345733046532 \t\n",
      "Epoch 46277 \t\t Training Loss: 0.0005807345733046532 \t\n",
      "Epoch 46278 \t\t Training Loss: 0.0005807345733046532 \t\n",
      "Epoch 46279 \t\t Training Loss: 0.0005807345733046532 \t\n",
      "Epoch 46280 \t\t Training Loss: 0.0005807345733046532 \t\n",
      "Epoch 46281 \t\t Training Loss: 0.0005807344568893313 \t\n",
      "Epoch 46282 \t\t Training Loss: 0.0005807344568893313 \t\n",
      "Epoch 46283 \t\t Training Loss: 0.0005807344568893313 \t\n",
      "Epoch 46284 \t\t Training Loss: 0.0005807343986816704 \t\n",
      "Epoch 46285 \t\t Training Loss: 0.0005807344568893313 \t\n",
      "Epoch 46286 \t\t Training Loss: 0.0005807344568893313 \t\n",
      "Epoch 46287 \t\t Training Loss: 0.0005807344568893313 \t\n",
      "Epoch 46288 \t\t Training Loss: 0.0005807344568893313 \t\n",
      "Epoch 46289 \t\t Training Loss: 0.0005807343986816704 \t\n",
      "Epoch 46290 \t\t Training Loss: 0.0005807343986816704 \t\n",
      "Epoch 46291 \t\t Training Loss: 0.0005807343986816704 \t\n",
      "Epoch 46292 \t\t Training Loss: 0.0005807343986816704 \t\n",
      "Epoch 46293 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46294 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46295 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46296 \t\t Training Loss: 0.0005807343986816704 \t\n",
      "Epoch 46297 \t\t Training Loss: 0.0005807343986816704 \t\n",
      "Epoch 46298 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46299 \t\t Training Loss: 0.0005807343404740095 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46300 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46301 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46302 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46303 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46304 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46305 \t\t Training Loss: 0.0005807342240586877 \t\n",
      "Epoch 46306 \t\t Training Loss: 0.0005807342822663486 \t\n",
      "Epoch 46307 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46308 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46309 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46310 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46311 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46312 \t\t Training Loss: 0.0005807342240586877 \t\n",
      "Epoch 46313 \t\t Training Loss: 0.0005807342240586877 \t\n",
      "Epoch 46314 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46315 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46316 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46317 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46318 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46319 \t\t Training Loss: 0.0005807342822663486 \t\n",
      "Epoch 46320 \t\t Training Loss: 0.0005807342240586877 \t\n",
      "Epoch 46321 \t\t Training Loss: 0.0005807342240586877 \t\n",
      "Epoch 46322 \t\t Training Loss: 0.0005807342240586877 \t\n",
      "Epoch 46323 \t\t Training Loss: 0.0005807342240586877 \t\n",
      "Epoch 46324 \t\t Training Loss: 0.0005807342240586877 \t\n",
      "Epoch 46325 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46326 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46327 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46328 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46329 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46330 \t\t Training Loss: 0.0005807343404740095 \t\n",
      "Epoch 46331 \t\t Training Loss: 0.0005807342240586877 \t\n",
      "Epoch 46332 \t\t Training Loss: 0.0005807342240586877 \t\n",
      "Epoch 46333 \t\t Training Loss: 0.0005807342240586877 \t\n",
      "Epoch 46334 \t\t Training Loss: 0.0005807341658510268 \t\n",
      "Epoch 46335 \t\t Training Loss: 0.0005807341658510268 \t\n",
      "Epoch 46336 \t\t Training Loss: 0.0005807341658510268 \t\n",
      "Epoch 46337 \t\t Training Loss: 0.0005807341658510268 \t\n",
      "Epoch 46338 \t\t Training Loss: 0.0005807341658510268 \t\n",
      "Epoch 46339 \t\t Training Loss: 0.0005807342240586877 \t\n",
      "Epoch 46340 \t\t Training Loss: 0.0005807341658510268 \t\n",
      "Epoch 46341 \t\t Training Loss: 0.0005807342240586877 \t\n",
      "Epoch 46342 \t\t Training Loss: 0.0005807341658510268 \t\n",
      "Epoch 46343 \t\t Training Loss: 0.0005807341658510268 \t\n",
      "Epoch 46344 \t\t Training Loss: 0.0005807341658510268 \t\n",
      "Epoch 46345 \t\t Training Loss: 0.0005807341658510268 \t\n",
      "Epoch 46346 \t\t Training Loss: 0.0005807341658510268 \t\n",
      "Epoch 46347 \t\t Training Loss: 0.0005807341658510268 \t\n",
      "Epoch 46348 \t\t Training Loss: 0.000580734049435705 \t\n",
      "Epoch 46349 \t\t Training Loss: 0.000580734049435705 \t\n",
      "Epoch 46350 \t\t Training Loss: 0.000580734049435705 \t\n",
      "Epoch 46351 \t\t Training Loss: 0.000580734049435705 \t\n",
      "Epoch 46352 \t\t Training Loss: 0.000580734049435705 \t\n",
      "Epoch 46353 \t\t Training Loss: 0.000580734049435705 \t\n",
      "Epoch 46354 \t\t Training Loss: 0.000580734049435705 \t\n",
      "Epoch 46355 \t\t Training Loss: 0.000580734049435705 \t\n",
      "Epoch 46356 \t\t Training Loss: 0.000580734049435705 \t\n",
      "Epoch 46357 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46358 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46359 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46360 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46361 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46362 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46363 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46364 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46365 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46366 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46367 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46368 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46369 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46370 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46371 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46372 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46373 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46374 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46375 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46376 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46377 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46378 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46379 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46380 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46381 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46382 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46383 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46384 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46385 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46386 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46387 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46388 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46389 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46390 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46391 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46392 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46393 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46394 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46395 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46396 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46397 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46398 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46399 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46400 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46401 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46402 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46403 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46404 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46405 \t\t Training Loss: 0.000580733991228044 \t\n",
      "Epoch 46406 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46407 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46408 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46409 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46410 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46411 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46412 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46413 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46414 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46415 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46416 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46417 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46418 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46419 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46420 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46421 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46422 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46423 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46424 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46425 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46426 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46427 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46428 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46429 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46430 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46431 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46432 \t\t Training Loss: 0.0005807339330203831 \t\n",
      "Epoch 46433 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46434 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46435 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46436 \t\t Training Loss: 0.0005807338166050613 \t\n",
      "Epoch 46437 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46438 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46439 \t\t Training Loss: 0.0005807338166050613 \t\n",
      "Epoch 46440 \t\t Training Loss: 0.0005807338166050613 \t\n",
      "Epoch 46441 \t\t Training Loss: 0.0005807338166050613 \t\n",
      "Epoch 46442 \t\t Training Loss: 0.0005807338748127222 \t\n",
      "Epoch 46443 \t\t Training Loss: 0.0005807338166050613 \t\n",
      "Epoch 46444 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46445 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46446 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46447 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46448 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46449 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46450 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46451 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46452 \t\t Training Loss: 0.0005807338166050613 \t\n",
      "Epoch 46453 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46454 \t\t Training Loss: 0.0005807337583974004 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46455 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46456 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46457 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46458 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46459 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46460 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46461 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46462 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46463 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46464 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46465 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46466 \t\t Training Loss: 0.0005807337001897395 \t\n",
      "Epoch 46467 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46468 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46469 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46470 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46471 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46472 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46473 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46474 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46475 \t\t Training Loss: 0.0005807337001897395 \t\n",
      "Epoch 46476 \t\t Training Loss: 0.0005807337583974004 \t\n",
      "Epoch 46477 \t\t Training Loss: 0.0005807336419820786 \t\n",
      "Epoch 46478 \t\t Training Loss: 0.0005807336419820786 \t\n",
      "Epoch 46479 \t\t Training Loss: 0.0005807336419820786 \t\n",
      "Epoch 46480 \t\t Training Loss: 0.0005807336419820786 \t\n",
      "Epoch 46481 \t\t Training Loss: 0.0005807335837744176 \t\n",
      "Epoch 46482 \t\t Training Loss: 0.0005807335837744176 \t\n",
      "Epoch 46483 \t\t Training Loss: 0.0005807336419820786 \t\n",
      "Epoch 46484 \t\t Training Loss: 0.0005807336419820786 \t\n",
      "Epoch 46485 \t\t Training Loss: 0.0005807336419820786 \t\n",
      "Epoch 46486 \t\t Training Loss: 0.0005807336419820786 \t\n",
      "Epoch 46487 \t\t Training Loss: 0.0005807336419820786 \t\n",
      "Epoch 46488 \t\t Training Loss: 0.0005807336419820786 \t\n",
      "Epoch 46489 \t\t Training Loss: 0.0005807335837744176 \t\n",
      "Epoch 46490 \t\t Training Loss: 0.0005807335837744176 \t\n",
      "Epoch 46491 \t\t Training Loss: 0.0005807335837744176 \t\n",
      "Epoch 46492 \t\t Training Loss: 0.0005807335255667567 \t\n",
      "Epoch 46493 \t\t Training Loss: 0.0005807335255667567 \t\n",
      "Epoch 46494 \t\t Training Loss: 0.0005807335255667567 \t\n",
      "Epoch 46495 \t\t Training Loss: 0.0005807335255667567 \t\n",
      "Epoch 46496 \t\t Training Loss: 0.0005807335255667567 \t\n",
      "Epoch 46497 \t\t Training Loss: 0.0005807335255667567 \t\n",
      "Epoch 46498 \t\t Training Loss: 0.0005807335255667567 \t\n",
      "Epoch 46499 \t\t Training Loss: 0.0005807335255667567 \t\n",
      "Epoch 46500 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46501 \t\t Training Loss: 0.0005807334673590958 \t\n",
      "Epoch 46502 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46503 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46504 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46505 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46506 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46507 \t\t Training Loss: 0.0005807334673590958 \t\n",
      "Epoch 46508 \t\t Training Loss: 0.0005807334673590958 \t\n",
      "Epoch 46509 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46510 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46511 \t\t Training Loss: 0.0005807334673590958 \t\n",
      "Epoch 46512 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46513 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46514 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46515 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46516 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46517 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46518 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46519 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46520 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46521 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46522 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46523 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46524 \t\t Training Loss: 0.0005807334091514349 \t\n",
      "Epoch 46525 \t\t Training Loss: 0.0005807332927361131 \t\n",
      "Epoch 46526 \t\t Training Loss: 0.0005807332927361131 \t\n",
      "Epoch 46527 \t\t Training Loss: 0.0005807332927361131 \t\n",
      "Epoch 46528 \t\t Training Loss: 0.0005807331763207912 \t\n",
      "Epoch 46529 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46530 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46531 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46532 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46533 \t\t Training Loss: 0.0005807331763207912 \t\n",
      "Epoch 46534 \t\t Training Loss: 0.0005807331763207912 \t\n",
      "Epoch 46535 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46536 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46537 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46538 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46539 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46540 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46541 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46542 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46543 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46544 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46545 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46546 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46547 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46548 \t\t Training Loss: 0.0005807331181131303 \t\n",
      "Epoch 46549 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46550 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46551 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46552 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46553 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46554 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46555 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46556 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46557 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46558 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46559 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46560 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46561 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46562 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46563 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46564 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46565 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46566 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46567 \t\t Training Loss: 0.0005807328852824867 \t\n",
      "Epoch 46568 \t\t Training Loss: 0.0005807328852824867 \t\n",
      "Epoch 46569 \t\t Training Loss: 0.0005807328852824867 \t\n",
      "Epoch 46570 \t\t Training Loss: 0.0005807328852824867 \t\n",
      "Epoch 46571 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46572 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46573 \t\t Training Loss: 0.0005807328852824867 \t\n",
      "Epoch 46574 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46575 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46576 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46577 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46578 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46579 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46580 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46581 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46582 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46583 \t\t Training Loss: 0.0005807330016978085 \t\n",
      "Epoch 46584 \t\t Training Loss: 0.0005807328852824867 \t\n",
      "Epoch 46585 \t\t Training Loss: 0.0005807328852824867 \t\n",
      "Epoch 46586 \t\t Training Loss: 0.0005807328852824867 \t\n",
      "Epoch 46587 \t\t Training Loss: 0.0005807328270748258 \t\n",
      "Epoch 46588 \t\t Training Loss: 0.0005807328270748258 \t\n",
      "Epoch 46589 \t\t Training Loss: 0.0005807328270748258 \t\n",
      "Epoch 46590 \t\t Training Loss: 0.0005807328270748258 \t\n",
      "Epoch 46591 \t\t Training Loss: 0.0005807328270748258 \t\n",
      "Epoch 46592 \t\t Training Loss: 0.0005807328270748258 \t\n",
      "Epoch 46593 \t\t Training Loss: 0.0005807328270748258 \t\n",
      "Epoch 46594 \t\t Training Loss: 0.0005807328270748258 \t\n",
      "Epoch 46595 \t\t Training Loss: 0.0005807328270748258 \t\n",
      "Epoch 46596 \t\t Training Loss: 0.0005807328270748258 \t\n",
      "Epoch 46597 \t\t Training Loss: 0.0005807328270748258 \t\n",
      "Epoch 46598 \t\t Training Loss: 0.0005807327688671649 \t\n",
      "Epoch 46599 \t\t Training Loss: 0.0005807327688671649 \t\n",
      "Epoch 46600 \t\t Training Loss: 0.0005807327688671649 \t\n",
      "Epoch 46601 \t\t Training Loss: 0.0005807327106595039 \t\n",
      "Epoch 46602 \t\t Training Loss: 0.0005807327106595039 \t\n",
      "Epoch 46603 \t\t Training Loss: 0.0005807327106595039 \t\n",
      "Epoch 46604 \t\t Training Loss: 0.0005807327106595039 \t\n",
      "Epoch 46605 \t\t Training Loss: 0.0005807327106595039 \t\n",
      "Epoch 46606 \t\t Training Loss: 0.0005807327106595039 \t\n",
      "Epoch 46607 \t\t Training Loss: 0.0005807327106595039 \t\n",
      "Epoch 46608 \t\t Training Loss: 0.0005807327106595039 \t\n",
      "Epoch 46609 \t\t Training Loss: 0.0005807327106595039 \t\n",
      "Epoch 46610 \t\t Training Loss: 0.0005807327106595039 \t\n",
      "Epoch 46611 \t\t Training Loss: 0.0005807325942441821 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46612 \t\t Training Loss: 0.0005807325360365212 \t\n",
      "Epoch 46613 \t\t Training Loss: 0.0005807325360365212 \t\n",
      "Epoch 46614 \t\t Training Loss: 0.0005807325942441821 \t\n",
      "Epoch 46615 \t\t Training Loss: 0.0005807325942441821 \t\n",
      "Epoch 46616 \t\t Training Loss: 0.0005807325942441821 \t\n",
      "Epoch 46617 \t\t Training Loss: 0.0005807325360365212 \t\n",
      "Epoch 46618 \t\t Training Loss: 0.0005807325942441821 \t\n",
      "Epoch 46619 \t\t Training Loss: 0.0005807325360365212 \t\n",
      "Epoch 46620 \t\t Training Loss: 0.0005807325360365212 \t\n",
      "Epoch 46621 \t\t Training Loss: 0.0005807325360365212 \t\n",
      "Epoch 46622 \t\t Training Loss: 0.0005807325360365212 \t\n",
      "Epoch 46623 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46624 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46625 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46626 \t\t Training Loss: 0.0005807325360365212 \t\n",
      "Epoch 46627 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46628 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46629 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46630 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46631 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46632 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46633 \t\t Training Loss: 0.0005807325360365212 \t\n",
      "Epoch 46634 \t\t Training Loss: 0.0005807325360365212 \t\n",
      "Epoch 46635 \t\t Training Loss: 0.0005807325360365212 \t\n",
      "Epoch 46636 \t\t Training Loss: 0.0005807325360365212 \t\n",
      "Epoch 46637 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46638 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46639 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46640 \t\t Training Loss: 0.0005807325360365212 \t\n",
      "Epoch 46641 \t\t Training Loss: 0.0005807325360365212 \t\n",
      "Epoch 46642 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46643 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46644 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46645 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46646 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46647 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46648 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46649 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46650 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46651 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46652 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46653 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46654 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46655 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46656 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46657 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46658 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46659 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46660 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46661 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46662 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46663 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46664 \t\t Training Loss: 0.0005807324778288603 \t\n",
      "Epoch 46665 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46666 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46667 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46668 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46669 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46670 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46671 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46672 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46673 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46674 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46675 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46676 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46677 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46678 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46679 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46680 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46681 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46682 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46683 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46684 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46685 \t\t Training Loss: 0.0005807324196211994 \t\n",
      "Epoch 46686 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46687 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46688 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46689 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46690 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46691 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46692 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46693 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46694 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46695 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46696 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46697 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46698 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46699 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46700 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46701 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46702 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46703 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46704 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46705 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46706 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46707 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46708 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46709 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46710 \t\t Training Loss: 0.0005807321285828948 \t\n",
      "Epoch 46711 \t\t Training Loss: 0.0005807321285828948 \t\n",
      "Epoch 46712 \t\t Training Loss: 0.0005807321285828948 \t\n",
      "Epoch 46713 \t\t Training Loss: 0.0005807321285828948 \t\n",
      "Epoch 46714 \t\t Training Loss: 0.0005807323032058775 \t\n",
      "Epoch 46715 \t\t Training Loss: 0.0005807321285828948 \t\n",
      "Epoch 46716 \t\t Training Loss: 0.0005807321285828948 \t\n",
      "Epoch 46717 \t\t Training Loss: 0.0005807321285828948 \t\n",
      "Epoch 46718 \t\t Training Loss: 0.0005807321285828948 \t\n",
      "Epoch 46719 \t\t Training Loss: 0.0005807321285828948 \t\n",
      "Epoch 46720 \t\t Training Loss: 0.0005807321285828948 \t\n",
      "Epoch 46721 \t\t Training Loss: 0.0005807321285828948 \t\n",
      "Epoch 46722 \t\t Training Loss: 0.0005807321285828948 \t\n",
      "Epoch 46723 \t\t Training Loss: 0.0005807321285828948 \t\n",
      "Epoch 46724 \t\t Training Loss: 0.0005807321285828948 \t\n",
      "Epoch 46725 \t\t Training Loss: 0.0005807321285828948 \t\n",
      "Epoch 46726 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46727 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46728 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46729 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46730 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46731 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46732 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46733 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46734 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46735 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46736 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46737 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46738 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46739 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46740 \t\t Training Loss: 0.0005807321285828948 \t\n",
      "Epoch 46741 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46742 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46743 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46744 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46745 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46746 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46747 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46748 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46749 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46750 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46751 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46752 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46753 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46754 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46755 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46756 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46757 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46758 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46759 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46760 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46761 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46762 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46763 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46764 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46765 \t\t Training Loss: 0.000580732012167573 \t\n",
      "Epoch 46766 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46767 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46768 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46769 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46770 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46771 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46772 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46773 \t\t Training Loss: 0.0005807318957522511 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46774 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46775 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46776 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46777 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46778 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46779 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46780 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46781 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46782 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46783 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46784 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46785 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46786 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46787 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46788 \t\t Training Loss: 0.0005807318375445902 \t\n",
      "Epoch 46789 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46790 \t\t Training Loss: 0.0005807318957522511 \t\n",
      "Epoch 46791 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46792 \t\t Training Loss: 0.0005807318375445902 \t\n",
      "Epoch 46793 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46794 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46795 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46796 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46797 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46798 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46799 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46800 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46801 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46802 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46803 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46804 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46805 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46806 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46807 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46808 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46809 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46810 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46811 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46812 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46813 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46814 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46815 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46816 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46817 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46818 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46819 \t\t Training Loss: 0.0005807317211292684 \t\n",
      "Epoch 46820 \t\t Training Loss: 0.0005807316047139466 \t\n",
      "Epoch 46821 \t\t Training Loss: 0.0005807316047139466 \t\n",
      "Epoch 46822 \t\t Training Loss: 0.0005807316047139466 \t\n",
      "Epoch 46823 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46824 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46825 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46826 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46827 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46828 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46829 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46830 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46831 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46832 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46833 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46834 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46835 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46836 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46837 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46838 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46839 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46840 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46841 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46842 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46843 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46844 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46845 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46846 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46847 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46848 \t\t Training Loss: 0.0005807316047139466 \t\n",
      "Epoch 46849 \t\t Training Loss: 0.0005807316047139466 \t\n",
      "Epoch 46850 \t\t Training Loss: 0.0005807316047139466 \t\n",
      "Epoch 46851 \t\t Training Loss: 0.0005807316047139466 \t\n",
      "Epoch 46852 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46853 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46854 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46855 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46856 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46857 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46858 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46859 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46860 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46861 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46862 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46863 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46864 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46865 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46866 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46867 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46868 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46869 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46870 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46871 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46872 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46873 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46874 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46875 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46876 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46877 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46878 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46879 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46880 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46881 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46882 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46883 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46884 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46885 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46886 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46887 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46888 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46889 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46890 \t\t Training Loss: 0.0005807315465062857 \t\n",
      "Epoch 46891 \t\t Training Loss: 0.0005807316047139466 \t\n",
      "Epoch 46892 \t\t Training Loss: 0.0005807316047139466 \t\n",
      "Epoch 46893 \t\t Training Loss: 0.0005807316047139466 \t\n",
      "Epoch 46894 \t\t Training Loss: 0.0005807316047139466 \t\n",
      "Epoch 46895 \t\t Training Loss: 0.0005807316047139466 \t\n",
      "Epoch 46896 \t\t Training Loss: 0.0005807316047139466 \t\n",
      "Epoch 46897 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46898 \t\t Training Loss: 0.0005807316047139466 \t\n",
      "Epoch 46899 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46900 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46901 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46902 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46903 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46904 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46905 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46906 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46907 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46908 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46909 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46910 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46911 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46912 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46913 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46914 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46915 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46916 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46917 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46918 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46919 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46920 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46921 \t\t Training Loss: 0.0005807314300909638 \t\n",
      "Epoch 46922 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46923 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46924 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46925 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46926 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46927 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46928 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46929 \t\t Training Loss: 0.000580731313675642 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46930 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46931 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46932 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46933 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46934 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46935 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46936 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46937 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46938 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46939 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46940 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46941 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46942 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46943 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46944 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46945 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46946 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46947 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46948 \t\t Training Loss: 0.0005807311972603202 \t\n",
      "Epoch 46949 \t\t Training Loss: 0.0005807311972603202 \t\n",
      "Epoch 46950 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46951 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46952 \t\t Training Loss: 0.000580731313675642 \t\n",
      "Epoch 46953 \t\t Training Loss: 0.0005807311972603202 \t\n",
      "Epoch 46954 \t\t Training Loss: 0.0005807311972603202 \t\n",
      "Epoch 46955 \t\t Training Loss: 0.0005807311972603202 \t\n",
      "Epoch 46956 \t\t Training Loss: 0.0005807311972603202 \t\n",
      "Epoch 46957 \t\t Training Loss: 0.0005807311972603202 \t\n",
      "Epoch 46958 \t\t Training Loss: 0.0005807311972603202 \t\n",
      "Epoch 46959 \t\t Training Loss: 0.0005807311972603202 \t\n",
      "Epoch 46960 \t\t Training Loss: 0.0005807311390526593 \t\n",
      "Epoch 46961 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46962 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46963 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46964 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46965 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46966 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46967 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46968 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46969 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46970 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46971 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46972 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46973 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46974 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46975 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46976 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46977 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46978 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46979 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46980 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46981 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 46982 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 46983 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46984 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46985 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46986 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46987 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46988 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 46989 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 46990 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 46991 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 46992 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46993 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 46994 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 46995 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 46996 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 46997 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 46998 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 46999 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 47000 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 47001 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 47002 \t\t Training Loss: 0.0005807310226373374 \t\n",
      "Epoch 47003 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 47004 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 47005 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 47006 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 47007 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 47008 \t\t Training Loss: 0.0005807308480143547 \t\n",
      "Epoch 47009 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 47010 \t\t Training Loss: 0.0005807308480143547 \t\n",
      "Epoch 47011 \t\t Training Loss: 0.0005807309062220156 \t\n",
      "Epoch 47012 \t\t Training Loss: 0.0005807308480143547 \t\n",
      "Epoch 47013 \t\t Training Loss: 0.0005807308480143547 \t\n",
      "Epoch 47014 \t\t Training Loss: 0.0005807308480143547 \t\n",
      "Epoch 47015 \t\t Training Loss: 0.0005807308480143547 \t\n",
      "Epoch 47016 \t\t Training Loss: 0.0005807308480143547 \t\n",
      "Epoch 47017 \t\t Training Loss: 0.0005807308480143547 \t\n",
      "Epoch 47018 \t\t Training Loss: 0.0005807308480143547 \t\n",
      "Epoch 47019 \t\t Training Loss: 0.0005807307315990329 \t\n",
      "Epoch 47020 \t\t Training Loss: 0.0005807307315990329 \t\n",
      "Epoch 47021 \t\t Training Loss: 0.0005807307315990329 \t\n",
      "Epoch 47022 \t\t Training Loss: 0.0005807307315990329 \t\n",
      "Epoch 47023 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47024 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47025 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47026 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47027 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47028 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47029 \t\t Training Loss: 0.0005807307315990329 \t\n",
      "Epoch 47030 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47031 \t\t Training Loss: 0.0005807307315990329 \t\n",
      "Epoch 47032 \t\t Training Loss: 0.0005807307315990329 \t\n",
      "Epoch 47033 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47034 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47035 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47036 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47037 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47038 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47039 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47040 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47041 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47042 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47043 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47044 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47045 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47046 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47047 \t\t Training Loss: 0.0005807304405607283 \t\n",
      "Epoch 47048 \t\t Training Loss: 0.0005807304405607283 \t\n",
      "Epoch 47049 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47050 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47051 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47052 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47053 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47054 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47055 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47056 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47057 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47058 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47059 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47060 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47061 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47062 \t\t Training Loss: 0.000580730615183711 \t\n",
      "Epoch 47063 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47064 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47065 \t\t Training Loss: 0.0005807305569760501 \t\n",
      "Epoch 47066 \t\t Training Loss: 0.0005807304405607283 \t\n",
      "Epoch 47067 \t\t Training Loss: 0.0005807304405607283 \t\n",
      "Epoch 47068 \t\t Training Loss: 0.0005807304405607283 \t\n",
      "Epoch 47069 \t\t Training Loss: 0.0005807304405607283 \t\n",
      "Epoch 47070 \t\t Training Loss: 0.0005807304405607283 \t\n",
      "Epoch 47071 \t\t Training Loss: 0.0005807304405607283 \t\n",
      "Epoch 47072 \t\t Training Loss: 0.0005807304405607283 \t\n",
      "Epoch 47073 \t\t Training Loss: 0.0005807304405607283 \t\n",
      "Epoch 47074 \t\t Training Loss: 0.0005807304405607283 \t\n",
      "Epoch 47075 \t\t Training Loss: 0.0005807304405607283 \t\n",
      "Epoch 47076 \t\t Training Loss: 0.0005807303241454065 \t\n",
      "Epoch 47077 \t\t Training Loss: 0.0005807303241454065 \t\n",
      "Epoch 47078 \t\t Training Loss: 0.0005807303241454065 \t\n",
      "Epoch 47079 \t\t Training Loss: 0.0005807303241454065 \t\n",
      "Epoch 47080 \t\t Training Loss: 0.0005807303241454065 \t\n",
      "Epoch 47081 \t\t Training Loss: 0.0005807303241454065 \t\n",
      "Epoch 47082 \t\t Training Loss: 0.0005807303241454065 \t\n",
      "Epoch 47083 \t\t Training Loss: 0.0005807303241454065 \t\n",
      "Epoch 47084 \t\t Training Loss: 0.0005807303241454065 \t\n",
      "Epoch 47085 \t\t Training Loss: 0.0005807303241454065 \t\n",
      "Epoch 47086 \t\t Training Loss: 0.0005807303241454065 \t\n",
      "Epoch 47087 \t\t Training Loss: 0.0005807303241454065 \t\n",
      "Epoch 47088 \t\t Training Loss: 0.0005807303241454065 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47089 \t\t Training Loss: 0.0005807303241454065 \t\n",
      "Epoch 47090 \t\t Training Loss: 0.0005807303241454065 \t\n",
      "Epoch 47091 \t\t Training Loss: 0.0005807303241454065 \t\n",
      "Epoch 47092 \t\t Training Loss: 0.0005807303241454065 \t\n",
      "Epoch 47093 \t\t Training Loss: 0.0005807302659377456 \t\n",
      "Epoch 47094 \t\t Training Loss: 0.0005807302659377456 \t\n",
      "Epoch 47095 \t\t Training Loss: 0.0005807302659377456 \t\n",
      "Epoch 47096 \t\t Training Loss: 0.0005807302077300847 \t\n",
      "Epoch 47097 \t\t Training Loss: 0.0005807302077300847 \t\n",
      "Epoch 47098 \t\t Training Loss: 0.0005807302077300847 \t\n",
      "Epoch 47099 \t\t Training Loss: 0.0005807302077300847 \t\n",
      "Epoch 47100 \t\t Training Loss: 0.0005807302077300847 \t\n",
      "Epoch 47101 \t\t Training Loss: 0.0005807302077300847 \t\n",
      "Epoch 47102 \t\t Training Loss: 0.0005807302077300847 \t\n",
      "Epoch 47103 \t\t Training Loss: 0.0005807302077300847 \t\n",
      "Epoch 47104 \t\t Training Loss: 0.0005807302077300847 \t\n",
      "Epoch 47105 \t\t Training Loss: 0.0005807301495224237 \t\n",
      "Epoch 47106 \t\t Training Loss: 0.0005807302077300847 \t\n",
      "Epoch 47107 \t\t Training Loss: 0.0005807301495224237 \t\n",
      "Epoch 47108 \t\t Training Loss: 0.0005807302077300847 \t\n",
      "Epoch 47109 \t\t Training Loss: 0.0005807301495224237 \t\n",
      "Epoch 47110 \t\t Training Loss: 0.0005807301495224237 \t\n",
      "Epoch 47111 \t\t Training Loss: 0.0005807301495224237 \t\n",
      "Epoch 47112 \t\t Training Loss: 0.0005807301495224237 \t\n",
      "Epoch 47113 \t\t Training Loss: 0.0005807301495224237 \t\n",
      "Epoch 47114 \t\t Training Loss: 0.0005807301495224237 \t\n",
      "Epoch 47115 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47116 \t\t Training Loss: 0.0005807301495224237 \t\n",
      "Epoch 47117 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47118 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47119 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47120 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47121 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47122 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47123 \t\t Training Loss: 0.0005807301495224237 \t\n",
      "Epoch 47124 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47125 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47126 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47127 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47128 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47129 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47130 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47131 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47132 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47133 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47134 \t\t Training Loss: 0.0005807301495224237 \t\n",
      "Epoch 47135 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47136 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47137 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47138 \t\t Training Loss: 0.0005807301495224237 \t\n",
      "Epoch 47139 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47140 \t\t Training Loss: 0.0005807301495224237 \t\n",
      "Epoch 47141 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47142 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47143 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47144 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47145 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47146 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47147 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47148 \t\t Training Loss: 0.0005807300913147628 \t\n",
      "Epoch 47149 \t\t Training Loss: 0.000580729974899441 \t\n",
      "Epoch 47150 \t\t Training Loss: 0.000580729974899441 \t\n",
      "Epoch 47151 \t\t Training Loss: 0.000580729974899441 \t\n",
      "Epoch 47152 \t\t Training Loss: 0.000580729974899441 \t\n",
      "Epoch 47153 \t\t Training Loss: 0.000580729974899441 \t\n",
      "Epoch 47154 \t\t Training Loss: 0.000580729974899441 \t\n",
      "Epoch 47155 \t\t Training Loss: 0.000580729974899441 \t\n",
      "Epoch 47156 \t\t Training Loss: 0.000580729974899441 \t\n",
      "Epoch 47157 \t\t Training Loss: 0.000580729974899441 \t\n",
      "Epoch 47158 \t\t Training Loss: 0.000580729974899441 \t\n",
      "Epoch 47159 \t\t Training Loss: 0.000580729974899441 \t\n",
      "Epoch 47160 \t\t Training Loss: 0.000580729974899441 \t\n",
      "Epoch 47161 \t\t Training Loss: 0.000580729974899441 \t\n",
      "Epoch 47162 \t\t Training Loss: 0.000580729974899441 \t\n",
      "Epoch 47163 \t\t Training Loss: 0.000580729974899441 \t\n",
      "Epoch 47164 \t\t Training Loss: 0.0005807299166917801 \t\n",
      "Epoch 47165 \t\t Training Loss: 0.0005807299166917801 \t\n",
      "Epoch 47166 \t\t Training Loss: 0.0005807299166917801 \t\n",
      "Epoch 47167 \t\t Training Loss: 0.0005807299166917801 \t\n",
      "Epoch 47168 \t\t Training Loss: 0.0005807298584841192 \t\n",
      "Epoch 47169 \t\t Training Loss: 0.0005807298002764583 \t\n",
      "Epoch 47170 \t\t Training Loss: 0.0005807298002764583 \t\n",
      "Epoch 47171 \t\t Training Loss: 0.0005807298002764583 \t\n",
      "Epoch 47172 \t\t Training Loss: 0.0005807298002764583 \t\n",
      "Epoch 47173 \t\t Training Loss: 0.0005807298002764583 \t\n",
      "Epoch 47174 \t\t Training Loss: 0.0005807298002764583 \t\n",
      "Epoch 47175 \t\t Training Loss: 0.0005807298002764583 \t\n",
      "Epoch 47176 \t\t Training Loss: 0.0005807298002764583 \t\n",
      "Epoch 47177 \t\t Training Loss: 0.0005807298002764583 \t\n",
      "Epoch 47178 \t\t Training Loss: 0.0005807298002764583 \t\n",
      "Epoch 47179 \t\t Training Loss: 0.0005807298002764583 \t\n",
      "Epoch 47180 \t\t Training Loss: 0.0005807297420687973 \t\n",
      "Epoch 47181 \t\t Training Loss: 0.0005807297420687973 \t\n",
      "Epoch 47182 \t\t Training Loss: 0.0005807297420687973 \t\n",
      "Epoch 47183 \t\t Training Loss: 0.0005807297420687973 \t\n",
      "Epoch 47184 \t\t Training Loss: 0.0005807296838611364 \t\n",
      "Epoch 47185 \t\t Training Loss: 0.0005807296838611364 \t\n",
      "Epoch 47186 \t\t Training Loss: 0.0005807296838611364 \t\n",
      "Epoch 47187 \t\t Training Loss: 0.0005807296838611364 \t\n",
      "Epoch 47188 \t\t Training Loss: 0.0005807296838611364 \t\n",
      "Epoch 47189 \t\t Training Loss: 0.0005807296838611364 \t\n",
      "Epoch 47190 \t\t Training Loss: 0.0005807296838611364 \t\n",
      "Epoch 47191 \t\t Training Loss: 0.0005807296838611364 \t\n",
      "Epoch 47192 \t\t Training Loss: 0.0005807296838611364 \t\n",
      "Epoch 47193 \t\t Training Loss: 0.0005807296838611364 \t\n",
      "Epoch 47194 \t\t Training Loss: 0.0005807296838611364 \t\n",
      "Epoch 47195 \t\t Training Loss: 0.0005807296256534755 \t\n",
      "Epoch 47196 \t\t Training Loss: 0.0005807296256534755 \t\n",
      "Epoch 47197 \t\t Training Loss: 0.0005807296256534755 \t\n",
      "Epoch 47198 \t\t Training Loss: 0.0005807296256534755 \t\n",
      "Epoch 47199 \t\t Training Loss: 0.0005807296256534755 \t\n",
      "Epoch 47200 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47201 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47202 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47203 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47204 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47205 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47206 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47207 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47208 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47209 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47210 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47211 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47212 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47213 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47214 \t\t Training Loss: 0.0005807296256534755 \t\n",
      "Epoch 47215 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47216 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47217 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47218 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47219 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47220 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47221 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47222 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47223 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47224 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47225 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47226 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47227 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47228 \t\t Training Loss: 0.0005807295092381537 \t\n",
      "Epoch 47229 \t\t Training Loss: 0.0005807295674458146 \t\n",
      "Epoch 47230 \t\t Training Loss: 0.0005807295092381537 \t\n",
      "Epoch 47231 \t\t Training Loss: 0.0005807295092381537 \t\n",
      "Epoch 47232 \t\t Training Loss: 0.0005807295092381537 \t\n",
      "Epoch 47233 \t\t Training Loss: 0.0005807295092381537 \t\n",
      "Epoch 47234 \t\t Training Loss: 0.0005807295092381537 \t\n",
      "Epoch 47235 \t\t Training Loss: 0.0005807293928228319 \t\n",
      "Epoch 47236 \t\t Training Loss: 0.0005807293928228319 \t\n",
      "Epoch 47237 \t\t Training Loss: 0.0005807293928228319 \t\n",
      "Epoch 47238 \t\t Training Loss: 0.0005807293928228319 \t\n",
      "Epoch 47239 \t\t Training Loss: 0.0005807293928228319 \t\n",
      "Epoch 47240 \t\t Training Loss: 0.0005807293928228319 \t\n",
      "Epoch 47241 \t\t Training Loss: 0.0005807295092381537 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47242 \t\t Training Loss: 0.0005807294510304928 \t\n",
      "Epoch 47243 \t\t Training Loss: 0.0005807293928228319 \t\n",
      "Epoch 47244 \t\t Training Loss: 0.0005807294510304928 \t\n",
      "Epoch 47245 \t\t Training Loss: 0.0005807294510304928 \t\n",
      "Epoch 47246 \t\t Training Loss: 0.0005807295092381537 \t\n",
      "Epoch 47247 \t\t Training Loss: 0.0005807295092381537 \t\n",
      "Epoch 47248 \t\t Training Loss: 0.0005807294510304928 \t\n",
      "Epoch 47249 \t\t Training Loss: 0.0005807293928228319 \t\n",
      "Epoch 47250 \t\t Training Loss: 0.0005807293928228319 \t\n",
      "Epoch 47251 \t\t Training Loss: 0.000580729334615171 \t\n",
      "Epoch 47252 \t\t Training Loss: 0.000580729334615171 \t\n",
      "Epoch 47253 \t\t Training Loss: 0.000580729334615171 \t\n",
      "Epoch 47254 \t\t Training Loss: 0.000580729334615171 \t\n",
      "Epoch 47255 \t\t Training Loss: 0.000580729334615171 \t\n",
      "Epoch 47256 \t\t Training Loss: 0.000580729334615171 \t\n",
      "Epoch 47257 \t\t Training Loss: 0.000580729334615171 \t\n",
      "Epoch 47258 \t\t Training Loss: 0.000580729334615171 \t\n",
      "Epoch 47259 \t\t Training Loss: 0.000580729334615171 \t\n",
      "Epoch 47260 \t\t Training Loss: 0.000580729334615171 \t\n",
      "Epoch 47261 \t\t Training Loss: 0.000580729334615171 \t\n",
      "Epoch 47262 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47263 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47264 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47265 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47266 \t\t Training Loss: 0.000580729334615171 \t\n",
      "Epoch 47267 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47268 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47269 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47270 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47271 \t\t Training Loss: 0.0005807292181998491 \t\n",
      "Epoch 47272 \t\t Training Loss: 0.0005807292181998491 \t\n",
      "Epoch 47273 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47274 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47275 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47276 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47277 \t\t Training Loss: 0.0005807292181998491 \t\n",
      "Epoch 47278 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47279 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47280 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47281 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47282 \t\t Training Loss: 0.00058072927640751 \t\n",
      "Epoch 47283 \t\t Training Loss: 0.0005807292181998491 \t\n",
      "Epoch 47284 \t\t Training Loss: 0.0005807292181998491 \t\n",
      "Epoch 47285 \t\t Training Loss: 0.0005807292181998491 \t\n",
      "Epoch 47286 \t\t Training Loss: 0.0005807292181998491 \t\n",
      "Epoch 47287 \t\t Training Loss: 0.0005807292181998491 \t\n",
      "Epoch 47288 \t\t Training Loss: 0.0005807292181998491 \t\n",
      "Epoch 47289 \t\t Training Loss: 0.0005807292181998491 \t\n",
      "Epoch 47290 \t\t Training Loss: 0.0005807292181998491 \t\n",
      "Epoch 47291 \t\t Training Loss: 0.0005807292181998491 \t\n",
      "Epoch 47292 \t\t Training Loss: 0.0005807291017845273 \t\n",
      "Epoch 47293 \t\t Training Loss: 0.0005807291017845273 \t\n",
      "Epoch 47294 \t\t Training Loss: 0.0005807291599921882 \t\n",
      "Epoch 47295 \t\t Training Loss: 0.0005807291017845273 \t\n",
      "Epoch 47296 \t\t Training Loss: 0.0005807291599921882 \t\n",
      "Epoch 47297 \t\t Training Loss: 0.0005807291017845273 \t\n",
      "Epoch 47298 \t\t Training Loss: 0.0005807291017845273 \t\n",
      "Epoch 47299 \t\t Training Loss: 0.0005807291017845273 \t\n",
      "Epoch 47300 \t\t Training Loss: 0.0005807290435768664 \t\n",
      "Epoch 47301 \t\t Training Loss: 0.0005807290435768664 \t\n",
      "Epoch 47302 \t\t Training Loss: 0.0005807290435768664 \t\n",
      "Epoch 47303 \t\t Training Loss: 0.0005807290435768664 \t\n",
      "Epoch 47304 \t\t Training Loss: 0.0005807290435768664 \t\n",
      "Epoch 47305 \t\t Training Loss: 0.0005807290435768664 \t\n",
      "Epoch 47306 \t\t Training Loss: 0.0005807290435768664 \t\n",
      "Epoch 47307 \t\t Training Loss: 0.0005807290435768664 \t\n",
      "Epoch 47308 \t\t Training Loss: 0.0005807290435768664 \t\n",
      "Epoch 47309 \t\t Training Loss: 0.0005807290435768664 \t\n",
      "Epoch 47310 \t\t Training Loss: 0.0005807290435768664 \t\n",
      "Epoch 47311 \t\t Training Loss: 0.0005807290435768664 \t\n",
      "Epoch 47312 \t\t Training Loss: 0.0005807290435768664 \t\n",
      "Epoch 47313 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47314 \t\t Training Loss: 0.0005807289853692055 \t\n",
      "Epoch 47315 \t\t Training Loss: 0.0005807289853692055 \t\n",
      "Epoch 47316 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47317 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47318 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47319 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47320 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47321 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47322 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47323 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47324 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47325 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47326 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47327 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47328 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47329 \t\t Training Loss: 0.0005807289853692055 \t\n",
      "Epoch 47330 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47331 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47332 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47333 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47334 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47335 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47336 \t\t Training Loss: 0.0005807288107462227 \t\n",
      "Epoch 47337 \t\t Training Loss: 0.0005807288107462227 \t\n",
      "Epoch 47338 \t\t Training Loss: 0.0005807288107462227 \t\n",
      "Epoch 47339 \t\t Training Loss: 0.0005807288107462227 \t\n",
      "Epoch 47340 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47341 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47342 \t\t Training Loss: 0.0005807288107462227 \t\n",
      "Epoch 47343 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47344 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47345 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47346 \t\t Training Loss: 0.0005807288107462227 \t\n",
      "Epoch 47347 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47348 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47349 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47350 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47351 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47352 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47353 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47354 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47355 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47356 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47357 \t\t Training Loss: 0.0005807289271615446 \t\n",
      "Epoch 47358 \t\t Training Loss: 0.0005807288107462227 \t\n",
      "Epoch 47359 \t\t Training Loss: 0.0005807288107462227 \t\n",
      "Epoch 47360 \t\t Training Loss: 0.0005807288107462227 \t\n",
      "Epoch 47361 \t\t Training Loss: 0.0005807288107462227 \t\n",
      "Epoch 47362 \t\t Training Loss: 0.0005807288107462227 \t\n",
      "Epoch 47363 \t\t Training Loss: 0.0005807288107462227 \t\n",
      "Epoch 47364 \t\t Training Loss: 0.0005807288107462227 \t\n",
      "Epoch 47365 \t\t Training Loss: 0.0005807287525385618 \t\n",
      "Epoch 47366 \t\t Training Loss: 0.0005807287525385618 \t\n",
      "Epoch 47367 \t\t Training Loss: 0.0005807287525385618 \t\n",
      "Epoch 47368 \t\t Training Loss: 0.0005807287525385618 \t\n",
      "Epoch 47369 \t\t Training Loss: 0.0005807287525385618 \t\n",
      "Epoch 47370 \t\t Training Loss: 0.0005807287525385618 \t\n",
      "Epoch 47371 \t\t Training Loss: 0.0005807287525385618 \t\n",
      "Epoch 47372 \t\t Training Loss: 0.0005807287525385618 \t\n",
      "Epoch 47373 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47374 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47375 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47376 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47377 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47378 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47379 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47380 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47381 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47382 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47383 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47384 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47385 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47386 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47387 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47388 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47389 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47390 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47391 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47392 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47393 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47394 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47395 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47396 \t\t Training Loss: 0.00058072863612324 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47397 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47398 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47399 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47400 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47401 \t\t Training Loss: 0.0005807285779155791 \t\n",
      "Epoch 47402 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47403 \t\t Training Loss: 0.0005807285779155791 \t\n",
      "Epoch 47404 \t\t Training Loss: 0.0005807285779155791 \t\n",
      "Epoch 47405 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47406 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47407 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47408 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47409 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47410 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47411 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47412 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47413 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47414 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47415 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47416 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47417 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47418 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47419 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47420 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47421 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47422 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47423 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47424 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47425 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47426 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47427 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47428 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47429 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47430 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47431 \t\t Training Loss: 0.0005807286943309009 \t\n",
      "Epoch 47432 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47433 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47434 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47435 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47436 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47437 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47438 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47439 \t\t Training Loss: 0.00058072863612324 \t\n",
      "Epoch 47440 \t\t Training Loss: 0.0005807285779155791 \t\n",
      "Epoch 47441 \t\t Training Loss: 0.0005807285779155791 \t\n",
      "Epoch 47442 \t\t Training Loss: 0.0005807285779155791 \t\n",
      "Epoch 47443 \t\t Training Loss: 0.0005807285779155791 \t\n",
      "Epoch 47444 \t\t Training Loss: 0.0005807285779155791 \t\n",
      "Epoch 47445 \t\t Training Loss: 0.0005807285779155791 \t\n",
      "Epoch 47446 \t\t Training Loss: 0.0005807285197079182 \t\n",
      "Epoch 47447 \t\t Training Loss: 0.0005807285197079182 \t\n",
      "Epoch 47448 \t\t Training Loss: 0.0005807285197079182 \t\n",
      "Epoch 47449 \t\t Training Loss: 0.0005807285197079182 \t\n",
      "Epoch 47450 \t\t Training Loss: 0.0005807285197079182 \t\n",
      "Epoch 47451 \t\t Training Loss: 0.0005807285197079182 \t\n",
      "Epoch 47452 \t\t Training Loss: 0.0005807285197079182 \t\n",
      "Epoch 47453 \t\t Training Loss: 0.0005807285197079182 \t\n",
      "Epoch 47454 \t\t Training Loss: 0.0005807285197079182 \t\n",
      "Epoch 47455 \t\t Training Loss: 0.0005807285197079182 \t\n",
      "Epoch 47456 \t\t Training Loss: 0.0005807284615002573 \t\n",
      "Epoch 47457 \t\t Training Loss: 0.0005807284615002573 \t\n",
      "Epoch 47458 \t\t Training Loss: 0.0005807284032925963 \t\n",
      "Epoch 47459 \t\t Training Loss: 0.0005807284032925963 \t\n",
      "Epoch 47460 \t\t Training Loss: 0.0005807284615002573 \t\n",
      "Epoch 47461 \t\t Training Loss: 0.0005807284032925963 \t\n",
      "Epoch 47462 \t\t Training Loss: 0.0005807284032925963 \t\n",
      "Epoch 47463 \t\t Training Loss: 0.0005807284032925963 \t\n",
      "Epoch 47464 \t\t Training Loss: 0.0005807284032925963 \t\n",
      "Epoch 47465 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47466 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47467 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47468 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47469 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47470 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47471 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47472 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47473 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47474 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47475 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47476 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47477 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47478 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47479 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47480 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47481 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47482 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47483 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47484 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47485 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47486 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47487 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47488 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47489 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47490 \t\t Training Loss: 0.0005807283450849354 \t\n",
      "Epoch 47491 \t\t Training Loss: 0.0005807282868772745 \t\n",
      "Epoch 47492 \t\t Training Loss: 0.0005807282286696136 \t\n",
      "Epoch 47493 \t\t Training Loss: 0.0005807282868772745 \t\n",
      "Epoch 47494 \t\t Training Loss: 0.0005807282286696136 \t\n",
      "Epoch 47495 \t\t Training Loss: 0.0005807282286696136 \t\n",
      "Epoch 47496 \t\t Training Loss: 0.0005807282286696136 \t\n",
      "Epoch 47497 \t\t Training Loss: 0.0005807282286696136 \t\n",
      "Epoch 47498 \t\t Training Loss: 0.0005807282286696136 \t\n",
      "Epoch 47499 \t\t Training Loss: 0.0005807282286696136 \t\n",
      "Epoch 47500 \t\t Training Loss: 0.0005807282286696136 \t\n",
      "Epoch 47501 \t\t Training Loss: 0.0005807282286696136 \t\n",
      "Epoch 47502 \t\t Training Loss: 0.0005807281122542918 \t\n",
      "Epoch 47503 \t\t Training Loss: 0.0005807281122542918 \t\n",
      "Epoch 47504 \t\t Training Loss: 0.0005807281122542918 \t\n",
      "Epoch 47505 \t\t Training Loss: 0.0005807281122542918 \t\n",
      "Epoch 47506 \t\t Training Loss: 0.0005807281122542918 \t\n",
      "Epoch 47507 \t\t Training Loss: 0.0005807281122542918 \t\n",
      "Epoch 47508 \t\t Training Loss: 0.0005807281122542918 \t\n",
      "Epoch 47509 \t\t Training Loss: 0.0005807281122542918 \t\n",
      "Epoch 47510 \t\t Training Loss: 0.0005807281122542918 \t\n",
      "Epoch 47511 \t\t Training Loss: 0.0005807281122542918 \t\n",
      "Epoch 47512 \t\t Training Loss: 0.0005807281122542918 \t\n",
      "Epoch 47513 \t\t Training Loss: 0.0005807281122542918 \t\n",
      "Epoch 47514 \t\t Training Loss: 0.0005807280540466309 \t\n",
      "Epoch 47515 \t\t Training Loss: 0.0005807280540466309 \t\n",
      "Epoch 47516 \t\t Training Loss: 0.00058072799583897 \t\n",
      "Epoch 47517 \t\t Training Loss: 0.00058072799583897 \t\n",
      "Epoch 47518 \t\t Training Loss: 0.00058072799583897 \t\n",
      "Epoch 47519 \t\t Training Loss: 0.00058072799583897 \t\n",
      "Epoch 47520 \t\t Training Loss: 0.00058072799583897 \t\n",
      "Epoch 47521 \t\t Training Loss: 0.00058072799583897 \t\n",
      "Epoch 47522 \t\t Training Loss: 0.00058072799583897 \t\n",
      "Epoch 47523 \t\t Training Loss: 0.00058072799583897 \t\n",
      "Epoch 47524 \t\t Training Loss: 0.00058072799583897 \t\n",
      "Epoch 47525 \t\t Training Loss: 0.00058072799583897 \t\n",
      "Epoch 47526 \t\t Training Loss: 0.00058072799583897 \t\n",
      "Epoch 47527 \t\t Training Loss: 0.000580727937631309 \t\n",
      "Epoch 47528 \t\t Training Loss: 0.000580727937631309 \t\n",
      "Epoch 47529 \t\t Training Loss: 0.000580727937631309 \t\n",
      "Epoch 47530 \t\t Training Loss: 0.00058072799583897 \t\n",
      "Epoch 47531 \t\t Training Loss: 0.000580727937631309 \t\n",
      "Epoch 47532 \t\t Training Loss: 0.0005807278794236481 \t\n",
      "Epoch 47533 \t\t Training Loss: 0.000580727937631309 \t\n",
      "Epoch 47534 \t\t Training Loss: 0.0005807278794236481 \t\n",
      "Epoch 47535 \t\t Training Loss: 0.0005807278794236481 \t\n",
      "Epoch 47536 \t\t Training Loss: 0.000580727937631309 \t\n",
      "Epoch 47537 \t\t Training Loss: 0.0005807278794236481 \t\n",
      "Epoch 47538 \t\t Training Loss: 0.0005807278794236481 \t\n",
      "Epoch 47539 \t\t Training Loss: 0.0005807278212159872 \t\n",
      "Epoch 47540 \t\t Training Loss: 0.0005807278794236481 \t\n",
      "Epoch 47541 \t\t Training Loss: 0.0005807278794236481 \t\n",
      "Epoch 47542 \t\t Training Loss: 0.0005807278794236481 \t\n",
      "Epoch 47543 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47544 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47545 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47546 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47547 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47548 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47549 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47550 \t\t Training Loss: 0.0005807278794236481 \t\n",
      "Epoch 47551 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47552 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47553 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47554 \t\t Training Loss: 0.0005807277630083263 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47555 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47556 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47557 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47558 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47559 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47560 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47561 \t\t Training Loss: 0.0005807278794236481 \t\n",
      "Epoch 47562 \t\t Training Loss: 0.0005807278794236481 \t\n",
      "Epoch 47563 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47564 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47565 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47566 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47567 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47568 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47569 \t\t Training Loss: 0.0005807277630083263 \t\n",
      "Epoch 47570 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47571 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47572 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47573 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47574 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47575 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47576 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47577 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47578 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47579 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47580 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47581 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47582 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47583 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47584 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47585 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47586 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47587 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47588 \t\t Training Loss: 0.0005807276465930045 \t\n",
      "Epoch 47589 \t\t Training Loss: 0.0005807276465930045 \t\n",
      "Epoch 47590 \t\t Training Loss: 0.0005807277048006654 \t\n",
      "Epoch 47591 \t\t Training Loss: 0.0005807276465930045 \t\n",
      "Epoch 47592 \t\t Training Loss: 0.0005807276465930045 \t\n",
      "Epoch 47593 \t\t Training Loss: 0.0005807276465930045 \t\n",
      "Epoch 47594 \t\t Training Loss: 0.0005807276465930045 \t\n",
      "Epoch 47595 \t\t Training Loss: 0.0005807276465930045 \t\n",
      "Epoch 47596 \t\t Training Loss: 0.0005807276465930045 \t\n",
      "Epoch 47597 \t\t Training Loss: 0.0005807276465930045 \t\n",
      "Epoch 47598 \t\t Training Loss: 0.0005807276465930045 \t\n",
      "Epoch 47599 \t\t Training Loss: 0.0005807276465930045 \t\n",
      "Epoch 47600 \t\t Training Loss: 0.0005807276465930045 \t\n",
      "Epoch 47601 \t\t Training Loss: 0.0005807276465930045 \t\n",
      "Epoch 47602 \t\t Training Loss: 0.0005807275883853436 \t\n",
      "Epoch 47603 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47604 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47605 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47606 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47607 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47608 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47609 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47610 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47611 \t\t Training Loss: 0.0005807276465930045 \t\n",
      "Epoch 47612 \t\t Training Loss: 0.0005807275883853436 \t\n",
      "Epoch 47613 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47614 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47615 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47616 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47617 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47618 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47619 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47620 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47621 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47622 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47623 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47624 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47625 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47626 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47627 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47628 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47629 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47630 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47631 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47632 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47633 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47634 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47635 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47636 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47637 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47638 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47639 \t\t Training Loss: 0.0005807275301776826 \t\n",
      "Epoch 47640 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47641 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47642 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47643 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47644 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47645 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47646 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47647 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47648 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47649 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47650 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47651 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47652 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47653 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47654 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47655 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47656 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47657 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47658 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47659 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47660 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47661 \t\t Training Loss: 0.0005807274719700217 \t\n",
      "Epoch 47662 \t\t Training Loss: 0.0005807273555546999 \t\n",
      "Epoch 47663 \t\t Training Loss: 0.0005807273555546999 \t\n",
      "Epoch 47664 \t\t Training Loss: 0.0005807273555546999 \t\n",
      "Epoch 47665 \t\t Training Loss: 0.0005807273555546999 \t\n",
      "Epoch 47666 \t\t Training Loss: 0.000580727297347039 \t\n",
      "Epoch 47667 \t\t Training Loss: 0.000580727297347039 \t\n",
      "Epoch 47668 \t\t Training Loss: 0.000580727297347039 \t\n",
      "Epoch 47669 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47670 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47671 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47672 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47673 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47674 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47675 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47676 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47677 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47678 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47679 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47680 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47681 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47682 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47683 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47684 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47685 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47686 \t\t Training Loss: 0.0005807271809317172 \t\n",
      "Epoch 47687 \t\t Training Loss: 0.0005807270645163953 \t\n",
      "Epoch 47688 \t\t Training Loss: 0.0005807270645163953 \t\n",
      "Epoch 47689 \t\t Training Loss: 0.0005807270645163953 \t\n",
      "Epoch 47690 \t\t Training Loss: 0.0005807270645163953 \t\n",
      "Epoch 47691 \t\t Training Loss: 0.0005807270645163953 \t\n",
      "Epoch 47692 \t\t Training Loss: 0.0005807270645163953 \t\n",
      "Epoch 47693 \t\t Training Loss: 0.0005807270645163953 \t\n",
      "Epoch 47694 \t\t Training Loss: 0.0005807270645163953 \t\n",
      "Epoch 47695 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47696 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47697 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47698 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47699 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47700 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47701 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47702 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47703 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47704 \t\t Training Loss: 0.0005807270645163953 \t\n",
      "Epoch 47705 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47706 \t\t Training Loss: 0.0005807270645163953 \t\n",
      "Epoch 47707 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47708 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47709 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47710 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47711 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47712 \t\t Training Loss: 0.0005807269481010735 \t\n",
      "Epoch 47713 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47714 \t\t Training Loss: 0.0005807270063087344 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47715 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47716 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47717 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47718 \t\t Training Loss: 0.0005807269481010735 \t\n",
      "Epoch 47719 \t\t Training Loss: 0.0005807269481010735 \t\n",
      "Epoch 47720 \t\t Training Loss: 0.0005807269481010735 \t\n",
      "Epoch 47721 \t\t Training Loss: 0.0005807269481010735 \t\n",
      "Epoch 47722 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47723 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47724 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47725 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47726 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47727 \t\t Training Loss: 0.0005807269481010735 \t\n",
      "Epoch 47728 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47729 \t\t Training Loss: 0.0005807269481010735 \t\n",
      "Epoch 47730 \t\t Training Loss: 0.0005807269481010735 \t\n",
      "Epoch 47731 \t\t Training Loss: 0.0005807269481010735 \t\n",
      "Epoch 47732 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47733 \t\t Training Loss: 0.0005807270063087344 \t\n",
      "Epoch 47734 \t\t Training Loss: 0.0005807269481010735 \t\n",
      "Epoch 47735 \t\t Training Loss: 0.0005807269481010735 \t\n",
      "Epoch 47736 \t\t Training Loss: 0.0005807269481010735 \t\n",
      "Epoch 47737 \t\t Training Loss: 0.0005807269481010735 \t\n",
      "Epoch 47738 \t\t Training Loss: 0.0005807269481010735 \t\n",
      "Epoch 47739 \t\t Training Loss: 0.0005807269481010735 \t\n",
      "Epoch 47740 \t\t Training Loss: 0.0005807269481010735 \t\n",
      "Epoch 47741 \t\t Training Loss: 0.0005807268898934126 \t\n",
      "Epoch 47742 \t\t Training Loss: 0.0005807268898934126 \t\n",
      "Epoch 47743 \t\t Training Loss: 0.0005807268898934126 \t\n",
      "Epoch 47744 \t\t Training Loss: 0.0005807268898934126 \t\n",
      "Epoch 47745 \t\t Training Loss: 0.0005807268898934126 \t\n",
      "Epoch 47746 \t\t Training Loss: 0.0005807268898934126 \t\n",
      "Epoch 47747 \t\t Training Loss: 0.0005807268898934126 \t\n",
      "Epoch 47748 \t\t Training Loss: 0.0005807268898934126 \t\n",
      "Epoch 47749 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47750 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47751 \t\t Training Loss: 0.0005807268898934126 \t\n",
      "Epoch 47752 \t\t Training Loss: 0.0005807268898934126 \t\n",
      "Epoch 47753 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47754 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47755 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47756 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47757 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47758 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47759 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47760 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47761 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47762 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47763 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47764 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47765 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47766 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47767 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47768 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47769 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47770 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47771 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47772 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47773 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47774 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47775 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47776 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47777 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47778 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47779 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47780 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47781 \t\t Training Loss: 0.0005807267734780908 \t\n",
      "Epoch 47782 \t\t Training Loss: 0.0005807266570627689 \t\n",
      "Epoch 47783 \t\t Training Loss: 0.000580726598855108 \t\n",
      "Epoch 47784 \t\t Training Loss: 0.000580726598855108 \t\n",
      "Epoch 47785 \t\t Training Loss: 0.0005807266570627689 \t\n",
      "Epoch 47786 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47787 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47788 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47789 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47790 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47791 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47792 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47793 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47794 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47795 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47796 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47797 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47798 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47799 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47800 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47801 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47802 \t\t Training Loss: 0.0005807265406474471 \t\n",
      "Epoch 47803 \t\t Training Loss: 0.0005807264824397862 \t\n",
      "Epoch 47804 \t\t Training Loss: 0.0005807264824397862 \t\n",
      "Epoch 47805 \t\t Training Loss: 0.0005807264824397862 \t\n",
      "Epoch 47806 \t\t Training Loss: 0.0005807264824397862 \t\n",
      "Epoch 47807 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47808 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47809 \t\t Training Loss: 0.0005807264824397862 \t\n",
      "Epoch 47810 \t\t Training Loss: 0.0005807264824397862 \t\n",
      "Epoch 47811 \t\t Training Loss: 0.0005807264824397862 \t\n",
      "Epoch 47812 \t\t Training Loss: 0.0005807264824397862 \t\n",
      "Epoch 47813 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47814 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47815 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47816 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47817 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47818 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47819 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47820 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47821 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47822 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47823 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47824 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47825 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47826 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47827 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47828 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47829 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47830 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47831 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47832 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47833 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47834 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47835 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47836 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47837 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47838 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47839 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47840 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47841 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47842 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47843 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47844 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47845 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47846 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47847 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47848 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47849 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47850 \t\t Training Loss: 0.0005807262496091425 \t\n",
      "Epoch 47851 \t\t Training Loss: 0.0005807262496091425 \t\n",
      "Epoch 47852 \t\t Training Loss: 0.0005807262496091425 \t\n",
      "Epoch 47853 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47854 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47855 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47856 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47857 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47858 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47859 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47860 \t\t Training Loss: 0.0005807262496091425 \t\n",
      "Epoch 47861 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47862 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47863 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47864 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47865 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47866 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47867 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47868 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47869 \t\t Training Loss: 0.0005807263660244644 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47870 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47871 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47872 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47873 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47874 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47875 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47876 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47877 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47878 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47879 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47880 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47881 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47882 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47883 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47884 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47885 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47886 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47887 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47888 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47889 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47890 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47891 \t\t Training Loss: 0.0005807262496091425 \t\n",
      "Epoch 47892 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47893 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47894 \t\t Training Loss: 0.0005807263660244644 \t\n",
      "Epoch 47895 \t\t Training Loss: 0.0005807262496091425 \t\n",
      "Epoch 47896 \t\t Training Loss: 0.0005807262496091425 \t\n",
      "Epoch 47897 \t\t Training Loss: 0.0005807262496091425 \t\n",
      "Epoch 47898 \t\t Training Loss: 0.0005807261914014816 \t\n",
      "Epoch 47899 \t\t Training Loss: 0.0005807261914014816 \t\n",
      "Epoch 47900 \t\t Training Loss: 0.0005807261914014816 \t\n",
      "Epoch 47901 \t\t Training Loss: 0.0005807261914014816 \t\n",
      "Epoch 47902 \t\t Training Loss: 0.0005807261914014816 \t\n",
      "Epoch 47903 \t\t Training Loss: 0.0005807261914014816 \t\n",
      "Epoch 47904 \t\t Training Loss: 0.0005807261914014816 \t\n",
      "Epoch 47905 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47906 \t\t Training Loss: 0.0005807261914014816 \t\n",
      "Epoch 47907 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47908 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47909 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47910 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47911 \t\t Training Loss: 0.0005807261914014816 \t\n",
      "Epoch 47912 \t\t Training Loss: 0.0005807261914014816 \t\n",
      "Epoch 47913 \t\t Training Loss: 0.0005807261914014816 \t\n",
      "Epoch 47914 \t\t Training Loss: 0.0005807261914014816 \t\n",
      "Epoch 47915 \t\t Training Loss: 0.0005807261914014816 \t\n",
      "Epoch 47916 \t\t Training Loss: 0.0005807261914014816 \t\n",
      "Epoch 47917 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47918 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47919 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47920 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47921 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47922 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47923 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47924 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47925 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47926 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47927 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47928 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47929 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47930 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47931 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47932 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47933 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47934 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47935 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47936 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47937 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47938 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47939 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47940 \t\t Training Loss: 0.000580725958570838 \t\n",
      "Epoch 47941 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47942 \t\t Training Loss: 0.000580725958570838 \t\n",
      "Epoch 47943 \t\t Training Loss: 0.000580725958570838 \t\n",
      "Epoch 47944 \t\t Training Loss: 0.000580725958570838 \t\n",
      "Epoch 47945 \t\t Training Loss: 0.000580725958570838 \t\n",
      "Epoch 47946 \t\t Training Loss: 0.000580725958570838 \t\n",
      "Epoch 47947 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47948 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47949 \t\t Training Loss: 0.000580725958570838 \t\n",
      "Epoch 47950 \t\t Training Loss: 0.000580725958570838 \t\n",
      "Epoch 47951 \t\t Training Loss: 0.0005807260749861598 \t\n",
      "Epoch 47952 \t\t Training Loss: 0.000580725958570838 \t\n",
      "Epoch 47953 \t\t Training Loss: 0.0005807259003631771 \t\n",
      "Epoch 47954 \t\t Training Loss: 0.0005807259003631771 \t\n",
      "Epoch 47955 \t\t Training Loss: 0.0005807259003631771 \t\n",
      "Epoch 47956 \t\t Training Loss: 0.0005807259003631771 \t\n",
      "Epoch 47957 \t\t Training Loss: 0.0005807259003631771 \t\n",
      "Epoch 47958 \t\t Training Loss: 0.0005807259003631771 \t\n",
      "Epoch 47959 \t\t Training Loss: 0.0005807259003631771 \t\n",
      "Epoch 47960 \t\t Training Loss: 0.0005807259003631771 \t\n",
      "Epoch 47961 \t\t Training Loss: 0.0005807259003631771 \t\n",
      "Epoch 47962 \t\t Training Loss: 0.0005807259003631771 \t\n",
      "Epoch 47963 \t\t Training Loss: 0.0005807259003631771 \t\n",
      "Epoch 47964 \t\t Training Loss: 0.0005807259003631771 \t\n",
      "Epoch 47965 \t\t Training Loss: 0.0005807259003631771 \t\n",
      "Epoch 47966 \t\t Training Loss: 0.0005807259003631771 \t\n",
      "Epoch 47967 \t\t Training Loss: 0.0005807259003631771 \t\n",
      "Epoch 47968 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47969 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47970 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47971 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47972 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47973 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47974 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47975 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47976 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47977 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47978 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47979 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47980 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47981 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47982 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47983 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47984 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47985 \t\t Training Loss: 0.0005807257839478552 \t\n",
      "Epoch 47986 \t\t Training Loss: 0.0005807256675325334 \t\n",
      "Epoch 47987 \t\t Training Loss: 0.0005807256675325334 \t\n",
      "Epoch 47988 \t\t Training Loss: 0.0005807256675325334 \t\n",
      "Epoch 47989 \t\t Training Loss: 0.0005807256675325334 \t\n",
      "Epoch 47990 \t\t Training Loss: 0.0005807256675325334 \t\n",
      "Epoch 47991 \t\t Training Loss: 0.0005807256093248725 \t\n",
      "Epoch 47992 \t\t Training Loss: 0.0005807256093248725 \t\n",
      "Epoch 47993 \t\t Training Loss: 0.0005807256675325334 \t\n",
      "Epoch 47994 \t\t Training Loss: 0.0005807256675325334 \t\n",
      "Epoch 47995 \t\t Training Loss: 0.0005807256093248725 \t\n",
      "Epoch 47996 \t\t Training Loss: 0.0005807256093248725 \t\n",
      "Epoch 47997 \t\t Training Loss: 0.0005807256093248725 \t\n",
      "Epoch 47998 \t\t Training Loss: 0.0005807256093248725 \t\n",
      "Epoch 47999 \t\t Training Loss: 0.0005807256093248725 \t\n",
      "Epoch 48000 \t\t Training Loss: 0.0005807256093248725 \t\n",
      "Epoch 48001 \t\t Training Loss: 0.0005807256093248725 \t\n",
      "Epoch 48002 \t\t Training Loss: 0.0005807256093248725 \t\n",
      "Epoch 48003 \t\t Training Loss: 0.0005807256093248725 \t\n",
      "Epoch 48004 \t\t Training Loss: 0.0005807256093248725 \t\n",
      "Epoch 48005 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48006 \t\t Training Loss: 0.0005807256093248725 \t\n",
      "Epoch 48007 \t\t Training Loss: 0.0005807256093248725 \t\n",
      "Epoch 48008 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48009 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48010 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48011 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48012 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48013 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48014 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48015 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48016 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48017 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48018 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48019 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48020 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48021 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48022 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48023 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48024 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48025 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48026 \t\t Training Loss: 0.0005807254929095507 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48027 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48028 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48029 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48030 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48031 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48032 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48033 \t\t Training Loss: 0.0005807253764942288 \t\n",
      "Epoch 48034 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48035 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48036 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48037 \t\t Training Loss: 0.0005807254929095507 \t\n",
      "Epoch 48038 \t\t Training Loss: 0.0005807253764942288 \t\n",
      "Epoch 48039 \t\t Training Loss: 0.0005807253764942288 \t\n",
      "Epoch 48040 \t\t Training Loss: 0.0005807253764942288 \t\n",
      "Epoch 48041 \t\t Training Loss: 0.0005807253764942288 \t\n",
      "Epoch 48042 \t\t Training Loss: 0.0005807253764942288 \t\n",
      "Epoch 48043 \t\t Training Loss: 0.0005807253764942288 \t\n",
      "Epoch 48044 \t\t Training Loss: 0.0005807253764942288 \t\n",
      "Epoch 48045 \t\t Training Loss: 0.0005807253764942288 \t\n",
      "Epoch 48046 \t\t Training Loss: 0.0005807253764942288 \t\n",
      "Epoch 48047 \t\t Training Loss: 0.0005807253764942288 \t\n",
      "Epoch 48048 \t\t Training Loss: 0.0005807253764942288 \t\n",
      "Epoch 48049 \t\t Training Loss: 0.0005807253764942288 \t\n",
      "Epoch 48050 \t\t Training Loss: 0.0005807253764942288 \t\n",
      "Epoch 48051 \t\t Training Loss: 0.0005807253764942288 \t\n",
      "Epoch 48052 \t\t Training Loss: 0.0005807253182865679 \t\n",
      "Epoch 48053 \t\t Training Loss: 0.0005807253182865679 \t\n",
      "Epoch 48054 \t\t Training Loss: 0.0005807253182865679 \t\n",
      "Epoch 48055 \t\t Training Loss: 0.000580725260078907 \t\n",
      "Epoch 48056 \t\t Training Loss: 0.000580725260078907 \t\n",
      "Epoch 48057 \t\t Training Loss: 0.000580725260078907 \t\n",
      "Epoch 48058 \t\t Training Loss: 0.0005807253182865679 \t\n",
      "Epoch 48059 \t\t Training Loss: 0.0005807253182865679 \t\n",
      "Epoch 48060 \t\t Training Loss: 0.000580725260078907 \t\n",
      "Epoch 48061 \t\t Training Loss: 0.000580725260078907 \t\n",
      "Epoch 48062 \t\t Training Loss: 0.000580725260078907 \t\n",
      "Epoch 48063 \t\t Training Loss: 0.000580725260078907 \t\n",
      "Epoch 48064 \t\t Training Loss: 0.000580725260078907 \t\n",
      "Epoch 48065 \t\t Training Loss: 0.0005807252018712461 \t\n",
      "Epoch 48066 \t\t Training Loss: 0.0005807252018712461 \t\n",
      "Epoch 48067 \t\t Training Loss: 0.0005807252018712461 \t\n",
      "Epoch 48068 \t\t Training Loss: 0.0005807250854559243 \t\n",
      "Epoch 48069 \t\t Training Loss: 0.0005807252018712461 \t\n",
      "Epoch 48070 \t\t Training Loss: 0.0005807250854559243 \t\n",
      "Epoch 48071 \t\t Training Loss: 0.0005807252018712461 \t\n",
      "Epoch 48072 \t\t Training Loss: 0.0005807250854559243 \t\n",
      "Epoch 48073 \t\t Training Loss: 0.0005807250854559243 \t\n",
      "Epoch 48074 \t\t Training Loss: 0.0005807250854559243 \t\n",
      "Epoch 48075 \t\t Training Loss: 0.0005807250854559243 \t\n",
      "Epoch 48076 \t\t Training Loss: 0.0005807250854559243 \t\n",
      "Epoch 48077 \t\t Training Loss: 0.0005807250854559243 \t\n",
      "Epoch 48078 \t\t Training Loss: 0.0005807250854559243 \t\n",
      "Epoch 48079 \t\t Training Loss: 0.0005807250854559243 \t\n",
      "Epoch 48080 \t\t Training Loss: 0.0005807250854559243 \t\n",
      "Epoch 48081 \t\t Training Loss: 0.0005807250854559243 \t\n",
      "Epoch 48082 \t\t Training Loss: 0.0005807250854559243 \t\n",
      "Epoch 48083 \t\t Training Loss: 0.0005807250854559243 \t\n",
      "Epoch 48084 \t\t Training Loss: 0.0005807250854559243 \t\n",
      "Epoch 48085 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48086 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48087 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48088 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48089 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48090 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48091 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48092 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48093 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48094 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48095 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48096 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48097 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48098 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48099 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48100 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48101 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48102 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48103 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48104 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48105 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48106 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48107 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48108 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48109 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48110 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48111 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48112 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48113 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48114 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48115 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48116 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48117 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48118 \t\t Training Loss: 0.0005807249690406024 \t\n",
      "Epoch 48119 \t\t Training Loss: 0.0005807249108329415 \t\n",
      "Epoch 48120 \t\t Training Loss: 0.0005807249108329415 \t\n",
      "Epoch 48121 \t\t Training Loss: 0.0005807249108329415 \t\n",
      "Epoch 48122 \t\t Training Loss: 0.0005807249108329415 \t\n",
      "Epoch 48123 \t\t Training Loss: 0.0005807249108329415 \t\n",
      "Epoch 48124 \t\t Training Loss: 0.0005807249108329415 \t\n",
      "Epoch 48125 \t\t Training Loss: 0.0005807249108329415 \t\n",
      "Epoch 48126 \t\t Training Loss: 0.0005807249108329415 \t\n",
      "Epoch 48127 \t\t Training Loss: 0.0005807249108329415 \t\n",
      "Epoch 48128 \t\t Training Loss: 0.0005807249108329415 \t\n",
      "Epoch 48129 \t\t Training Loss: 0.0005807247944176197 \t\n",
      "Epoch 48130 \t\t Training Loss: 0.0005807247944176197 \t\n",
      "Epoch 48131 \t\t Training Loss: 0.0005807247944176197 \t\n",
      "Epoch 48132 \t\t Training Loss: 0.0005807247944176197 \t\n",
      "Epoch 48133 \t\t Training Loss: 0.0005807247944176197 \t\n",
      "Epoch 48134 \t\t Training Loss: 0.0005807247944176197 \t\n",
      "Epoch 48135 \t\t Training Loss: 0.0005807247944176197 \t\n",
      "Epoch 48136 \t\t Training Loss: 0.0005807247944176197 \t\n",
      "Epoch 48137 \t\t Training Loss: 0.0005807247944176197 \t\n",
      "Epoch 48138 \t\t Training Loss: 0.0005807247944176197 \t\n",
      "Epoch 48139 \t\t Training Loss: 0.0005807247944176197 \t\n",
      "Epoch 48140 \t\t Training Loss: 0.0005807247944176197 \t\n",
      "Epoch 48141 \t\t Training Loss: 0.0005807247944176197 \t\n",
      "Epoch 48142 \t\t Training Loss: 0.0005807247944176197 \t\n",
      "Epoch 48143 \t\t Training Loss: 0.0005807247944176197 \t\n",
      "Epoch 48144 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48145 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48146 \t\t Training Loss: 0.0005807247944176197 \t\n",
      "Epoch 48147 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48148 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48149 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48150 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48151 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48152 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48153 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48154 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48155 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48156 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48157 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48158 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48159 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48160 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48161 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48162 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48163 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48164 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48165 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48166 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48167 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48168 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48169 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48170 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48171 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48172 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48173 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48174 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48175 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48176 \t\t Training Loss: 0.0005807246780022979 \t\n",
      "Epoch 48177 \t\t Training Loss: 0.000580724619794637 \t\n",
      "Epoch 48178 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48179 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48180 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48181 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48182 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48183 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48184 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48185 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48186 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48187 \t\t Training Loss: 0.000580724561586976 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48188 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48189 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48190 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48191 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48192 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48193 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48194 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48195 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48196 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48197 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48198 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48199 \t\t Training Loss: 0.000580724561586976 \t\n",
      "Epoch 48200 \t\t Training Loss: 0.0005807245033793151 \t\n",
      "Epoch 48201 \t\t Training Loss: 0.0005807245033793151 \t\n",
      "Epoch 48202 \t\t Training Loss: 0.0005807245033793151 \t\n",
      "Epoch 48203 \t\t Training Loss: 0.0005807245033793151 \t\n",
      "Epoch 48204 \t\t Training Loss: 0.0005807245033793151 \t\n",
      "Epoch 48205 \t\t Training Loss: 0.0005807245033793151 \t\n",
      "Epoch 48206 \t\t Training Loss: 0.0005807245033793151 \t\n",
      "Epoch 48207 \t\t Training Loss: 0.0005807245033793151 \t\n",
      "Epoch 48208 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48209 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48210 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48211 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48212 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48213 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48214 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48215 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48216 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48217 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48218 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48219 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48220 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48221 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48222 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48223 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48224 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48225 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48226 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48227 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48228 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48229 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48230 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48231 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48232 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48233 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48234 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48235 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48236 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48237 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48238 \t\t Training Loss: 0.0005807243287563324 \t\n",
      "Epoch 48239 \t\t Training Loss: 0.0005807243287563324 \t\n",
      "Epoch 48240 \t\t Training Loss: 0.0005807243287563324 \t\n",
      "Epoch 48241 \t\t Training Loss: 0.0005807243869639933 \t\n",
      "Epoch 48242 \t\t Training Loss: 0.0005807243287563324 \t\n",
      "Epoch 48243 \t\t Training Loss: 0.0005807243287563324 \t\n",
      "Epoch 48244 \t\t Training Loss: 0.0005807243287563324 \t\n",
      "Epoch 48245 \t\t Training Loss: 0.0005807242705486715 \t\n",
      "Epoch 48246 \t\t Training Loss: 0.0005807242705486715 \t\n",
      "Epoch 48247 \t\t Training Loss: 0.0005807242705486715 \t\n",
      "Epoch 48248 \t\t Training Loss: 0.0005807242705486715 \t\n",
      "Epoch 48249 \t\t Training Loss: 0.0005807242705486715 \t\n",
      "Epoch 48250 \t\t Training Loss: 0.0005807242705486715 \t\n",
      "Epoch 48251 \t\t Training Loss: 0.0005807242705486715 \t\n",
      "Epoch 48252 \t\t Training Loss: 0.0005807242705486715 \t\n",
      "Epoch 48253 \t\t Training Loss: 0.0005807242705486715 \t\n",
      "Epoch 48254 \t\t Training Loss: 0.0005807242123410106 \t\n",
      "Epoch 48255 \t\t Training Loss: 0.0005807242705486715 \t\n",
      "Epoch 48256 \t\t Training Loss: 0.0005807242123410106 \t\n",
      "Epoch 48257 \t\t Training Loss: 0.0005807242123410106 \t\n",
      "Epoch 48258 \t\t Training Loss: 0.0005807242123410106 \t\n",
      "Epoch 48259 \t\t Training Loss: 0.0005807242123410106 \t\n",
      "Epoch 48260 \t\t Training Loss: 0.0005807242123410106 \t\n",
      "Epoch 48261 \t\t Training Loss: 0.0005807242123410106 \t\n",
      "Epoch 48262 \t\t Training Loss: 0.0005807242705486715 \t\n",
      "Epoch 48263 \t\t Training Loss: 0.0005807242705486715 \t\n",
      "Epoch 48264 \t\t Training Loss: 0.0005807242705486715 \t\n",
      "Epoch 48265 \t\t Training Loss: 0.0005807242123410106 \t\n",
      "Epoch 48266 \t\t Training Loss: 0.0005807242123410106 \t\n",
      "Epoch 48267 \t\t Training Loss: 0.0005807242123410106 \t\n",
      "Epoch 48268 \t\t Training Loss: 0.0005807242123410106 \t\n",
      "Epoch 48269 \t\t Training Loss: 0.0005807242123410106 \t\n",
      "Epoch 48270 \t\t Training Loss: 0.0005807242123410106 \t\n",
      "Epoch 48271 \t\t Training Loss: 0.0005807241541333497 \t\n",
      "Epoch 48272 \t\t Training Loss: 0.0005807241541333497 \t\n",
      "Epoch 48273 \t\t Training Loss: 0.0005807241541333497 \t\n",
      "Epoch 48274 \t\t Training Loss: 0.0005807241541333497 \t\n",
      "Epoch 48275 \t\t Training Loss: 0.0005807241541333497 \t\n",
      "Epoch 48276 \t\t Training Loss: 0.0005807240959256887 \t\n",
      "Epoch 48277 \t\t Training Loss: 0.0005807240959256887 \t\n",
      "Epoch 48278 \t\t Training Loss: 0.0005807240959256887 \t\n",
      "Epoch 48279 \t\t Training Loss: 0.0005807240377180278 \t\n",
      "Epoch 48280 \t\t Training Loss: 0.0005807240377180278 \t\n",
      "Epoch 48281 \t\t Training Loss: 0.0005807240377180278 \t\n",
      "Epoch 48282 \t\t Training Loss: 0.0005807240959256887 \t\n",
      "Epoch 48283 \t\t Training Loss: 0.0005807240959256887 \t\n",
      "Epoch 48284 \t\t Training Loss: 0.0005807240959256887 \t\n",
      "Epoch 48285 \t\t Training Loss: 0.0005807239795103669 \t\n",
      "Epoch 48286 \t\t Training Loss: 0.0005807239795103669 \t\n",
      "Epoch 48287 \t\t Training Loss: 0.0005807240959256887 \t\n",
      "Epoch 48288 \t\t Training Loss: 0.0005807240959256887 \t\n",
      "Epoch 48289 \t\t Training Loss: 0.0005807240959256887 \t\n",
      "Epoch 48290 \t\t Training Loss: 0.0005807240959256887 \t\n",
      "Epoch 48291 \t\t Training Loss: 0.0005807240959256887 \t\n",
      "Epoch 48292 \t\t Training Loss: 0.0005807240959256887 \t\n",
      "Epoch 48293 \t\t Training Loss: 0.0005807239795103669 \t\n",
      "Epoch 48294 \t\t Training Loss: 0.0005807239795103669 \t\n",
      "Epoch 48295 \t\t Training Loss: 0.0005807239795103669 \t\n",
      "Epoch 48296 \t\t Training Loss: 0.0005807239795103669 \t\n",
      "Epoch 48297 \t\t Training Loss: 0.0005807239795103669 \t\n",
      "Epoch 48298 \t\t Training Loss: 0.0005807239795103669 \t\n",
      "Epoch 48299 \t\t Training Loss: 0.0005807239795103669 \t\n",
      "Epoch 48300 \t\t Training Loss: 0.0005807239795103669 \t\n",
      "Epoch 48301 \t\t Training Loss: 0.0005807239795103669 \t\n",
      "Epoch 48302 \t\t Training Loss: 0.0005807239795103669 \t\n",
      "Epoch 48303 \t\t Training Loss: 0.000580723921302706 \t\n",
      "Epoch 48304 \t\t Training Loss: 0.000580723921302706 \t\n",
      "Epoch 48305 \t\t Training Loss: 0.000580723921302706 \t\n",
      "Epoch 48306 \t\t Training Loss: 0.0005807238630950451 \t\n",
      "Epoch 48307 \t\t Training Loss: 0.0005807238630950451 \t\n",
      "Epoch 48308 \t\t Training Loss: 0.0005807238048873842 \t\n",
      "Epoch 48309 \t\t Training Loss: 0.0005807238630950451 \t\n",
      "Epoch 48310 \t\t Training Loss: 0.0005807238630950451 \t\n",
      "Epoch 48311 \t\t Training Loss: 0.0005807238630950451 \t\n",
      "Epoch 48312 \t\t Training Loss: 0.0005807238048873842 \t\n",
      "Epoch 48313 \t\t Training Loss: 0.0005807238048873842 \t\n",
      "Epoch 48314 \t\t Training Loss: 0.0005807238048873842 \t\n",
      "Epoch 48315 \t\t Training Loss: 0.0005807238630950451 \t\n",
      "Epoch 48316 \t\t Training Loss: 0.0005807238048873842 \t\n",
      "Epoch 48317 \t\t Training Loss: 0.0005807238048873842 \t\n",
      "Epoch 48318 \t\t Training Loss: 0.0005807238048873842 \t\n",
      "Epoch 48319 \t\t Training Loss: 0.0005807238048873842 \t\n",
      "Epoch 48320 \t\t Training Loss: 0.0005807238048873842 \t\n",
      "Epoch 48321 \t\t Training Loss: 0.0005807238048873842 \t\n",
      "Epoch 48322 \t\t Training Loss: 0.0005807238048873842 \t\n",
      "Epoch 48323 \t\t Training Loss: 0.0005807238048873842 \t\n",
      "Epoch 48324 \t\t Training Loss: 0.0005807238048873842 \t\n",
      "Epoch 48325 \t\t Training Loss: 0.0005807237466797233 \t\n",
      "Epoch 48326 \t\t Training Loss: 0.0005807237466797233 \t\n",
      "Epoch 48327 \t\t Training Loss: 0.0005807237466797233 \t\n",
      "Epoch 48328 \t\t Training Loss: 0.0005807237466797233 \t\n",
      "Epoch 48329 \t\t Training Loss: 0.0005807237466797233 \t\n",
      "Epoch 48330 \t\t Training Loss: 0.0005807237466797233 \t\n",
      "Epoch 48331 \t\t Training Loss: 0.0005807237466797233 \t\n",
      "Epoch 48332 \t\t Training Loss: 0.0005807237466797233 \t\n",
      "Epoch 48333 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48334 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48335 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48336 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48337 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48338 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48339 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48340 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48341 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48342 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48343 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48344 \t\t Training Loss: 0.0005807236884720623 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48345 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48346 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48347 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48348 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48349 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48350 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48351 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48352 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48353 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48354 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48355 \t\t Training Loss: 0.0005807236302644014 \t\n",
      "Epoch 48356 \t\t Training Loss: 0.0005807236302644014 \t\n",
      "Epoch 48357 \t\t Training Loss: 0.0005807236302644014 \t\n",
      "Epoch 48358 \t\t Training Loss: 0.0005807236302644014 \t\n",
      "Epoch 48359 \t\t Training Loss: 0.0005807236302644014 \t\n",
      "Epoch 48360 \t\t Training Loss: 0.0005807236302644014 \t\n",
      "Epoch 48361 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48362 \t\t Training Loss: 0.0005807236884720623 \t\n",
      "Epoch 48363 \t\t Training Loss: 0.0005807236302644014 \t\n",
      "Epoch 48364 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48365 \t\t Training Loss: 0.0005807236302644014 \t\n",
      "Epoch 48366 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48367 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48368 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48369 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48370 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48371 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48372 \t\t Training Loss: 0.0005807234556414187 \t\n",
      "Epoch 48373 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48374 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48375 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48376 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48377 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48378 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48379 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48380 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48381 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48382 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48383 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48384 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48385 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48386 \t\t Training Loss: 0.0005807234556414187 \t\n",
      "Epoch 48387 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48388 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48389 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48390 \t\t Training Loss: 0.0005807234556414187 \t\n",
      "Epoch 48391 \t\t Training Loss: 0.0005807234556414187 \t\n",
      "Epoch 48392 \t\t Training Loss: 0.0005807234556414187 \t\n",
      "Epoch 48393 \t\t Training Loss: 0.0005807233974337578 \t\n",
      "Epoch 48394 \t\t Training Loss: 0.0005807234556414187 \t\n",
      "Epoch 48395 \t\t Training Loss: 0.0005807234556414187 \t\n",
      "Epoch 48396 \t\t Training Loss: 0.0005807234556414187 \t\n",
      "Epoch 48397 \t\t Training Loss: 0.0005807234556414187 \t\n",
      "Epoch 48398 \t\t Training Loss: 0.0005807234556414187 \t\n",
      "Epoch 48399 \t\t Training Loss: 0.0005807234556414187 \t\n",
      "Epoch 48400 \t\t Training Loss: 0.0005807234556414187 \t\n",
      "Epoch 48401 \t\t Training Loss: 0.0005807235138490796 \t\n",
      "Epoch 48402 \t\t Training Loss: 0.0005807234556414187 \t\n",
      "Epoch 48403 \t\t Training Loss: 0.0005807234556414187 \t\n",
      "Epoch 48404 \t\t Training Loss: 0.0005807234556414187 \t\n",
      "Epoch 48405 \t\t Training Loss: 0.0005807233974337578 \t\n",
      "Epoch 48406 \t\t Training Loss: 0.0005807234556414187 \t\n",
      "Epoch 48407 \t\t Training Loss: 0.0005807233974337578 \t\n",
      "Epoch 48408 \t\t Training Loss: 0.0005807233974337578 \t\n",
      "Epoch 48409 \t\t Training Loss: 0.0005807233974337578 \t\n",
      "Epoch 48410 \t\t Training Loss: 0.0005807233974337578 \t\n",
      "Epoch 48411 \t\t Training Loss: 0.0005807233974337578 \t\n",
      "Epoch 48412 \t\t Training Loss: 0.0005807233974337578 \t\n",
      "Epoch 48413 \t\t Training Loss: 0.0005807233974337578 \t\n",
      "Epoch 48414 \t\t Training Loss: 0.0005807233974337578 \t\n",
      "Epoch 48415 \t\t Training Loss: 0.0005807233974337578 \t\n",
      "Epoch 48416 \t\t Training Loss: 0.0005807233974337578 \t\n",
      "Epoch 48417 \t\t Training Loss: 0.0005807233392260969 \t\n",
      "Epoch 48418 \t\t Training Loss: 0.0005807233392260969 \t\n",
      "Epoch 48419 \t\t Training Loss: 0.0005807233392260969 \t\n",
      "Epoch 48420 \t\t Training Loss: 0.0005807233392260969 \t\n",
      "Epoch 48421 \t\t Training Loss: 0.0005807233392260969 \t\n",
      "Epoch 48422 \t\t Training Loss: 0.0005807233392260969 \t\n",
      "Epoch 48423 \t\t Training Loss: 0.0005807233392260969 \t\n",
      "Epoch 48424 \t\t Training Loss: 0.0005807233392260969 \t\n",
      "Epoch 48425 \t\t Training Loss: 0.0005807233392260969 \t\n",
      "Epoch 48426 \t\t Training Loss: 0.0005807233392260969 \t\n",
      "Epoch 48427 \t\t Training Loss: 0.0005807233392260969 \t\n",
      "Epoch 48428 \t\t Training Loss: 0.000580723281018436 \t\n",
      "Epoch 48429 \t\t Training Loss: 0.000580723281018436 \t\n",
      "Epoch 48430 \t\t Training Loss: 0.000580723222810775 \t\n",
      "Epoch 48431 \t\t Training Loss: 0.000580723222810775 \t\n",
      "Epoch 48432 \t\t Training Loss: 0.000580723222810775 \t\n",
      "Epoch 48433 \t\t Training Loss: 0.000580723222810775 \t\n",
      "Epoch 48434 \t\t Training Loss: 0.000580723222810775 \t\n",
      "Epoch 48435 \t\t Training Loss: 0.000580723281018436 \t\n",
      "Epoch 48436 \t\t Training Loss: 0.000580723281018436 \t\n",
      "Epoch 48437 \t\t Training Loss: 0.000580723281018436 \t\n",
      "Epoch 48438 \t\t Training Loss: 0.000580723281018436 \t\n",
      "Epoch 48439 \t\t Training Loss: 0.000580723281018436 \t\n",
      "Epoch 48440 \t\t Training Loss: 0.000580723222810775 \t\n",
      "Epoch 48441 \t\t Training Loss: 0.000580723222810775 \t\n",
      "Epoch 48442 \t\t Training Loss: 0.000580723281018436 \t\n",
      "Epoch 48443 \t\t Training Loss: 0.000580723281018436 \t\n",
      "Epoch 48444 \t\t Training Loss: 0.000580723281018436 \t\n",
      "Epoch 48445 \t\t Training Loss: 0.000580723281018436 \t\n",
      "Epoch 48446 \t\t Training Loss: 0.000580723281018436 \t\n",
      "Epoch 48447 \t\t Training Loss: 0.000580723281018436 \t\n",
      "Epoch 48448 \t\t Training Loss: 0.000580723281018436 \t\n",
      "Epoch 48449 \t\t Training Loss: 0.000580723281018436 \t\n",
      "Epoch 48450 \t\t Training Loss: 0.000580723281018436 \t\n",
      "Epoch 48451 \t\t Training Loss: 0.000580723222810775 \t\n",
      "Epoch 48452 \t\t Training Loss: 0.000580723222810775 \t\n",
      "Epoch 48453 \t\t Training Loss: 0.000580723222810775 \t\n",
      "Epoch 48454 \t\t Training Loss: 0.000580723222810775 \t\n",
      "Epoch 48455 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48456 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48457 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48458 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48459 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48460 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48461 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48462 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48463 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48464 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48465 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48466 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48467 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48468 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48469 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48470 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48471 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48472 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48473 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48474 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48475 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48476 \t\t Training Loss: 0.000580723222810775 \t\n",
      "Epoch 48477 \t\t Training Loss: 0.000580723222810775 \t\n",
      "Epoch 48478 \t\t Training Loss: 0.000580723222810775 \t\n",
      "Epoch 48479 \t\t Training Loss: 0.000580723222810775 \t\n",
      "Epoch 48480 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48481 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48482 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48483 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48484 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48485 \t\t Training Loss: 0.0005807231063954532 \t\n",
      "Epoch 48486 \t\t Training Loss: 0.0005807231063954532 \t\n",
      "Epoch 48487 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48488 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48489 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48490 \t\t Training Loss: 0.0005807231063954532 \t\n",
      "Epoch 48491 \t\t Training Loss: 0.0005807231063954532 \t\n",
      "Epoch 48492 \t\t Training Loss: 0.0005807231063954532 \t\n",
      "Epoch 48493 \t\t Training Loss: 0.0005807231063954532 \t\n",
      "Epoch 48494 \t\t Training Loss: 0.0005807231063954532 \t\n",
      "Epoch 48495 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48496 \t\t Training Loss: 0.0005807231646031141 \t\n",
      "Epoch 48497 \t\t Training Loss: 0.0005807231063954532 \t\n",
      "Epoch 48498 \t\t Training Loss: 0.0005807231063954532 \t\n",
      "Epoch 48499 \t\t Training Loss: 0.0005807231063954532 \t\n",
      "Epoch 48500 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48501 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48502 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48503 \t\t Training Loss: 0.0005807229899801314 \t\n",
      "Epoch 48504 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48505 \t\t Training Loss: 0.0005807230481877923 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48506 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48507 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48508 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48509 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48510 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48511 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48512 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48513 \t\t Training Loss: 0.0005807229899801314 \t\n",
      "Epoch 48514 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48515 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48516 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48517 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48518 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48519 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48520 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48521 \t\t Training Loss: 0.0005807230481877923 \t\n",
      "Epoch 48522 \t\t Training Loss: 0.0005807229317724705 \t\n",
      "Epoch 48523 \t\t Training Loss: 0.0005807229317724705 \t\n",
      "Epoch 48524 \t\t Training Loss: 0.0005807229317724705 \t\n",
      "Epoch 48525 \t\t Training Loss: 0.0005807229317724705 \t\n",
      "Epoch 48526 \t\t Training Loss: 0.0005807229317724705 \t\n",
      "Epoch 48527 \t\t Training Loss: 0.0005807229317724705 \t\n",
      "Epoch 48528 \t\t Training Loss: 0.0005807229317724705 \t\n",
      "Epoch 48529 \t\t Training Loss: 0.0005807229317724705 \t\n",
      "Epoch 48530 \t\t Training Loss: 0.0005807229317724705 \t\n",
      "Epoch 48531 \t\t Training Loss: 0.0005807229317724705 \t\n",
      "Epoch 48532 \t\t Training Loss: 0.0005807228735648096 \t\n",
      "Epoch 48533 \t\t Training Loss: 0.0005807228735648096 \t\n",
      "Epoch 48534 \t\t Training Loss: 0.0005807229317724705 \t\n",
      "Epoch 48535 \t\t Training Loss: 0.0005807229317724705 \t\n",
      "Epoch 48536 \t\t Training Loss: 0.0005807229317724705 \t\n",
      "Epoch 48537 \t\t Training Loss: 0.0005807228735648096 \t\n",
      "Epoch 48538 \t\t Training Loss: 0.0005807228735648096 \t\n",
      "Epoch 48539 \t\t Training Loss: 0.0005807228735648096 \t\n",
      "Epoch 48540 \t\t Training Loss: 0.0005807228735648096 \t\n",
      "Epoch 48541 \t\t Training Loss: 0.0005807228735648096 \t\n",
      "Epoch 48542 \t\t Training Loss: 0.0005807228735648096 \t\n",
      "Epoch 48543 \t\t Training Loss: 0.0005807228735648096 \t\n",
      "Epoch 48544 \t\t Training Loss: 0.0005807228735648096 \t\n",
      "Epoch 48545 \t\t Training Loss: 0.0005807228735648096 \t\n",
      "Epoch 48546 \t\t Training Loss: 0.0005807228735648096 \t\n",
      "Epoch 48547 \t\t Training Loss: 0.0005807228735648096 \t\n",
      "Epoch 48548 \t\t Training Loss: 0.0005807228153571486 \t\n",
      "Epoch 48549 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48550 \t\t Training Loss: 0.0005807228153571486 \t\n",
      "Epoch 48551 \t\t Training Loss: 0.0005807228153571486 \t\n",
      "Epoch 48552 \t\t Training Loss: 0.0005807228735648096 \t\n",
      "Epoch 48553 \t\t Training Loss: 0.0005807228153571486 \t\n",
      "Epoch 48554 \t\t Training Loss: 0.0005807228153571486 \t\n",
      "Epoch 48555 \t\t Training Loss: 0.0005807228153571486 \t\n",
      "Epoch 48556 \t\t Training Loss: 0.0005807228153571486 \t\n",
      "Epoch 48557 \t\t Training Loss: 0.0005807228153571486 \t\n",
      "Epoch 48558 \t\t Training Loss: 0.0005807228153571486 \t\n",
      "Epoch 48559 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48560 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48561 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48562 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48563 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48564 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48565 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48566 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48567 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48568 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48569 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48570 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48571 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48572 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48573 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48574 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48575 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48576 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48577 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48578 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48579 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48580 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48581 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48582 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48583 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48584 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48585 \t\t Training Loss: 0.0005807227571494877 \t\n",
      "Epoch 48586 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48587 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48588 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48589 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48590 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48591 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48592 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48593 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48594 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48595 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48596 \t\t Training Loss: 0.0005807226407341659 \t\n",
      "Epoch 48597 \t\t Training Loss: 0.0005807226407341659 \t\n",
      "Epoch 48598 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48599 \t\t Training Loss: 0.0005807226407341659 \t\n",
      "Epoch 48600 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48601 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48602 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48603 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48604 \t\t Training Loss: 0.0005807226407341659 \t\n",
      "Epoch 48605 \t\t Training Loss: 0.0005807226407341659 \t\n",
      "Epoch 48606 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48607 \t\t Training Loss: 0.0005807226989418268 \t\n",
      "Epoch 48608 \t\t Training Loss: 0.0005807226407341659 \t\n",
      "Epoch 48609 \t\t Training Loss: 0.0005807226407341659 \t\n",
      "Epoch 48610 \t\t Training Loss: 0.0005807226407341659 \t\n",
      "Epoch 48611 \t\t Training Loss: 0.0005807226407341659 \t\n",
      "Epoch 48612 \t\t Training Loss: 0.000580722582526505 \t\n",
      "Epoch 48613 \t\t Training Loss: 0.000580722582526505 \t\n",
      "Epoch 48614 \t\t Training Loss: 0.0005807225243188441 \t\n",
      "Epoch 48615 \t\t Training Loss: 0.000580722582526505 \t\n",
      "Epoch 48616 \t\t Training Loss: 0.000580722582526505 \t\n",
      "Epoch 48617 \t\t Training Loss: 0.0005807225243188441 \t\n",
      "Epoch 48618 \t\t Training Loss: 0.000580722582526505 \t\n",
      "Epoch 48619 \t\t Training Loss: 0.0005807226407341659 \t\n",
      "Epoch 48620 \t\t Training Loss: 0.000580722582526505 \t\n",
      "Epoch 48621 \t\t Training Loss: 0.000580722582526505 \t\n",
      "Epoch 48622 \t\t Training Loss: 0.0005807225243188441 \t\n",
      "Epoch 48623 \t\t Training Loss: 0.0005807225243188441 \t\n",
      "Epoch 48624 \t\t Training Loss: 0.0005807224661111832 \t\n",
      "Epoch 48625 \t\t Training Loss: 0.0005807225243188441 \t\n",
      "Epoch 48626 \t\t Training Loss: 0.0005807225243188441 \t\n",
      "Epoch 48627 \t\t Training Loss: 0.0005807224661111832 \t\n",
      "Epoch 48628 \t\t Training Loss: 0.0005807224661111832 \t\n",
      "Epoch 48629 \t\t Training Loss: 0.0005807224661111832 \t\n",
      "Epoch 48630 \t\t Training Loss: 0.0005807224661111832 \t\n",
      "Epoch 48631 \t\t Training Loss: 0.0005807224661111832 \t\n",
      "Epoch 48632 \t\t Training Loss: 0.0005807224661111832 \t\n",
      "Epoch 48633 \t\t Training Loss: 0.0005807224661111832 \t\n",
      "Epoch 48634 \t\t Training Loss: 0.0005807224661111832 \t\n",
      "Epoch 48635 \t\t Training Loss: 0.0005807224661111832 \t\n",
      "Epoch 48636 \t\t Training Loss: 0.0005807223496958613 \t\n",
      "Epoch 48637 \t\t Training Loss: 0.0005807223496958613 \t\n",
      "Epoch 48638 \t\t Training Loss: 0.0005807223496958613 \t\n",
      "Epoch 48639 \t\t Training Loss: 0.0005807223496958613 \t\n",
      "Epoch 48640 \t\t Training Loss: 0.0005807223496958613 \t\n",
      "Epoch 48641 \t\t Training Loss: 0.0005807223496958613 \t\n",
      "Epoch 48642 \t\t Training Loss: 0.0005807222914882004 \t\n",
      "Epoch 48643 \t\t Training Loss: 0.0005807222914882004 \t\n",
      "Epoch 48644 \t\t Training Loss: 0.0005807223496958613 \t\n",
      "Epoch 48645 \t\t Training Loss: 0.0005807223496958613 \t\n",
      "Epoch 48646 \t\t Training Loss: 0.0005807222914882004 \t\n",
      "Epoch 48647 \t\t Training Loss: 0.0005807223496958613 \t\n",
      "Epoch 48648 \t\t Training Loss: 0.0005807222914882004 \t\n",
      "Epoch 48649 \t\t Training Loss: 0.0005807222914882004 \t\n",
      "Epoch 48650 \t\t Training Loss: 0.0005807222914882004 \t\n",
      "Epoch 48651 \t\t Training Loss: 0.0005807222914882004 \t\n",
      "Epoch 48652 \t\t Training Loss: 0.0005807223496958613 \t\n",
      "Epoch 48653 \t\t Training Loss: 0.0005807223496958613 \t\n",
      "Epoch 48654 \t\t Training Loss: 0.0005807222914882004 \t\n",
      "Epoch 48655 \t\t Training Loss: 0.0005807222914882004 \t\n",
      "Epoch 48656 \t\t Training Loss: 0.0005807222914882004 \t\n",
      "Epoch 48657 \t\t Training Loss: 0.0005807222914882004 \t\n",
      "Epoch 48658 \t\t Training Loss: 0.0005807222914882004 \t\n",
      "Epoch 48659 \t\t Training Loss: 0.0005807222914882004 \t\n",
      "Epoch 48660 \t\t Training Loss: 0.0005807221750728786 \t\n",
      "Epoch 48661 \t\t Training Loss: 0.0005807221750728786 \t\n",
      "Epoch 48662 \t\t Training Loss: 0.0005807221750728786 \t\n",
      "Epoch 48663 \t\t Training Loss: 0.0005807221750728786 \t\n",
      "Epoch 48664 \t\t Training Loss: 0.0005807221750728786 \t\n",
      "Epoch 48665 \t\t Training Loss: 0.0005807221750728786 \t\n",
      "Epoch 48666 \t\t Training Loss: 0.0005807221750728786 \t\n",
      "Epoch 48667 \t\t Training Loss: 0.0005807221750728786 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48668 \t\t Training Loss: 0.0005807221750728786 \t\n",
      "Epoch 48669 \t\t Training Loss: 0.0005807221750728786 \t\n",
      "Epoch 48670 \t\t Training Loss: 0.0005807221750728786 \t\n",
      "Epoch 48671 \t\t Training Loss: 0.0005807221750728786 \t\n",
      "Epoch 48672 \t\t Training Loss: 0.0005807221168652177 \t\n",
      "Epoch 48673 \t\t Training Loss: 0.0005807221168652177 \t\n",
      "Epoch 48674 \t\t Training Loss: 0.0005807221168652177 \t\n",
      "Epoch 48675 \t\t Training Loss: 0.0005807221168652177 \t\n",
      "Epoch 48676 \t\t Training Loss: 0.0005807221168652177 \t\n",
      "Epoch 48677 \t\t Training Loss: 0.0005807221168652177 \t\n",
      "Epoch 48678 \t\t Training Loss: 0.0005807221168652177 \t\n",
      "Epoch 48679 \t\t Training Loss: 0.0005807220586575568 \t\n",
      "Epoch 48680 \t\t Training Loss: 0.0005807221168652177 \t\n",
      "Epoch 48681 \t\t Training Loss: 0.0005807221168652177 \t\n",
      "Epoch 48682 \t\t Training Loss: 0.0005807220586575568 \t\n",
      "Epoch 48683 \t\t Training Loss: 0.0005807220586575568 \t\n",
      "Epoch 48684 \t\t Training Loss: 0.0005807220586575568 \t\n",
      "Epoch 48685 \t\t Training Loss: 0.0005807220586575568 \t\n",
      "Epoch 48686 \t\t Training Loss: 0.0005807220586575568 \t\n",
      "Epoch 48687 \t\t Training Loss: 0.0005807221168652177 \t\n",
      "Epoch 48688 \t\t Training Loss: 0.0005807221168652177 \t\n",
      "Epoch 48689 \t\t Training Loss: 0.0005807220586575568 \t\n",
      "Epoch 48690 \t\t Training Loss: 0.0005807220586575568 \t\n",
      "Epoch 48691 \t\t Training Loss: 0.0005807220586575568 \t\n",
      "Epoch 48692 \t\t Training Loss: 0.0005807220004498959 \t\n",
      "Epoch 48693 \t\t Training Loss: 0.0005807220004498959 \t\n",
      "Epoch 48694 \t\t Training Loss: 0.000580721884034574 \t\n",
      "Epoch 48695 \t\t Training Loss: 0.000580721884034574 \t\n",
      "Epoch 48696 \t\t Training Loss: 0.0005807219422422349 \t\n",
      "Epoch 48697 \t\t Training Loss: 0.000580721884034574 \t\n",
      "Epoch 48698 \t\t Training Loss: 0.0005807219422422349 \t\n",
      "Epoch 48699 \t\t Training Loss: 0.000580721884034574 \t\n",
      "Epoch 48700 \t\t Training Loss: 0.000580721884034574 \t\n",
      "Epoch 48701 \t\t Training Loss: 0.000580721884034574 \t\n",
      "Epoch 48702 \t\t Training Loss: 0.000580721884034574 \t\n",
      "Epoch 48703 \t\t Training Loss: 0.000580721884034574 \t\n",
      "Epoch 48704 \t\t Training Loss: 0.000580721884034574 \t\n",
      "Epoch 48705 \t\t Training Loss: 0.000580721884034574 \t\n",
      "Epoch 48706 \t\t Training Loss: 0.000580721884034574 \t\n",
      "Epoch 48707 \t\t Training Loss: 0.0005807218258269131 \t\n",
      "Epoch 48708 \t\t Training Loss: 0.0005807218258269131 \t\n",
      "Epoch 48709 \t\t Training Loss: 0.0005807218258269131 \t\n",
      "Epoch 48710 \t\t Training Loss: 0.0005807218258269131 \t\n",
      "Epoch 48711 \t\t Training Loss: 0.0005807218258269131 \t\n",
      "Epoch 48712 \t\t Training Loss: 0.0005807218258269131 \t\n",
      "Epoch 48713 \t\t Training Loss: 0.0005807218258269131 \t\n",
      "Epoch 48714 \t\t Training Loss: 0.0005807217676192522 \t\n",
      "Epoch 48715 \t\t Training Loss: 0.0005807217676192522 \t\n",
      "Epoch 48716 \t\t Training Loss: 0.0005807218258269131 \t\n",
      "Epoch 48717 \t\t Training Loss: 0.0005807218258269131 \t\n",
      "Epoch 48718 \t\t Training Loss: 0.0005807218258269131 \t\n",
      "Epoch 48719 \t\t Training Loss: 0.0005807218258269131 \t\n",
      "Epoch 48720 \t\t Training Loss: 0.0005807217676192522 \t\n",
      "Epoch 48721 \t\t Training Loss: 0.0005807217676192522 \t\n",
      "Epoch 48722 \t\t Training Loss: 0.0005807217676192522 \t\n",
      "Epoch 48723 \t\t Training Loss: 0.0005807217676192522 \t\n",
      "Epoch 48724 \t\t Training Loss: 0.0005807217676192522 \t\n",
      "Epoch 48725 \t\t Training Loss: 0.0005807217094115913 \t\n",
      "Epoch 48726 \t\t Training Loss: 0.0005807217676192522 \t\n",
      "Epoch 48727 \t\t Training Loss: 0.0005807217676192522 \t\n",
      "Epoch 48728 \t\t Training Loss: 0.0005807217094115913 \t\n",
      "Epoch 48729 \t\t Training Loss: 0.0005807217094115913 \t\n",
      "Epoch 48730 \t\t Training Loss: 0.0005807217094115913 \t\n",
      "Epoch 48731 \t\t Training Loss: 0.0005807217094115913 \t\n",
      "Epoch 48732 \t\t Training Loss: 0.0005807217094115913 \t\n",
      "Epoch 48733 \t\t Training Loss: 0.0005807216512039304 \t\n",
      "Epoch 48734 \t\t Training Loss: 0.0005807216512039304 \t\n",
      "Epoch 48735 \t\t Training Loss: 0.0005807217094115913 \t\n",
      "Epoch 48736 \t\t Training Loss: 0.0005807217094115913 \t\n",
      "Epoch 48737 \t\t Training Loss: 0.0005807217094115913 \t\n",
      "Epoch 48738 \t\t Training Loss: 0.0005807216512039304 \t\n",
      "Epoch 48739 \t\t Training Loss: 0.0005807216512039304 \t\n",
      "Epoch 48740 \t\t Training Loss: 0.0005807216512039304 \t\n",
      "Epoch 48741 \t\t Training Loss: 0.0005807216512039304 \t\n",
      "Epoch 48742 \t\t Training Loss: 0.0005807216512039304 \t\n",
      "Epoch 48743 \t\t Training Loss: 0.0005807215929962695 \t\n",
      "Epoch 48744 \t\t Training Loss: 0.0005807215929962695 \t\n",
      "Epoch 48745 \t\t Training Loss: 0.0005807215929962695 \t\n",
      "Epoch 48746 \t\t Training Loss: 0.0005807215929962695 \t\n",
      "Epoch 48747 \t\t Training Loss: 0.0005807215929962695 \t\n",
      "Epoch 48748 \t\t Training Loss: 0.0005807215929962695 \t\n",
      "Epoch 48749 \t\t Training Loss: 0.0005807215929962695 \t\n",
      "Epoch 48750 \t\t Training Loss: 0.0005807215929962695 \t\n",
      "Epoch 48751 \t\t Training Loss: 0.0005807215929962695 \t\n",
      "Epoch 48752 \t\t Training Loss: 0.0005807216512039304 \t\n",
      "Epoch 48753 \t\t Training Loss: 0.0005807215929962695 \t\n",
      "Epoch 48754 \t\t Training Loss: 0.0005807216512039304 \t\n",
      "Epoch 48755 \t\t Training Loss: 0.0005807215929962695 \t\n",
      "Epoch 48756 \t\t Training Loss: 0.0005807215929962695 \t\n",
      "Epoch 48757 \t\t Training Loss: 0.0005807216512039304 \t\n",
      "Epoch 48758 \t\t Training Loss: 0.0005807216512039304 \t\n",
      "Epoch 48759 \t\t Training Loss: 0.0005807216512039304 \t\n",
      "Epoch 48760 \t\t Training Loss: 0.0005807215929962695 \t\n",
      "Epoch 48761 \t\t Training Loss: 0.0005807215347886086 \t\n",
      "Epoch 48762 \t\t Training Loss: 0.0005807215347886086 \t\n",
      "Epoch 48763 \t\t Training Loss: 0.0005807215347886086 \t\n",
      "Epoch 48764 \t\t Training Loss: 0.0005807215347886086 \t\n",
      "Epoch 48765 \t\t Training Loss: 0.0005807215347886086 \t\n",
      "Epoch 48766 \t\t Training Loss: 0.0005807214765809476 \t\n",
      "Epoch 48767 \t\t Training Loss: 0.0005807214765809476 \t\n",
      "Epoch 48768 \t\t Training Loss: 0.0005807214765809476 \t\n",
      "Epoch 48769 \t\t Training Loss: 0.0005807214765809476 \t\n",
      "Epoch 48770 \t\t Training Loss: 0.0005807214183732867 \t\n",
      "Epoch 48771 \t\t Training Loss: 0.0005807214183732867 \t\n",
      "Epoch 48772 \t\t Training Loss: 0.0005807214183732867 \t\n",
      "Epoch 48773 \t\t Training Loss: 0.0005807213019579649 \t\n",
      "Epoch 48774 \t\t Training Loss: 0.0005807213019579649 \t\n",
      "Epoch 48775 \t\t Training Loss: 0.0005807213019579649 \t\n",
      "Epoch 48776 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48777 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48778 \t\t Training Loss: 0.0005807213019579649 \t\n",
      "Epoch 48779 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48780 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48781 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48782 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48783 \t\t Training Loss: 0.0005807214183732867 \t\n",
      "Epoch 48784 \t\t Training Loss: 0.0005807214183732867 \t\n",
      "Epoch 48785 \t\t Training Loss: 0.0005807214183732867 \t\n",
      "Epoch 48786 \t\t Training Loss: 0.0005807214183732867 \t\n",
      "Epoch 48787 \t\t Training Loss: 0.0005807214183732867 \t\n",
      "Epoch 48788 \t\t Training Loss: 0.0005807214183732867 \t\n",
      "Epoch 48789 \t\t Training Loss: 0.0005807214183732867 \t\n",
      "Epoch 48790 \t\t Training Loss: 0.0005807214183732867 \t\n",
      "Epoch 48791 \t\t Training Loss: 0.0005807214183732867 \t\n",
      "Epoch 48792 \t\t Training Loss: 0.0005807213019579649 \t\n",
      "Epoch 48793 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48794 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48795 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48796 \t\t Training Loss: 0.0005807213019579649 \t\n",
      "Epoch 48797 \t\t Training Loss: 0.0005807213019579649 \t\n",
      "Epoch 48798 \t\t Training Loss: 0.0005807213019579649 \t\n",
      "Epoch 48799 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48800 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48801 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48802 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48803 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48804 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48805 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48806 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48807 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48808 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48809 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48810 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48811 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48812 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48813 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48814 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48815 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48816 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48817 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48818 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48819 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48820 \t\t Training Loss: 0.0005807211273349822 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48821 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48822 \t\t Training Loss: 0.000580721243750304 \t\n",
      "Epoch 48823 \t\t Training Loss: 0.0005807211273349822 \t\n",
      "Epoch 48824 \t\t Training Loss: 0.0005807211273349822 \t\n",
      "Epoch 48825 \t\t Training Loss: 0.0005807211273349822 \t\n",
      "Epoch 48826 \t\t Training Loss: 0.0005807211273349822 \t\n",
      "Epoch 48827 \t\t Training Loss: 0.0005807211273349822 \t\n",
      "Epoch 48828 \t\t Training Loss: 0.0005807210691273212 \t\n",
      "Epoch 48829 \t\t Training Loss: 0.0005807210691273212 \t\n",
      "Epoch 48830 \t\t Training Loss: 0.0005807210691273212 \t\n",
      "Epoch 48831 \t\t Training Loss: 0.0005807210691273212 \t\n",
      "Epoch 48832 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48833 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48834 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48835 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48836 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48837 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48838 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48839 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48840 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48841 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48842 \t\t Training Loss: 0.0005807210691273212 \t\n",
      "Epoch 48843 \t\t Training Loss: 0.0005807210691273212 \t\n",
      "Epoch 48844 \t\t Training Loss: 0.0005807210691273212 \t\n",
      "Epoch 48845 \t\t Training Loss: 0.0005807210691273212 \t\n",
      "Epoch 48846 \t\t Training Loss: 0.0005807210691273212 \t\n",
      "Epoch 48847 \t\t Training Loss: 0.0005807210691273212 \t\n",
      "Epoch 48848 \t\t Training Loss: 0.0005807210691273212 \t\n",
      "Epoch 48849 \t\t Training Loss: 0.0005807210691273212 \t\n",
      "Epoch 48850 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48851 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48852 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48853 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48854 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48855 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48856 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48857 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48858 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48859 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48860 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48861 \t\t Training Loss: 0.0005807210109196603 \t\n",
      "Epoch 48862 \t\t Training Loss: 0.0005807209527119994 \t\n",
      "Epoch 48863 \t\t Training Loss: 0.0005807209527119994 \t\n",
      "Epoch 48864 \t\t Training Loss: 0.0005807209527119994 \t\n",
      "Epoch 48865 \t\t Training Loss: 0.0005807209527119994 \t\n",
      "Epoch 48866 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48867 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48868 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48869 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48870 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48871 \t\t Training Loss: 0.0005807209527119994 \t\n",
      "Epoch 48872 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48873 \t\t Training Loss: 0.0005807209527119994 \t\n",
      "Epoch 48874 \t\t Training Loss: 0.0005807209527119994 \t\n",
      "Epoch 48875 \t\t Training Loss: 0.0005807209527119994 \t\n",
      "Epoch 48876 \t\t Training Loss: 0.0005807209527119994 \t\n",
      "Epoch 48877 \t\t Training Loss: 0.0005807209527119994 \t\n",
      "Epoch 48878 \t\t Training Loss: 0.0005807209527119994 \t\n",
      "Epoch 48879 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48880 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48881 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48882 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48883 \t\t Training Loss: 0.0005807209527119994 \t\n",
      "Epoch 48884 \t\t Training Loss: 0.0005807209527119994 \t\n",
      "Epoch 48885 \t\t Training Loss: 0.0005807209527119994 \t\n",
      "Epoch 48886 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48887 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48888 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48889 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48890 \t\t Training Loss: 0.0005807209527119994 \t\n",
      "Epoch 48891 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48892 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48893 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48894 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48895 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48896 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48897 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48898 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48899 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48900 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48901 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48902 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48903 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48904 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48905 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48906 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48907 \t\t Training Loss: 0.0005807208362966776 \t\n",
      "Epoch 48908 \t\t Training Loss: 0.0005807207198813558 \t\n",
      "Epoch 48909 \t\t Training Loss: 0.0005807207198813558 \t\n",
      "Epoch 48910 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48911 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48912 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48913 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48914 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48915 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48916 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48917 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48918 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48919 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48920 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48921 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48922 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48923 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48924 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48925 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48926 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48927 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48928 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48929 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48930 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48931 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48932 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48933 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48934 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48935 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48936 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48937 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48938 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48939 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48940 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48941 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48942 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48943 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48944 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48945 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48946 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48947 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48948 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48949 \t\t Training Loss: 0.0005807206616736948 \t\n",
      "Epoch 48950 \t\t Training Loss: 0.000580720545258373 \t\n",
      "Epoch 48951 \t\t Training Loss: 0.000580720545258373 \t\n",
      "Epoch 48952 \t\t Training Loss: 0.000580720545258373 \t\n",
      "Epoch 48953 \t\t Training Loss: 0.000580720545258373 \t\n",
      "Epoch 48954 \t\t Training Loss: 0.000580720545258373 \t\n",
      "Epoch 48955 \t\t Training Loss: 0.000580720545258373 \t\n",
      "Epoch 48956 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48957 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48958 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48959 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48960 \t\t Training Loss: 0.000580720545258373 \t\n",
      "Epoch 48961 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48962 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48963 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48964 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48965 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48966 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48967 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48968 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48969 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48970 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48971 \t\t Training Loss: 0.000580720545258373 \t\n",
      "Epoch 48972 \t\t Training Loss: 0.000580720545258373 \t\n",
      "Epoch 48973 \t\t Training Loss: 0.000580720545258373 \t\n",
      "Epoch 48974 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48975 \t\t Training Loss: 0.0005807204288430512 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48976 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 48977 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48978 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48979 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48980 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48981 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48982 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48983 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 48984 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 48985 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 48986 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 48987 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 48988 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 48989 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 48990 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 48991 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48992 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48993 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48994 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48995 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48996 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48997 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 48998 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 48999 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49000 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49001 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 49002 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 49003 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 49004 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 49005 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 49006 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 49007 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 49008 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 49009 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 49010 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 49011 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 49012 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 49013 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 49014 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49015 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49016 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49017 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49018 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49019 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49020 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49021 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 49022 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49023 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49024 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 49025 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49026 \t\t Training Loss: 0.0005807204288430512 \t\n",
      "Epoch 49027 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49028 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49029 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49030 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49031 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49032 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49033 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49034 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49035 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49036 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49037 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49038 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49039 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49040 \t\t Training Loss: 0.0005807203706353903 \t\n",
      "Epoch 49041 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49042 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49043 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49044 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49045 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49046 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49047 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49048 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49049 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49050 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49051 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49052 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49053 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49054 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49055 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49056 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49057 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49058 \t\t Training Loss: 0.0005807202542200685 \t\n",
      "Epoch 49059 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49060 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49061 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49062 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49063 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49064 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49065 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49066 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49067 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49068 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49069 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49070 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49071 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49072 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49073 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49074 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49075 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49076 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49077 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49078 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49079 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49080 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49081 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49082 \t\t Training Loss: 0.0005807201378047466 \t\n",
      "Epoch 49083 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49084 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49085 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49086 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49087 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49088 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49089 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49090 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49091 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49092 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49093 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49094 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49095 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49096 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49097 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49098 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49099 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49100 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49101 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49102 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49103 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49104 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49105 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49106 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49107 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49108 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49109 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49110 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49111 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49112 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49113 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49114 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49115 \t\t Training Loss: 0.0005807200213894248 \t\n",
      "Epoch 49116 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49117 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49118 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49119 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49120 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49121 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49122 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49123 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49124 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49125 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49126 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49127 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49128 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49129 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49130 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49131 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49132 \t\t Training Loss: 0.0005807199631817639 \t\n",
      "Epoch 49133 \t\t Training Loss: 0.0005807198467664421 \t\n",
      "Epoch 49134 \t\t Training Loss: 0.0005807198467664421 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49135 \t\t Training Loss: 0.0005807198467664421 \t\n",
      "Epoch 49136 \t\t Training Loss: 0.0005807198467664421 \t\n",
      "Epoch 49137 \t\t Training Loss: 0.0005807198467664421 \t\n",
      "Epoch 49138 \t\t Training Loss: 0.0005807198467664421 \t\n",
      "Epoch 49139 \t\t Training Loss: 0.0005807198467664421 \t\n",
      "Epoch 49140 \t\t Training Loss: 0.0005807198467664421 \t\n",
      "Epoch 49141 \t\t Training Loss: 0.0005807198467664421 \t\n",
      "Epoch 49142 \t\t Training Loss: 0.0005807198467664421 \t\n",
      "Epoch 49143 \t\t Training Loss: 0.0005807198467664421 \t\n",
      "Epoch 49144 \t\t Training Loss: 0.0005807198467664421 \t\n",
      "Epoch 49145 \t\t Training Loss: 0.0005807198467664421 \t\n",
      "Epoch 49146 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49147 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49148 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49149 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49150 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49151 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49152 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49153 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49154 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49155 \t\t Training Loss: 0.0005807198467664421 \t\n",
      "Epoch 49156 \t\t Training Loss: 0.0005807198467664421 \t\n",
      "Epoch 49157 \t\t Training Loss: 0.0005807198467664421 \t\n",
      "Epoch 49158 \t\t Training Loss: 0.0005807198467664421 \t\n",
      "Epoch 49159 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49160 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49161 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49162 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49163 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49164 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49165 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49166 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49167 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49168 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49169 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49170 \t\t Training Loss: 0.0005807197303511202 \t\n",
      "Epoch 49171 \t\t Training Loss: 0.0005807195557281375 \t\n",
      "Epoch 49172 \t\t Training Loss: 0.0005807195557281375 \t\n",
      "Epoch 49173 \t\t Training Loss: 0.0005807195557281375 \t\n",
      "Epoch 49174 \t\t Training Loss: 0.0005807195557281375 \t\n",
      "Epoch 49175 \t\t Training Loss: 0.0005807195557281375 \t\n",
      "Epoch 49176 \t\t Training Loss: 0.0005807195557281375 \t\n",
      "Epoch 49177 \t\t Training Loss: 0.0005807195557281375 \t\n",
      "Epoch 49178 \t\t Training Loss: 0.0005807195557281375 \t\n",
      "Epoch 49179 \t\t Training Loss: 0.0005807195557281375 \t\n",
      "Epoch 49180 \t\t Training Loss: 0.0005807195557281375 \t\n",
      "Epoch 49181 \t\t Training Loss: 0.0005807195557281375 \t\n",
      "Epoch 49182 \t\t Training Loss: 0.0005807195557281375 \t\n",
      "Epoch 49183 \t\t Training Loss: 0.0005807194393128157 \t\n",
      "Epoch 49184 \t\t Training Loss: 0.0005807194393128157 \t\n",
      "Epoch 49185 \t\t Training Loss: 0.0005807194393128157 \t\n",
      "Epoch 49186 \t\t Training Loss: 0.0005807194393128157 \t\n",
      "Epoch 49187 \t\t Training Loss: 0.0005807194393128157 \t\n",
      "Epoch 49188 \t\t Training Loss: 0.0005807194393128157 \t\n",
      "Epoch 49189 \t\t Training Loss: 0.0005807194393128157 \t\n",
      "Epoch 49190 \t\t Training Loss: 0.0005807194393128157 \t\n",
      "Epoch 49191 \t\t Training Loss: 0.0005807194393128157 \t\n",
      "Epoch 49192 \t\t Training Loss: 0.0005807194393128157 \t\n",
      "Epoch 49193 \t\t Training Loss: 0.0005807194393128157 \t\n",
      "Epoch 49194 \t\t Training Loss: 0.0005807194393128157 \t\n",
      "Epoch 49195 \t\t Training Loss: 0.0005807194393128157 \t\n",
      "Epoch 49196 \t\t Training Loss: 0.0005807194393128157 \t\n",
      "Epoch 49197 \t\t Training Loss: 0.0005807194393128157 \t\n",
      "Epoch 49198 \t\t Training Loss: 0.0005807194393128157 \t\n",
      "Epoch 49199 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49200 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49201 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49202 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49203 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49204 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49205 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49206 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49207 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49208 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49209 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49210 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49211 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49212 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49213 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49214 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49215 \t\t Training Loss: 0.0005807193811051548 \t\n",
      "Epoch 49216 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49217 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49218 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49219 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49220 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49221 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49222 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49223 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49224 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49225 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49226 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49227 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49228 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49229 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49230 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49231 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49232 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49233 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49234 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49235 \t\t Training Loss: 0.0005807192646898329 \t\n",
      "Epoch 49236 \t\t Training Loss: 0.0005807191482745111 \t\n",
      "Epoch 49237 \t\t Training Loss: 0.0005807191482745111 \t\n",
      "Epoch 49238 \t\t Training Loss: 0.0005807191482745111 \t\n",
      "Epoch 49239 \t\t Training Loss: 0.0005807191482745111 \t\n",
      "Epoch 49240 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49241 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49242 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49243 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49244 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49245 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49246 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49247 \t\t Training Loss: 0.0005807190318591893 \t\n",
      "Epoch 49248 \t\t Training Loss: 0.0005807190318591893 \t\n",
      "Epoch 49249 \t\t Training Loss: 0.0005807190318591893 \t\n",
      "Epoch 49250 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49251 \t\t Training Loss: 0.0005807190318591893 \t\n",
      "Epoch 49252 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49253 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49254 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49255 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49256 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49257 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49258 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49259 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49260 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49261 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49262 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49263 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49264 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49265 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49266 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49267 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49268 \t\t Training Loss: 0.0005807190900668502 \t\n",
      "Epoch 49269 \t\t Training Loss: 0.0005807189736515284 \t\n",
      "Epoch 49270 \t\t Training Loss: 0.0005807189736515284 \t\n",
      "Epoch 49271 \t\t Training Loss: 0.0005807189736515284 \t\n",
      "Epoch 49272 \t\t Training Loss: 0.0005807189736515284 \t\n",
      "Epoch 49273 \t\t Training Loss: 0.0005807189736515284 \t\n",
      "Epoch 49274 \t\t Training Loss: 0.0005807190318591893 \t\n",
      "Epoch 49275 \t\t Training Loss: 0.0005807190318591893 \t\n",
      "Epoch 49276 \t\t Training Loss: 0.0005807189736515284 \t\n",
      "Epoch 49277 \t\t Training Loss: 0.0005807189736515284 \t\n",
      "Epoch 49278 \t\t Training Loss: 0.0005807189736515284 \t\n",
      "Epoch 49279 \t\t Training Loss: 0.0005807189736515284 \t\n",
      "Epoch 49280 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49281 \t\t Training Loss: 0.0005807189736515284 \t\n",
      "Epoch 49282 \t\t Training Loss: 0.0005807189736515284 \t\n",
      "Epoch 49283 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49284 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49285 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49286 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49287 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49288 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49289 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49290 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49291 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49292 \t\t Training Loss: 0.0005807188572362065 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49293 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49294 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49295 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49296 \t\t Training Loss: 0.0005807187408208847 \t\n",
      "Epoch 49297 \t\t Training Loss: 0.0005807187408208847 \t\n",
      "Epoch 49298 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49299 \t\t Training Loss: 0.0005807187408208847 \t\n",
      "Epoch 49300 \t\t Training Loss: 0.0005807187408208847 \t\n",
      "Epoch 49301 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49302 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49303 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49304 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49305 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49306 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49307 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49308 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49309 \t\t Training Loss: 0.0005807188572362065 \t\n",
      "Epoch 49310 \t\t Training Loss: 0.0005807187408208847 \t\n",
      "Epoch 49311 \t\t Training Loss: 0.0005807187408208847 \t\n",
      "Epoch 49312 \t\t Training Loss: 0.0005807186826132238 \t\n",
      "Epoch 49313 \t\t Training Loss: 0.0005807186826132238 \t\n",
      "Epoch 49314 \t\t Training Loss: 0.0005807186826132238 \t\n",
      "Epoch 49315 \t\t Training Loss: 0.0005807186826132238 \t\n",
      "Epoch 49316 \t\t Training Loss: 0.0005807186826132238 \t\n",
      "Epoch 49317 \t\t Training Loss: 0.0005807186826132238 \t\n",
      "Epoch 49318 \t\t Training Loss: 0.0005807186826132238 \t\n",
      "Epoch 49319 \t\t Training Loss: 0.0005807186826132238 \t\n",
      "Epoch 49320 \t\t Training Loss: 0.0005807186244055629 \t\n",
      "Epoch 49321 \t\t Training Loss: 0.0005807186244055629 \t\n",
      "Epoch 49322 \t\t Training Loss: 0.000580718566197902 \t\n",
      "Epoch 49323 \t\t Training Loss: 0.000580718566197902 \t\n",
      "Epoch 49324 \t\t Training Loss: 0.000580718566197902 \t\n",
      "Epoch 49325 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49326 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49327 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49328 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49329 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49330 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49331 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49332 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49333 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49334 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49335 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49336 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49337 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49338 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49339 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49340 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49341 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49342 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49343 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49344 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49345 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49346 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49347 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49348 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49349 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49350 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49351 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49352 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49353 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49354 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49355 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49356 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49357 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49358 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49359 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49360 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49361 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49362 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49363 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49364 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49365 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49366 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49367 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49368 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49369 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49370 \t\t Training Loss: 0.0005807183915749192 \t\n",
      "Epoch 49371 \t\t Training Loss: 0.0005807183915749192 \t\n",
      "Epoch 49372 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49373 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49374 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49375 \t\t Training Loss: 0.0005807183915749192 \t\n",
      "Epoch 49376 \t\t Training Loss: 0.0005807183915749192 \t\n",
      "Epoch 49377 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49378 \t\t Training Loss: 0.0005807183915749192 \t\n",
      "Epoch 49379 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49380 \t\t Training Loss: 0.0005807184497825801 \t\n",
      "Epoch 49381 \t\t Training Loss: 0.0005807183915749192 \t\n",
      "Epoch 49382 \t\t Training Loss: 0.0005807183915749192 \t\n",
      "Epoch 49383 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49384 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49385 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49386 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49387 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49388 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49389 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49390 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49391 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49392 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49393 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49394 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49395 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49396 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49397 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49398 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49399 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49400 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49401 \t\t Training Loss: 0.0005807183333672583 \t\n",
      "Epoch 49402 \t\t Training Loss: 0.0005807182751595974 \t\n",
      "Epoch 49403 \t\t Training Loss: 0.0005807182751595974 \t\n",
      "Epoch 49404 \t\t Training Loss: 0.0005807182751595974 \t\n",
      "Epoch 49405 \t\t Training Loss: 0.0005807182751595974 \t\n",
      "Epoch 49406 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49407 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49408 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49409 \t\t Training Loss: 0.0005807182751595974 \t\n",
      "Epoch 49410 \t\t Training Loss: 0.0005807182751595974 \t\n",
      "Epoch 49411 \t\t Training Loss: 0.0005807182751595974 \t\n",
      "Epoch 49412 \t\t Training Loss: 0.0005807182751595974 \t\n",
      "Epoch 49413 \t\t Training Loss: 0.0005807182751595974 \t\n",
      "Epoch 49414 \t\t Training Loss: 0.0005807182751595974 \t\n",
      "Epoch 49415 \t\t Training Loss: 0.0005807182751595974 \t\n",
      "Epoch 49416 \t\t Training Loss: 0.0005807182751595974 \t\n",
      "Epoch 49417 \t\t Training Loss: 0.0005807182751595974 \t\n",
      "Epoch 49418 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49419 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49420 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49421 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49422 \t\t Training Loss: 0.0005807181005366147 \t\n",
      "Epoch 49423 \t\t Training Loss: 0.0005807181005366147 \t\n",
      "Epoch 49424 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49425 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49426 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49427 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49428 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49429 \t\t Training Loss: 0.0005807182751595974 \t\n",
      "Epoch 49430 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49431 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49432 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49433 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49434 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49435 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49436 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49437 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49438 \t\t Training Loss: 0.0005807181005366147 \t\n",
      "Epoch 49439 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49440 \t\t Training Loss: 0.0005807181005366147 \t\n",
      "Epoch 49441 \t\t Training Loss: 0.0005807181005366147 \t\n",
      "Epoch 49442 \t\t Training Loss: 0.0005807181005366147 \t\n",
      "Epoch 49443 \t\t Training Loss: 0.0005807181005366147 \t\n",
      "Epoch 49444 \t\t Training Loss: 0.0005807181587442756 \t\n",
      "Epoch 49445 \t\t Training Loss: 0.0005807181005366147 \t\n",
      "Epoch 49446 \t\t Training Loss: 0.0005807181005366147 \t\n",
      "Epoch 49447 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49448 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49449 \t\t Training Loss: 0.0005807181005366147 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49450 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49451 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49452 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49453 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49454 \t\t Training Loss: 0.0005807179259136319 \t\n",
      "Epoch 49455 \t\t Training Loss: 0.0005807179841212928 \t\n",
      "Epoch 49456 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49457 \t\t Training Loss: 0.0005807179841212928 \t\n",
      "Epoch 49458 \t\t Training Loss: 0.0005807179259136319 \t\n",
      "Epoch 49459 \t\t Training Loss: 0.0005807179841212928 \t\n",
      "Epoch 49460 \t\t Training Loss: 0.0005807179841212928 \t\n",
      "Epoch 49461 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49462 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49463 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49464 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49465 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49466 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49467 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49468 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49469 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49470 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49471 \t\t Training Loss: 0.0005807179841212928 \t\n",
      "Epoch 49472 \t\t Training Loss: 0.0005807179841212928 \t\n",
      "Epoch 49473 \t\t Training Loss: 0.0005807179259136319 \t\n",
      "Epoch 49474 \t\t Training Loss: 0.0005807179841212928 \t\n",
      "Epoch 49475 \t\t Training Loss: 0.0005807180423289537 \t\n",
      "Epoch 49476 \t\t Training Loss: 0.0005807179259136319 \t\n",
      "Epoch 49477 \t\t Training Loss: 0.0005807179841212928 \t\n",
      "Epoch 49478 \t\t Training Loss: 0.0005807179841212928 \t\n",
      "Epoch 49479 \t\t Training Loss: 0.0005807179259136319 \t\n",
      "Epoch 49480 \t\t Training Loss: 0.0005807179259136319 \t\n",
      "Epoch 49481 \t\t Training Loss: 0.0005807179259136319 \t\n",
      "Epoch 49482 \t\t Training Loss: 0.0005807179259136319 \t\n",
      "Epoch 49483 \t\t Training Loss: 0.0005807179841212928 \t\n",
      "Epoch 49484 \t\t Training Loss: 0.0005807179259136319 \t\n",
      "Epoch 49485 \t\t Training Loss: 0.0005807179259136319 \t\n",
      "Epoch 49486 \t\t Training Loss: 0.0005807179259136319 \t\n",
      "Epoch 49487 \t\t Training Loss: 0.0005807179259136319 \t\n",
      "Epoch 49488 \t\t Training Loss: 0.0005807179259136319 \t\n",
      "Epoch 49489 \t\t Training Loss: 0.0005807179259136319 \t\n",
      "Epoch 49490 \t\t Training Loss: 0.000580717867705971 \t\n",
      "Epoch 49491 \t\t Training Loss: 0.000580717867705971 \t\n",
      "Epoch 49492 \t\t Training Loss: 0.0005807178094983101 \t\n",
      "Epoch 49493 \t\t Training Loss: 0.000580717867705971 \t\n",
      "Epoch 49494 \t\t Training Loss: 0.0005807178094983101 \t\n",
      "Epoch 49495 \t\t Training Loss: 0.0005807178094983101 \t\n",
      "Epoch 49496 \t\t Training Loss: 0.0005807178094983101 \t\n",
      "Epoch 49497 \t\t Training Loss: 0.0005807178094983101 \t\n",
      "Epoch 49498 \t\t Training Loss: 0.0005807178094983101 \t\n",
      "Epoch 49499 \t\t Training Loss: 0.0005807178094983101 \t\n",
      "Epoch 49500 \t\t Training Loss: 0.0005807178094983101 \t\n",
      "Epoch 49501 \t\t Training Loss: 0.0005807178094983101 \t\n",
      "Epoch 49502 \t\t Training Loss: 0.0005807178094983101 \t\n",
      "Epoch 49503 \t\t Training Loss: 0.0005807178094983101 \t\n",
      "Epoch 49504 \t\t Training Loss: 0.0005807178094983101 \t\n",
      "Epoch 49505 \t\t Training Loss: 0.0005807178094983101 \t\n",
      "Epoch 49506 \t\t Training Loss: 0.0005807177512906492 \t\n",
      "Epoch 49507 \t\t Training Loss: 0.0005807177512906492 \t\n",
      "Epoch 49508 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49509 \t\t Training Loss: 0.0005807177512906492 \t\n",
      "Epoch 49510 \t\t Training Loss: 0.0005807177512906492 \t\n",
      "Epoch 49511 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49512 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49513 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49514 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49515 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49516 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49517 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49518 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49519 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49520 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49521 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49522 \t\t Training Loss: 0.0005807177512906492 \t\n",
      "Epoch 49523 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49524 \t\t Training Loss: 0.0005807176348753273 \t\n",
      "Epoch 49525 \t\t Training Loss: 0.0005807176348753273 \t\n",
      "Epoch 49526 \t\t Training Loss: 0.0005807176348753273 \t\n",
      "Epoch 49527 \t\t Training Loss: 0.0005807176348753273 \t\n",
      "Epoch 49528 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49529 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49530 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49531 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49532 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49533 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49534 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49535 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49536 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49537 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49538 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49539 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49540 \t\t Training Loss: 0.0005807176348753273 \t\n",
      "Epoch 49541 \t\t Training Loss: 0.0005807176348753273 \t\n",
      "Epoch 49542 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49543 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49544 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49545 \t\t Training Loss: 0.0005807176348753273 \t\n",
      "Epoch 49546 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49547 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49548 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49549 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49550 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49551 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49552 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49553 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49554 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49555 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49556 \t\t Training Loss: 0.0005807176930829883 \t\n",
      "Epoch 49557 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49558 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49559 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49560 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49561 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49562 \t\t Training Loss: 0.0005807176348753273 \t\n",
      "Epoch 49563 \t\t Training Loss: 0.0005807176348753273 \t\n",
      "Epoch 49564 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49565 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49566 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49567 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49568 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49569 \t\t Training Loss: 0.0005807174602523446 \t\n",
      "Epoch 49570 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49571 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49572 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49573 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49574 \t\t Training Loss: 0.0005807175184600055 \t\n",
      "Epoch 49575 \t\t Training Loss: 0.0005807174602523446 \t\n",
      "Epoch 49576 \t\t Training Loss: 0.0005807174602523446 \t\n",
      "Epoch 49577 \t\t Training Loss: 0.0005807174020446837 \t\n",
      "Epoch 49578 \t\t Training Loss: 0.0005807174020446837 \t\n",
      "Epoch 49579 \t\t Training Loss: 0.0005807174020446837 \t\n",
      "Epoch 49580 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49581 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49582 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49583 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49584 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49585 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49586 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49587 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49588 \t\t Training Loss: 0.0005807174020446837 \t\n",
      "Epoch 49589 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49590 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49591 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49592 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49593 \t\t Training Loss: 0.0005807172856293619 \t\n",
      "Epoch 49594 \t\t Training Loss: 0.000580717227421701 \t\n",
      "Epoch 49595 \t\t Training Loss: 0.000580717227421701 \t\n",
      "Epoch 49596 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49597 \t\t Training Loss: 0.0005807172856293619 \t\n",
      "Epoch 49598 \t\t Training Loss: 0.0005807172856293619 \t\n",
      "Epoch 49599 \t\t Training Loss: 0.000580717227421701 \t\n",
      "Epoch 49600 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49601 \t\t Training Loss: 0.000580717227421701 \t\n",
      "Epoch 49602 \t\t Training Loss: 0.0005807172856293619 \t\n",
      "Epoch 49603 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49604 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49605 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49606 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49607 \t\t Training Loss: 0.0005807173438370228 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49608 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49609 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49610 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49611 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49612 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49613 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49614 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49615 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49616 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49617 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49618 \t\t Training Loss: 0.0005807173438370228 \t\n",
      "Epoch 49619 \t\t Training Loss: 0.000580717227421701 \t\n",
      "Epoch 49620 \t\t Training Loss: 0.000580717227421701 \t\n",
      "Epoch 49621 \t\t Training Loss: 0.000580717227421701 \t\n",
      "Epoch 49622 \t\t Training Loss: 0.000580717227421701 \t\n",
      "Epoch 49623 \t\t Training Loss: 0.000580717227421701 \t\n",
      "Epoch 49624 \t\t Training Loss: 0.000580717227421701 \t\n",
      "Epoch 49625 \t\t Training Loss: 0.000580717227421701 \t\n",
      "Epoch 49626 \t\t Training Loss: 0.000580717227421701 \t\n",
      "Epoch 49627 \t\t Training Loss: 0.000580717227421701 \t\n",
      "Epoch 49628 \t\t Training Loss: 0.00058071716921404 \t\n",
      "Epoch 49629 \t\t Training Loss: 0.00058071716921404 \t\n",
      "Epoch 49630 \t\t Training Loss: 0.0005807170527987182 \t\n",
      "Epoch 49631 \t\t Training Loss: 0.0005807170527987182 \t\n",
      "Epoch 49632 \t\t Training Loss: 0.00058071716921404 \t\n",
      "Epoch 49633 \t\t Training Loss: 0.0005807170527987182 \t\n",
      "Epoch 49634 \t\t Training Loss: 0.0005807171110063791 \t\n",
      "Epoch 49635 \t\t Training Loss: 0.0005807171110063791 \t\n",
      "Epoch 49636 \t\t Training Loss: 0.00058071716921404 \t\n",
      "Epoch 49637 \t\t Training Loss: 0.00058071716921404 \t\n",
      "Epoch 49638 \t\t Training Loss: 0.0005807171110063791 \t\n",
      "Epoch 49639 \t\t Training Loss: 0.0005807171110063791 \t\n",
      "Epoch 49640 \t\t Training Loss: 0.00058071716921404 \t\n",
      "Epoch 49641 \t\t Training Loss: 0.00058071716921404 \t\n",
      "Epoch 49642 \t\t Training Loss: 0.0005807171110063791 \t\n",
      "Epoch 49643 \t\t Training Loss: 0.0005807171110063791 \t\n",
      "Epoch 49644 \t\t Training Loss: 0.0005807171110063791 \t\n",
      "Epoch 49645 \t\t Training Loss: 0.0005807171110063791 \t\n",
      "Epoch 49646 \t\t Training Loss: 0.0005807170527987182 \t\n",
      "Epoch 49647 \t\t Training Loss: 0.0005807170527987182 \t\n",
      "Epoch 49648 \t\t Training Loss: 0.0005807171110063791 \t\n",
      "Epoch 49649 \t\t Training Loss: 0.0005807170527987182 \t\n",
      "Epoch 49650 \t\t Training Loss: 0.0005807169945910573 \t\n",
      "Epoch 49651 \t\t Training Loss: 0.0005807169945910573 \t\n",
      "Epoch 49652 \t\t Training Loss: 0.0005807169945910573 \t\n",
      "Epoch 49653 \t\t Training Loss: 0.0005807169945910573 \t\n",
      "Epoch 49654 \t\t Training Loss: 0.0005807169945910573 \t\n",
      "Epoch 49655 \t\t Training Loss: 0.0005807169945910573 \t\n",
      "Epoch 49656 \t\t Training Loss: 0.0005807169945910573 \t\n",
      "Epoch 49657 \t\t Training Loss: 0.0005807169363833964 \t\n",
      "Epoch 49658 \t\t Training Loss: 0.0005807169363833964 \t\n",
      "Epoch 49659 \t\t Training Loss: 0.0005807169363833964 \t\n",
      "Epoch 49660 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49661 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49662 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49663 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49664 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49665 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49666 \t\t Training Loss: 0.0005807169363833964 \t\n",
      "Epoch 49667 \t\t Training Loss: 0.0005807169363833964 \t\n",
      "Epoch 49668 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49669 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49670 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49671 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49672 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49673 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49674 \t\t Training Loss: 0.0005807169363833964 \t\n",
      "Epoch 49675 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49676 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49677 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49678 \t\t Training Loss: 0.0005807169363833964 \t\n",
      "Epoch 49679 \t\t Training Loss: 0.0005807169363833964 \t\n",
      "Epoch 49680 \t\t Training Loss: 0.0005807169363833964 \t\n",
      "Epoch 49681 \t\t Training Loss: 0.0005807169363833964 \t\n",
      "Epoch 49682 \t\t Training Loss: 0.0005807169363833964 \t\n",
      "Epoch 49683 \t\t Training Loss: 0.0005807169363833964 \t\n",
      "Epoch 49684 \t\t Training Loss: 0.0005807169945910573 \t\n",
      "Epoch 49685 \t\t Training Loss: 0.0005807170527987182 \t\n",
      "Epoch 49686 \t\t Training Loss: 0.0005807170527987182 \t\n",
      "Epoch 49687 \t\t Training Loss: 0.0005807169945910573 \t\n",
      "Epoch 49688 \t\t Training Loss: 0.0005807170527987182 \t\n",
      "Epoch 49689 \t\t Training Loss: 0.0005807170527987182 \t\n",
      "Epoch 49690 \t\t Training Loss: 0.0005807170527987182 \t\n",
      "Epoch 49691 \t\t Training Loss: 0.0005807170527987182 \t\n",
      "Epoch 49692 \t\t Training Loss: 0.0005807170527987182 \t\n",
      "Epoch 49693 \t\t Training Loss: 0.0005807170527987182 \t\n",
      "Epoch 49694 \t\t Training Loss: 0.0005807170527987182 \t\n",
      "Epoch 49695 \t\t Training Loss: 0.0005807170527987182 \t\n",
      "Epoch 49696 \t\t Training Loss: 0.0005807169363833964 \t\n",
      "Epoch 49697 \t\t Training Loss: 0.0005807169945910573 \t\n",
      "Epoch 49698 \t\t Training Loss: 0.0005807169945910573 \t\n",
      "Epoch 49699 \t\t Training Loss: 0.0005807169945910573 \t\n",
      "Epoch 49700 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49701 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49702 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49703 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49704 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49705 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49706 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49707 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49708 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49709 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49710 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49711 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49712 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49713 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49714 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49715 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49716 \t\t Training Loss: 0.0005807168781757355 \t\n",
      "Epoch 49717 \t\t Training Loss: 0.0005807168199680746 \t\n",
      "Epoch 49718 \t\t Training Loss: 0.0005807168199680746 \t\n",
      "Epoch 49719 \t\t Training Loss: 0.0005807168199680746 \t\n",
      "Epoch 49720 \t\t Training Loss: 0.0005807167617604136 \t\n",
      "Epoch 49721 \t\t Training Loss: 0.0005807167617604136 \t\n",
      "Epoch 49722 \t\t Training Loss: 0.0005807167617604136 \t\n",
      "Epoch 49723 \t\t Training Loss: 0.0005807167617604136 \t\n",
      "Epoch 49724 \t\t Training Loss: 0.0005807167617604136 \t\n",
      "Epoch 49725 \t\t Training Loss: 0.0005807167617604136 \t\n",
      "Epoch 49726 \t\t Training Loss: 0.0005807167617604136 \t\n",
      "Epoch 49727 \t\t Training Loss: 0.0005807167035527527 \t\n",
      "Epoch 49728 \t\t Training Loss: 0.0005807167035527527 \t\n",
      "Epoch 49729 \t\t Training Loss: 0.0005807167035527527 \t\n",
      "Epoch 49730 \t\t Training Loss: 0.0005807167035527527 \t\n",
      "Epoch 49731 \t\t Training Loss: 0.0005807167035527527 \t\n",
      "Epoch 49732 \t\t Training Loss: 0.0005807167035527527 \t\n",
      "Epoch 49733 \t\t Training Loss: 0.0005807166453450918 \t\n",
      "Epoch 49734 \t\t Training Loss: 0.0005807166453450918 \t\n",
      "Epoch 49735 \t\t Training Loss: 0.0005807166453450918 \t\n",
      "Epoch 49736 \t\t Training Loss: 0.0005807166453450918 \t\n",
      "Epoch 49737 \t\t Training Loss: 0.0005807166453450918 \t\n",
      "Epoch 49738 \t\t Training Loss: 0.0005807166453450918 \t\n",
      "Epoch 49739 \t\t Training Loss: 0.0005807166453450918 \t\n",
      "Epoch 49740 \t\t Training Loss: 0.0005807166453450918 \t\n",
      "Epoch 49741 \t\t Training Loss: 0.0005807166453450918 \t\n",
      "Epoch 49742 \t\t Training Loss: 0.0005807166453450918 \t\n",
      "Epoch 49743 \t\t Training Loss: 0.00058071652892977 \t\n",
      "Epoch 49744 \t\t Training Loss: 0.00058071652892977 \t\n",
      "Epoch 49745 \t\t Training Loss: 0.0005807164707221091 \t\n",
      "Epoch 49746 \t\t Training Loss: 0.0005807164707221091 \t\n",
      "Epoch 49747 \t\t Training Loss: 0.0005807164707221091 \t\n",
      "Epoch 49748 \t\t Training Loss: 0.0005807164707221091 \t\n",
      "Epoch 49749 \t\t Training Loss: 0.0005807164707221091 \t\n",
      "Epoch 49750 \t\t Training Loss: 0.0005807164707221091 \t\n",
      "Epoch 49751 \t\t Training Loss: 0.0005807164707221091 \t\n",
      "Epoch 49752 \t\t Training Loss: 0.0005807164707221091 \t\n",
      "Epoch 49753 \t\t Training Loss: 0.0005807164707221091 \t\n",
      "Epoch 49754 \t\t Training Loss: 0.0005807164707221091 \t\n",
      "Epoch 49755 \t\t Training Loss: 0.0005807164707221091 \t\n",
      "Epoch 49756 \t\t Training Loss: 0.0005807164707221091 \t\n",
      "Epoch 49757 \t\t Training Loss: 0.0005807164707221091 \t\n",
      "Epoch 49758 \t\t Training Loss: 0.0005807164125144482 \t\n",
      "Epoch 49759 \t\t Training Loss: 0.0005807164125144482 \t\n",
      "Epoch 49760 \t\t Training Loss: 0.0005807164125144482 \t\n",
      "Epoch 49761 \t\t Training Loss: 0.0005807164125144482 \t\n",
      "Epoch 49762 \t\t Training Loss: 0.0005807164125144482 \t\n",
      "Epoch 49763 \t\t Training Loss: 0.0005807163543067873 \t\n",
      "Epoch 49764 \t\t Training Loss: 0.0005807162960991263 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49765 \t\t Training Loss: 0.0005807162960991263 \t\n",
      "Epoch 49766 \t\t Training Loss: 0.0005807162960991263 \t\n",
      "Epoch 49767 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49768 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49769 \t\t Training Loss: 0.0005807162960991263 \t\n",
      "Epoch 49770 \t\t Training Loss: 0.0005807163543067873 \t\n",
      "Epoch 49771 \t\t Training Loss: 0.0005807163543067873 \t\n",
      "Epoch 49772 \t\t Training Loss: 0.0005807163543067873 \t\n",
      "Epoch 49773 \t\t Training Loss: 0.0005807163543067873 \t\n",
      "Epoch 49774 \t\t Training Loss: 0.0005807163543067873 \t\n",
      "Epoch 49775 \t\t Training Loss: 0.0005807163543067873 \t\n",
      "Epoch 49776 \t\t Training Loss: 0.0005807162960991263 \t\n",
      "Epoch 49777 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49778 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49779 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49780 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49781 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49782 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49783 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49784 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49785 \t\t Training Loss: 0.0005807163543067873 \t\n",
      "Epoch 49786 \t\t Training Loss: 0.0005807163543067873 \t\n",
      "Epoch 49787 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49788 \t\t Training Loss: 0.0005807162960991263 \t\n",
      "Epoch 49789 \t\t Training Loss: 0.0005807162960991263 \t\n",
      "Epoch 49790 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49791 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49792 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49793 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49794 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49795 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49796 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49797 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49798 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49799 \t\t Training Loss: 0.0005807162378914654 \t\n",
      "Epoch 49800 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49801 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49802 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49803 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49804 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49805 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49806 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49807 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49808 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49809 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49810 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49811 \t\t Training Loss: 0.0005807161214761436 \t\n",
      "Epoch 49812 \t\t Training Loss: 0.0005807161214761436 \t\n",
      "Epoch 49813 \t\t Training Loss: 0.0005807161214761436 \t\n",
      "Epoch 49814 \t\t Training Loss: 0.0005807161214761436 \t\n",
      "Epoch 49815 \t\t Training Loss: 0.0005807161214761436 \t\n",
      "Epoch 49816 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49817 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49818 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49819 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49820 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49821 \t\t Training Loss: 0.0005807161796838045 \t\n",
      "Epoch 49822 \t\t Training Loss: 0.0005807161214761436 \t\n",
      "Epoch 49823 \t\t Training Loss: 0.0005807161214761436 \t\n",
      "Epoch 49824 \t\t Training Loss: 0.0005807160050608218 \t\n",
      "Epoch 49825 \t\t Training Loss: 0.0005807160050608218 \t\n",
      "Epoch 49826 \t\t Training Loss: 0.0005807160050608218 \t\n",
      "Epoch 49827 \t\t Training Loss: 0.0005807160050608218 \t\n",
      "Epoch 49828 \t\t Training Loss: 0.0005807160050608218 \t\n",
      "Epoch 49829 \t\t Training Loss: 0.0005807160050608218 \t\n",
      "Epoch 49830 \t\t Training Loss: 0.0005807160050608218 \t\n",
      "Epoch 49831 \t\t Training Loss: 0.0005807160050608218 \t\n",
      "Epoch 49832 \t\t Training Loss: 0.0005807160050608218 \t\n",
      "Epoch 49833 \t\t Training Loss: 0.0005807160050608218 \t\n",
      "Epoch 49834 \t\t Training Loss: 0.0005807160050608218 \t\n",
      "Epoch 49835 \t\t Training Loss: 0.0005807160050608218 \t\n",
      "Epoch 49836 \t\t Training Loss: 0.0005807160050608218 \t\n",
      "Epoch 49837 \t\t Training Loss: 0.0005807160050608218 \t\n",
      "Epoch 49838 \t\t Training Loss: 0.0005807160050608218 \t\n",
      "Epoch 49839 \t\t Training Loss: 0.0005807159468531609 \t\n",
      "Epoch 49840 \t\t Training Loss: 0.0005807159468531609 \t\n",
      "Epoch 49841 \t\t Training Loss: 0.0005807159468531609 \t\n",
      "Epoch 49842 \t\t Training Loss: 0.0005807159468531609 \t\n",
      "Epoch 49843 \t\t Training Loss: 0.0005807158886454999 \t\n",
      "Epoch 49844 \t\t Training Loss: 0.0005807158886454999 \t\n",
      "Epoch 49845 \t\t Training Loss: 0.000580715830437839 \t\n",
      "Epoch 49846 \t\t Training Loss: 0.000580715830437839 \t\n",
      "Epoch 49847 \t\t Training Loss: 0.000580715830437839 \t\n",
      "Epoch 49848 \t\t Training Loss: 0.000580715830437839 \t\n",
      "Epoch 49849 \t\t Training Loss: 0.0005807157722301781 \t\n",
      "Epoch 49850 \t\t Training Loss: 0.0005807157722301781 \t\n",
      "Epoch 49851 \t\t Training Loss: 0.0005807157722301781 \t\n",
      "Epoch 49852 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49853 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49854 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49855 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49856 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49857 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49858 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49859 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49860 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49861 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49862 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49863 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49864 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49865 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49866 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49867 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49868 \t\t Training Loss: 0.0005807156558148563 \t\n",
      "Epoch 49869 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49870 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49871 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49872 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49873 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49874 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49875 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49876 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49877 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49878 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49879 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49880 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49881 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49882 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49883 \t\t Training Loss: 0.0005807157140225172 \t\n",
      "Epoch 49884 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49885 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49886 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49887 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49888 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49889 \t\t Training Loss: 0.0005807156558148563 \t\n",
      "Epoch 49890 \t\t Training Loss: 0.0005807156558148563 \t\n",
      "Epoch 49891 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49892 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49893 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49894 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49895 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49896 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49897 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49898 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49899 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49900 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49901 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49902 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49903 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49904 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49905 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49906 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49907 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49908 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49909 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49910 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49911 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49912 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49913 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49914 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49915 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49916 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49917 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49918 \t\t Training Loss: 0.0005807155976071954 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49919 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49920 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49921 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49922 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49923 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49924 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49925 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49926 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49927 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49928 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49929 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49930 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49931 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49932 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49933 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49934 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49935 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49936 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49937 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49938 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49939 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49940 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49941 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49942 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49943 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49944 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49945 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49946 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49947 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49948 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49949 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49950 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49951 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49952 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49953 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49954 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49955 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49956 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49957 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49958 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49959 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49960 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49961 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49962 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49963 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49964 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49965 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49966 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49967 \t\t Training Loss: 0.0005807155976071954 \t\n",
      "Epoch 49968 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49969 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49970 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49971 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49972 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49973 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49974 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49975 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49976 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49977 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49978 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49979 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49980 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49981 \t\t Training Loss: 0.0005807155393995345 \t\n",
      "Epoch 49982 \t\t Training Loss: 0.0005807154229842126 \t\n",
      "Epoch 49983 \t\t Training Loss: 0.0005807154229842126 \t\n",
      "Epoch 49984 \t\t Training Loss: 0.0005807154229842126 \t\n",
      "Epoch 49985 \t\t Training Loss: 0.0005807154229842126 \t\n",
      "Epoch 49986 \t\t Training Loss: 0.0005807154229842126 \t\n",
      "Epoch 49987 \t\t Training Loss: 0.0005807154229842126 \t\n",
      "Epoch 49988 \t\t Training Loss: 0.0005807154229842126 \t\n",
      "Epoch 49989 \t\t Training Loss: 0.0005807154229842126 \t\n",
      "Epoch 49990 \t\t Training Loss: 0.0005807154229842126 \t\n",
      "Epoch 49991 \t\t Training Loss: 0.0005807154229842126 \t\n",
      "Epoch 49992 \t\t Training Loss: 0.0005807154229842126 \t\n",
      "Epoch 49993 \t\t Training Loss: 0.0005807154229842126 \t\n",
      "Epoch 49994 \t\t Training Loss: 0.0005807154229842126 \t\n",
      "Epoch 49995 \t\t Training Loss: 0.0005807154229842126 \t\n",
      "Epoch 49996 \t\t Training Loss: 0.0005807153647765517 \t\n",
      "Epoch 49997 \t\t Training Loss: 0.0005807154229842126 \t\n",
      "Epoch 49998 \t\t Training Loss: 0.0005807153065688908 \t\n",
      "Epoch 49999 \t\t Training Loss: 0.0005807153065688908 \t\n",
      "Epoch 50000 \t\t Training Loss: 0.0005807153065688908 \t\n"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "\n",
    "min_val_loss = np.inf\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    count_train_loss = 0.0\n",
    "    \n",
    "    \n",
    "    pred_y = model1(x_train)\n",
    "\n",
    "    # Compute and print loss\n",
    "    train_loss = criterion(pred_y, y_train)\n",
    "    \n",
    "    count_train_loss+= train_loss.item()\n",
    "    \n",
    "    train_loss_tens[epoch] = train_loss\n",
    "    \n",
    "    \n",
    "\n",
    "    # Zero gradients, perform a backward pass,\n",
    "    # and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    #print(loss)\n",
    "    print(f'Epoch {epoch +1} \\t\\t Training Loss: {train_loss / len(x_train)} \\t')\n",
    "    \n",
    "train_loss_arr = train_loss_tens.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f838a6614b0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_arr, 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self,input_shape):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,32)\n",
    "        self.fc2 = nn.Linear(32,64)\n",
    "        self.fc3 = nn.Linear(64,1)  \n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m Net1(input_shape\u001b[38;5;241m=\u001b[39m\u001b[43mx\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m      3\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "model = Net1(input_shape=x.shape[1])\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a a row of the dataframe\n",
    "# select 10 random sequences with class 0 (normal) and 10 with class 1 (abnormal)\n",
    "df_ptbd_normal = df_ptbd[df_ptbd[\"label\"] == 0].sample(10)\n",
    "df_ptbd_abnormal = df_ptbd[df_ptbd[\"label\"] == 1].sample(10)\n",
    "\n",
    "# create a figure\n",
    "plt.figure(figsize = (20, 10))\n",
    "# plot againts the column names (time)\n",
    "# plot the normal sequences\n",
    "for i in range(df_ptbd_normal.shape[0]):\n",
    "    if i == 0:\n",
    "        plt.plot(df_ptbd_normal.columns[:-1], df_ptbd_normal.iloc[i, :-1], color = \"blue\", label = \"normal\")\n",
    "    else:\n",
    "        plt.plot(df_ptbd_normal.columns[:-1], df_ptbd_normal.iloc[i, :-1],color = \"blue\")\n",
    "# plot the abnormal sequences\n",
    "for i in range(df_ptbd_abnormal.shape[0]):\n",
    "    if i == 0:\n",
    "        plt.plot(df_ptbd_abnormal.columns[:-1], df_ptbd_abnormal.iloc[i, :-1], color = \"red\", label = \"abnormal\")\n",
    "    else:   \n",
    "        plt.plot(df_ptbd_abnormal.columns[:-1], df_ptbd_abnormal.iloc[i, :-1], color = \"red\")\n",
    "# create a legend for the plot, blue for normal and red for abnormal\n",
    "plt.legend()\n",
    "# set the title\n",
    "plt.title(\"ECG sequences\")\n",
    "# y lable is \"Normalized value\"\n",
    "plt.ylabel(\"Normalized value\")\n",
    "# x label is \"Time (ms)\"\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "# show the plot\n",
    "plt.show()\n",
    "# close\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>16.0</th>\n",
       "      <th>24.0</th>\n",
       "      <th>32.0</th>\n",
       "      <th>40.0</th>\n",
       "      <th>48.0</th>\n",
       "      <th>56.0</th>\n",
       "      <th>64.0</th>\n",
       "      <th>72.0</th>\n",
       "      <th>...</th>\n",
       "      <th>1424.0</th>\n",
       "      <th>1432.0</th>\n",
       "      <th>1440.0</th>\n",
       "      <th>1448.0</th>\n",
       "      <th>1456.0</th>\n",
       "      <th>1464.0</th>\n",
       "      <th>1472.0</th>\n",
       "      <th>1480.0</th>\n",
       "      <th>1488.0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.956608</td>\n",
       "      <td>0.304140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107882</td>\n",
       "      <td>0.142118</td>\n",
       "      <td>0.153264</td>\n",
       "      <td>0.142118</td>\n",
       "      <td>0.136545</td>\n",
       "      <td>0.133758</td>\n",
       "      <td>0.132564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3445</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.523354</td>\n",
       "      <td>0.300110</td>\n",
       "      <td>0.059581</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>0.061052</td>\n",
       "      <td>0.115851</td>\n",
       "      <td>0.112541</td>\n",
       "      <td>0.113645</td>\n",
       "      <td>0.117690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.641743</td>\n",
       "      <td>0.273771</td>\n",
       "      <td>0.100971</td>\n",
       "      <td>0.092140</td>\n",
       "      <td>0.045923</td>\n",
       "      <td>0.023550</td>\n",
       "      <td>0.017074</td>\n",
       "      <td>0.009714</td>\n",
       "      <td>0.010303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.792068</td>\n",
       "      <td>0.342210</td>\n",
       "      <td>0.075921</td>\n",
       "      <td>0.061190</td>\n",
       "      <td>0.033428</td>\n",
       "      <td>0.020397</td>\n",
       "      <td>0.010765</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.009065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.598419</td>\n",
       "      <td>0.196838</td>\n",
       "      <td>0.118182</td>\n",
       "      <td>0.124506</td>\n",
       "      <td>0.081818</td>\n",
       "      <td>0.055336</td>\n",
       "      <td>0.048617</td>\n",
       "      <td>0.042688</td>\n",
       "      <td>0.039921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>0.995811</td>\n",
       "      <td>0.706752</td>\n",
       "      <td>0.479547</td>\n",
       "      <td>0.282159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.065796</td>\n",
       "      <td>0.132578</td>\n",
       "      <td>0.168802</td>\n",
       "      <td>0.161163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>0.925185</td>\n",
       "      <td>0.428665</td>\n",
       "      <td>0.095694</td>\n",
       "      <td>0.063071</td>\n",
       "      <td>0.093301</td>\n",
       "      <td>0.078295</td>\n",
       "      <td>0.062636</td>\n",
       "      <td>0.063071</td>\n",
       "      <td>0.062418</td>\n",
       "      <td>0.060461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3296</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615490</td>\n",
       "      <td>0.379484</td>\n",
       "      <td>0.173863</td>\n",
       "      <td>0.162440</td>\n",
       "      <td>0.189399</td>\n",
       "      <td>0.196482</td>\n",
       "      <td>0.192598</td>\n",
       "      <td>0.193512</td>\n",
       "      <td>0.197852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.381696</td>\n",
       "      <td>0.080357</td>\n",
       "      <td>0.092634</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.252790</td>\n",
       "      <td>0.266741</td>\n",
       "      <td>0.269531</td>\n",
       "      <td>0.271763</td>\n",
       "      <td>0.276228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.434179</td>\n",
       "      <td>0.012688</td>\n",
       "      <td>0.048374</td>\n",
       "      <td>0.098335</td>\n",
       "      <td>0.118557</td>\n",
       "      <td>0.103886</td>\n",
       "      <td>0.103886</td>\n",
       "      <td>0.101507</td>\n",
       "      <td>0.112213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0.0       8.0      16.0      24.0      32.0      40.0      48.0  \\\n",
       "291   0.956608  0.304140  0.000000  0.107882  0.142118  0.153264  0.142118   \n",
       "3445  1.000000  0.523354  0.300110  0.059581  0.005149  0.061052  0.115851   \n",
       "2438  1.000000  0.641743  0.273771  0.100971  0.092140  0.045923  0.023550   \n",
       "214   1.000000  0.792068  0.342210  0.075921  0.061190  0.033428  0.020397   \n",
       "1819  1.000000  0.598419  0.196838  0.118182  0.124506  0.081818  0.055336   \n",
       "2216  0.995811  0.706752  0.479547  0.282159  0.000000  0.000493  0.065796   \n",
       "1715  0.925185  0.428665  0.095694  0.063071  0.093301  0.078295  0.062636   \n",
       "3296  1.000000  0.615490  0.379484  0.173863  0.162440  0.189399  0.196482   \n",
       "1349  1.000000  0.381696  0.080357  0.092634  0.187500  0.252790  0.266741   \n",
       "1461  1.000000  0.434179  0.012688  0.048374  0.098335  0.118557  0.103886   \n",
       "\n",
       "          56.0      64.0      72.0  ...  1424.0  1432.0  1440.0  1448.0  \\\n",
       "291   0.136545  0.133758  0.132564  ...     0.0     0.0     0.0     0.0   \n",
       "3445  0.112541  0.113645  0.117690  ...     0.0     0.0     0.0     0.0   \n",
       "2438  0.017074  0.009714  0.010303  ...     0.0     0.0     0.0     0.0   \n",
       "214   0.010765  0.005099  0.009065  ...     0.0     0.0     0.0     0.0   \n",
       "1819  0.048617  0.042688  0.039921  ...     0.0     0.0     0.0     0.0   \n",
       "2216  0.132578  0.168802  0.161163  ...     0.0     0.0     0.0     0.0   \n",
       "1715  0.063071  0.062418  0.060461  ...     0.0     0.0     0.0     0.0   \n",
       "3296  0.192598  0.193512  0.197852  ...     0.0     0.0     0.0     0.0   \n",
       "1349  0.269531  0.271763  0.276228  ...     0.0     0.0     0.0     0.0   \n",
       "1461  0.103886  0.101507  0.112213  ...     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      1456.0  1464.0  1472.0  1480.0  1488.0  label  \n",
       "291      0.0     0.0     0.0     0.0     0.0    0.0  \n",
       "3445     0.0     0.0     0.0     0.0     0.0    0.0  \n",
       "2438     0.0     0.0     0.0     0.0     0.0    0.0  \n",
       "214      0.0     0.0     0.0     0.0     0.0    0.0  \n",
       "1819     0.0     0.0     0.0     0.0     0.0    0.0  \n",
       "2216     0.0     0.0     0.0     0.0     0.0    0.0  \n",
       "1715     0.0     0.0     0.0     0.0     0.0    0.0  \n",
       "3296     0.0     0.0     0.0     0.0     0.0    0.0  \n",
       "1349     0.0     0.0     0.0     0.0     0.0    0.0  \n",
       "1461     0.0     0.0     0.0     0.0     0.0    0.0  \n",
       "\n",
       "[10 rows x 188 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ptbd_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
